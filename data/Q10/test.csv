abstract
"  Probabilistic principal component analysis (PPCA) seeks a low dimensional
representation of a data set in the presence of independent spherical Gaussian
noise, Sigma = (sigma^2)*I. The maximum likelihood solution for the model is an
eigenvalue problem on the sample covariance matrix. In this paper we consider
the situation where the data variance is already partially explained by other
factors, e.g. covariates of interest, or temporal correlations leaving some
residual variance. We decompose the residual variance into its components
through a generalized eigenvalue problem, which we call residual component
analysis (RCA). We show that canonical covariates analysis (CCA) is a special
case of our algorithm and explore a range of new algorithms that arise from the
framework. We illustrate the ideas on a gene expression time series data set
and the recovery of human pose from silhouette.
"
"  Sensor-based degradation signals measure the accumulation of damage of an
engineering system using sensor technology. Degradation signals can be used to
estimate, for example, the distribution of the remaining life of partially
degraded systems and/or their components. In this paper we present a
nonparametric degradation modeling framework for making inference on the
evolution of degradation signals that are observed sparsely or over short
intervals of times. Furthermore, an empirical Bayes approach is used to update
the stochastic parameters of the degradation model in real-time using training
degradation signals for online monitoring of components operating in the field.
The primary application of this Bayesian framework is updating the residual
lifetime up to a degradation threshold of partially degraded components. We
validate our degradation modeling approach using a real-world crack growth data
set as well as a case study of simulated degradation signals.
"
"  The generic identification problem is to decide whether a stochastic process
$(X_t)$ is a hidden Markov process and if yes to infer its parameters for all
but a subset of parametrizations that form a lower-dimensional subvariety in
parameter space. Partial answers so far available depend on extra assumptions
on the processes, which are usually centered around stationarity. Here we
present a general solution for binary-valued hidden Markov processes. Our
approach is rooted in algebraic statistics hence it is geometric in nature. We
find that the algebraic varieties associated with the probability distributions
of binary-valued hidden Markov processes are zero sets of determinantal
equations which draws a connection to well-studied objects from algebra. As a
consequence, our solution allows for algorithmic implementation based on
elementary (linear) algebraic routines.
"
"  We introduce a new class of lower bounds on the log partition function of a
Markov random field which makes use of a reversed Jensen's inequality. In
particular, our method approximates the intractable distribution using a linear
combination of spanning trees with negative weights. This technique is a
lower-bound counterpart to the tree-reweighted belief propagation algorithm,
which uses a convex combination of spanning trees with positive weights to
provide corresponding upper bounds. We develop algorithms to optimize and
tighten the lower bounds over the non-convex set of valid parameter values. Our
algorithm generalizes mean field approaches (including naive and structured
mean field approximations), which it includes as a limiting case.
"
"  Regularization is a powerful technique for extracting useful information from
noisy data. Typically, it is implemented by adding some sort of norm constraint
to an objective function and then exactly optimizing the modified objective
function. This procedure often leads to optimization problems that are
computationally more expensive than the original problem, a fact that is
clearly problematic if one is interested in large-scale applications. On the
other hand, a large body of empirical work has demonstrated that heuristics,
and in some cases approximation algorithms, developed to speed up computations
sometimes have the side-effect of performing regularization implicitly. Thus,
we consider the question: What is the regularized optimization objective that
an approximation algorithm is exactly optimizing?
  We address this question in the context of computing approximations to the
smallest nontrivial eigenvector of a graph Laplacian; and we consider three
random-walk-based procedures: one based on the heat kernel of the graph, one
based on computing the the PageRank vector associated with the graph, and one
based on a truncated lazy random walk. In each case, we provide a precise
characterization of the manner in which the approximation method can be viewed
as implicitly computing the exact solution to a regularized problem.
Interestingly, the regularization is not on the usual vector form of the
optimization problem, but instead it is on a related semidefinite program.
"
"  In mixtures-of-experts (ME) model, where a number of submodels (experts) are
combined, there have been two longstanding problems: (i) how many experts
should be chosen, given the size of the training data? (ii) given the total
number of parameters, is it better to use a few very complex experts, or is it
better to combine many simple experts? In this paper, we try to provide some
insights to these problems through a theoretic study on a ME structure where
$m$ experts are mixed, with each expert being related to a polynomial
regression model of order $k$. We study the convergence rate of the maximum
likelihood estimator (MLE), in terms of how fast the Kullback-Leibler
divergence of the estimated density converges to the true density, when the
sample size $n$ increases. The convergence rate is found to be dependent on
both $m$ and $k$, and certain choices of $m$ and $k$ are found to produce
optimal convergence rates. Therefore, these results shed light on the two
aforementioned important problems: on how to choose $m$, and on how $m$ and $k$
should be compromised, for achieving good convergence rates.
"
"  Realistic modeling of vehicular mobility has been particularly challenging
due to a lack of large libraries of measurements in the research community. In
this paper we introduce a novel method for large-scale monitoring, analysis,
and identification of spatio-temporal models for vehicular mobility using the
freely available online webcams in cities across the globe. We collect
vehicular mobility traces from 2,700 traffic webcams in 10 different cities for
several months and generate a mobility dataset of 7.5 Terabytes consisting of
125 million of images. To the best of our knowl- edge, this is the largest data
set ever used in such study. To process and analyze this data, we propose an
efficient and scalable algorithm to estimate traffic density based on
background image subtraction. Initial results show that at least 82% of
individual cameras with less than 5% deviation from four cities follow
Loglogistic distribution and also 94% cameras from Toronto follow gamma
distribution. The aggregate results from each city also demonstrate that Log-
Logistic and gamma distribution pass the KS-test with 95% confidence.
Furthermore, many of the camera traces exhibit long range dependence, with
self-similarity evident in the aggregates of traffic (per city). We believe our
novel data collection method and dataset provide a much needed contribution to
the research community for realistic modeling of vehicular networks and
mobility.
"
"  The Bonferroni multiple testing procedure is commonly perceived as being
overly conservative in large-scale simultaneous testing situations such as
those that arise in microarray data analysis. The objective of the present
study is to show that this popular belief is due to overly stringent
requirements that are typically imposed on the procedure rather than to its
conservative nature. To get over its notorious conservatism, we advocate using
the Bonferroni selection rule as a procedure that controls the per family error
rate (PFER). The present paper reports the first study of stability properties
of the Bonferroni and Benjamini--Hochberg procedures. The Bonferroni procedure
shows a superior stability in terms of the variance of both the number of true
discoveries and the total number of discoveries, a property that is especially
important in the presence of correlations between individual $p$-values. Its
stability and the ability to provide strong control of the PFER make the
Bonferroni procedure an attractive choice in microarray studies.
"
"  Discussion of ""Latent variable graphical model selection via convex
optimization"" by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky
[arXiv:1008.1290].
"
"  The problem of detecting changes in the statistical properties of a
stochastic system and time series arises in various branches of science and
engineering. It has a wide spectrum of important applications ranging from
machine monitoring to biomedical signal processing. In all of these
applications the observations being monitored undergo a change in distribution
in response to a change or anomaly in the environment, and the goal is to
detect the change as quickly as possibly, subject to false alarm constraints.
In this chapter, two formulations of the quickest change detection problem,
Bayesian and minimax, are introduced, and optimal or asymptotically optimal
solutions to these formulations are discussed. Then some generalizations and
extensions of the quickest change detection problem are described. The chapter
is concluded with a discussion of applications and open issues.
"
"  Dimensionality reduction is a topic of recent interest. In this paper, we
present the classification constrained dimensionality reduction (CCDR)
algorithm to account for label information. The algorithm can account for
multiple classes as well as the semi-supervised setting. We present an
out-of-sample expressions for both labeled and unlabeled data. For unlabeled
data, we introduce a method of embedding a new point as preprocessing to a
classifier. For labeled data, we introduce a method that improves the embedding
during the training phase using the out-of-sample extension. We investigate
classification performance using the CCDR algorithm on hyper-spectral satellite
imagery data. We demonstrate the performance gain for both local and global
classifiers and demonstrate a 10% improvement of the $k$-nearest neighbors
algorithm performance. We present a connection between intrinsic dimension
estimation and the optimal embedding dimension obtained using the CCDR
algorithm.
"
"  Exploring the genetic basis of heritable traits remains one of the central
challenges in biomedical research. In simple cases, single polymorphic loci
explain a significant fraction of the phenotype variability. However, many
traits of interest appear to be subject to multifactorial control by groups of
genetic loci instead. Accurate detection of such multivariate associations is
nontrivial and often hindered by limited power. At the same time, confounding
influences such as population structure cause spurious association signals that
result in false positive findings if they are not accounted for in the model.
Here, we propose LMM-Lasso, a mixed model that allows for both, multi-locus
mapping and correction for confounding effects. Our approach is simple and free
of tuning parameters, effectively controls for population structure and scales
to genome-wide datasets. We show practical use in genome-wide association
studies and linkage mapping through retrospective analyses. In data from
Arabidopsis thaliana and mouse, our method is able to find a genetic cause for
significantly greater fractions of phenotype variation in 91% of the phenotypes
considered. At the same time, our model dissects this variability into
components that result from individual SNP effects and population structure. In
addition to this increase of genetic heritability, enrichment of known
candidate genes suggests that the associations retrieved by LMM-Lasso are more
likely to be genuine.
"
"  In this article, we advocate the ensemble approach for variable selection. We
point out that the stochastic mechanism used to generate the variable-selection
ensemble (VSE) must be picked with care. We construct a VSE using a stochastic
stepwise algorithm, and compare its performance with numerous state-of-the-art
algorithms.
"
"  The prevalence of common chronic non-communicable diseases (CNCDs) far
overshadows the prevalence of both monogenic and infectious diseases combined.
All CNCDs, also called complex genetic diseases, have a heritable genetic
component that can be used for pre-symptomatic risk assessment. Common single
nucleotide polymorphisms (SNPs) that tag risk haplotypes across the genome
currently account for a non-trivial portion of the germ-line genetic risk and
we will likely continue to identify the remaining missing heritability in the
form of rare variants, copy number variants and epigenetic modifications. Here,
we describe a novel measure for calculating the lifetime risk of a disease,
called the genetic composite index (GCI), and demonstrate its predictive value
as a clinical classifier. The GCI only considers summary statistics of the
effects of genetic variation and hence does not require the results of
large-scale studies simultaneously assessing multiple risk factors. Combining
GCI scores with environmental risk information provides an additional tool for
clinical decision-making. The GCI can be populated with heritable risk
information of any type, and thus represents a framework for CNCD
pre-symptomatic risk assessment that can be populated as additional risk
information is identified through next-generation technologies.
"
"  Within the private-values paradigm, we construct a tractable empirical model
of equilibrium behavior at first-price auctions when bidders' valuations are
potentially dependent, but not necessarily affiliated. We develop a test of
affiliation and apply our framework to data from low-price, sealed-bid auctions
held by the Department of Transportation in the State of Michigan to procure
road-resurfacing services: we do not reject the hypothesis of affiliation in
cost signals.
"
"  Item recommendation is the task of predicting a personalized ranking on a set
of items (e.g. websites, movies, products). In this paper, we investigate the
most common scenario with implicit feedback (e.g. clicks, purchases). There are
many methods for item recommendation from implicit feedback like matrix
factorization (MF) or adaptive knearest-neighbor (kNN). Even though these
methods are designed for the item prediction task of personalized ranking, none
of them is directly optimized for ranking. In this paper we present a generic
optimization criterion BPR-Opt for personalized ranking that is the maximum
posterior estimator derived from a Bayesian analysis of the problem. We also
provide a generic learning algorithm for optimizing models with respect to
BPR-Opt. The learning method is based on stochastic gradient descent with
bootstrap sampling. We show how to apply our method to two state-of-the-art
recommender models: matrix factorization and adaptive kNN. Our experiments
indicate that for the task of personalized ranking our optimization method
outperforms the standard learning techniques for MF and kNN. The results show
the importance of optimizing models for the right criterion.
"
"  A one dimensional diffusion process $X=\{X_t, 0\leq t \leq T\}$, with drift
$b(x)$ and diffusion coefficient $\sigma(\theta, x)=\sqrt{\theta} \sigma(x)$
known up to $\theta>0$, is supposed to switch volatility regime at some point
$t^*\in (0,T)$. On the basis of discrete time observations from $X$, the
problem is the one of estimating the instant of change in the volatility
structure $t^*$ as well as the two values of $\theta$, say $\theta_1$ and
$\theta_2$, before and after the change point. It is assumed that the sampling
occurs at regularly spaced times intervals of length $\Delta_n$ with
$n\Delta_n=T$. To work out our statistical problem we use a least squares
approach. Consistency, rates of convergence and distributional results of the
estimators are presented under an high frequency scheme. We also study the case
of a diffusion process with unknown drift and unknown volatility but constant.
"
"  This article presents a short case study in text analysis: the scoring of
Twitter posts for positive, negative, or neutral sentiment directed towards
particular US politicians. The study requires selection of a sub-sample of
representative posts for sentiment scoring, a common and costly aspect of
sentiment mining. As a general contribution, our application is preceded by a
proposed algorithm for maximizing sampling efficiency. In particular, we
outline and illustrate greedy selection of documents to build designs that are
D-optimal in a topic-factor decomposition of the original text. The strategy is
applied to our motivating dataset of political posts, and we outline a new
technique for predicting both generic and subject-specific document sentiment
through use of variable interactions in multinomial inverse regression. Results
are presented for analysis of 2.1 million Twitter posts around February 2012.
"
"  To address an important risk classification issue that arises in clinical
practice, we propose a new mixture model via latent cure rate markers for
survival data with a cure fraction. In the proposed model, the latent cure rate
markers are modeled via a multinomial logistic regression and patients who
share the same cure rate are classified into the same risk group. Compared to
available cure rate models, the proposed model fits better to data from a
prostate cancer clinical trial. In addition, the proposed model can be used to
determine the number of risk groups and to develop a predictive classification
algorithm.
"
"  In this paper, we explore and detail our experiments in a
high-dimensionality, multi-class image classification problem often found in
the automatic recognition of Sign Languages. Here, our efforts are directed
towards comparing the characteristics, advantages and drawbacks of creating and
training Support Vector Machines disposed in a Directed Acyclic Graph and
Artificial Neural Networks to classify signs from the Brazilian Sign Language
(LIBRAS). We explore how the different heuristics, hyperparameters and
multi-class decision schemes affect the performance, efficiency and ease of use
for each classifier. We provide hyperparameter surface maps capturing accuracy
and efficiency, comparisons between DDAGs and 1-vs-1 SVMs, and effects of
heuristics when training ANNs with Resilient Backpropagation. We report
statistically significant results using Cohen's Kappa statistic for contingency
tables.
"
"  In response to a 1997 problem of M. Vidyasagar, we state a criterion for PAC
learnability of a concept class $\mathscr C$ under the family of all non-atomic
(diffuse) measures on the domain $\Omega$. The uniform Glivenko--Cantelli
property with respect to non-atomic measures is no longer a necessary
condition, and consistent learnability cannot in general be expected. Our
criterion is stated in terms of a combinatorial parameter $\VC({\mathscr
C}\,{\mathrm{mod}}\,\omega_1)$ which we call the VC dimension of $\mathscr C$
modulo countable sets. The new parameter is obtained by ""thickening up"" single
points in the definition of VC dimension to uncountable ""clusters"".
Equivalently, $\VC(\mathscr C\modd\omega_1)\leq d$ if and only if every
countable subclass of $\mathscr C$ has VC dimension $\leq d$ outside a
countable subset of $\Omega$. The new parameter can be also expressed as the
classical VC dimension of $\mathscr C$ calculated on a suitable subset of a
compactification of $\Omega$. We do not make any measurability assumptions on
$\mathscr C$, assuming instead the validity of Martin's Axiom (MA). Similar
results are obtained for function learning in terms of fat-shattering dimension
modulo countable sets, but, just like in the classical distribution-free case,
the finiteness of this parameter is sufficient but not necessary for PAC
learnability under non-atomic measures.
"
"  A pivotal problem in Bayesian nonparametrics is the construction of prior
distributions on the space M(V) of probability measures on a given domain V. In
principle, such distributions on the infinite-dimensional space M(V) can be
constructed from their finite-dimensional marginals---the most prominent
example being the construction of the Dirichlet process from finite-dimensional
Dirichlet distributions. This approach is both intuitive and applicable to the
construction of arbitrary distributions on M(V), but also hamstrung by a number
of technical difficulties. We show how these difficulties can be resolved if
the domain V is a Polish topological space, and give a representation theorem
directly applicable to the construction of any probability distribution on M(V)
whose first moment measure is well-defined. The proof draws on a projective
limit theorem of Bochner, and on properties of set functions on Polish spaces
to establish countable additivity of the resulting random probabilities.
"
"  In this work we introduce a mixture of GPs to address the data association
problem, i.e. to label a group of observations according to the sources that
generated them. Unlike several previously proposed GP mixtures, the novel
mixture has the distinct characteristic of using no gating function to
determine the association of samples and mixture components. Instead, all the
GPs in the mixture are global and samples are clustered following
""trajectories"" across input space. We use a non-standard variational Bayesian
algorithm to efficiently recover sample labels and learn the hyperparameters.
We show how multi-object tracking problems can be disambiguated and also
explore the characteristics of the model in traditional regression settings.
"
"  Parallel least mean square-partial parallel interference cancelation
(PLMS-PPIC) is a partial interference cancelation which employs adaptive
multistage structure. In this algorithm the channel phases for all users are
assumed to be known. Having only their quarters in (0,2\pi), a modified version
of PLMS-PPIC is proposed in this paper to simultaneously estimate the channel
phases and the cancelation weights. Simulation examples are given in the cases
of balanced, unbalanced and time varying channels to show the performance of
the modified PLMS-PPIC method.
"
"  Particle physics experiments such as those run in the Large Hadron Collider
result in huge quantities of data, which are boiled down to a few numbers from
which it is hoped that a signal will be detected. We discuss a simple
probability model for this and derive frequentist and noninformative Bayesian
procedures for inference about the signal. Both are highly accurate in
realistic cases, with the frequentist procedure having the edge for interval
estimation, and the Bayesian procedure yielding slightly better point
estimates. We also argue that the significance, or $p$-value, function based on
the modified likelihood root provides a comprehensive presentation of the
information in the data and should be used for inference.
"
"  An increasing number of scholars are using longitudinal social network data
to try to obtain estimates of peer or social influence effects. These data may
provide additional statistical leverage, but they can introduce new inferential
problems. In particular, while the confounding effects of homophily in
friendship formation are widely appreciated, homophily in friendship retention
may also confound causal estimates of social influence in longitudinal network
data. We provide evidence for this claim in a Monte Carlo analysis of the
statistical model used by Christakis, Fowler, and their colleagues in numerous
articles estimating ""contagion"" effects in social networks. Our results
indicate that homophily in friendship retention induces significant upward bias
and decreased coverage levels in the Christakis and Fowler model if there is
non-negligible friendship attrition over time.
"
"  With the unprecedented photometric precision of the Kepler Spacecraft,
significant systematic and stochastic errors on transit signal levels are
observable in the Kepler photometric data. These errors, which include
discontinuities, outliers, systematic trends and other instrumental signatures,
obscure astrophysical signals. The Presearch Data Conditioning (PDC) module of
the Kepler data analysis pipeline tries to remove these errors while preserving
planet transits and other astrophysically interesting signals. The completely
new noise and stellar variability regime observed in Kepler data poses a
significant problem to standard cotrending methods such as SYSREM and TFA.
Variable stars are often of particular astrophysical interest so the
preservation of their signals is of significant importance to the astrophysical
community. We present a Bayesian Maximum A Posteriori (MAP) approach where a
subset of highly correlated and quiet stars is used to generate a cotrending
basis vector set which is in turn used to establish a range of ""reasonable""
robust fit parameters. These robust fit parameters are then used to generate a
Bayesian Prior and a Bayesian Posterior Probability Distribution Function (PDF)
which when maximized finds the best fit that simultaneously removes systematic
effects while reducing the signal distortion and noise injection which commonly
afflicts simple least-squares (LS) fitting. A numerical and empirical approach
is taken where the Bayesian Prior PDFs are generated from fits to the light
curve distributions themselves.
"
"  We analyze a class of distributed quantized consen- sus algorithms for
arbitrary networks. In the initial setting, each node in the network has an
integer value. Nodes exchange their current estimate of the mean value in the
network, and then update their estimation by communicating with their neighbors
in a limited capacity channel in an asynchronous clock setting. Eventually, all
nodes reach consensus with quantized precision. We start the analysis with a
special case of a distributed binary voting algorithm, then proceed to the
expected convergence time for the general quantized consensus algorithm
proposed by Kashyap et al. We use the theory of electric networks, random
walks, and couplings of Markov chains to derive an O(N^3log N) upper bound for
the expected convergence time on an arbitrary graph of size N, improving on the
state of art bound of O(N^4logN) for binary consensus and O(N^5) for quantized
consensus algorithms. Our result is not dependent on graph topology.
Simulations on special graphs such as star networks, line graphs, lollipop
graphs, and Erd\""os-R\'enyi random graphs are performed to validate the
analysis. This work has applications to load balancing, coordination of
autonomous agents, estimation and detection, decision-making networks,
peer-to-peer systems, etc.
"
"  Despite the huge amount of literature on h-index, few papers have been
devoted to the statistical analysis of h-index when a probabilistic
distribution is assumed for citation counts. The present contribution relies on
showing the available inferential techniques, by providing the details for
proper point and set estimation of the theoretical h-index. Moreover, some
issues on simultaneous inference - aimed to produce suitable scholar
comparisons - are carried out. Finally, the analysis of the citation dataset
for the Nobel Laureates (in the last five years) and for the Fields medallists
(from 2002 onward) is proposed.
"
"  The Hierarchical Mixture of Experts (HME) is a well-known tree-based model
for regression and classification, based on soft probabilistic splits. In its
original formulation it was trained by maximum likelihood, and is therefore
prone to over-fitting. Furthermore the maximum likelihood framework offers no
natural metric for optimizing the complexity and structure of the tree.
Previous attempts to provide a Bayesian treatment of the HME model have relied
either on ad-hoc local Gaussian approximations or have dealt with related
models representing the joint distribution of both input and output variables.
In this paper we describe a fully Bayesian treatment of the HME model based on
variational inference. By combining local and global variational methods we
obtain a rigourous lower bound on the marginal probability of the data under
the model. This bound is optimized during the training phase, and its resulting
value can be used for model order selection. We present results using this
approach for a data set describing robot arm kinematics.
"
"  Several approximate policy iteration schemes without value functions, which
focus on policy representation using classifiers and address policy learning as
a supervised learning problem, have been proposed recently. Finding good
policies with such methods requires not only an appropriate classifier, but
also reliable examples of best actions, covering the state space sufficiently.
Up to this time, little work has been done on appropriate covering schemes and
on methods for reducing the sample complexity of such methods, especially in
continuous state spaces. This paper focuses on the simplest possible covering
scheme (a discretized grid over the state space) and performs a
sample-complexity comparison between the simplest (and previously commonly
used) rollout sampling allocation strategy, which allocates samples equally at
each state under consideration, and an almost as simple method, which allocates
samples only as needed and requires significantly fewer samples.
"
"  We analyze the occurrence frequencies of over 15 million words recorded in
millions of books published during the past two centuries in seven different
languages. For all languages and chronological subsets of the data we confirm
that two scaling regimes characterize the word frequency distributions, with
only the more common words obeying the classic Zipf law. Using corpora of
unprecedented size, we test the allometric scaling relation between the corpus
size and the vocabulary size of growing languages to demonstrate a decreasing
marginal need for new words, a feature that is likely related to the underlying
correlations between words. We calculate the annual growth fluctuations of word
use which has a decreasing trend as the corpus size increases, indicating a
slowdown in linguistic evolution following language expansion. This ""cooling
pattern"" forms the basis of a third statistical regularity, which unlike the
Zipf and the Heaps law, is dynamical in nature.
"
"  A significant challenge to make learning techniques more suitable for general
purpose use is to move beyond i) complete supervision, ii) low dimensional
data, iii) a single task and single view per instance. Solving these challenges
allows working with ""Big Data"" problems that are typically high dimensional
with multiple (but possibly incomplete) labelings and views. While other work
has addressed each of these problems separately, in this paper we show how to
address them together, namely semi-supervised dimension reduction for
multi-task and multi-view learning (SSDR-MML), which performs optimization for
dimension reduction and label inference in semi-supervised setting. The
proposed framework is designed to handle both multi-task and multi-view
learning settings, and can be easily adapted to many useful applications.
Information obtained from all tasks and views is combined via reconstruction
errors in a linear fashion that can be efficiently solved using an alternating
optimization scheme. Our formulation has a number of advantages. We explicitly
model the information combining mechanism as a data structure (a
weight/nearest-neighbor matrix) which allows investigating fundamental
questions in multi-task and multi-view learning. We address one such question
by presenting a general measure to quantify the success of simultaneous
learning of multiple tasks or from multiple views. We show that our SSDR-MML
approach can outperform many state-of-the-art baseline methods and demonstrate
the effectiveness of connecting dimension reduction and learning.
"
"  Fisher score is one of the most widely used supervised feature selection
methods. However, it selects each feature independently according to their
scores under the Fisher criterion, which leads to a suboptimal subset of
features. In this paper, we present a generalized Fisher score to jointly
select features. It aims at finding an subset of features, which maximize the
lower bound of traditional Fisher score. The resulting feature selection
problem is a mixed integer programming, which can be reformulated as a
quadratically constrained linear programming (QCLP). It is solved by cutting
plane algorithm, in each iteration of which a multiple kernel learning problem
is solved alternatively by multivariate ridge regression and projected gradient
descent. Experiments on benchmark data sets indicate that the proposed method
outperforms Fisher score as well as many other state-of-the-art feature
selection methods.
"
"  The partition function of a factor graph can sometimes be accurately
estimated by Monte Carlo methods. In this paper, such methods are extended to
factor graphs with negative and complex factors.
"
"  This paper deals with the comparison of several stationary processes with
unequal sample sizes. We provide a detailed theoretical framework on the
testing problem for equality of spectral densities in the bivariate case, after
which the generalization of our approach to the m dimensional case and to other
statistical applications (like testing for zero correlation or clustering of
time series data with different length) is straightforward. We prove asymptotic
normality of an appropriately standardized version of the test statistic both
under the null and the alternative and investigate the finite sample properties
of our method in a simulation study. Furthermore we apply our approach to
cluster financial time series data with different sample length.
"
"  We study the problem of finding the smallest $m$ such that every element of
an exponential family can be written as a mixture of $m$ elements of another
exponential family. We propose an approach based on coverings and packings of
the face lattice of the corresponding convex support polytopes and results from
coding theory. We show that $m=q^{N-1}$ is the smallest number for which any
distribution of $N$ $q$-ary variables can be written as mixture of $m$
independent $q$-ary variables. Furthermore, we show that any distribution of
$N$ binary variables is a mixture of $m = 2^{N-(k+1)}(1+ 1/(2^k-1))$ elements
of the $k$-interaction exponential family.
"
"  We propose a method that infers whether linear relations between two
high-dimensional variables X and Y are due to a causal influence from X to Y or
from Y to X. The earlier proposed so-called Trace Method is extended to the
regime where the dimension of the observed variables exceeds the sample size.
Based on previous work, we postulate conditions that characterize a causal
relation between X and Y. Moreover, we describe a statistical test and argue
that both causal directions are typically rejected if there is a common cause.
A full theoretical analysis is presented for the deterministic case but our
approach seems to be valid for the noisy case, too, for which we additionally
present an approach based on a sparsity constraint. The discussed method yields
promising results for both simulated and real world data.
"
"  In this paper a new dissimilarity measure to identify groups of assets
dynamics is proposed. The underlying generating process is assumed to be a
diffusion process solution of stochastic differential equations and observed at
discrete time. The mesh of observations is not required to shrink to zero. As
distance between two observed paths, the quadratic distance of the
corresponding estimated Markov operators is considered. Analysis of both
synthetic data and real financial data from NYSE/NASDAQ stocks, give evidence
that this distance seems capable to catch differences in both the drift and
diffusion coefficients contrary to other commonly used metrics.
"
"  The Interbank Offered Rate is a vital benchmark interest rate in the
financial markets of every country to which financial contracts are tied. In
the light of the recent LIBOR manipulation incident, this paper seeks to
address the fear that Interbank Offered Rate are entirely controlled by the
bank. The paper will focus on the comparison between LIBOR and SIBOR especially
with regards to the behavior of the interest rate with time. Because of the
nature of IBORs, banks will naturally be submitting similar rates which should
not differ excessively from the market as well as the other banks. We will
compare the LIBOR and SIBOR from 2005 to 2011 with respect to the 1 month rates
on an annual basis. We will present the result that the SIBOR is not
manipulated like LIBOR.
"
"  Trace norm regularization is a popular method of multitask learning. We give
excess risk bounds with explicit dependence on the number of tasks, the number
of examples per task and properties of the data distribution. The bounds are
independent of the dimension of the input space, which may be infinite as in
the case of reproducing kernel Hilbert spaces. A byproduct of the proof are
bounds on the expected norm of sums of random positive semidefinite matrices
with subexponential moments.
"
"  The problem of estimating the L\'evy density of a partially observed
multidimensional affine process from low-frequency and mixed-frequency data is
considered. The estimation methodology is based on the log-affine
representation of the conditional characteristic function of an affine process
and local linear smoothing in time. We derive almost sure uniform rates of
convergence for the estimated L\'evy density both in mixed-frequency and
low-frequency setups and prove that these rates are optimal in the minimax
sense. Finally, the performance of the estimation algorithms is illustrated in
the case of the Bates stochastic volatility model.
"
"  Minwise hashing is the standard technique in the context of search and
databases for efficiently estimating set (e.g., high-dimensional 0/1 vector)
similarities. Recently, b-bit minwise hashing was proposed which significantly
improves upon the original minwise hashing in practice by storing only the
lowest b bits of each hashed value, as opposed to using 64 bits. b-bit hashing
is particularly effective in applications which mainly concern sets of high
similarities (e.g., the resemblance >0.5). However, there are other important
applications in which not just pairs of high similarities matter. For example,
many learning algorithms require all pairwise similarities and it is expected
that only a small fraction of the pairs are similar. Furthermore, many
applications care more about containment (e.g., how much one object is
contained by another object) than the resemblance. In this paper, we show that
the estimators for minwise hashing and b-bit minwise hashing used in the
current practice can be systematically improved and the improvements are most
significant for set pairs of low resemblance and high containment.
"
"  In this paper, we develop an approach to recursively estimate the quadratic
risk for matrix recovery problems regularized with spectral functions. Toward
this end, in the spirit of the SURE theory, a key step is to compute the (weak)
derivative and divergence of a solution with respect to the observations. As
such a solution is not available in closed form, but rather through a proximal
splitting algorithm, we propose to recursively compute the divergence from the
sequence of iterates. A second challenge that we unlocked is the computation of
the (weak) derivative of the proximity operator of a spectral function. To show
the potential applicability of our approach, we exemplify it on a matrix
completion problem to objectively and automatically select the regularization
parameter.
"
"  In this work we develop Curvature Propagation (CP), a general technique for
efficiently computing unbiased approximations of the Hessian of any function
that is computed using a computational graph. At the cost of roughly two
gradient evaluations, CP can give a rank-1 approximation of the whole Hessian,
and can be repeatedly applied to give increasingly precise unbiased estimates
of any or all of the entries of the Hessian. Of particular interest is the
diagonal of the Hessian, for which no general approach is known to exist that
is both efficient and accurate. We show in experiments that CP turns out to
work well in practice, giving very accurate estimates of the Hessian of neural
networks, for example, with a relatively small amount of work. We also apply CP
to Score Matching, where a diagonal of a Hessian plays an integral role in the
Score Matching objective, and where it is usually computed exactly using
inefficient algorithms which do not scale to larger and more complex models.
"
"  This book presents a methodology and philosophy of empirical science based on
large scale lossless data compression. In this view a theory is scientific if
it can be used to build a data compression program, and it is valuable if it
can compress a standard benchmark database to a small size, taking into account
the length of the compressor itself. This methodology therefore includes an
Occam principle as well as a solution to the problem of demarcation. Because of
the fundamental difficulty of lossless compression, this type of research must
be empirical in nature: compression can only be achieved by discovering and
characterizing empirical regularities in the data. Because of this, the
philosophy provides a way to reformulate fields such as computer vision and
computational linguistics as empirical sciences: the former by attempting to
compress databases of natural images, the latter by attempting to compress
large text databases. The book argues that the rigor and objectivity of the
compression principle should set the stage for systematic progress in these
fields. The argument is especially strong in the context of computer vision,
which is plagued by chronic problems of evaluation.
  The book also considers the field of machine learning. Here the traditional
approach requires that the models proposed to solve learning problems be
extremely simple, in order to avoid overfitting. However, the world may contain
intrinsically complex phenomena, which would require complex models to
understand. The compression philosophy can justify complex models because of
the large quantity of data being modeled (if the target database is 100 Gb, it
is easy to justify a 10 Mb model). The complex models and abstractions learned
on the basis of the raw data (images, language, etc) can then be reused to
solve any specific learning problem, such as face recognition or machine
translation.
"
"  Missing data estimation is an important challenge with high-dimensional data
arranged in the form of a matrix. Typically this data matrix is transposable,
meaning that either the rows, columns or both can be treated as features. To
model transposable data, we present a modification of the matrix-variate
normal, the mean-restricted matrix-variate normal, in which the rows and
columns each have a separate mean vector and covariance matrix. By placing
additive penalties on the inverse covariance matrices of the rows and columns,
these so-called transposable regularized covariance models allow for maximum
likelihood estimation of the mean and nonsingular covariance matrices. Using
these models, we formulate EM-type algorithms for missing data imputation in
both the multivariate and transposable frameworks. We present theoretical
results exploiting the structure of our transposable models that allow these
models and imputation methods to be applied to high-dimensional data.
Simulations and results on microarray data and the Netflix data show that these
imputation techniques often outperform existing methods and offer a greater
degree of flexibility.
"
"  We study sparse principal components analysis in the high-dimensional
setting, where $p$ (the number of variables) can be much larger than $n$ (the
number of observations). We prove optimal, non-asymptotic lower and upper
bounds on the minimax estimation error for the leading eigenvector when it
belongs to an $\ell_q$ ball for $q \in [0,1]$. Our bounds are sharp in $p$ and
$n$ for all $q \in [0, 1]$ over a wide class of distributions. The upper bound
is obtained by analyzing the performance of $\ell_q$-constrained PCA. In
particular, our results provide convergence rates for $\ell_1$-constrained PCA.
"
"  Determination of the minimum inhibitory concentration (MIC) of a drug that
prevents microbial growth is an important step for managing patients with
infections. In this paper we present a novel probabilistic approach that
accurately estimates MICs based on a panel of multiple curves reflecting
features of bacterial growth. We develop a probabilistic model for determining
whether a given dilution of an antimicrobial agent is the MIC given features of
the growth curves over time. Because of the potentially large collection of
features, we utilize Bayesian model selection to narrow the collection of
predictors to the most important variables. In addition to point estimates of
MICs, we are able to provide posterior probabilities that each dilution is the
MIC based on the observed growth curves. The methods are easily automated and
have been incorporated into the Becton--Dickinson PHOENIX automated
susceptibility system that rapidly and accurately classifies the resistance of
a large number of microorganisms in clinical samples. Over seventy-five studies
to date have shown this new method provides improved estimation of MICs over
existing approaches.
"
"  We consider the least-square linear regression problem with regularization by
the l1-norm, a problem usually referred to as the Lasso. In this paper, we
present a detailed asymptotic analysis of model consistency of the Lasso. For
various decays of the regularization parameter, we compute asymptotic
equivalents of the probability of correct model selection (i.e., variable
selection). For a specific rate decay, we show that the Lasso selects all the
variables that should enter the model with probability tending to one
exponentially fast, while it selects all other variables with strictly positive
probability. We show that this property implies that if we run the Lasso for
several bootstrapped replications of a given sample, then intersecting the
supports of the Lasso bootstrap estimates leads to consistent model selection.
This novel variable selection algorithm, referred to as the Bolasso, is
compared favorably to other linear regression methods on synthetic data and
datasets from the UCI machine learning repository.
"
"  This document derives the CPHD filter for extended targets. Only the update
step is derived here. Target generated measurements, false alarms and prior are
all assumed to be independent identically distributed cluster processes. We
also prove here that the derived CPHD filter for extended targets reduce to PHD
filter for extended targets and CPHD filter for standard targets under suitable
assumptions.
"
"  Advances in nanotechnology have allowed scientists to study biological
processes on an unprecedented nanoscale molecule-by-molecule basis, opening the
door to addressing many important biological problems. A phenomenon observed in
recent nanoscale single-molecule biophysics experiments is subdiffusion, which
largely departs from the classical Brownian diffusion theory. In this paper, by
incorporating fractional Gaussian noise into the generalized Langevin equation,
we formulate a model to describe subdiffusion. We conduct a detailed analysis
of the model, including (i) a spectral analysis of the stochastic
integro-differential equations introduced in the model and (ii) a microscopic
derivation of the model from a system of interacting particles. In addition to
its analytical tractability and clear physical underpinning, the model is
capable of explaining data collected in fluorescence studies on single protein
molecules. Excellent agreement between the model prediction and the
single-molecule experimental data is seen.
"
"  Motivation: NMR spectra are widely used in metabolomics to obtain metabolite
profiles in complex biological mixtures. Common methods used to assign and
estimate concentrations of metabolites involve either an expert manual peak
fitting or extra pre-processing steps, such as peak alignment and binning. Peak
fitting is very time consuming and is subject to human error. Conversely,
alignment and binning can introduce artefacts and limit immediate biological
interpretation of models. Results: We present the Bayesian AuTomated Metabolite
Analyser for NMR spectra (BATMAN), an R package which deconvolutes peaks from
1-dimensional NMR spectra, automatically assigns them to specific metabolites
from a target list and obtains concentration estimates. The Bayesian model
incorporates information on charac-teristic peak patterns of metabolites and is
able to account for shifts in the position of peaks commonly seen in NMR
spectra of biological samples. It applies a Markov Chain Monte Carlo (MCMC)
algorithm to sample from a joint posterior distribution of the model parameters
and obtains concentration estimates with reduced error compared with
conventional numerical integration and comparable to manual deconvolution by
experienced spectroscopists. Availability:
http://www1.imperial.ac.uk/medicine/people/t.ebbels/ Contact:
t.ebbels@imperial.ac.uk
"
"  We develop a geometrical interpretation of ternary probabilistic forecasts in
which forecasts and observations are regarded as points inside a triangle.
Within the triangle, we define a continuous colour palette in which hue and
colour saturation are defined with reference to the observed climatology. In
contrast to current methods, forecast maps created with this colour scheme
convey all of the information present in each ternary forecast. The geometrical
interpretation is then extended to verification under quadratic scoring rules
(of which the Brier Score and the Ranked Probability Score are well--known
examples). Each scoring rule defines an associated triangle in which the square
roots of the score, the reliability, the uncertainty and the resolution all
have natural interpretations as root--mean--square distances. This leads to our
proposal for a Ternary Reliability Diagram in which data relating to
verification and calibration can be summarised. We illustrate these ideas with
data relating to seasonal forecasting of precipitation in South America,
including an example of nonlinear forecast calibration. Codes implementing
these ideas have been produced using the statistical software package R and are
available from the authors.
"
"  In recent years, a growing need has arisen in different fields for the
development of computational systems for automated analysis of large amounts of
data (high-throughput). Dealing with nonstandard noise structure and outliers,
that could have been detected and corrected in manual analysis, must now be
built into the system with the aid of robust methods. We discuss such problems
and present insights and solutions in the context of behavior genetics, where
data consists of a time series of locations of a mouse in a circular arena. In
order to estimate the location, velocity and acceleration of the mouse, and
identify stops, we use a nonstandard mix of robust and resistant methods:
LOWESS and repeated running median. In addition, we argue that protection
against small deviations from experimental protocols can be handled
automatically using statistical methods. In our case, it is of biological
interest to measure a rodent's distance from the arena's wall, but this measure
is corrupted if the arena is not a perfect circle, as required in the protocol.
The problem is addressed by estimating robustly the actual boundary of the
arena and its center using a nonparametric regression quantile of the
behavioral data, with the aid of a fast algorithm developed for that purpose.
"
"  After building a classifier with modern tools of machine learning we
typically have a black box at hand that is able to predict well for unseen
data. Thus, we get an answer to the question what is the most likely label of a
given unseen data point. However, most methods will provide no answer why the
model predicted the particular label for a single instance and what features
were most influential for that particular instance. The only method that is
currently able to provide such explanations are decision trees. This paper
proposes a procedure which (based on a set of assumptions) allows to explain
the decisions of any classification method.
"
"  We study graph estimation and density estimation in high dimensions, using a
family of density estimators based on forest structured undirected graphical
models. For density estimation, we do not assume the true distribution
corresponds to a forest; rather, we form kernel density estimates of the
bivariate and univariate marginals, and apply Kruskal's algorithm to estimate
the optimal forest on held out data. We prove an oracle inequality on the
excess risk of the resulting estimator relative to the risk of the best forest.
For graph estimation, we consider the problem of estimating forests with
restricted tree sizes. We prove that finding a maximum weight spanning forest
with restricted tree size is NP-hard, and develop an approximation algorithm
for this problem. Viewing the tree size as a complexity parameter, we then
select a forest using data splitting, and prove bounds on excess risk and
structure selection consistency of the procedure. Experiments with simulated
data and microarray data indicate that the methods are a practical alternative
to Gaussian graphical models.
"
"  The regularization path of the Lasso can be shown to be piecewise linear,
making it possible to ""follow"" and explicitly compute the entire path. We
analyze in this paper this popular strategy, and prove that its worst case
complexity is exponential in the number of variables. We then oppose this
pessimistic result to an (optimistic) approximate analysis: We show that an
approximate path with at most O(1/sqrt(epsilon)) linear segments can always be
obtained, where every point on the path is guaranteed to be optimal up to a
relative epsilon-duality gap. We complete our theoretical analysis with a
practical algorithm to compute these approximate paths.
"
"  We are interested in the estimation and prediction of a parametric model on a
short dataset upon which it is expected to overfit and perform badly. To
overcome the lack of data (relatively to the dimension of the model) we propose
the construction of an informative hierarchical Bayesian prior based upon
another longer dataset which is assumed to share some similarities with the
original, short dataset. We illustrate the performance of our prior on
simulated dataset from three standard models. Then we apply the methodology to
a working model for the electricity load forecasting on real datasets, where it
leads to a substantial improvement of the quality of the predictions.
"
"  The ""mind-brain supervenience"" conjecture suggests that all mental properties
are derived from the physical properties of the brain. To address the question
of whether the mind supervenes on the brain, we frame a supervenience
hypothesis in rigorous statistical terms. Specifically, we propose a modified
version of supervenience (called epsilon-supervenience) that is amenable to
experimental investigation and statistical analysis. To illustrate this
approach, we perform a thought experiment that illustrates how the
probabilistic theory of pattern recognition can be used to make a one-sided
determination of epsilon-supervenience. The physical property of the brain
employed in this analysis is the graph describing brain connectivity (i.e., the
brain-graph or connectome). epsilon-supervenience allows us to determine
whether a particular mental property can be inferred from one's connectome to
within any given positive misclassification rate, regardless of the
relationship between the two. This may provide motivation for
cross-disciplinary research between neuroscientists and statisticians.
"
"  Walley's Imprecise Dirichlet Model (IDM) for categorical i.i.d. data extends
the classical Dirichlet model to a set of priors. It overcomes several
fundamental problems which other approaches to uncertainty suffer from. Yet, to
be useful in practice, one needs efficient ways for computing the
imprecise=robust sets or intervals. The main objective of this work is to
derive exact, conservative, and approximate, robust and credible interval
estimates under the IDM for a large class of statistical estimators, including
the entropy and mutual information.
"
"  Spatially explicit data layers of tree species assemblages, referred to as
forest types or forest type groups, are a key component in large-scale
assessments of forest sustainability, biodiversity, timber biomass, carbon
sinks and forest health monitoring. This paper explores the utility of coupling
georeferenced national forest inventory (NFI) data with readily available and
spatially complete environmental predictor variables through spatially-varying
multinomial logistic regression models to predict forest type groups across
large forested landscapes. These models exploit underlying spatial associations
within the NFI plot array and the spatially-varying impact of predictor
variables to improve the accuracy of forest type group predictions. The
richness of these models incurs onerous computational burdens and we discuss
dimension reducing spatial processes that retain the richness in modeling. We
illustrate using NFI data from Michigan, USA, where we provide a comprehensive
analysis of this large study area and demonstrate improved prediction with
associated measures of uncertainty.
"
"  The estimation of the total of an attribute defined over a continuous planar
domain is required in many applied settings, such as the estimation of canopy
coverage in the Monterano Nature Reserve in Italy. If the design-based approach
is considered, the scheme for the placement of the sample sites over the domain
is fundamental in order to implement the survey. In real situations, a commonly
adopted scheme is based on partitioning the domain into suitable strata, in
such a way that a single sample site is uniformly placed (i.e., selected with
uniform probability density) in each stratum and sample sites are independently
located. Under mild conditions on the function representing the target
attribute, it is shown that this scheme gives rise to an unbiased spatial total
estimator which is ""superefficient"" with respect to the estimator based on the
uniform placement of independent sample sites over the domain. In addition, the
large-sample normality of the estimator is proven and variance estimation
issues are discussed.
"
"  Many random processes can be simulated as the output of a deterministic model
accepting random inputs. Such a model usually describes a complex mathematical
or physical stochastic system and the randomness is introduced in the input
variables of the model. When the statistics of the output event are known,
these input variables have to be chosen in a specific way for the output to
have the prescribed statistics. Because the probability distribution of the
input random variables is not directly known but dictated implicitly by the
statistics of the output random variables, this problem is usually intractable
for classical sampling methods. Based on Markov Chain Monte Carlo we propose a
novel method to sample random inputs to such models by introducing a
modification to the standard Metropolis-Hastings algorithm. As an example we
consider a system described by a stochastic differential equation (sde) and
demonstrate how sample paths of a random process satisfying this sde can be
generated with our technique.
"
"  In clinical trials, a covariate-adjusted response-adaptive (CARA) design
allows a subject newly entering a trial a better chance of being allocated to a
superior treatment regimen based on cumulative information from previous
subjects, and adjusts the allocation according to individual covariate
information.
  Since this design allocates subjects sequentially, it is natural to apply a
sequential method for estimating the treatment effect in order to make the data
analysis more efficient.
  In this paper, we study the sequential estimation of treatment effect for a
general CARA design. A stopping criterion is proposed such that the estimates
satisfy a prescribed precision when the sampling is stopped. The properties of
estimates and stopping time} are obtained under the proposed stopping rule. In
addition, we show that the asymptotic properties of the allocation function,
under the proposed stopping rule, are the same as those obtained in the
non-sequential/fixed sample size counterpart.
  We then illustrate the performance of the proposed procedure with some
simulation results using logistic models. The properties, such as the coverage
probability of treatment effect, correct allocation proportion and average
sample size, for diverse combinations of initial sample sizes and tuning
parameters in the utility function are discussed.
"
"  Lasso is a widely used regression technique to find sparse representations.
When the dimension of the feature space and the number of samples are extremely
large, solving the Lasso problem remains challenging. To improve the efficiency
of solving large-scale Lasso problems, El Ghaoui and his colleagues have
proposed the SAFE rules which are able to quickly identify the inactive
predictors, i.e., predictors that have $0$ components in the solution vector.
Then, the inactive predictors or features can be removed from the optimization
problem to reduce its scale. By transforming the standard Lasso to its dual
form, it can be shown that the inactive predictors include the set of inactive
constraints on the optimal dual solution. In this paper, we propose an
efficient and effective screening rule via Dual Polytope Projections (DPP),
which is mainly based on the uniqueness and nonexpansiveness of the optimal
dual solution due to the fact that the feasible set in the dual space is a
convex and closed polytope. Moreover, we show that our screening rule can be
extended to identify inactive groups in group Lasso. To the best of our
knowledge, there is currently no ""exact"" screening rule for group Lasso. We
have evaluated our screening rule using synthetic and real data sets. Results
show that our rule is more effective in identifying inactive predictors than
existing state-of-the-art screening rules for Lasso.
"
"  In recent years, non-parametric methods utilizing random walks on graphs have
been used to solve a wide range of machine learning problems, but in their
simplest form they do not scale well due to the quadratic complexity. In this
paper, a new dual-tree based variational approach for approximating the
transition matrix and efficiently performing the random walk is proposed. The
approach exploits a connection between kernel density estimation, mixture
modeling, and random walk on graphs in an optimization of the transition matrix
for the data graph that ties together edge transitions probabilities that are
similar. Compared to the de facto standard approximation method based on
k-nearestneighbors, we demonstrate order of magnitudes speedup without
sacrificing accuracy for Label Propagation tasks on benchmark data sets in
semi-supervised learning.
"
"  We introduce in this paper a new way of optimizing the natural extension of
the quantization error using in k-means clustering to dissimilarity data. The
proposed method is based on hierarchical clustering analysis combined with
multi-level heuristic refinement. The method is computationally efficient and
achieves better quantization errors than the
"
"  We comment on a paper by Kaminski et al. (2001) and show that their claim of
a relationship between the directed transfer function (DTF) and the concept of
Granger causality is false.
"
"  Representations based on random walks can exploit discrete data distributions
for clustering and classification. We extend such representations from discrete
to continuous distributions. Transition probabilities are now calculated using
a diffusion equation with a diffusion coefficient that inversely depends on the
data density. We relate this diffusion equation to a path integral and derive
the corresponding path probability measure. The framework is useful for
incorporating continuous data densities and prior knowledge.
"
"  We consider the dimensionality-reduction problem (finding a subspace
approximation of observed data) for contaminated data in the high dimensional
regime, where the number of observations is of the same magnitude as the number
of variables of each observation, and the data set contains some (arbitrarily)
corrupted observations. We propose a High-dimensional Robust Principal
Component Analysis (HR-PCA) algorithm that is tractable, robust to contaminated
points, and easily kernelizable. The resulting subspace has a bounded deviation
from the desired one, achieves maximal robustness -- a breakdown point of 50%
while all existing algorithms have a breakdown point of zero, and unlike
ordinary PCA algorithms, achieves optimality in the limit case where the
proportion of corrupted points goes to zero.
"
"  In many real-world applications of machine learning classifiers, it is
essential to predict the probability of an example belonging to a particular
class. This paper proposes a simple technique for predicting probabilities
based on optimizing a ranking loss, followed by isotonic regression. This
semi-parametric technique offers both good ranking and regression performance,
and models a richer set of probability distributions than statistical
workhorses such as logistic regression. We provide experimental results that
show the effectiveness of this technique on real-world applications of
probability prediction.
"
"  In this article, we develop an algorithm for probabilistic and constrained
projection pursuit. Our algorithm called ADIS (automated decomposition into
sources) accepts arbitrary non-linear contrast functions and constraints from
the user and performs non-square blind source separation (BSS). In the first
stage, we estimate the latent dimensionality using a combination of bootstrap
and cross validation techniques. In the second stage, we apply our
state-of-the-art optimization algorithm to perform BSS. We validate the latent
dimensionality estimation procedure via simulations on sources with different
kurtosis excess properties. Our optimization algorithm is benchmarked via
standard benchmarks from GAMS performance library. We develop two different
algorithmic frameworks for improving the quality of local solution for BSS. Our
algorithm also outputs extensive convergence diagnostics that validate the
convergence to an optimal solution for each extracted component. The quality of
extracted sources from ADIS is compared to other well known algorithms such as
Fixed Point ICA (FPICA), efficient Fast ICA (EFICA), Joint Approximate
Diagonalization (JADE) and others using the ICALAB toolbox for algorithm
comparison. In several cases, ADIS outperforms these algorithms. Finally we
apply our algorithm to a standard functional MRI data-set as a case study.
"
"  Decision making under severe lack of information is a ubiquitous situation in
nearly every applied field of engineering, policy, and science. A severe lack
of information precludes our ability to determine a frequency of occurrence of
events or conditions that impact the decision; therefore, decision
uncertainties due to a severe lack of information cannot be characterized
probabilistically. To circumvent this problem, information gap (info-gap)
theory has been developed to explicitly recognize and quantify the implications
of information gaps in decision making. This paper presents a decision analysis
based on info-gap theory developed for a contaminant remediation scenario. The
analysis provides decision support in determining the fraction of contaminant
mass to remove from the environment in the presence of a lack of information
related to the contaminant mass flux into an aquifer. An info-gap uncertainty
model is developed to characterize uncertainty due to a lack of information
concerning the contaminant flux. The info-gap uncertainty model groups nested,
convex sets of functions defining contaminant flux over time based on their
level of deviation from a nominal contaminant flux. The nominal contaminant
flux defines a reasonable contaminant flux over time based on existing
information. A robustness function is derived to quantify the maximum level of
deviation from nominal that still ensures compliance for each decision. An
opportuneness function is derived to characterize the possibility of meeting a
desired contaminant concentration level. The decision analysis evaluates how
the robustness and opportuneness change as a function of time since remediation
and as a function of the fraction of contaminant mass removed.
"
"  Ex ante forecast outcomes should be interpreted as counterfactuals (potential
histories), with errors as the spread between outcomes. Reapplying measurements
of uncertainty about the estimation errors of the estimation errors of an
estimation leads to branching counterfactuals. Such recursions of epistemic
uncertainty have markedly different distributial properties from conventional
sampling error. Nested counterfactuals of error rates invariably lead to fat
tails, regardless of the probability distribution used, and to powerlaws under
some conditions. A mere .01% branching error rate about the STD (itself an
error rate), and .01% branching error rate about that error rate, etc.
(recursing all the way) results in explosive (and infinite) higher moments than
1. Missing any degree of regress leads to the underestimation of small
probabilities and concave payoffs (a standard example of which is Fukushima).
The paper states the conditions under which higher order rates of uncertainty
(expressed in spreads of counterfactuals) alters the shapes the of final
distribution and shows which a priori beliefs about conterfactuals are needed
to accept the reliability of conventional probabilistic methods (thin tails or
mildly fat tails).
"
"  This paper deals with the trace regression model where $n$ entries or linear
combinations of entries of an unknown $m_1\times m_2$ matrix $A_0$ corrupted by
noise are observed. We propose a new nuclear norm penalized estimator of $A_0$
and establish a general sharp oracle inequality for this estimator for
arbitrary values of $n,m_1,m_2$ under the condition of isometry in expectation.
Then this method is applied to the matrix completion problem. In this case, the
estimator admits a simple explicit form and we prove that it satisfies oracle
inequalities with faster rates of convergence than in the previous works. They
are valid, in particular, in the high-dimensional setting $m_1m_2\gg n$. We
show that the obtained rates are optimal up to logarithmic factors in a minimax
sense and also derive, for any fixed matrix $A_0$, a non-minimax lower bound on
the rate of convergence of our estimator, which coincides with the upper bound
up to a constant factor. Finally, we show that our procedure provides an exact
recovery of the rank of $A_0$ with probability close to 1. We also discuss the
statistical learning setting where there is no underlying model determined by
$A_0$ and the aim is to find the best trace regression model approximating the
data.
"
"  Discussion on ""Brownian distance covariance"" by G\'{a}bor J. Sz\'{e}kely,
Maria L. Rizzo [arXiv:1010.0297]
"
"  We study the causal effect of winning an Oscar Award on an actor or actress's
survival. Does the increase in social rank from a performer winning an Oscar
increase the performer's life expectancy? Previous studies of this issue have
suffered from healthy performer survivor bias, that is, candidates who are
healthier will be able to act in more films and have more chance to win Oscar
Awards. To correct this bias, we adapt Robins' rank preserving structural
accelerated failure time model and $g$-estimation method. We show in simulation
studies that this approach corrects the bias contained in previous studies. We
estimate that the effect of winning an Oscar Award on survival is 4.2 years,
with a 95% confidence interval of $[-0.4,8.4]$ years. There is not strong
evidence that winning an Oscar increases life expectancy.
"
"  Estimating the cosmological microwave background is of utmost importance for
cosmology. However, its estimation from full-sky surveys such as WMAP or more
recently Planck is challenging: CMB maps are generally estimated via the
application of some source separation techniques which never prevent the final
map from being contaminated with noise and foreground residuals. These spurious
contaminations whether noise or foreground residuals are well-known to be a
plague for most cosmologically relevant tests or evaluations; this includes CMB
lensing reconstruction or non-Gaussian signatures search. Noise reduction is
generally performed by applying a simple Wiener filter in spherical harmonics;
however this does not account for the non-stationarity of the noise. Foreground
contamination is usually tackled by masking the most intense residuals detected
in the map, which makes CMB evaluation harder to perform. In this paper, we
introduce a novel noise reduction framework coined LIW-Filtering for Linear
Iterative Wavelet Filtering which is able to account for the noise spatial
variability thanks to a wavelet-based modeling while keeping the highly desired
linearity of the Wiener filter. We further show that the same filtering
technique can effectively perform foreground contamination reduction thus
providing a globally cleaner CMB map. Numerical results on simulated but
realistic Planck data are provided.
"
"  Latent variable models for network data extract a summary of the relational
structure underlying an observed network. The simplest possible models
subdivide nodes of the network into clusters; the probability of a link between
any two nodes then depends only on their cluster assignment. Currently
available models can be classified by whether clusters are disjoint or are
allowed to overlap. These models can explain a ""flat"" clustering structure.
Hierarchical Bayesian models provide a natural approach to capture more complex
dependencies. We propose a model in which objects are characterised by a latent
feature vector. Each feature is itself partitioned into disjoint groups
(subclusters), corresponding to a second layer of hierarchy. In experimental
comparisons, the model achieves significantly improved predictive performance
on social and biological link prediction tasks. The results indicate that
models with a single layer hierarchy over-simplify real networks.
"
"  Discussion of ""A statistical analysis of multiple temperature proxies: Are
reconstructions of surface temperatures over the last 1000 years reliable?"" by
B.B. McShane and A.J. Wyner [arXiv:1104.4002]
"
"  This survey is an introduction to positive definite kernels and the set of
methods they have inspired in the machine learning literature, namely kernel
methods. We first discuss some properties of positive definite kernels as well
as reproducing kernel Hibert spaces, the natural extension of the set of
functions $\{k(x,\cdot),x\in\mathcal{X}\}$ associated with a kernel $k$ defined
on a space $\mathcal{X}$. We discuss at length the construction of kernel
functions that take advantage of well-known statistical models. We provide an
overview of numerous data-analysis methods which take advantage of reproducing
kernel Hilbert spaces and discuss the idea of combining several kernels to
improve the performance on certain tasks. We also provide a short cookbook of
different kernels which are particularly useful for certain data-types such as
images, graphs or speech segments.
"
"  Clustering ensemble is one of the most recent advances in unsupervised
learning. It aims to combine the clustering results obtained using different
algorithms or from different runs of the same clustering algorithm for the same
data set, this is accomplished using on a consensus function, the efficiency
and accuracy of this method has been proven in many works in literature. In the
first part of this paper we make a comparison among current approaches to
clustering ensemble in literature. All of these approaches consist of two main
steps: the ensemble generation and consensus function. In the second part of
the paper, we suggest engaging supervision in the clustering ensemble procedure
to get more enhancements on the clustering results. Supervision can be applied
in two places: either by using semi-supervised algorithms in the clustering
ensemble generation step or in the form of a feedback used by the consensus
function stage. Also, we introduce a flexible two parameter weighting
mechanism, the first parameter describes the compatibility between the datasets
under study and the semi-supervised clustering algorithms used to generate the
base partitions, the second parameter is used to provide the user feedback on
the these partitions. The two parameters are engaged in a ""relabeling and
voting"" based consensus function to produce the final clustering.
"
"  We show that the stick-breaking construction of the beta process due to
Paisley, et al. (2010) can be obtained from the characterization of the beta
process as a Poisson process. Specifically, we show that the mean measure of
the underlying Poisson process is equal to that of the beta process. We use
this underlying representation to derive error bounds on truncated beta
processes that are tighter than those in the literature. We also develop a new
MCMC inference algorithm for beta processes, based in part on our new Poisson
process construction.
"
"  If learning methods are to scale to the massive sizes of modern datasets, it
is essential for the field of machine learning to embrace parallel and
distributed computing. Inspired by the recent development of matrix
factorization methods with rich theory but poor computational complexity and by
the relative ease of mapping matrices onto distributed architectures, we
introduce a scalable divide-and-conquer framework for noisy matrix
factorization. We present a thorough theoretical analysis of this framework in
which we characterize the statistical errors introduced by the ""divide"" step
and control their magnitude in the ""conquer"" step, so that the overall
algorithm enjoys high-probability estimation guarantees comparable to those of
its base algorithm. We also present experiments in collaborative filtering and
video background modeling that demonstrate the near-linear to superlinear
speed-ups attainable with this approach.
"
"  Lasso and other regularization procedures are attractive methods for variable
selection, subject to a proper choice of shrinkage parameter. Given a set of
potential subsets produced by a regularization algorithm, a consistent model
selection criterion is proposed to select the best one among this preselected
set. The approach leads to a fast and efficient procedure for variable
selection, especially in high-dimensional settings. Model selection consistency
of the suggested criterion is proven when the number of covariates d is fixed.
Simulation studies suggest that the criterion still enjoys model selection
consistency when d is much larger than the sample size. The simulations also
show that our approach for variable selection works surprisingly well in
comparison with existing competitors. The method is also applied to a real data
set.
"
"  We extend the herding algorithm to continuous spaces by using the kernel
trick. The resulting ""kernel herding"" algorithm is an infinite memory
deterministic process that learns to approximate a PDF with a collection of
samples. We show that kernel herding decreases the error of expectations of
functions in the Hilbert space at a rate O(1/T) which is much faster than the
usual O(1/pT) for iid random samples. We illustrate kernel herding by
approximating Bayesian predictive distributions.
"
"  We address the problem of estimating the difference between two probability
densities. A naive approach is a two-step procedure of first estimating two
densities separately and then computing their difference. However, such a
two-step procedure does not necessarily work well because the first step is
performed without regard to the second step and thus a small error incurred in
the first stage can cause a big error in the second stage. In this paper, we
propose a single-shot procedure for directly estimating the density difference
without separately estimating two densities. We derive a non-parametric
finite-sample error bound for the proposed single-shot density-difference
estimator and show that it achieves the optimal convergence rate. The
usefulness of the proposed method is also demonstrated experimentally.
"
"  Regression-based adjusted plus-minus statistics were developed in basketball
and have recently come to hockey. The purpose of these statistics is to provide
an estimate of each player's contribution to his team, independent of the
strength of his teammates, the strength of his opponents, and other variables
that are out of his control. One of the main downsides of the ordinary least
squares regression models is that the estimates have large error bounds. Since
certain pairs of teammates play together frequently, collinearity is present in
the data and is one reason for the large errors. In hockey, the relative lack
of scoring compared to basketball is another reason. To deal with these issues,
we use ridge regression, a method that is commonly used in lieu of ordinary
least squares regression when collinearity is present in the data. We also
create models that use not only goals, but also shots, Fenwick rating (shots
plus missed shots), and Corsi rating (shots, missed shots, and blocked shots).
One benefit of using these statistics is that there are roughly ten times as
many shots as goals, so there is much more data when using these statistics and
the resulting estimates have smaller error bounds. The results of our ridge
regression models are estimates of the offensive and defensive contributions of
forwards and defensemen during even strength, power play, and short handed
situations, in terms of goals per 60 minutes. The estimates are independent of
strength of teammates, strength of opponents, and the zone in which a player's
shift begins.
"
"  We consider processes on social networks that can potentially involve three
factors: homophily, or the formation of social ties due to matching individual
traits; social contagion, also known as social influence; and the causal effect
of an individual's covariates on their behavior or other measurable responses.
We show that, generically, all of these are confounded with each other.
Distinguishing them from one another requires strong assumptions on the
parametrization of the social process or on the adequacy of the covariates used
(or both). In particular we demonstrate, with simple examples, that asymmetries
in regression coefficients cannot identify causal effects, and that very simple
models of imitation (a form of social contagion) can produce substantial
correlations between an individual's enduring traits and their choices, even
when there is no intrinsic affinity between them. We also suggest some possible
constructive responses to these results.
"
"  I present several new relations between mutual information (MI) and
statistical estimation error for a system that can be regarded simultaneously
as a communication channel and as an estimator of an input parameter. I first
derive a second-order result between MI and Fisher information (FI) that is
valid for sufficiently narrow priors, but arbitrary channels. A second relation
furnishes a lower bound on the MI in terms of the minimum mean-squared error
(MMSE) on the Bayesian estimation of the input parameter from the channel
output, one that is valid for arbitrary channels and priors. The existence of
such a lower bound, while extending previous work relating the MI to the FI
that is valid only in the asymptotic and high-SNR limits, elucidates further
the fundamental connection between information and estimation theoretic
measures of fidelity. The remaining relations I present are inequalities and
correspondences among MI, FI, and MMSE in the presence of nuisance parameters.
"
"  Recently, Li et al. (Bioinformatics 27(19), 2686-91, 2011) proposed a method,
called Differential Equation-based Local Dynamic Bayesian Network (DELDBN), for
reverse engineering gene regulatory networks from time-course data. We commend
the authors for an interesting paper that draws attention to the close
relationship between dynamic Bayesian networks (DBNs) and differential
equations (DEs). Their central claim is that modifying a DBN to model Euler
approximations to the gradient rather than expression levels themselves is
beneficial for network inference. The empirical evidence provided is based on
time-course data with equally-spaced observations. However, as we discuss
below, in the particular case of equally-spaced observations, Euler
approximations and conventional DBNs lead to equivalent statistical models
that, absent artefacts due to the estimation procedure, yield networks with
identical inter-gene edge sets. Here, we discuss further the relationship
between DEs and conventional DBNs and present new empirical results on
unequally spaced data which demonstrate that modelling Euler approximations in
a DBN can lead to improved network reconstruction.
"
"  Motivation: Biomarker discovery from high-dimensional data is a crucial
problem with enormous applications in biology and medicine. It is also
extremely challenging from a statistical viewpoint, but surprisingly few
studies have investigated the relative strengths and weaknesses of the plethora
of existing feature selection methods. Methods: We compare 32 feature selection
methods on 4 public gene expression datasets for breast cancer prognosis, in
terms of predictive performance, stability and functional interpretability of
the signatures they produce. Results: We observe that the feature selection
method has a significant influence on the accuracy, stability and
interpretability of signatures. Simple filter methods generally outperform more
complex embedded or wrapper methods, and ensemble feature selection has
generally no positive effect. Overall a simple Student's t-test seems to
provide the best results. Availability: Code and data are publicly available at
http://cbio.ensmp.fr/~ahaury/.
"
"  We study the problem of learning Bayesian network structures from data. We
develop an algorithm for finding the k-best Bayesian network structures. We
propose to compute the posterior probabilities of hypotheses of interest by
Bayesian model averaging over the k-best Bayesian networks. We present
empirical results on structural discovery over several real and synthetic data
sets and show that the method outperforms the model selection method and the
state of-the-art MCMC methods.
"
"  Continuous time Bayesian networks (CTBNs) describe structured stochastic
processes with finitely many states that evolve over continuous time. A CTBN is
a directed (possibly cyclic) dependency graph over a set of variables, each of
which represents a finite state continuous time Markov process whose transition
model is a function of its parents. We address the problem of learning
parameters and structure of a CTBN from fully observed data. We define a
conjugate prior for CTBNs, and show how it can be used both for Bayesian
parameter estimation and as the basis of a Bayesian score for structure
learning. Because acyclicity is not a constraint in CTBNs, we can show that the
structure learning problem is significantly easier, both in theory and in
practice, than structure learning for dynamic Bayesian networks (DBNs).
Furthermore, as CTBNs can tailor the parameters and dependency structure to the
different time granularities of the evolution of different variables, they can
provide a better fit to continuous-time processes than DBNs with a fixed time
granularity.
"
"  This paper considers stochastic bandits with side observations, a model that
accounts for both the exploration/exploitation dilemma and relationships
between arms. In this setting, after pulling an arm i, the decision maker also
observes the rewards for some other actions related to i. We will see that this
model is suited to content recommendation in social networks, where users'
reactions may be endorsed or not by their friends. We provide efficient
algorithms based on upper confidence bounds (UCBs) to leverage this additional
information and derive new bounds improving on standard regret guarantees. We
also evaluate these policies in the context of movie recommendation in social
networks: experiments on real datasets show substantial learning rate speedups
ranging from 2.2x to 14x on dense networks.
"
"  Herding and kernel herding are deterministic methods of choosing samples
which summarise a probability distribution. A related task is choosing samples
for estimating integrals using Bayesian quadrature. We show that the criterion
minimised when selecting samples in kernel herding is equivalent to the
posterior variance in Bayesian quadrature. We then show that sequential
Bayesian quadrature can be viewed as a weighted version of kernel herding which
achieves performance superior to any other weighted herding method. We
demonstrate empirically a rate of convergence faster than O(1/N). Our results
also imply an upper bound on the empirical error of the Bayesian quadrature
estimate.
"
"  Line transect sampling is a method used to estimate wildlife populations,
with the resulting data often grouped in intervals. Estimating the density from
grouped data can be challenging. In this paper we propose a kernel density
estimator of wildlife population density for such grouped data. Our method uses
a combined cross-validation and smoothed bootstrap approach to select the
optimal bandwidth for grouped data. Our simulation study shows that with the
smoothing parameter selected with this method, the estimated density from
grouped data matches the true density more closely than with other approaches.
Using smoothed bootstrap, we also construct bias-adjusted confidence intervals
for the value of the density at the boundary. We apply the proposed method to
two grouped data sets, one from a wooden stake study where the true density is
known, and the other from a survey of kangaroos in Australia.
"
"  Anti-retroviral drugs can reduce the infectiousness of people living with HIV
by about 96%--treatment as prevention or TasP--and can reduce the risk of being
infected by an HIV positive person by about 70%--pre-exposure prophylaxis or
PrEP--raising the prospect of using anti-retroviral drugs to stop the epidemic
of HIV. The question as to which is more effective, more affordable and more
cost effective, and under what conditions, continues to be debated in the
scientific literature. Here we compare TasP and PreP in order to determine the
conditions under which each strategy is favourable. This analysis suggests that
where the incidence of HIV is less than 5% or the risk-reduction under PrEP is
less than 50%, TasP is favoured over PrEP; otherwise PrEP is favoured over
TasP. The potential for using PreP should therefore be restricted to those
among whom the annual incidence of HIV is greater than 5% and TasP reduces
transmission by more than 50%. PreP should be considered for commercial sex
workers, young women aged about 20 to 25 years, men-who-have-sex with men, or
intravenous drug users, but only where the incidence of HIV is high.
"
"  In many applications, input data are sampled functions taking their values in
infinite dimensional spaces rather than standard vectors. This fact has complex
consequences on data analysis algorithms that motivate modifications of them.
In fact most of the traditional data analysis tools for regression,
classification and clustering have been adapted to functional inputs under the
general name of functional Data Analysis (FDA). In this paper, we investigate
the use of Support Vector Machines (SVMs) for functional data analysis and we
focus on the problem of curves discrimination. SVMs are large margin classifier
tools based on implicit non linear mappings of the considered data into high
dimensional spaces thanks to kernels. We show how to define simple kernels that
take into account the unctional nature of the data and lead to consistent
classification. Experiments conducted on real world data emphasize the benefit
of taking into account some functional aspects of the problems.
"
"  Low-rank structure have been profoundly studied in data mining and machine
learning. In this paper, we show a dense matrix $X$'s low-rank approximation
can be rapidly built from its left and right random projections $Y_1=XA_1$ and
$Y_2=X^TA_2$, or bilateral random projection (BRP). We then show power scheme
can further improve the precision. The deterministic, average and deviation
bounds of the proposed method and its power scheme modification are proved
theoretically. The effectiveness and the efficiency of BRP based low-rank
approximation is empirically verified on both artificial and real datasets.
"
"  We present a hybrid algorithm for optimizing a convex, smooth function over
the cone of positive semidefinite matrices. Our algorithm converges to the
global optimal solution and can be used to solve general large-scale
semidefinite programs and hence can be readily applied to a variety of machine
learning problems. We show experimental results on three machine learning
problems (matrix completion, metric learning, and sparse PCA) . Our approach
outperforms state-of-the-art algorithms.
"
"  Graph clustering involves the task of dividing nodes into clusters, so that
the edge density is higher within clusters as opposed to across clusters. A
natural, classic and popular statistical setting for evaluating solutions to
this problem is the stochastic block model, also referred to as the planted
partition model.
  In this paper we present a new algorithm--a convexified version of Maximum
Likelihood--for graph clustering. We show that, in the classic stochastic block
model setting, it outperforms existing methods by polynomial factors when the
cluster size is allowed to have general scalings. In fact, it is within
logarithmic factors of known lower bounds for spectral methods, and there is
evidence suggesting that no polynomial time algorithm would do significantly
better.
  We then show that this guarantee carries over to a more general extension of
the stochastic block model. Our method can handle the settings of semi-random
graphs, heterogeneous degree distributions, unequal cluster sizes, unaffiliated
nodes, partially observed graphs and planted clique/coloring etc. In
particular, our results provide the best exact recovery guarantees to date for
the planted partition, planted k-disjoint-cliques and planted noisy coloring
models with general cluster sizes; in other settings, we match the best
existing results up to logarithmic factors.
"
"  This paper proposes an empirical test of financial contagion in European
equity markets during the tumultuous period of 2008-2011. Our analysis shows
that traditional GARCH and Gaussian stochastic-volatility models are unable to
explain two key stylized features of global markets during presumptive
contagion periods: shocks to aggregate market volatility can be sudden and
explosive, and they are associated with specific directional biases in the
cross-section of country-level returns. Our model repairs this deficit by
assuming that the random shocks to volatility are heavy-tailed and correlated
cross-sectionally, both with each other and with returns. The fundamental
conclusion of our analysis is that great care is needed in modeling volatility
if one wishes to characterize the relationship between volatility and contagion
that is predicted by economic theory.
  In analyzing daily data, we find evidence for significant contagion effects
during the major EU crisis periods of May 2010 and August 2011, where contagion
is defined as excess correlation in the residuals from a factor model
incorporating global and regional market risk factors. Some of this excess
correlation can be explained by quantifying the impact of shocks to aggregate
volatility in the cross-section of expected returns - but only, it turns out,
if one is extremely careful in accounting for the explosive nature of these
shocks. We show that global markets have time-varying cross-sectional
sensitivities to these shocks, and that high sensitivities strongly predict
periods of financial crisis. Moreover, the pattern of temporal changes in
correlation structure between volatility and returns is readily interpretable
in terms of the major events of the periods in question.
"
"  Continuous state spaces and stochastic, switching dynamics characterize a
number of rich, realworld domains, such as robot navigation across varying
terrain. We describe a reinforcementlearning algorithm for learning in these
domains and prove for certain environments the algorithm is probably
approximately correct with a sample complexity that scales polynomially with
the state-space dimension. Unfortunately, no optimal planning techniques exist
in general for such problems; instead we use fitted value iteration to solve
the learned MDP, and include the error due to approximate planning in our
bounds. Finally, we report an experiment using a robotic car driving over
varying terrain to demonstrate that these dynamics representations adequately
capture real-world dynamics and that our algorithm can be used to efficiently
solve such problems.
"
"  A fundamental problem in control is to learn a model of a system from
observations that is useful for controller synthesis. To provide good
performance guarantees, existing methods must assume that the real system is in
the class of models considered during learning. We present an iterative method
with strong guarantees even in the agnostic case where the system is not in the
class. In particular, we show that any no-regret online learning algorithm can
be used to obtain a near-optimal policy, provided some model achieves low
training error and access to a good exploration distribution. Our approach
applies to both discrete and continuous domains. We demonstrate its efficacy
and scalability on a challenging helicopter domain from the literature.
"
"  The sum-product or belief propagation (BP) algorithm is a widely-used
message-passing algorithm for computing marginal distributions in graphical
models with discrete variables. At the core of the BP message updates, when
applied to a graphical model with pairwise interactions, lies a matrix-vector
product with complexity that is quadratic in the state dimension $d$, and
requires transmission of a $(d-1)$-dimensional vector of real numbers
(messages) to its neighbors. Since various applications involve very large
state dimensions, such computation and communication complexities can be
prohibitively complex. In this paper, we propose a low-complexity variant of
BP, referred to as stochastic belief propagation (SBP). As suggested by the
name, it is an adaptively randomized version of the BP message updates in which
each node passes randomly chosen information to each of its neighbors. The SBP
message updates reduce the computational complexity (per iteration) from
quadratic to linear in $d$, without assuming any particular structure of the
potentials, and also reduce the communication complexity significantly,
requiring only $\log{d}$ bits transmission per edge. Moreover, we establish a
number of theoretical guarantees for the performance of SBP, showing that it
converges almost surely to the BP fixed point for any tree-structured graph,
and for graphs with cycles satisfying a contractivity condition. In addition,
for these graphical models, we provide non-asymptotic upper bounds on the
convergence rate, showing that the $\ell_{\infty}$ norm of the error vector
decays no slower than $O(1/\sqrt{t})$ with the number of iterations $t$ on
trees and the mean square error decays as $O(1/t)$ for general graphs. These
analysis show that SBP can provably yield reductions in computational and
communication complexities for various classes of graphical models.
"
"  Distributions over exchangeable matrices with infinitely many columns, such
as the Indian buffet process, are useful in constructing nonparametric latent
variable models. However, the distribution implied by such models over the
number of features exhibited by each data point may be poorly- suited for many
modeling tasks. In this paper, we propose a class of exchangeable nonparametric
priors obtained by restricting the domain of existing models. Such models allow
us to specify the distribution over the number of features per data point, and
can achieve better performance on data sets where the number of features is not
well-modeled by the original distribution.
"
"  We consider the problem of testing the significance of features in
high-dimensional settings. In particular, we test for differentially-expressed
genes in a microarray experiment. We wish to identify genes that are associated
with some type of outcome, such as survival time or cancer type. We propose a
new procedure, called Lassoed Principal Components (LPC), that builds upon
existing methods and can provide a sizable improvement. For instance, in the
case of two-class data, a standard (albeit simple) approach might be to compute
a two-sample $t$-statistic for each gene. The LPC method involves projecting
these conventional gene scores onto the eigenvectors of the gene expression
data covariance matrix and then applying an $L_1$ penalty in order to de-noise
the resulting projections. We present a theoretical framework under which LPC
is the logical choice for identifying significant genes, and we show that LPC
can provide a marked reduction in false discovery rates over the conventional
methods on both real and simulated data. Moreover, this flexible procedure can
be applied to a variety of types of data and can be used to improve many
existing methods for the identification of significant features.
"
"  In this correspondence, we obtain exact recovery conditions for regularized
modified basis pursuit (reg-mod-BP) and discuss when the obtained conditions
are weaker than those for modified-CS or for basis pursuit (BP). The discussion
is also supported by simulation comparisons. Reg-mod-BP provides a solution to
the sparse recovery problem when both an erroneous estimate of the signal's
support, denoted by $T$, and an erroneous estimate of the signal values on $T$
are available.
"
"  We introduce supervised latent Dirichlet allocation (sLDA), a statistical
model of labelled documents. The model accommodates a variety of response
types. We derive an approximate maximum-likelihood procedure for parameter
estimation, which relies on variational methods to handle intractable posterior
expectations. Prediction problems motivate this research: we use the fitted
model to predict response values for new documents. We test sLDA on two
real-world problems: movie ratings predicted from reviews, and the political
tone of amendments in the U.S. Senate based on the amendment text. We
illustrate the benefits of sLDA versus modern regularized regression, as well
as versus an unsupervised LDA analysis followed by a separate regression.
"
"  We describe regularized methods for image reconstruction and focus on the
question of hyperparameter and instrument parameter estimation, i.e.
unsupervised and myopic problems. We developed a Bayesian framework that is
based on the \post density for all unknown quantities, given the observations.
This density is explored by a Markov Chain Monte-Carlo sampling technique based
on a Gibbs loop and including a Metropolis-Hastings step. The numerical
evaluation relies on the SPIRE instrument of the Herschel observatory. Using
simulated and real observations, we show that the hyperparameters and
instrument parameters are correctly estimated, which opens up many perspectives
for imaging in astrophysics.
"
"  Exponential random graph models (ERGMs), also known as p* models, have been
utilized extensively in the social science literature to study complex networks
and how their global structure depends on underlying structural components.
However, the literature on their use in biological networks (especially brain
networks) has remained sparse. Descriptive models based on a specific feature
of the graph (clustering coefficient, degree distribution, etc.) have dominated
connectivity research in neuroscience. Corresponding generative models have
been developed to reproduce one of these features. However, the complexity
inherent in whole-brain network data necessitates the development and use of
tools that allow the systematic exploration of several features simultaneously
and how they interact to form the global network architecture. ERGMs provide a
statistically principled approach to the assessment of how a set of interacting
local brain network features gives rise to the global structure. We illustrate
the utility of ERGMs for modeling, analyzing, and simulating complex
whole-brain networks with network data from normal subjects. We also provide a
foundation for the selection of important local features through the
implementation and assessment of three selection approaches: a traditional
p-value based backward selection approach, an information criterion approach
(AIC), and a graphical goodness of fit (GOF) approach. The graphical GOF
approach serves as the best method given the scientific interest in being able
to capture and reproduce the structure of fitted brain networks.
"
"  We consider the problem of detecting and estimating the strength of
association between a trait of interest and alleles or haplotypes in a small
genomic region (e.g. a gene or a gene complex), when no direct information on
that region is available but the values of neighbouring DNA-markers are at
hand. We argue that the effects of the non-observable haplotypes of the genomic
regions can and should be represented by factors representing disjoint groups
of marker-alleles. A theoretical argument based on a hypothetical phylogenetic
tree supports this general claim.
  The techniques described allow to identify and to infer the number of
detectable haplotypes in the genomic region that are associated with a trait.
The methods proposed use an exhaustive combinatorial search coupled with the
maximization of a version of the likelihood function penalized for the number
of parameters. This procedure can easily be implemented with standard
statistical methods for a moderate number of marker-alleles.
"
"  We establish some uniform limit results in the setting of additive regression
model estimation. Our results allow to give an asymptotic 100% confidence bands
for these components. These results are stated in the framework of i.i.d random
vectors when the marginal integration estimation method is used.
"
"  Patterns of isolation-by-distance arise when population differentiation
increases with increasing geographic distances. Patterns of
isolation-by-distance are usually caused by local spatial dispersal, which
explains why differences of allele frequencies between populations accumulate
with distance. However, spatial variations of demographic parameters such as
migration rate or population density can generate non-stationary patterns of
isolation-by-distance where the rate at which genetic differentiation
accumulates varies across space. To characterize non-stationary patterns of
isolation-by-distance, we infer local genetic differentiation based on Bayesian
kriging. Local genetic differentiation for a sampled population is defined as
the average genetic differentiation between the sampled population and fictive
neighboring populations. To avoid defining populations in advance, the method
can also be applied at the scale of individuals making it relevant for
landscape genetics. Inference of local genetic differentiation relies on a
matrix of pairwise similarity or dissimilarity between populations or
individuals such as matrices of FST between pairs of populations. Simulation
studies show that maps of local genetic differentiation can reveal barriers to
gene flow but also other patterns such as continuous variations of gene flow
across habitat. The potential of the method is illustrated with 2 data sets:
genome-wide SNP data for human Swedish populations and AFLP markers for alpine
plant species. The software LocalDiff implementing the method is available at
http://membres-timc.imag.fr/Michael.Blum/LocalDiff.html
"
"  We have developed an efficient algorithm for the maximum likelihood joint
tracking and association problem in a strong clutter for GMTI data. By using an
iterative procedure of the dynamic logic process ""from vague-to-crisp,"" the new
tracker overcomes combinatorial complexity of tracking in highly-cluttered
scenarios and results in a significant improvement in signal-to-clutter ratio.
"
"  A number of problems in statistical physics and computer science can be
expressed as the computation of marginal probabilities over a Markov random
field. Belief propagation, an iterative message-passing algorithm, computes
exactly such marginals when the underlying graph is a tree. But it has gained
its popularity as an efficient way to approximate them in the more general
case, even if it can exhibits multiple fixed points and is not guaranteed to
converge. In this paper, we express a new sufficient condition for local
stability of a belief propagation fixed point in terms of the graph structure
and the beliefs values at the fixed point. This gives credence to the usual
understanding that Belief Propagation performs better on sparse graphs.
"
"  Sparsity in the eigenvectors of signal covariance matrices is exploited in
this paper for compression and denoising. Dimensionality reduction (DR) and
quantization modules present in many practical compression schemes such as
transform codecs, are designed to capitalize on this form of sparsity and
achieve improved reconstruction performance compared to existing
sparsity-agnostic codecs. Using training data that may be noisy a novel
sparsity-aware linear DR scheme is developed to fully exploit sparsity in the
covariance eigenvectors and form noise-resilient estimates of the principal
covariance eigenbasis. Sparsity is effected via norm-one regularization, and
the associated minimization problems are solved using computationally efficient
coordinate descent iterations. The resulting eigenspace estimator is shown
capable of identifying a subset of the unknown support of the eigenspace basis
vectors even when the observation noise covariance matrix is unknown, as long
as the noise power is sufficiently low. It is proved that the sparsity-aware
estimator is asymptotically normal, and the probability to correctly identify
the signal subspace basis support approaches one, as the number of training
data grows large. Simulations using synthetic data and images, corroborate that
the proposed algorithms achieve improved reconstruction quality relative to
alternatives.
"
"  Large-scale statistical analysis of data sets associated with genome
sequences plays an important role in modern biology. A key component of such
statistical analyses is the computation of $p$-values and confidence bounds for
statistics defined on the genome. Currently such computation is commonly
achieved through ad hoc simulation measures. The method of randomization, which
is at the heart of these simulation procedures, can significantly affect the
resulting statistical conclusions. Most simulation schemes introduce a variety
of hidden assumptions regarding the nature of the randomness in the data,
resulting in a failure to capture biologically meaningful relationships. To
address the need for a method of assessing the significance of observations
within large scale genomic studies, where there often exists a complex
dependency structure between observations, we propose a unified solution built
upon a data subsampling approach. We propose a piecewise stationary model for
genome sequences and show that the subsampling approach gives correct answers
under this model. We illustrate the method on three simulation studies and two
real data examples.
"
"  Sphere packings are essential to the development of physical models for
powders, composite materials, and the atomic structure of the liquid state.
There is a strong scientific need to be able to assess the fit of packing
models to data, but this is complicated by the lack of formal probabilistic
models for packings. Without formal models, simulation algorithms and
collections of physical objects must be used as models. Identification of
common aspects of different realizations of the same packing process requires
the use of new descriptive statistics, many of which have yet to be developed.
Model assessment will require the use of large samples of independent and
identically distributed realizations, rather than the large single stationary
realizations found in conventional spatial statistics. The development of
procedures for model assessment will resemble the development of thermodynamic
models, and will be based on much exploration and experimentation rather than
on extensions of established statistical methods.
"
"  Many models for sparse regression typically assume that the covariates are
known completely, and without noise. Particularly in high-dimensional
applications, this is often not the case. This paper develops efficient
OMP-like algorithms to deal with precisely this setting. Our algorithms are as
efficient as OMP, and improve on the best-known results for missing and noisy
data in regression, both in the high-dimensional setting where we seek to
recover a sparse vector from only a few measurements, and in the classical
low-dimensional setting where we recover an unstructured regressor. In the
high-dimensional setting, our support-recovery algorithm requires no knowledge
of even the statistics of the noise. Along the way, we also obtain improved
performance guarantees for OMP for the standard sparse regression problem with
Gaussian noise.
"
"  We present an efficient, principled, and interpretable technique for
inferring module assignments and for identifying the optimal number of modules
in a given network. We show how several existing methods for finding modules
can be described as variant, special, or limiting cases of our work, and how
the method overcomes the resolution limit problem, accurately recovering the
true number of modules. Our approach is based on Bayesian methods for model
selection which have been used with success for almost a century, implemented
using a variational technique developed only in the past decade. We apply the
technique to synthetic and real networks and outline how the method naturally
allows selection among competing models.
"
"  Manifold matching works to identify embeddings of multiple disparate data
spaces into the same low-dimensional space, where joint inference can be
pursued. It is an enabling methodology for fusion and inference from multiple
and massive disparate data sources. In this paper we focus on a method called
Canonical Correlation Analysis (CCA) and its generalization Generalized
Canonical Correlation Analysis (GCCA), which belong to the more general Reduced
Rank Regression (RRR) framework. We present an efficiency investigation of CCA
and GCCA under different training conditions for a particular text document
classification task.
"
"  This work presents GROUSE (Grassmanian Rank-One Update Subspace Estimation),
an efficient online algorithm for tracking subspaces from highly incomplete
observations. GROUSE requires only basic linear algebraic manipulations at each
iteration, and each subspace update can be performed in linear time in the
dimension of the subspace. The algorithm is derived by analyzing incremental
gradient descent on the Grassmannian manifold of subspaces. With a slight
modification, GROUSE can also be used as an online incremental algorithm for
the matrix completion problem of imputing missing entries of a low-rank matrix.
GROUSE performs exceptionally well in practice both in tracking subspaces and
as an online algorithm for matrix completion.
"
"  This paper provides a generic framework of component analysis (CA) methods
introducing a new expression for scatter matrices and Gram matrices, called
Generalized Pairwise Expression (GPE). This expression is quite compact but
highly powerful: The framework includes not only (1) the standard CA methods
but also (2) several regularization techniques, (3) weighted extensions, (4)
some clustering methods, and (5) their semi-supervised extensions. This paper
also presents quite a simple methodology for designing a desired CA method from
the proposed framework: Adopting the known GPEs as templates, and generating a
new method by combining these templates appropriately.
"
"  We introduce a new wavelet transform suitable for analyzing functions on
point clouds and graphs. Our construction is based on a generalization of the
average interpolating refinement scheme of Donoho. The most important
ingredient of the original scheme that needs to be altered is the choice of the
interpolant. Here, we define the interpolant as the minimizer of a smoothness
functional, namely a generalization of the Laplacian energy, subject to the
averaging constraints. In the continuous setting, we derive a formula for the
optimal solution in terms of the poly-harmonic Green's function. The form of
this solution is used to motivate our construction in the setting of graphs and
point clouds. We highlight the empirical convergence of our refinement scheme
and the potential applications of the resulting wavelet transform through
experiments on a number of data stets.
"
"  In classical analysis of variance, dispersion is measured by considering
squared distances of sample elements from the sample mean. We consider a
measure of dispersion for univariate or multivariate response based on all
pairwise distances between-sample elements, and derive an analogous distance
components (DISCO) decomposition for powers of distance in $(0,2]$. The ANOVA F
statistic is obtained when the index (exponent) is 2. For each index in
$(0,2)$, this decomposition determines a nonparametric test for the
multi-sample hypothesis of equal distributions that is statistically consistent
against general alternatives.
"
"  Mixture models are a fundamental tool in applied statistics and machine
learning for treating data taken from multiple subpopulations. The current
practice for estimating the parameters of such models relies on local search
heuristics (e.g., the EM algorithm) which are prone to failure, and existing
consistent methods are unfavorable due to their high computational and sample
complexity which typically scale exponentially with the number of mixture
components. This work develops an efficient method of moments approach to
parameter estimation for a broad class of high-dimensional mixture models with
many components, including multi-view mixtures of Gaussians (such as mixtures
of axis-aligned Gaussians) and hidden Markov models. The new method leads to
rigorous unsupervised learning results for mixture models that were not
achieved by previous works; and, because of its simplicity, it offers a viable
alternative to EM for practical deployment.
"
"  We give a risk-averse solution to the problem of estimating the reliability
of a parallel-series system. We adopt a beta-binomial model for components
reliabilities, and assume that the total sample size for the experience is
fixed. The allocation at subsystems or components level may be random. Based on
the sampling schemes for parallel and series systems separately, we propose a
hybrid sequential scheme for the parallel-series system. Asymptotic optimality
of the Bayes risk associated with quadratic loss is proved with the help of
martingale convergence properties.
"
"  Given a set of experiments in which varying subsets of observed variables are
subject to intervention, we consider the problem of identifiability of causal
models exhibiting latent confounding. While identifiability is trivial when
each experiment intervenes on a large number of variables, the situation is
more complicated when only one or a few variables are subject to intervention
per experiment. For linear causal models with latent variables Hyttinen et al.
(2010) gave precise conditions for when such data are sufficient to identify
the full model. While their result cannot be extended to discrete-valued
variables with arbitrary cause-effect relationships, we show that a similar
result can be obtained for the class of causal models whose conditional
probability distributions are restricted to a `noisy-OR' parameterization. We
further show that identification is preserved under an extension of the model
that allows for negative influences, and present learning algorithms that we
test for accuracy, scalability and robustness.
"
"  In this paper, we address the problem of embedded feature selection for
ranking on top of the list problems. We pose this problem as a regularized
empirical risk minimization with $p$-norm push loss function ($p=\infty$) and
sparsity inducing regularizers. We leverage the issues related to this
challenging optimization problem by considering an alternating direction method
of multipliers algorithm which is built upon proximal operators of the loss
function and the regularizer. Our main technical contribution is thus to
provide a numerical scheme for computing the infinite push loss function
proximal operator. Experimental results on toy, DNA microarray and BCI problems
show how our novel algorithm compares favorably to competitors for ranking on
top while using fewer variables in the scoring function.
"
"  In this paper, two-state Markov switching models are proposed to study
accident frequencies. These models assume that there are two unobserved states
of roadway safety, and that roadway entities (roadway segments) can switch
between these states over time. The states are distinct, in the sense that in
the different states accident frequencies are generated by separate counting
processes (by separate Poisson or negative binomial processes). To demonstrate
the applicability of the approach presented herein, two-state Markov switching
negative binomial models are estimated using five-year accident frequencies on
Indiana interstate highway segments. Bayesian inference methods and Markov
Chain Monte Carlo (MCMC) simulations are used for model estimation. The
estimated Markov switching models result in a superior statistical fit relative
to the standard (single-state) negative binomial model. It is found that the
more frequent state is safer and it is correlated with better weather
conditions. The less frequent state is found to be less safe and to be
correlated with adverse weather conditions.
"
"  Least Angle Regression is a promising technique for variable selection
applications, offering a nice alternative to stepwise regression. It provides
an explanation for the similar behavior of LASSO ($\ell_1$-penalized
regression) and forward stagewise regression, and provides a fast
implementation of both. The idea has caught on rapidly, and sparked a great
deal of research interest. In this paper, we give an overview of Least Angle
Regression and the current state of related research.
"
"  We analyze a power distribution line with high penetration of distributed
generation and strong variations of power consumption and generation levels. In
the presence of uncertainty the statistical description of the system is
required to assess the risks of power outages. In order to find the probability
of exceeding the constraints for voltage levels we introduce the probability
distribution of maximal voltage drop and propose an algorithm for finding this
distribution. The algorithm is based on the assumption of random but
statistically independent distribution of loads on buses. Linear complexity in
the number of buses is achieved through the dynamic programming technique. We
illustrate the performance of the algorithm by analyzing a simple 4-bus system
with high variations of load levels.
"
"  Estimating the level set of a signal from measurements is a task that arises
in a variety of fields, including medical imaging, astronomy, and digital
elevation mapping. Motivated by scenarios where accurate and complete
measurements of the signal may not available, we examine here a simple
procedure for estimating the level set of a signal from highly incomplete
measurements, which may additionally be corrupted by additive noise. The
proposed procedure is based on box-constrained Total Variation (TV)
regularization. We demonstrate the performance of our approach, relative to
existing state-of-the-art techniques for level set estimation from compressive
measurements, via several simulation examples.
"
"  Finding the most probable assignment (MAP) in a general graphical model is
known to be NP hard but good approximations have been attained with max-product
belief propagation (BP) and its variants. In particular, it is known that using
BP on a single-cycle graph or tree reweighted BP on an arbitrary graph will
give the MAP solution if the beliefs have no ties. In this paper we extend the
setting under which BP can be used to provably extract the MAP. We define
Convex BP as BP algorithms based on a convex free energy approximation and show
that this class includes ordinary BP with single-cycle, tree reweighted BP and
many other BP variants. We show that when there are no ties, fixed-points of
convex max-product BP will provably give the MAP solution. We also show that
convex sum-product BP at sufficiently small temperatures can be used to solve
linear programs that arise from relaxing the MAP problem. Finally, we derive a
novel condition that allows us to derive the MAP solution even if some of the
convex BP beliefs have ties. In experiments, we show that our theorems allow us
to find the MAP in many real-world instances of graphical models where exact
inference using junction-tree is impossible.
"
"  In this paper, we study the classical problem of estimating the proportion of
a finite population. First, we consider a fixed sample size method and derive
an explicit sample size formula which ensures a mixed criterion of absolute and
relative errors. Second, we consider an inverse sampling scheme such that the
sampling is continue until the number of units having a certain attribute
reaches a threshold value or the whole population is examined. We have
established a simple method to determine the threshold so that a prescribed
relative precision is guaranteed. Finally, we develop a multistage sampling
scheme for constructing fixed-width confidence interval for the proportion of a
finite population. Powerful computational techniques are introduced to make it
possible that the fixed-width confidence interval ensures prescribed level of
coverage probability.
"
"  In probabilistic approaches to classification and information extraction, one
typically builds a statistical model of words under the assumption that future
data will exhibit the same regularities as the training data. In many data
sets, however, there are scope-limited features whose predictive power is only
applicable to a certain subset of the data. For example, in information
extraction from web pages, word formatting may be indicative of extraction
category in different ways on different web pages. The difficulty with using
such features is capturing and exploiting the new regularities encountered in
previously unseen data. In this paper, we propose a hierarchical probabilistic
model that uses both local/scope-limited features, such as word formatting, and
global features, such as word content. The local regularities are modeled as an
unobserved random parameter which is drawn once for each local data set. This
random parameter is estimated during the inference process and then used to
perform classification with both the local and global features--- a procedure
which is akin to automatically retuning the classifier to the local
regularities on each newly encountered web page. Exact inference is intractable
and we present approximations via point estimates and variational methods.
Empirical results on large collections of web data demonstrate that this method
significantly improves performance from traditional models of global features
alone.
"
"  We examine the use of a structured thresholding algorithm for sparse
underwater channel estimation using compressed sensing. This method shows some
improvements over standard algorithms for sparse channel estimation such as
matching pursuit, iterative detection and least squares.
"
"  We present two methods for detecting patterns and clusters in high
dimensional time-dependent functional data. Our methods are based on
wavelet-based similarity measures, since wavelets are well suited for
identifying highly discriminant local time and scale features. The
multiresolution aspect of the wavelet transform provides a time-scale
decomposition of the signals allowing to visualize and to cluster the
functional data into homogeneous groups. For each input function, through its
empirical orthogonal wavelet transform the first method uses the distribution
of energy across scales generate a handy number of features that can be
sufficient to still make the signals well distinguishable. Our new similarity
measure combined with an efficient feature selection technique in the wavelet
domain is then used within more or less classical clustering algorithms to
effectively differentiate among high dimensional populations. The second method
uses dissimilarity measures between the whole time-scale representations and
are based on wavelet-coherence tools. The clustering is then performed using a
k-centroid algorithm starting from these dissimilarities. Practical performance
of these methods that jointly designs both the feature selection in the wavelet
domain and the classification distance is demonstrated through simulations as
well as daily profiles of the French electricity power demand.
"
"  This article describes a fast iterative algorithm for image denoising and
deconvolution with signal-dependent observation noise. We use an optimization
strategy based on variable splitting that adapts traditional Gaussian
noise-based restoration algorithms to account for the observed image being
corrupted by mixed Poisson-Gaussian noise and quantization errors.
"
"  We use the method of Maximum (relative) Entropy to process information in the
form of observed data and moment constraints. The generic ""canonical"" form of
the posterior distribution for the problem of simultaneous updating with data
and moments is obtained. We discuss the general problem of non-commuting
constraints, when they should be processed sequentially and when
simultaneously. As an illustration, the multinomial example of die tosses is
solved in detail for two superficially similar but actually very different
problems.
"
"  In this paper, we consider the classic measurement error regression scenario
in which our independent, or design, variables are observed with several
sources of additive noise. We will show that our motivating example's
replicated measurements on both the design and dependent variables may be
leveraged to enhance a sparse regression algorithm. Specifically, we estimate
the variance and use it to scale our design variables. We demonstrate the
efficacy of scaling from several points of view and validate it empirically
with a biomass characterization data set using two of the most widely used
sparse algorithms: least angle regression (LARS) and the Dantzig selector (DS).
"
"  Accurate goodness-of-fit tests for the extreme tails of empirical
distributions is a very important issue, relevant in many contexts, including
geophysics, insurance, and finance. We have derived exact asymptotic results
for a generalization of the large-sample Kolmogorov-Smirnov test, well suited
to testing these extreme tails. In passing, we have rederived and made more
precise the approximate limit solutions found originally in unrelated fields,
first in [L. Turban, J. Phys. A 25, 127 (1992)] and later in [P. L. Krapivsky
and S. Redner, Am. J. Phys. 64, 546 (1996)].
"
"  A mixture of shifted asymmetric Laplace distributions is introduced and used
for clustering and classification. A variant of the EM algorithm is developed
for parameter estimation by exploiting the relationship with the general
inverse Gaussian distribution. This approach is mathematically elegant and
relatively computationally straightforward. Our novel mixture modelling
approach is demonstrated on both simulated and real data to illustrate
clustering and classification applications. In these analyses, our mixture of
shifted asymmetric Laplace distributions performs favourably when compared to
the popular Gaussian approach. This work, which marks an important step in the
non-Gaussian model-based clustering and classification direction, concludes
with discussion as well as suggestions for future work.
"
"  We introduce a trade strategy representation theorem for performance
measurement and portable alpha in high frequency trading, by embedding a robust
trading algorithm that describe portfolio manager market timing behavior, in a
canonical multifactor asset pricing model. First, we present a spectral test
for market timing based on behavioral transformation of the hedge factors
design matrix. Second, we find that the typical trade strategy process is a
local martingale with a background driving Brownian bridge that mimics
portfolio manager price reversal strategies. Third, we show that equilibrium
asset pricing models like the CAPM exists on a set with P-measure zero. So that
excess returns, i.e. positive alpha, relative to a benchmark index is robust to
no arbitrage pricing in turbulent capital markets. Fourth, the path properties
of alpha are such that it is positive between suitably chosen stopping times
for trading. Fifth, we demonstrate how, and why, econometric tests of portfolio
performance tend to under report positive alpha.
"
"  Respondent-driven sampling is a widely-used network sampling technique,
designed to sample from hard-to-reach populations. Estimation from the
resulting samples is an area of active research, with software available to
compute at least four estimators of a population proportion. Each estimator is
claimed to address deficiencies in previous estimators, however those claims
are often unsubstantiated. In this study we provide a simulation-based
comparison of five existing estimators, focussing on sampling conditions which
a recent estimator is designed to address. We find no estimator consistently
out-performs all others, and highlight sampling conditions in which each is to
be preferred.
"
"  In modeling multivariate time series, it is important to allow time-varying
smoothness in the mean and covariance process. In particular, there may be
certain time intervals exhibiting rapid changes and others in which changes are
slow. If such time-varying smoothness is not accounted for, one can obtain
misleading inferences and predictions, with over-smoothing across erratic time
intervals and under-smoothing across times exhibiting slow variation. This can
lead to mis-calibration of predictive intervals, which can be substantially too
narrow or wide depending on the time. We propose a locally adaptive factor
process for characterizing multivariate mean-covariance changes in continuous
time, allowing locally varying smoothness in both the mean and covariance
matrix. This process is constructed utilizing latent dictionary functions
evolving in time through nested Gaussian processes and linearly related to the
observed data with a sparse mapping. Using a differential equation
representation, we bypass usual computational bottlenecks in obtaining MCMC and
online algorithms for approximate Bayesian inference. The performance is
assessed in simulations and illustrated in a financial application.
"
"  In many signal detection and classification problems, we have knowledge of
the distribution under each hypothesis, but not the prior probabilities. This
paper is aimed at providing theory to quantify the performance of detection via
estimating prior probabilities from either labeled or unlabeled training data.
The error or {\em risk} is considered as a function of the prior probabilities.
We show that the risk function is locally Lipschitz in the vicinity of the true
prior probabilities, and the error of detectors based on estimated prior
probabilities depends on the behavior of the risk function in this locality. In
general, we show that the error of detectors based on the Maximum Likelihood
Estimate (MLE) of the prior probabilities converges to the Bayes error at a
rate of $n^{-1/2}$, where $n$ is the number of training data. If the behavior
of the risk function is more favorable, then detectors based on the MLE have
errors converging to the corresponding Bayes errors at optimal rates of the
form $n^{-(1+\alpha)/2}$, where $\alpha>0$ is a parameter governing the
behavior of the risk function with a typical value $\alpha = 1$. The limit
$\alpha \rightarrow \infty$ corresponds to a situation where the risk function
is flat near the true probabilities, and thus insensitive to small errors in
the MLE; in this case the error of the detector based on the MLE converges to
the Bayes error exponentially fast with $n$. We show the bounds are achievable
no matter given labeled or unlabeled training data and are minimax-optimal in
labeled case.
"
"  Method for detection and visualization of trends, periodicities, local
peculiarities in measurement series (dL-method) based on DFA technology
(Detrended fluctuation analysis) is proposed. The essence of the method lies in
reflecting the values of absolute deviation of measurement accumulation series
points from the respective values of linear approximation. It is shown that
dL-method in some cases allows better determination of local peculiarities than
wavelet-analysis. Easy-to-realize approach that is proposed can be used in the
analysis of time series in such fields as economics and sociology.
"
"  To investigate interactions between parasite species in a host, a population
of field voles was studied longitudinally, with presence or absence of six
different parasites measured repeatedly. Although trapping sessions were
regular, a different set of voles was caught at each session leading to
incomplete profiles for all subjects. We use a discrete-time hidden Markov
model for each disease with transition probabilities dependent on covariates
via a set of logistic regressions. For each disease the hidden states for each
of the other diseases at a given time point form part of the covariate set for
the Markov transition probabilities from that time point. This allows us to
gauge the influence of each parasite species on the transition probabilities
for each of the other parasite species. Inference is performed via a Gibbs
sampler, which cycles through each of the diseases, first using an adaptive
Metropolis-Hastings step to sample from the conditional posterior of the
covariate parameters for that particular disease given the hidden states for
all other diseases and then sampling from the hidden states for that disease
given the parameters. We find evidence for interactions between several pairs
of parasites and of an acquired immune response for two of the parasites.
"
"  We present a procedure for effective estimation of entropy and mutual
information from small-sample data, and apply it to the problem of inferring
high-dimensional gene association networks. Specifically, we develop a
James-Stein-type shrinkage estimator, resulting in a procedure that is highly
efficient statistically as well as computationally. Despite its simplicity, we
show that it outperforms eight other entropy estimation procedures across a
diverse range of sampling scenarios and data-generating models, even in cases
of severe undersampling. We illustrate the approach by analyzing E. coli gene
expression data and computing an entropy-based gene-association network from
gene expression data. A computer program is available that implements the
proposed shrinkage estimator.
"
"  Stochastic gradient descent is a simple approach to find the local minima of
a cost function whose evaluations are corrupted by noise. In this paper, we
develop a procedure extending stochastic gradient descent algorithms to the
case where the function is defined on a Riemannian manifold. We prove that, as
in the Euclidian case, the gradient descent algorithm converges to a critical
point of the cost function. The algorithm has numerous potential applications,
and is illustrated here by four examples. In particular a novel gossip
algorithm on the set of covariance matrices is derived and tested numerically.
"
"  Nonparametric Bayesian models are often based on the assumption that the
objects being modeled are exchangeable. While appropriate in some applications
(e.g., bag-of-words models for documents), exchangeability is sometimes assumed
simply for computational reasons; non-exchangeable models might be a better
choice for applications based on subject matter. Drawing on ideas from
graphical models and phylogenetics, we describe a non-exchangeable prior for a
class of nonparametric latent feature models that is nearly as efficient
computationally as its exchangeable counterpart. Our model is applicable to the
general setting in which the dependencies between objects can be expressed
using a tree, where edge lengths indicate the strength of relationships. We
demonstrate an application to modeling probabilistic choice.
"
"  This paper presents a forecasting model designed using WSNs (Wireless Sensor
Networks) to predict flood in rivers using simple and fast calculations to
provide real-time results and save the lives of people who may be affected by
the flood. Our prediction model uses multiple variable robust linear regression
which is easy to understand and simple and cost effective in implementation, is
speed efficient, but has low resource utilization and yet provides real time
predictions with reliable accuracy, thus having features which are desirable in
any real world algorithm. Our prediction model is independent of the number of
parameters, i.e. any number of parameters may be added or removed based on the
on-site requirements. When the water level rises, we represent it using a
polynomial whose nature is used to determine if the water level may exceed the
flood line in the near future. We compare our work with a contemporary
algorithm to demonstrate our improvements over it. Then we present our
simulation results for the predicted water level compared to the actual water
level.
"
"  Automatically classifying the tissues types of Region of Interest (ROI) in
medical imaging has been an important application in Computer-Aided Diagnosis
(CAD), such as classification of breast parenchymal tissue in the mammogram,
classify lung disease patterns in High-Resolution Computed Tomography (HRCT)
etc. Recently, bag-of-features method has shown its power in this field,
treating each ROI as a set of local features. In this paper, we investigate
using the bag-of-features strategy to classify the tissue types in medical
imaging applications. Two important issues are considered here: the visual
vocabulary learning and weighting. Although there are already plenty of
algorithms to deal with them, all of them treat them independently, namely, the
vocabulary learned first and then the histogram weighted. Inspired by
Auto-Context who learns the features and classifier jointly, we try to develop
a novel algorithm that learns the vocabulary and weights jointly. The new
algorithm, called Joint-ViVo, works in an iterative way. In each iteration, we
first learn the weights for each visual word by maximizing the margin of ROI
triplets, and then select the most discriminate visual words based on the
learned weights for the next iteration. We test our algorithm on three tissue
classification tasks: identifying brain tissue type in magnetic resonance
imaging (MRI), classifying lung tissue in HRCT images, and classifying breast
tissue density in mammograms. The results show that Joint-ViVo can perform
effectively for classifying tissues.
"
"  We show that when $\set{X_j}$ is a sequence of independent (but not
necessarily identically distributed) random variables which satisfies a
condition similar to the Lindeberg condition, the properly normalized geometric
sum $\sum_{j=1}^{\nu_p}X_j$ (where $\nu_p$ is a geometric random variable with
mean $1/p$) converges in distribution to a Laplace distribution as $p\to 0$.
The same conclusion holds for the multivariate case. This theorem provides a
reason for the ubiquity of the double power law in economic and financial data.
"
"  We face network data from various sources, such as protein interactions and
online social networks. A critical problem is to model network interactions and
identify latent groups of network nodes. This problem is challenging due to
many reasons. For example, the network nodes are interdependent instead of
independent of each other, and the data are known to be very noisy (e.g.,
missing edges). To address these challenges, we propose a new relational model
for network data, Sparse Matrix-variate Gaussian process Blockmodel (SMGB). Our
model generalizes popular bilinear generative models and captures nonlinear
network interactions using a matrix-variate Gaussian process with latent
membership variables. We also assign sparse prior distributions on the latent
membership variables to learn sparse group assignments for individual network
nodes. To estimate the latent variables efficiently from data, we develop an
efficient variational expectation maximization method. We compared our
approaches with several state-of-the-art network models on both synthetic and
real-world network datasets. Experimental results demonstrate SMGBs outperform
the alternative approaches in terms of discovering latent classes or predicting
unknown interactions.
"
"  Spatial concurrent linear models, in which the model coefficients are spatial
processes varying at a local level, are flexible and useful tools for analyzing
spatial data. One approach places stationary Gaussian process priors on the
spatial processes, but in applications the data may display strong
nonstationary patterns. In this article, we propose a Bayesian variable
selection approach based on wavelet tools to address this problem. The proposed
approach does not involve any stationarity assumptions on the priors, and
instead we impose a mixture prior directly on each wavelet coefficient. We
introduce an option to control the priors such that high resolution
coefficients are more likely to be zero. Computationally efficient MCMC
procedures are provided to address posterior sampling, and uncertainty in the
estimation is assessed through posterior means and standard deviations.
Examples based on simulated data demonstrate the estimation accuracy and
advantages of the proposed method. We also illustrate the performance of the
proposed method for real data obtained through remote sensing.
"
"  Learning linear combinations of multiple kernels is an appealing strategy
when the right choice of features is unknown. Previous approaches to multiple
kernel learning (MKL) promote sparse kernel combinations to support
interpretability and scalability. Unfortunately, this 1-norm MKL is rarely
observed to outperform trivial baselines in practical applications. To allow
for robust kernel mixtures, we generalize MKL to arbitrary norms. We devise new
insights on the connection between several existing MKL formulations and
develop two efficient interleaved optimization strategies for arbitrary norms,
like p-norms with p>1. Empirically, we demonstrate that the interleaved
optimization strategies are much faster compared to the commonly used wrapper
approaches. A theoretical analysis and an experiment on controlled artificial
data experiment sheds light on the appropriateness of sparse, non-sparse and
$\ell_\infty$-norm MKL in various scenarios. Empirical applications of p-norm
MKL to three real-world problems from computational biology show that
non-sparse MKL achieves accuracies that go beyond the state-of-the-art.
"
"  Compressive sensing (CS) is a technique for estimating a sparse signal from
the random measurements and the measurement matrix. Traditional sparse signal
recovery methods have seriously degeneration with the measurement matrix
uncertainty (MMU). Here the MMU is modeled as a bounded additive error. An
anti-uncertainty constraint in the form of a mixed L2 and L1 norm is deduced
from the sparse signal model with MMU. Then we combine the sparse constraint
with the anti-uncertainty constraint to get an anti-uncertainty sparse signal
recovery operator. Numerical simulations demonstrate that the proposed operator
has a better reconstructing performance with the MMU than traditional methods.
"
"  We consider Gaussian ensembles of m N x N complex matrices. We identify an
enhanced symmetry in the system and the resultant closed subsector, which is
naturally associated with the radial sector of the theory. The density of
radial eigenvalues is obtained in the large N limit. It is of the Wigner form
only for m=1. For m \ge 2, the new form of the density is obtained.
"
"  Cryo-electron microscopy (cryo-EM) studies using single particle
reconstruction are extensively used to reveal structural information on
macromolecular complexes. Aiming at the highest achievable resolution, state of
the art electron microscopes automatically acquire thousands of high-quality
micrographs. Particles are detected on and boxed out from each micrograph using
fully- or semi-automated approaches. However, the obtained particles still
require laborious manual post-picking classification, which is one major
bottleneck for single particle analysis of large datasets. We introduce MAPPOS,
a supervised post-picking strategy for the classification of boxed particle
images, as additional strategy adding to the already efficient automated
particle picking routines. MAPPOS employs machine learning techniques to train
a robust classifier from a small number of characteristic image features. In
order to accurately quantify the performance of MAPPOS we used simulated
particle and non-particle images. In addition, we verified our method by
applying it to an experimental cryo-EM dataset and comparing the results to the
manual classification of the same dataset. Comparisons between MAPPOS and
manual post-picking classification by several human experts demonstrated that
merely a few hundred sample images are sufficient for MAPPOS to classify an
entire dataset with a human-like performance. MAPPOS was shown to greatly
accelerate the throughput of large datasets by reducing the manual workload by
orders of magnitude while maintaining a reliable identification of non-particle
images.
"
"  The purpose of this paper is to provide a discussion, with illustrating
examples, on Bayesian forecasting for dynamic generalized linear models
(DGLMs). Adopting approximate Bayesian analysis, based on conjugate forms and
on Bayes linear estimation, we describe the theoretical framework and then we
provide detailed examples of response distributions, including binomial,
Poisson, negative binomial, geometric, normal, log-normal, gamma, exponential,
Weibull, Pareto, beta, and inverse Gaussian. We give numerical illustrations
for all distributions (except for the normal). Putting together all the above
distributions, we give a unified Bayesian approach to non-Gaussian time series
analysis, with applications from finance and medicine to biology and the
behavioural sciences. Throughout the models we discuss Bayesian forecasting
and, for each model, we derive the multi-step forecast mean. Finally, we
describe model assessment using the likelihood function, and Bayesian model
monitoring.
"
"  Evaluation of a variable Yd from certain measured variable(s) Xi(s), by
making use of their system-specific-relationship (SSR), is generally referred
as the indirect measurement. Naturally the SSR may stand for a simple
data-translation process in a given case, but a set of equations, or even a
cascade of different such processes, in some other case. Further, though the
measurements are a priori ensured to be accurate, there is no definite method
for examining whether the result obtained at the end of an SSR, specifically a
cascade of SSRs, is really representative as the measured Xi-values.
  Of Course, it was recently shown that the uncertainty (ed) in the estimate
(yd) of a specified Yd is given by a specified linear combination of
corresponding measurement-uncertainties (uis). Here, further insight into this
principle is provided by its application to the cases represented by
cascade-SSRs. It is exemplified how the different stage-wise uncertainties
(Ied, IIed, ... ed), that is to say the requirements for the evaluation to be
successful, could even a priori be predicted. The theoretical tools (SSRs) have
resemblance with the real world measuring devices (MDs), and hence are referred
as also the data transformation scales (DTSs). However, non-uniform behavior
appears to be the feature of the DTSs rather than of the MDs.
"
"  Cardinality potentials are a generally useful class of high order potential
that affect probabilities based on how many of D binary variables are active.
Maximum a posteriori (MAP) inference for cardinality potential models is
well-understood, with efficient computations taking O(DlogD) time. Yet
efficient marginalization and sampling have not been addressed as thoroughly in
the machine learning community. We show that there exists a simple algorithm
for computing marginal probabilities and drawing exact joint samples that runs
in O(Dlog2 D) time, and we show how to frame the algorithm as efficient belief
propagation in a low order tree-structured model that includes additional
auxiliary variables. We then develop a new, more general class of models,
termed Recursive Cardinality models, which take advantage of this efficiency.
Finally, we show how to do efficient exact inference in models composed of a
tree structure and a cardinality potential. We explore the expressive power of
Recursive Cardinality models and empirically demonstrate their utility.
"
"  In many signal processing problems, it may be fruitful to represent the
signal under study in a frame. If a probabilistic approach is adopted, it
becomes then necessary to estimate the hyper-parameters characterizing the
probability distribution of the frame coefficients. This problem is difficult
since in general the frame synthesis operator is not bijective. Consequently,
the frame coefficients are not directly observable. This paper introduces a
hierarchical Bayesian model for frame representation. The posterior
distribution of the frame coefficients and model hyper-parameters is derived.
Hybrid Markov Chain Monte Carlo algorithms are subsequently proposed to sample
from this posterior distribution. The generated samples are then exploited to
estimate the hyper-parameters and the frame coefficients of the target signal.
Validation experiments show that the proposed algorithms provide an accurate
estimation of the frame coefficients and hyper-parameters. Application to
practical problems of image denoising show the impact of the resulting Bayesian
estimation on the recovered signal quality.
"
"  We consider the inverse reinforcement learning problem, that is, the problem
of learning from, and then predicting or mimicking a controller based on
state/action data. We propose a statistical model for such data, derived from
the structure of a Markov decision process. Adopting a Bayesian approach to
inference, we show how latent variables of the model can be estimated, and how
predictions about actions can be made, in a unified framework. A new Markov
chain Monte Carlo (MCMC) sampler is devised for simulation from the posterior
distribution. This step includes a parameter expansion step, which is shown to
be essential for good convergence properties of the MCMC sampler. As an
illustration, the method is applied to learning a human controller.
"
"  A novel multi-resolution cluster detection (MCD) method is proposed to
identify irregularly shaped clusters in space. Multi-scale test statistic on a
single cell is derived based on likelihood ratio statistic for Bernoulli
sequence, Poisson sequence and Normal sequence. A neighborhood variability
measure is defined to select the optimal test threshold. The MCD method is
compared with single scale testing methods controlling for false discovery rate
and the spatial scan statistics using simulation and f-MRI data. The MCD method
is shown to be more effective for discovering irregularly shaped clusters, and
the implementation of this method does not require heavy computation, making it
suitable for cluster detection for large spatial data.
"
"  Food authenticity studies are concerned with determining if food samples have
been correctly labeled or not. Discriminant analysis methods are an integral
part of the methodology for food authentication. Motivated by food authenticity
applications, a model-based discriminant analysis method that includes variable
selection is presented. The discriminant analysis model is fitted in a
semi-supervised manner using both labeled and unlabeled data. The method is
shown to give excellent classification performance on several high-dimensional
multiclass food authenticity data sets with more variables than observations.
The variables selected by the proposed method provide information about which
variables are meaningful for classification purposes. A headlong search
strategy for variable selection is shown to be efficient in terms of
computation and achieves excellent classification performance. In applications
to several food authenticity data sets, our proposed method outperformed
default implementations of Random Forests, AdaBoost, transductive SVMs and
Bayesian Multinomial Regression by substantial margins.
"
"  Most of the available methods for longitudinal data analysis are designed and
validated for the situation where the number of subjects is large and the
number of observations per subject is relatively small. Motivated by the
Naturalistic Teenage Driving Study (NTDS), which represents the exact opposite
situation, we examine standard and propose new methodology for marginal
analysis of longitudinal count data in a small number of very long sequences.
We consider standard methods based on generalized estimating equations, under
working independence or an appropriate correlation structure, and find them
unsatisfactory for dealing with time-dependent covariates when the counts are
low. For this situation, we explore a within-cluster resampling (WCR) approach
that involves repeated analyses of random subsamples with a final analysis that
synthesizes results across subsamples. This leads to a novel WCR method which
operates on separated blocks within subjects and which performs better than all
of the previously considered methods. The methods are applied to the NTDS data
and evaluated in simulation experiments mimicking the NTDS.
"
"  Determinantal point processes (DPPs), which arise in random matrix theory and
quantum physics, are natural models for subset selection problems where
diversity is preferred. Among many remarkable properties, DPPs offer tractable
algorithms for exact inference, including computing marginal probabilities and
sampling; however, an important open question has been how to learn a DPP from
labeled training data. In this paper we propose a natural feature-based
parameterization of conditional DPPs, and show how it leads to a convex and
efficient learning formulation. We analyze the relationship between our model
and binary Markov random fields with repulsive potentials, which are
qualitatively similar but computationally intractable. Finally, we apply our
approach to the task of extractive summarization, where the goal is to choose a
small subset of sentences conveying the most important information from a set
of documents. In this task there is a fundamental tradeoff between sentences
that are highly relevant to the collection as a whole, and sentences that are
diverse and not repetitive. Our parameterization allows us to naturally balance
these two characteristics. We evaluate our system on data from the DUC 2003/04
multi-document summarization task, achieving state-of-the-art results.
"
"  We propose a flexible change-point model for inhomogeneous Poisson Processes,
which arise naturally from next-generation DNA sequencing, and derive score and
generalized likelihood statistics for shifts in intensity functions. We
construct a modified Bayesian information criterion (mBIC) to guide model
selection, and point-wise approximate Bayesian confidence intervals for
assessing the confidence in the segmentation. The model is applied to DNA Copy
Number profiling with sequencing data and evaluated on simulated spike-in and
real data sets.
"
"  In a typical online learning scenario, a learner is required to process a
large data stream using a small memory buffer. Such a requirement is usually in
conflict with a learner's primary pursuit of prediction accuracy. To address
this dilemma, we introduce a novel Bayesian online classi cation algorithm,
called the Virtual Vector Machine. The virtual vector machine allows you to
smoothly trade-off prediction accuracy with memory size. The virtual vector
machine summarizes the information contained in the preceding data stream by a
Gaussian distribution over the classi cation weights plus a constant number of
virtual data points. The virtual data points are designed to add extra
non-Gaussian information about the classi cation weights. To maintain the
constant number of virtual points, the virtual vector machine adds the current
real data point into the virtual point set, merges two most similar virtual
points into a new virtual point or deletes a virtual point that is far from the
decision boundary. The information lost in this process is absorbed into the
Gaussian distribution. The extra information provided by the virtual points
leads to improved predictive accuracy over previous online classification
algorithms.
"
"  This article describes posterior maximization for topic models, identifying
computational and conceptual gains from inference under a non-standard
parametrization. We then show that fitted parameters can be used as the basis
for a novel approach to marginal likelihood estimation, via block-diagonal
approximation to the information matrix,that facilitates choosing the number of
latent topics. This likelihood-based model selection is complemented with a
goodness-of-fit analysis built around estimated residual dispersion. Examples
are provided to illustrate model selection as well as to compare our estimation
against standard alternative techniques.
"
"  Discussion of ""A statistical analysis of multiple temperature proxies: Are
reconstructions of surface temperatures over the last 1000 years reliable?"" by
B.B. McShane and A.J. Wyner [arXiv:1104.4002]
"
"  We introduce a novel and efficient sampling algorithm for the Multiplicative
Attribute Graph Model (MAGM - Kim and Leskovec (2010)}). Our algorithm is
\emph{strictly} more efficient than the algorithm proposed by Yun and
Vishwanathan (2012), in the sense that our method extends the \emph{best} time
complexity guarantee of their algorithm to a larger fraction of parameter
space. Both in theory and in empirical evaluation on sparse graphs, our new
algorithm outperforms the previous one. To design our algorithm, we first
define a stochastic \emph{ball-dropping process} (BDP). Although a special case
of this process was introduced as an efficient approximate sampling algorithm
for the Kronecker Product Graph Model (KPGM - Leskovec et al. (2010)}), neither
\emph{why} such an approximation works nor \emph{what} is the actual
distribution this process is sampling from has been addressed so far to the
best of our knowledge. Our rigorous treatment of the BDP enables us to clarify
the rational behind a BDP approximation of KPGM, and design an efficient
sampling algorithm for the MAGM.
"
"  In literature there are several studies on the performance of Bayesian
network structure learning algorithms. The focus of these studies is almost
always the heuristics the learning algorithms are based on, i.e. the
maximisation algorithms (in score-based algorithms) or the techniques for
learning the dependencies of each variable (in constraint-based algorithms). In
this paper we investigate how the use of permutation tests instead of
parametric ones affects the performance of Bayesian network structure learning
from discrete data. Shrinkage tests are also covered to provide a broad
overview of the techniques developed in current literature.
"
"  Submodular functions have many applications. Matchings have many
applications. The bitext word alignment problem can be modeled as the problem
of maximizing a nonnegative, monotone, submodular function constrained to
matchings in a complete bipartite graph where each vertex corresponds to a word
in the two input sentences and each edge represents a potential word-to-word
translation. We propose a more general problem of maximizing a nonnegative,
monotone, submodular function defined on the edge set of a complete graph
constrained to matchings; we call this problem the CSM-Matching problem.
CSM-Matching also generalizes the maximum-weight matching problem, which has a
polynomial-time algorithm; however, we show that it is NP-hard to approximate
CSM-Matching within a factor of e/(e-1) by reducing the max k-cover problem to
it. Our main result is a simple, greedy, 3-approximation algorithm for
CSM-Matching. Then we reduce CSM-Matching to maximizing a nonnegative,
monotone, submodular function over two matroids, i.e., CSM-2-Matroids.
CSM-2-Matroids has a (2+epsilon)-approximation algorithm - called LSV2. We show
that we can find a (4+epsilon)-approximate solution to CSM-Matching using LSV2.
We extend this approach to similar problems.
"
"  The Bradley-Terry model is a popular approach to describe probabilities of
the possible outcomes when elements of a set are repeatedly compared with one
another in pairs. It has found many applications including animal behaviour,
chess ranking and multiclass classification. Numerous extensions of the basic
model have also been proposed in the literature including models with ties,
multiple comparisons, group comparisons and random graphs. From a computational
point of view, Hunter (2004) has proposed efficient iterative MM
(minorization-maximization) algorithms to perform maximum likelihood estimation
for these generalized Bradley-Terry models whereas Bayesian inference is
typically performed using MCMC (Markov chain Monte Carlo) algorithms based on
tailored Metropolis-Hastings (M-H) proposals. We show here that these MM\
algorithms can be reinterpreted as special instances of
Expectation-Maximization (EM) algorithms associated to suitable sets of latent
variables and propose some original extensions. These latent variables allow us
to derive simple Gibbs samplers for Bayesian inference. We demonstrate
experimentally the efficiency of these algorithms on a variety of applications.
"
"  The detection of change-points in heterogeneous sequences is a statistical
challenge with applications across a wide variety of fields. In bioinformatics,
a vast amount of methodology exists to identify an ideal set of change-points
for detecting Copy Number Variation (CNV). While considerable efficient
algorithms are currently available for finding the best segmentation of the
data in CNV, relatively few approaches consider the important problem of
assessing the uncertainty of the change-point location. Asymptotic and
stochastic approaches exist but often require additional model assumptions to
speed up the computations, while exact methods have quadratic complexity which
usually are intractable for large datasets of tens of thousands points or more.
In this paper, we suggest an exact method for obtaining the posterior
distribution of change-points with linear complexity, based on a constrained
hidden Markov model. The methods are implemented in the R package postCP, which
uses the results of a given change-point detection algorithm to estimate the
probability that each observation is a change-point. We present the results of
the package on a publicly available CNV data set (n=120). Due to its
frequentist framework, postCP obtains less conservative confidence intervals
than previously published Bayesian methods, but with linear complexity instead
of quadratic. Simulations showed that postCP provided comparable loss to a
Bayesian MCMC method when estimating posterior means, specifically when
assessing larger-scale changes, while being more computationally efficient. On
another high-resolution CNV data set (n=14,241), the implementation processed
information in less than one second on a mid-range laptop computer.
"
"  While considerable advance has been made to account for statistical
uncertainties in astronomical analyses, systematic instrumental uncertainties
have been generally ignored. This can be crucial to a proper interpretation of
analysis results because instrumental calibration uncertainty is a form of
systematic uncertainty. Ignoring it can underestimate error bars and introduce
bias into the fitted values of model parameters. Accounting for such
uncertainties currently requires extensive case-specific simulations if using
existing analysis packages. Here we present general statistical methods that
incorporate calibration uncertainties into spectral analysis of high-energy
data. We first present a method based on multiple imputation that can be
applied with any fitting method, but is necessarily approximate. We then
describe a more exact Bayesian approach that works in conjunction with a Markov
chain Monte Carlo based fitting. We explore methods for improving computational
efficiency, and in particular detail a method of summarizing calibration
uncertainties with a principal component analysis of samples of plausible
calibration files. This method is implemented using recently codified Chandra
effective area uncertainties for low-resolution spectral analysis and is
verified using both simulated and actual Chandra data. Our procedure for
incorporating effective area uncertainty is easily generalized to other types
of calibration uncertainties.
"
"  Slovenia's Current Research Information System (SICRIS) currently hosts
86,443 publications with citation data from 8,359 researchers working on the
whole plethora of social and natural sciences from 1970 till present. Using
these data, we show that the citation distributions derived from individual
publications have Zipfian properties in that they can be fitted by a power law
$P(x) \sim x^{-\alpha}$, with $\alpha$ between 2.4 and 3.1 depending on the
institution and field of research. Distributions of indexes that quantify the
success of researchers rather than individual publications, on the other hand,
cannot be associated with a power law. We find that for Egghe's g-index and
Hirsch's h-index the log-normal form $P(x) \sim \exp[-a\ln x -b(\ln x)^2]$
applies best, with $a$ and $b$ depending moderately on the underlying set of
researchers. In special cases, particularly for institutions with a strongly
hierarchical constitution and research fields with high self-citation rates,
exponential distributions can be observed as well. Both indexes yield
distributions with equivalent statistical properties, which is a strong
indicator for their consistency and logical connectedness. At the same time,
differences in the assessment of citation histories of individual researchers
strengthen their importance for properly evaluating the quality and impact of
scientific output.
"
"  We consider integrative modeling of multiple gene networks and diverse
genomic data, including protein-DNA binding, gene expression and DNA sequence
data, to accurately identify the regulatory target genes of a transcription
factor (TF). Rather than treating all the genes equally and independently a
priori in existing joint modeling approaches, we incorporate the biological
prior knowledge that neighboring genes on a gene network tend to be (or not to
be) regulated together by a TF. A key contribution of our work is that, to
maximize the use of all existing biological knowledge, we allow incorporation
of multiple gene networks into joint modeling of genomic data by introducing a
mixture model based on the use of multiple Markov random fields (MRFs). Another
important contribution of our work is to allow different genomic data to be
correlated and to examine the validity and effect of the independence
assumption as adopted in existing methods. Due to a fully Bayesian approach,
inference about model parameters can be carried out based on MCMC samples.
Application to an E. coli data set, together with simulation studies,
demonstrates the utility and statistical efficiency gains with the proposed
joint model.
"
"  In intractable, undirected graphical models, an intuitive way of creating
structured mean field approximations is to select an acyclic tractable
subgraph. We show that the hardness of computing the objective function and
gradient of the mean field objective qualitatively depends on a simple graph
property. If the tractable subgraph has this property- we call such subgraphs
v-acyclic-a very fast block coordinate ascent algorithm is possible. If not,
optimization is harder, but we show a new algorithm based on the construction
of an auxiliary exponential family that can be used to make inference possible
in this case as well. We discuss the advantages and disadvantages of each
regime and compare the algorithms empirically.
"
"  We propose a method for nonparametric density estimation that exhibits
robustness to contamination of the training sample. This method achieves
robustness by combining a traditional kernel density estimator (KDE) with ideas
from classical $M$-estimation. We interpret the KDE based on a radial, positive
semi-definite kernel as a sample mean in the associated reproducing kernel
Hilbert space. Since the sample mean is sensitive to outliers, we estimate it
robustly via $M$-estimation, yielding a robust kernel density estimator (RKDE).
  An RKDE can be computed efficiently via a kernelized iteratively re-weighted
least squares (IRWLS) algorithm. Necessary and sufficient conditions are given
for kernelized IRWLS to converge to the global minimizer of the $M$-estimator
objective function. The robustness of the RKDE is demonstrated with a
representer theorem, the influence function, and experimental results for
density estimation and anomaly detection.
"
"  Motivated by vision tasks such as robust face and object recognition, we
consider the following general problem: given a collection of low-dimensional
linear subspaces in a high-dimensional ambient (image) space and a query point
(image), efficiently determine the nearest subspace to the query in $\ell^1$
distance. We show in theory that Cauchy random embedding of the objects into
significantly-lower-dimensional spaces helps preserve the identity of the
nearest subspace with constant probability. This offers the possibility of
efficiently selecting several candidates for accurate search. We sketch
preliminary experiments on robust face and digit recognition to corroborate our
theory.
"
"  In the industries that involved either chemistry or biology, such as
pharmaceutical industries, chemical industries or food industry, the analytical
methods are the necessary eyes and hear of all the material produced or used.
If the quality of an analytical method is doubtful, then the whole set of
decision that will be based on those measures is questionable. For those
reasons, being able to assess the quality of an analytical method is far more
than a statistical challenge; it's a matter of ethic and good business
practices. Many regulatory documents have been releases, primarily ICH and FDA
documents in the pharmaceutical industry (FDA, 1995, 1997, 2001) to address
that issue.
"
"  When related learning tasks are naturally arranged in a hierarchy, an
appealing approach for coping with scarcity of instances is that of transfer
learning using a hierarchical Bayes framework. As fully Bayesian computations
can be difficult and computationally demanding, it is often desirable to use
posterior point estimates that facilitate (relatively) efficient prediction.
However, the hierarchical Bayes framework does not always lend itself naturally
to this maximum aposteriori goal. In this work we propose an undirected
reformulation of hierarchical Bayes that relies on priors in the form of
similarity measures. We introduce the notion of ""degree of transfer"" weights on
components of these similarity measures, and show how they can be automatically
learned within a joint probabilistic framework. Importantly, our reformulation
results in a convex objective for many learning problems, thus facilitating
optimal posterior point estimation using standard optimization techniques. In
addition, we no longer require proper priors, allowing for flexible and
straightforward specification of joint distributions over transfer hierarchies.
We show that our framework is effective for learning models that are part of
transfer hierarchies for two real-life tasks: object shape modeling using
Gaussian density estimation and document classification.
"
"  Educational research often studies subjects that are in naturally clustered
groups of classrooms or schools. When designing a randomized experiment to
evaluate an intervention directed at teachers, but with effects on teachers and
their students, the power or anticipated variance for the treatment effect
needs to be examined at both levels. If the treatment is applied to clusters,
power is usually reduced. At the same time, a cluster design decreases the
probability of contamination, and contamination can also reduce power to detect
a treatment effect. Designs that are optimal at one level may be inefficient
for estimating the treatment effect at another level. In this paper we study
the efficiency of three designs and their ability to detect a treatment effect:
randomize schools to treatment, randomize teachers within schools to treatment,
and completely randomize teachers to treatment. The three designs are compared
for both the teacher and student level within the mixed model framework, and a
simulation study is conducted to compare expected treatment variances for the
three designs with various levels of correlation within and between clusters.
We present a computer program that study designers can use to explore the
anticipated variances of treatment effects under proposed experimental designs
and settings.
"
"  A popular method for selecting the number of clusters is based on stability
arguments: one chooses the number of clusters such that the corresponding
clustering results are ""most stable"". In recent years, a series of papers has
analyzed the behavior of this method from a theoretical point of view. However,
the results are very technical and difficult to interpret for non-experts. In
this paper we give a high-level overview about the existing literature on
clustering stability. In addition to presenting the results in a slightly
informal but accessible way, we relate them to each other and discuss their
different implications.
"
"  Ebrahimi (1996) has shown that the measure of residual entropy characterizes
the distribution function uniquely. In this communication we study an analogous
result for past entropy.
"
"  We provide a unifying framework linking two classes of statistics used in
two-sample and independence testing: on the one hand, the energy distances and
distance covariances from the statistics literature; on the other, distances
between embeddings of distributions to reproducing kernel Hilbert spaces
(RKHS), as established in machine learning. The equivalence holds when energy
distances are computed with semimetrics of negative type, in which case a
kernel may be defined such that the RKHS distance between distributions
corresponds exactly to the energy distance. We determine the class of
probability distributions for which kernels induced by semimetrics are
characteristic (that is, for which embeddings of the distributions to an RKHS
are injective). Finally, we investigate the performance of this family of
kernels in two-sample and independence tests: we show in particular that the
energy distance most commonly employed in statistics is just one member of a
parametric family of kernels, and that other choices from this family can yield
more powerful tests.
"
"  It is shown that bootstrap approximations of an estimator which is based on a
continuous operator from the set of Borel probability measures defined on a
compact metric space into a complete separable metric space is stable in the
sense of qualitative robustness. Support vector machines based on shifted loss
functions are treated as special cases.
"
"  Mean-field variational methods are widely used for approximate posterior
inference in many probabilistic models. In a typical application, mean-field
methods approximately compute the posterior with a coordinate-ascent
optimization algorithm. When the model is conditionally conjugate, the
coordinate updates are easily derived and in closed form. However, many models
of interest---like the correlated topic model and Bayesian logistic
regression---are nonconjuate. In these models, mean-field methods cannot be
directly applied and practitioners have had to develop variational algorithms
on a case-by-case basis. In this paper, we develop two generic methods for
nonconjugate models, Laplace variational inference and delta method variational
inference. Our methods have several advantages: they allow for easily derived
variational algorithms with a wide class of nonconjugate models; they extend
and unify some of the existing algorithms that have been derived for specific
models; and they work well on real-world datasets. We studied our methods on
the correlated topic model, Bayesian logistic regression, and hierarchical
Bayesian logistic regression.
"
"  In the field of structural reliability, the Monte-Carlo estimator is
considered as the reference probability estimator. However, it is still
untractable for real engineering cases since it requires a high number of runs
of the model. In order to reduce the number of computer experiments, many other
approaches known as reliability methods have been proposed. A certain approach
consists in replacing the original experiment by a surrogate which is much
faster to evaluate. Nevertheless, it is often difficult (or even impossible) to
quantify the error made by this substitution. In this paper an alternative
approach is developed. It takes advantage of the kriging meta-modeling and
importance sampling techniques. The proposed alternative estimator is finally
applied to a finite element based structural reliability analysis.
"
"  Our goal is to estimate causal interactions in multivariate time series.
Using vector autoregressive (VAR) models, these can be defined based on
non-vanishing coefficients belonging to respective time-lagged instances. As in
most cases a parsimonious causality structure is assumed, a promising approach
to causal discovery consists in fitting VAR models with an additional
sparsity-promoting regularization. Along this line we here propose that
sparsity should be enforced for the subgroups of coefficients that belong to
each pair of time series, as the absence of a causal relation requires the
coefficients for all time-lags to become jointly zero. Such behavior can be
achieved by means of l1-l2-norm regularized regression, for which an efficient
active set solver has been proposed recently. Our method is shown to outperform
standard methods in recovering simulated causality graphs. The results are on
par with a second novel approach which uses multiple statistical testing.
"
"  We consider a class of learning problems that involve a structured
sparsity-inducing norm defined as the sum of $\ell_\infty$-norms over groups of
variables. Whereas a lot of effort has been put in developing fast optimization
methods when the groups are disjoint or embedded in a specific hierarchical
structure, we address here the case of general overlapping groups. To this end,
we show that the corresponding optimization problem is related to network flow
optimization. More precisely, the proximal problem associated with the norm we
consider is dual to a quadratic min-cost flow problem. We propose an efficient
procedure which computes its solution exactly in polynomial time. Our algorithm
scales up to millions of variables, and opens up a whole new range of
applications for structured sparse models. We present several experiments on
image and video data, demonstrating the applicability and scalability of our
approach for various problems.
"
"  Motivated by the problem of identifying correlations between genes or
features of two related biological systems, we propose a model of \emph{feature
selection} in which only a subset of the predictors $X_t$ are dependent on the
multidimensional variate $Y$, and the remainder of the predictors constitute a
""noise set"" $X_u$ independent of $Y$. Using Monte Carlo simulations, we
investigated the relative performance of two methods: thresholding and
singular-value decomposition, in combination with stochastic optimization to
determine ""empirical bounds"" on the small-sample accuracy of an asymptotic
approximation. We demonstrate utility of the thresholding and SVD feature
selection methods to with respect to a recent infant intestinal gene expression
and metagenomics dataset.
"
"  Accurately assessing a patient's risk of a given event is essential in making
informed treatment decisions. One approach is to stratify patients into two or
more distinct risk groups with respect to a specific outcome using both
clinical and demographic variables. Outcomes may be categorical or continuous
in nature; important examples in cancer studies might include level of toxicity
or time to recurrence. Recursive partitioning methods are ideal for building
such risk groups. Two such methods are Classification and Regression Trees
(CART) and a more recent competitor known as the partitioning
Deletion/Substitution/Addition (partDSA) algorithm, both which also utilize
loss functions (e.g. squared error for a continuous outcome) as the basis for
building, selecting and assessing predictors but differ in the manner by which
regression trees are constructed.
  Recently, we have shown that partDSA often outperforms CART in so-called
""full data"" (e.g., uncensored) settings. However, when confronted with censored
outcome data, the loss functions used by both procedures must be modified.
There have been several attempts to adapt CART for right-censored data. This
article describes two such extensions for \emph{partDSA} that make use of
observed data (i.e. possibly censored) loss functions. These observed data loss
functions, constructed using inverse probability of censoring weights, are
consistent estimates of their uncensored counterparts provided that the
corresponding censoring model is correctly specified. The relative performance
of these new methods is evaluated via simulation studies and illustrated
through an analysis of clinical trial data on brain cancer patients.
"
"  The exploration/exploitation (E/E) dilemma arises naturally in many subfields
of Science. Multi-armed bandit problems formalize this dilemma in its canonical
form. Most current research in this field focuses on generic solutions that can
be applied to a wide range of problems. However, in practice, it is often the
case that a form of prior information is available about the specific class of
target problems. Prior knowledge is rarely used in current solutions due to the
lack of a systematic approach to incorporate it into the E/E strategy.
  To address a specific class of E/E problems, we propose to proceed in three
steps: (i) model prior knowledge in the form of a probability distribution over
the target class of E/E problems; (ii) choose a large hypothesis space of
candidate E/E strategies; and (iii), solve an optimization problem to find a
candidate E/E strategy of maximal average performance over a sample of problems
drawn from the prior distribution.
  We illustrate this meta-learning approach with two different hypothesis
spaces: one where E/E strategies are numerically parameterized and another
where E/E strategies are represented as small symbolic formulas. We propose
appropriate optimization algorithms for both cases. Our experiments, with
two-armed Bernoulli bandit problems and various playing budgets, show that the
meta-learnt E/E strategies outperform generic strategies of the literature
(UCB1, UCB1-Tuned, UCB-v, KL-UCB and epsilon greedy); they also evaluate the
robustness of the learnt E/E strategies, by tests carried out on arms whose
rewards follow a truncated Gaussian distribution.
"
"  Although many studies collect biomedical time series signals from multiple
subjects, there is a dearth of models and methods for assessing the association
between frequency domain properties of time series and other study outcomes.
This article introduces the random Cramer representation as a joint model for
collections of time series and static outcomes where power spectra are random
functions that are correlated with the outcomes. A canonical correlation
analysis between cepstral coefficients and static outcomes is developed to
provide a flexible yet interpretable measure of association. Estimates of the
canonical correlations and weight functions are obtained from a canonical
correlation analysis between the static outcomes and maximum Whittle likelihood
estimates of truncated cepstral coefficients. The proposed methodology is used
to analyze the association between the spectrum of heart rate variability and
measures of sleep duration and fragmentation in a study of older adults who
serve as the primary caregiver for their ill spouse.
"
"  We introduce a new spatial data structure for high dimensional data called
the \emph{approximate principal direction tree} (APD tree) that adapts to the
intrinsic dimension of the data. Our algorithm ensures vector-quantization
accuracy similar to that of computationally-expensive PCA trees with similar
time-complexity to that of lower-accuracy RP trees.
  APD trees use a small number of power-method iterations to find splitting
planes for recursively partitioning the data. As such they provide a natural
trade-off between the running-time and accuracy achieved by RP and PCA trees.
Our theoretical results establish a) strong performance guarantees regardless
of the convergence rate of the power-method and b) that $O(\log d)$ iterations
suffice to establish the guarantee of PCA trees when the intrinsic dimension is
$d$. We demonstrate this trade-off and the efficacy of our data structure on
both the CPU and GPU.
"
"  Given a set of aligned sequences of independent noisy observations, we are
concerned with detecting intervals where the mean values of the observations
change simultaneously in a subset of the sequences. The intervals of changed
means are typically short relative to the length of the sequences, the subset
where the change occurs, the ""carriers,"" can be relatively small, and the sizes
of the changes can vary from one sequence to another. This problem is motivated
by the scientific problem of detecting inherited copy number variants in
aligned DNA samples. We suggest a statistic based on the assumption that for
any given interval of changed means there is a given fraction of samples that
carry the change. We derive an analytic approximation for the false positive
error probability of a scan, which is shown by simulations to be reasonably
accurate. We show that the new method usually improves on methods that analyze
a single sample at a time and on our earlier multi-sample method, which is most
efficient when the carriers form a large fraction of the set of sequences. The
proposed procedure is also shown to be robust with respect to the assumed
fraction of carriers of the changes.
"
"  Previous work in hierarchical reinforcement learning has faced a dilemma:
either ignore the values of different possible exit states from a subroutine,
thereby risking suboptimal behavior, or represent those values explicitly
thereby incurring a possibly large representation cost because exit values
refer to nonlocal aspects of the world (i.e., all subsequent rewards). This
paper shows that, in many cases, one can avoid both of these problems. The
solution is based on recursively decomposing the exit value function in terms
of Q-functions at higher levels of the hierarchy. This leads to an intuitively
appealing runtime architecture in which a parent subroutine passes to its child
a value function on the exit states and the child reasons about how its choices
affect the exit value. We also identify structural conditions on the value
function and transition distributions that allow much more concise
representations of exit state distributions, leading to further state
abstraction. In essence, the only variables whose exit values need be
considered are those that the parent cares about and the child affects. We
demonstrate the utility of our algorithms on a series of increasingly complex
environments.
"
"  This paper develops a general theoretical framework to analyze structured
sparse recovery problems using the notation of dual certificate. Although
certain aspects of the dual certificate idea have already been used in some
previous work, due to the lack of a general and coherent theory, the analysis
has so far only been carried out in limited scopes for specific problems. In
this context the current paper makes two contributions. First, we introduce a
general definition of dual certificate, which we then use to develop a unified
theory of sparse recovery analysis for convex programming. Second, we present a
class of structured sparsity regularization called structured Lasso for which
calculations can be readily performed under our theoretical framework. This new
theory includes many seemingly loosely related previous work as special cases;
it also implies new results that improve existing ones even for standard
formulations such as L1 regularization.
"
"  Given a heterogeneous time-series sample, the objective is to find points in
time (called change points) where the probability distribution generating the
data has changed. The data are assumed to have been generated by arbitrary
unknown stationary ergodic distributions. No modelling, independence or mixing
assumptions are made. A novel, computationally efficient, nonparametric method
is proposed, and is shown to be asymptotically consistent in this general
framework. The theoretical results are complemented with experimental
evaluations.
"
"  We present a novel modulation level classification (MLC) method based on
probability distribution distance functions. The proposed method uses modified
Kuiper and Kolmogorov-Smirnov distances to achieve low computational complexity
and outperforms the state of the art methods based on cumulants and
goodness-of-fit tests. We derive the theoretical performance of the proposed
MLC method and verify it via simulations. The best classification accuracy,
under AWGN with SNR mismatch and phase jitter, is achieved with the proposed
MLC method using Kuiper distances.
"
"  We study minority games in efficient regime. By incorporating the utility
function and aggregating agents with similar strategies we develop an effective
mesoscale notion of state of the game. Using this approach, the game can be
represented as a Markov process with substantially reduced number of states
with explicitly computable probabilities. For any payoff, the finiteness of the
number of states is proved. Interesting features of an extensive random
variable, called aggregated demand, viz. its strong inhomogeneity and presence
of patterns in time, can be easily interpreted. Using Markov theory and
quenched disorder approach, we can explain important macroscopic
characteristics of the game: behavior of variance per capita and predictability
of the aggregated demand. We prove that in case of linear payoff many
attractors in the state space are possible.
"
"  In this paper we build on previous work which uses inferences techniques, in
particular Markov Chain Monte Carlo (MCMC) methods, to solve parameterized
control problems. We propose a number of modifications in order to make this
approach more practical in general, higher-dimensional spaces. We first
introduce a new target distribution which is able to incorporate more reward
information from sampled trajectories. We also show how to break strong
correlations between the policy parameters and sampled trajectories in order to
sample more freely. Finally, we show how to incorporate these techniques in a
principled manner to obtain estimates of the optimal policy.
"
"  We introduce a new method to price American-style options on underlying
investments governed by stochastic volatility (SV) models. The method does not
require the volatility process to be observed. Instead, it exploits the fact
that the optimal decision functions in the corresponding dynamic programming
problem can be expressed as functions of conditional distributions of
volatility, given observed data. By constructing statistics summarizing
information about these conditional distributions, one can obtain high quality
approximate solutions. Although the required conditional distributions are in
general intractable, they can be arbitrarily precisely approximated using
sequential Monte Carlo schemes. The drawback, as with many Monte Carlo schemes,
is potentially heavy computational demand. We present two variants of the
algorithm, one closely related to the well-known least-squares Monte Carlo
algorithm of Longstaff and Schwartz [The Review of Financial Studies 14 (2001)
113-147], and the other solving the same problem using a ""brute force"" gridding
approach. We estimate an illustrative SV model using Markov chain Monte Carlo
(MCMC) methods for three equities. We also demonstrate the use of our algorithm
by estimating the posterior distribution of the market price of volatility risk
for each of the three equities.
"
"  We introduce a factor analysis model that summarizes the dependencies between
observed variable groups, instead of dependencies between individual variables
as standard factor analysis does. A group may correspond to one view of the
same set of objects, one of many data sets tied by co-occurrence, or a set of
alternative variables collected from statistics tables to measure one property
of interest. We show that by assuming group-wise sparse factors, active in a
subset of the sets, the variation can be decomposed into factors explaining
relationships between the sets and factors explaining away set-specific
variation. We formulate the assumptions in a Bayesian model which provides the
factors, and apply the model to two data analysis tasks, in neuroimaging and
chemical systems biology.
"
"  We are concerned with the estimation of the exterior surface and interior
summaries of tube-shaped anatomical structures. This interest is motivated by
two distinct scientific goals, one dealing with the distribution of HIV
microbicide in the colon and the other with measuring degradation in
white-matter tracts in the brain. Our problem is posed as the estimation of the
support of a distribution in three dimensions from a sample from that
distribution, possibly measured with error. We propose a novel tube-fitting
algorithm to construct such estimators. Further, we conduct a simulation study
to aid in the choice of a key parameter of the algorithm, and we test our
algorithm with validation study tailored to the motivating data sets. Finally,
we apply the tube-fitting algorithm to a colon image produced by single photon
emission computed tomography (SPECT) and to a white-matter tract image produced
using diffusion tensor imaging (DTI).
"
"  High dimensional structured data such as text and images is often poorly
understood and misrepresented in statistical modeling. The standard histogram
representation suffers from high variance and performs poorly in general. We
explore novel connections between statistical translation, heat kernels on
manifolds and graphs, and expected distances. These connections provide a new
framework for unsupervised metric learning for text documents. Experiments
indicate that the resulting distances are generally superior to their more
standard counterparts.
"
"  When analysing gene expression time series data an often overlooked but
crucial aspect of the model is that the regulatory network structure may change
over time. Whilst some approaches have addressed this problem previously in the
literature, many are not well suited to the sequential nature of the data. Here
we present a method that allows us to infer regulatory network structures that
may vary between time points, utilising a set of hidden states that describe
the network structure at a given time point. To model the distribution of the
hidden states we have applied the Hierarchical Dirichlet Process Hideen Markov
Model, a nonparametric extension of the traditional Hidden Markov Model, that
does not require us to fix the number of hidden states in advance. We apply our
method to exisiting microarray expression data as well as demonstrating is
efficacy on simulated test data.
"
"  Auto-Associative models cover a large class of methods used in data analysis.
In this paper, we describe the generals properties of these models when the
projection component is linear and we propose and test an easy to implement
Probabilistic Semi-Linear Auto- Associative model in a Gaussian setting. We
show it is a generalization of the PCA model to the semi-linear case. Numerical
experiments on simulated datasets and a real astronomical application highlight
the interest of this approach
"
"  The use of L1 regularisation for sparse learning has generated immense
research interest, with successful application in such diverse areas as signal
acquisition, image coding, genomics and collaborative filtering. While existing
work highlights the many advantages of L1 methods, in this paper we find that
L1 regularisation often dramatically underperforms in terms of predictive
performance when compared with other methods for inferring sparsity. We focus
on unsupervised latent variable models, and develop L1 minimising factor
models, Bayesian variants of ""L1"", and Bayesian models with a stronger L0-like
sparsity induced through spike-and-slab distributions. These spike-and-slab
Bayesian factor models encourage sparsity while accounting for uncertainty in a
principled manner and avoiding unnecessary shrinkage of non-zero values. We
demonstrate on a number of data sets that in practice spike-and-slab Bayesian
methods outperform L1 minimisation, even on a computational budget. We thus
highlight the need to re-assess the wide use of L1 methods in sparsity-reliant
applications, particularly when we care about generalising to previously unseen
data, and provide an alternative that, over many varying conditions, provides
improved generalisation performance.
"
"  We study the problem of estimating from data, a sparse approximation to the
inverse covariance matrix. Estimating a sparsity constrained inverse covariance
matrix is a key component in Gaussian graphical model learning, but one that is
numerically very challenging. We address this challenge by developing a new
adaptive gradient-based method that carefully combines gradient information
with an adaptive step-scaling strategy, which results in a scalable, highly
competitive method. Our algorithm, like its predecessors, maximizes an
$\ell_1$-norm penalized log-likelihood and has the same per iteration
arithmetic complexity as the best methods in its class. Our experiments reveal
that our approach outperforms state-of-the-art competitors, often significantly
so, for large problems.
"
"  The asymptotic pseudo-trajectory approach to stochastic approximation of
Benaim, Hofbauer and Sorin is extended for asynchronous stochastic
approximations with a set-valued mean field. The asynchronicity of the process
is incorporated into the mean field to produce convergence results which remain
similar to those of an equivalent synchronous process. In addition, this allows
many of the restrictive assumptions previously associated with asynchronous
stochastic approximation to be removed. The framework is extended for a coupled
asynchronous stochastic approximation process with set-valued mean fields.
Two-timescales arguments are used here in a similar manner to the original work
in this area by Borkar. The applicability of this approach is demonstrated
through learning in a Markov decision process.
"
"  The Sleep Heart Health Study (SHHS) is a comprehensive landmark study of
sleep and its impacts on health outcomes. A primary metric of the SHHS is the
in-home polysomnogram, which includes two electroencephalographic (EEG)
channels for each subject, at two visits. The volume and importance of this
data presents enormous challenges for analysis. To address these challenges, we
introduce multilevel functional principal component analysis (MFPCA), a novel
statistical methodology designed to extract core intra- and inter-subject
geometric components of multilevel functional data. Though motivated by the
SHHS, the proposed methodology is generally applicable, with potential
relevance to many modern scientific studies of hierarchical or longitudinal
functional outcomes. Notably, using MFPCA, we identify and quantify
associations between EEG activity during sleep and adverse cardiovascular
outcomes.
"
"  In this paper we address the problem of modeling relational data, which
appear in many applications such as social network analysis, recommender
systems and bioinformatics. Previous studies either consider latent feature
based models but disregarding local structure in the network, or focus
exclusively on capturing local structure of objects based on latent blockmodels
without coupling with latent characteristics of objects. To combine the
benefits of the previous work, we propose a novel model that can simultaneously
incorporate the effect of latent features and covariates if any, as well as the
effect of latent structure that may exist in the data. To achieve this, we
model the relation graph as a function of both latent feature factors and
latent cluster memberships of objects to collectively discover globally
predictive intrinsic properties of objects and capture latent block structure
in the network to improve prediction performance. We also develop an
optimization transfer algorithm based on the generalized EM-style strategy to
learn the latent factors. We prove the efficacy of our proposed model through
the link prediction task and cluster analysis task, and extensive experiments
on the synthetic data and several real world datasets suggest that our proposed
LFBM model outperforms the other state of the art approaches in the evaluated
tasks.
"
"  Over the past decades, the competition for academic resources has gradually
intensified, and worsened with the current financial crisis. To optimize the
resource allocation, individualized assessment of research results is being
actively studied but the current indices, such as the number of papers, the
number of citations, the h-factor and its variants have limitations, especially
their inability of determining co-authors' credit shares fairly. Here we
establish an axiomatic system and quantify co-authors' relative contributions.
Our methodology avoids subjective assignment of co-authors' credits using the
inflated, fractional or harmonic methods, and provides a quantitative tool for
scientific management such as funding and tenure decisions.
"
"  This paper aims at the problem of link pattern prediction in collections of
objects connected by multiple relation types, where each type may play a
distinct role. While common link analysis models are limited to single-type
link prediction, we attempt here to capture the correlations among different
relation types and reveal the impact of various relation types on performance
quality. For that, we define the overall relations between object pairs as a
\textit{link pattern} which consists in interaction pattern and connection
structure in the network, and then use tensor formalization to jointly model
and predict the link patterns, which we refer to as \textit{Link Pattern
Prediction} (LPP) problem. To address the issue, we propose a Probabilistic
Latent Tensor Factorization (PLTF) model by introducing another latent factor
for multiple relation types and furnish the Hierarchical Bayesian treatment of
the proposed probabilistic model to avoid overfitting for solving the LPP
problem. To learn the proposed model we develop an efficient Markov Chain Monte
Carlo sampling method. Extensive experiments are conducted on several real
world datasets and demonstrate significant improvements over several existing
state-of-the-art methods.
"
"  The recent developments of basis pursuit and compressed sensing seek to
extract information from as few samples as possible. In such applications,
since the number of samples is restricted, one should deploy the sampling
points wisely. We are motivated to study the optimal distribution of finite
sampling points. Formulation under the framework of optimal reconstruction
yields a minimization problem. In the discrete case, we estimate the distance
between the optimal subspace resulting from a general Karhunen-Loeve transform
and the kernel space to obtain another algorithm that is computationally
favorable. Numerical experiments are then presented to illustrate the
performance of the algorithms for the searching of optimal sampling points.
"
"  Longitudinal imaging studies have moved to the forefront of medical research
due to their ability to characterize spatio-temporal features of biological
structures across the lifespan. Credible models of the correlations in
longitudinal imaging require two or more pattern components. Valid inference
requires enough flexibility of the correlation model to allow reasonable
fidelity to the true pattern. On the other hand, the existence of computable
estimates demands a parsimonious parameterization of the correlation structure.
For many one-dimensional spatial or temporal arrays, the linear exponent
autoregressive (LEAR) correlation structure meets these two opposing goals in
one model. The LEAR structure is a flexible two-parameter correlation model
that applies in situations in which the within-subject correlation decreases
exponentially in time or space. It allows for an attenuation or acceleration of
the exponential decay rate imposed by the commonly used continuous-time AR(1)
structure. Here we propose the Kronecker product LEAR correlation structure for
multivariate repeated measures data in which the correlation between
measurements for a given subject is induced by two factors. We also provide a
scientifically informed approach to assessing the adequacy of a Kronecker
product LEAR model and a general unstructured Kronecker product model. The
approach provides useful guidance for high dimension, low sample size data that
preclude using standard likelihood based tests. Longitudinal medical imaging
data of caudate morphology in schizophrenia illustrates the appeal of the
Kronecker product LEAR correlation structure.
"
"  This is a note on logistic regression models and logistic kernel machine
models. It contains derivations to some of the expressions in a paper -- SNP
Set Analysis for Detecting Disease Association Using Exon Sequence Data --
submitted to BMC proceedings by these authors.
"
"  The graphical lasso \citep{FHT2007a} is an algorithm for learning the
structure in an undirected Gaussian graphical model, using $\ell_1$
regularization to control the number of zeros in the precision matrix
${\B\Theta}={\B\Sigma}^{-1}$ \citep{BGA2008,yuan_lin_07}. The {\texttt R}
package \GL\ \citep{FHT2007a} is popular, fast, and allows one to efficiently
build a path of models for different values of the tuning parameter.
Convergence of \GL\ can be tricky; the converged precision matrix might not be
the inverse of the estimated covariance, and occasionally it fails to converge
with warm starts. In this paper we explain this behavior, and propose new
algorithms that appear to outperform \GL.
  By studying the ""normal equations"" we see that, \GL\ is solving the {\em
dual} of the graphical lasso penalized likelihood, by block coordinate ascent;
a result which can also be found in \cite{BGA2008}.
  In this dual, the target of estimation is $\B\Sigma$, the covariance matrix,
rather than the precision matrix $\B\Theta$. We propose similar primal
algorithms \PGL\ and \DPGL, that also operate by block-coordinate descent,
where $\B\Theta$ is the optimization target. We study all of these algorithms,
and in particular different approaches to solving their coordinate
sub-problems. We conclude that \DPGL\ is superior from several points of view.
"
"  In computer experiments, a mathematical model implemented on a computer is
used to represent complex physical phenomena. These models, known as computer
simulators, enable experimental study of a virtual representation of the
complex phenomena. Simulators can be thought of as complex functions that take
many inputs and provide an output. Often these simulators are themselves
expensive to compute, and may be approximated by ""surrogate models"" such as
statistical regression models. In this paper we consider a new kind of
surrogate model, a Bayesian ensemble of trees (Chipman et al. 2010), with the
specific goal of learning enough about the simulator that a particular feature
of the simulator can be estimated. We focus on identifying the simulator's
global minimum. Utilizing the Bayesian version of the Expected Improvement
criterion (Jones et al. 1998), we show that this ensemble is particularly
effective when the simulator is ill-behaved, exhibiting nonstationarity or
abrupt changes in the response. A number of illustrations of the approach are
given, including a tidal power application.
"
"  Parallel MRI is a fast imaging technique that enables the acquisition of
highly resolved images in space or/and in time. The performance of parallel
imaging strongly depends on the reconstruction algorithm, which can proceed
either in the original k-space (GRAPPA, SMASH) or in the image domain
(SENSE-like methods). To improve the performance of the widely used SENSE
algorithm, 2D- or slice-specific regularization in the wavelet domain has been
deeply investigated. In this paper, we extend this approach using 3D-wavelet
representations in order to handle all slices together and address
reconstruction artifacts which propagate across adjacent slices. The gain
induced by such extension (3D-Unconstrained Wavelet Regularized -SENSE:
3D-UWR-SENSE) is validated on anatomical image reconstruction where no temporal
acquisition is considered. Another important extension accounts for temporal
correlations that exist between successive scans in functional MRI (fMRI). In
addition to the case of 2D+t acquisition schemes addressed by some other
methods like kt-FOCUSS, our approach allows us to deal with 3D+t acquisition
schemes which are widely used in neuroimaging. The resulting 3D-UWR-SENSE and
4D-UWR-SENSE reconstruction schemes are fully unsupervised in the sense that
all regularization parameters are estimated in the maximum likelihood sense on
a reference scan. The gain induced by such extensions is illustrated on both
anatomical and functional image reconstruction, and also measured in terms of
statistical sensitivity for the 4D-UWR-SENSE approach during a fast
event-related fMRI protocol. Our 4D-UWR-SENSE algorithm outperforms the SENSE
reconstruction at the subject and group levels (15 subjects) for different
contrasts of interest (eg, motor or computation tasks) and using different
parallel acceleration factors (R=2 and R=4) on 2x2x3mm3 EPI images.
"
"  Hungaria asteroids, whose orbits occupy the region in element space between
$1.78< a< 2.03$ AU, $e<0.19$, $12^\circ<i<31^\circ$, are a possible source of
Near-Earth Asteroids (NEAs). Named after (434) Hungaria these asteroids are
relatively small, since the largest member of the group has a diameter of just
about 11 km. They are mainly perturbed by Jupiter and Mars, possibly becoming
Mars-crossers and, later, they may even cross the orbits of Earth and Venus. In
this paper we analyze the close encounters and possible impacts of escaped
Hungarias with the terrestrial planets. Out of about 8000 known Hungarias we
selected 200 objects which are on the edge of the group. We integrated their
orbits over 100 million years in a simplified model of the planetary system
(Mars to Saturn) subject only to gravitational forces. We picked out a sample
of 11 objects (each with 50 clones) with large variations in semi-major axis
and restarted the numerical integration in a gravitational model including the
planets from Venus to Saturn. Due to close encounters, some of them achieve
high inclinations and eccentricities which, in turn, lead to relatively high
velocity impacts on Venus, Earth, and Mars. We statistically analyze all close
encounters and impacts with the terrestrial planets and determine the encounter
and impact velocities of these fictitious Hungarias.
"
"  We consider the application of a popular penalised regression method, Ridge
Regression, to data with very high dimensions and many more covariates than
observations. Our motivation is the problem of out-of-sample prediction and the
setting is high-density genotype data from a genome-wide association or
resequencing study. Ridge regression has previously been shown to offer
improved performance for prediction when compared with other penalised
regression methods. One problem with ridge regression is the choice of an
appropriate parameter for controlling the amount of shrinkage of the
coefficient estimates. Here we propose a method for choosing the ridge
parameter based on controlling the variance of the predicted observations in
the model.
  Using simulated data, we demonstrate that our method outperforms subset
selection based on univariate tests of association and another penalised
regression method, HyperLasso regression, in terms of improved prediction
error. We extend our approach to regression problems when the outcomes are
binary (representing cases and controls, as is typically the setting for
genome-wide association studies) and demonstrate the method on a real data
example consisting of case-control and genotype data on Bipolar Disorder, taken
from the Wellcome Trust Case Control Consortium and the Genetic Association
Information Network.
"
"  We propose an adjusted likelihood ratio test of two-factor separability
(Kronecker product structure) for unbalanced multivariate repeated measures
data. Here we address the particular case where the within subject correlation
is believed to decrease exponentially in both dimensions (e.g., temporal and
spatial dimensions). However, the test can be easily generalized to factor
specific matrices of any structure. A simulation study is conducted to assess
the inference accuracy of the proposed test. Longitudinal medical imaging data
concerning schizophrenia and caudate morphology illustrates the methodology.
"
"  We show how to reduce the process of predicting general order statistics (and
the median in particular) to solving classification. The accompanying
theoretical statement shows that the regret of the classifier bounds the regret
of the quantile regression under a quantile loss. We also test this reduction
empirically against existing quantile regression methods on large real-world
datasets and discover that it provides state-of-the-art performance.
"
"  Most nations of the world periodically publish N x N origin-destination
tables, recording the number of people who lived in geographic subdivision i at
time t and j at t+1. We have developed and widely applied to such national
tables and other analogous (weighted, directed) socioeconomic networks, a
two-stage--double-standardization and (strong component) hierarchical
clustering--procedure. Previous applications of this methodology and related
analytical issues are discussed. Its use is illustrated in a large-scale study,
employing recorded United States internal migration flows between the 3,000+
county-level units of the nation for the periods 1965-1970 and 1995-2000.
Prominent, important features--such as ''cosmopolitan hubs'' and ``functional
regions''--are extracted from master dendrograms. The extent to which such
characteristics have varied over the intervening thirty years is evaluated.
"
"  Collaborative filtering is a rapidly advancing research area. Every year
several new techniques are proposed and yet it is not clear which of the
techniques work best and under what conditions. In this paper we conduct a
study comparing several collaborative filtering techniques -- both classic and
recent state-of-the-art -- in a variety of experimental contexts. Specifically,
we report conclusions controlling for number of items, number of users,
sparsity level, performance criteria, and computational complexity. Our
conclusions identify what algorithms work well and in what conditions, and
contribute to both industrial deployment collaborative filtering algorithms and
to the research community.
"
"  The distance metric plays an important role in nearest neighbor (NN)
classification. Usually the Euclidean distance metric is assumed or a
Mahalanobis distance metric is optimized to improve the NN performance. In this
paper, we study the problem of embedding arbitrary metric spaces into a
Euclidean space with the goal to improve the accuracy of the NN classifier. We
propose a solution by appealing to the framework of regularization in a
reproducing kernel Hilbert space and prove a representer-like theorem for NN
classification. The embedding function is then determined by solving a
semidefinite program which has an interesting connection to the soft-margin
linear binary support vector machine classifier. Although the main focus of
this paper is to present a general, theoretical framework for metric embedding
in a NN setting, we demonstrate the performance of the proposed method on some
benchmark datasets and show that it performs better than the Mahalanobis metric
learning algorithm in terms of leave-one-out and generalization errors.
"
"  Latent force models (LFMs) are hybrid models combining mechanistic principles
with non-parametric components. In this article, we shall show how LFMs can be
equivalently formulated and solved using the state variable approach. We shall
also show how the Gaussian process prior used in LFMs can be equivalently
formulated as a linear statespace model driven by a white noise process and how
inference on the resulting model can be efficiently implemented using Kalman
filter and smoother. Then we shall show how the recently proposed switching LFM
can be reformulated using the state variable approach, and how we can construct
a probabilistic model for the switches by formulating a similar switching LFM
as a switching linear dynamic system (SLDS). We illustrate the performance of
the proposed methodology in simulated scenarios and apply it to inferring the
switching points in GPS data collected from car movement data in urban
environment.
"
"  Government statistical agencies often apply statistical disclosure limitation
techniques to survey microdata to protect the confidentiality of respondents.
There is a need for valid and practical ways to assess the protection provided.
This paper develops some simple methods for disclosure limitation techniques
which perturb the values of categorical identifying variables. The methods are
applied in numerical experiments based upon census data from the United Kingdom
which are subject to two perturbation techniques: data swapping (random and
targeted) and the post randomization method. Some simplifying approximations to
the measure of risk are found to work well in capturing the impacts of these
techniques. These approximations provide simple extensions of existing risk
assessment methods based upon Poisson log-linear models. A numerical experiment
is also undertaken to assess the impact of multivariate misclassification with
an increasing number of identifying variables. It is found that the
misclassification dominates the usual monotone increasing relationship between
this number and risk so that the risk eventually declines, implying less
sensitivity of risk to choice of identifying variables. The methods developed
in this paper may also be used to obtain more realistic assessments of risk
which take account of the kinds of measurement and other nonsampling errors
commonly arising in surveys.
"
"  Regional flood frequency analysis is a convenient way to reduce estimation
uncertainty when few data are available at the gauging site. In this work, a
model that allows a non-null probability to a regional fixed shape parameter is
presented. This methodology is integrated within a Bayesian framework and uses
reversible jump techniques. The performance on stochastic data of this new
estimator is compared to two other models: a conventional Bayesian analysis and
the index flood approach. Results show that the proposed estimator is
absolutely suited to regional estimation when only a few data are available at
the target site. Moreover, unlike the index flood estimator, target site index
flood error estimation seems to have less impact on Bayesian estimators. Some
suggestions about configurations of the pooling groups are also presented to
increase the performance of each estimator.
"
"  In recent years, there is a growing interest in learning Bayesian networks
with continuous variables. Learning the structure of such networks is a
computationally expensive procedure, which limits most applications to
parameter learning. This problem is even more acute when learning networks with
hidden variables. We present a general method for significantly speeding the
structure search algorithm for continuous variable networks with common
parametric distributions. Importantly, our method facilitates the addition of
new hidden variables into the network structure efficiently. We demonstrate the
method on several data sets, both for learning structure on fully observable
data, and for introducing new hidden variables during structure search.
"
"  We describe a model for capturing the statistical structure of local
amplitude and local spatial phase in natural images. The model is based on a
recently developed, factorized third-order Boltzmann machine that was shown to
be effective at capturing higher-order structure in images by modeling
dependencies among squared filter outputs (Ranzato and Hinton, 2010). Here, we
extend this model to $L_p$-spherically symmetric subspaces. In order to model
local amplitude and phase structure in images, we focus on the case of two
dimensional subspaces, and the $L_2$-norm. When trained on natural images the
model learns subspaces resembling quadrature-pair Gabor filters. We then
introduce an additional set of hidden units that model the dependencies among
subspace phases. These hidden units form a combinatorial mixture of phase
coupling distributions, concentrated in the sum and difference of phase pairs.
When adapted to natural images, these distributions capture local spatial phase
structure in natural images.
"
"  A desirable goal of scientific management is to introduce, if it exists, a
simple and reliable way to measure the scientific excellence of publicly-funded
research institutions and universities to serve as a basis for their ranking
and financing. While citation-based indicators and metrics are easily
accessible, they are far from being universally accepted as way to automate or
inform evaluation processes or to replace evaluations based on peer review.
Here we consider absolute measurements of research excellence at an
amalgamated, institutional level and specific measures of research excellence
as performance per head. Using biology research institutions in the UK as a
test case, we examine the correlations between peer-review-based and
citation-based measures of research excellence on these two scales. We find
that citation-based indicators are very highly correlated with peer-evaluated
measures of group strength but are poorly correlated with group quality. Thus,
and almost paradoxically, our analysis indicates that citation counts could
possibly form a basis for deciding on how to fund research institutions but
they should not be used as a basis for ranking them in terms of quality.
"
"  Contextual bandit algorithms have become popular for online recommendation
systems such as Digg, Yahoo! Buzz, and news recommendation in general.
\emph{Offline} evaluation of the effectiveness of new algorithms in these
applications is critical for protecting online user experiences but very
challenging due to their ""partial-label"" nature. Common practice is to create a
simulator which simulates the online environment for the problem at hand and
then run an algorithm against this simulator. However, creating simulator
itself is often difficult and modeling bias is usually unavoidably introduced.
In this paper, we introduce a \emph{replay} methodology for contextual bandit
algorithm evaluation. Different from simulator-based approaches, our method is
completely data-driven and very easy to adapt to different applications. More
importantly, our method can provide provably unbiased evaluations. Our
empirical results on a large-scale news article recommendation dataset
collected from Yahoo! Front Page conform well with our theoretical results.
Furthermore, comparisons between our offline replay and online bucket
evaluation of several contextual bandit algorithms show accuracy and
effectiveness of our offline evaluation method.
"
"  The partial least squares (PLS) is a popular modeling technique commonly used
in social sciences. The traditional PLS algorithm deals with variables measured
on interval scales while data are often collected on ordinal scales: a
reformulation of the algorithm, named ordinal PLS (OPLS), is introduced, which
properly deals with ordinal variables. An application to customer satisfaction
data and some simulations are also presented. The technique seems to perform
better than the traditional PLS when the number of categories of the items in
the questionnaire is small (4 or 5) which is typical in the most common
practical situations.
"
"  Global expression analyses using microarray technologies are becoming more
common in genomic research, therefore, new statistical challenges associated
with combining information from multiple studies must be addressed. In this
paper we will describe our proposal for an adaptively weighted (AW) statistic
to combine multiple genomic studies for detecting differentially expressed
genes. We will also present our results from comparisons of our proposed AW
statistic to Fisher's equally weighted (EW), Tippett's minimum p-value (minP)
and Pearson's (PR) statistics. Due to the absence of a uniformly powerful test,
we used a simplified Gaussian scenario to compare the four methods. Our AW
statistic consistently produced the best or near-best power for a range of
alternative hypotheses. AW-obtained weights also have the additional advantage
of filtering discordant biomarkers and providing natural detected gene
categories for further biological investigation. Here we will demonstrate the
superior performance of our proposed AW statistic based on a mix of power
analyses, simulations and applications using data sets for multi-tissue energy
metabolism mouse, multi-lab prostate cancer and lung cancer.
"
"  The goal of machine learning is to provide solutions which are trained by
data or by experience coming from the environment. Many training algorithms
exist and some brilliant successes were achieved. But even in structured
environments for machine learning (e.g. data mining or board games), most
applications beyond the level of toy problems need careful hand-tuning or human
ingenuity (i.e. detection of interesting patterns) or both. We discuss several
aspects how self-configuration can help to alleviate these problems. One aspect
is the self-configuration by tuning of algorithms, where recent advances have
been made in the area of SPO (Sequen- tial Parameter Optimization). Another
aspect is the self-configuration by pattern detection or feature construction.
Forming multiple features (e.g. random boolean functions) and using algorithms
(e.g. random forests) which easily digest many fea- tures can largely increase
learning speed. However, a full-fledged theory of feature construction is not
yet available and forms a current barrier in machine learning. We discuss
several ideas for systematic inclusion of feature construction. This may lead
to partly self-configuring machine learning solutions which show robustness,
flexibility, and fast learning in potentially changing environments.
"
"  We develop the distance dependent Chinese restaurant process (CRP), a
flexible class of distributions over partitions that allows for
non-exchangeability. This class can be used to model many kinds of dependencies
between data in infinite clustering models, including dependencies across time
or space. We examine the properties of the distance dependent CRP, discuss its
connections to Bayesian nonparametric mixture models, and derive a Gibbs
sampler for both observed and mixture settings. We study its performance with
three text corpora. We show that relaxing the assumption of exchangeability
with distance dependent CRPs can provide a better fit to sequential data. We
also show its alternative formulation of the traditional CRP leads to a
faster-mixing Gibbs sampling algorithm than the one based on the original
formulation.
"
"  Structured prediction tasks pose a fundamental trade-off between the need for
model complexity to increase predictive power and the limited computational
resources for inference in the exponentially-sized output spaces such models
require. We formulate and develop the Structured Prediction Cascade
architecture: a sequence of increasingly complex models that progressively
filter the space of possible outputs. The key principle of our approach is that
each model in the cascade is optimized to accurately filter and refine the
structured output state space of the next model, speeding up both learning and
inference in the next layer of the cascade. We learn cascades by optimizing a
novel convex loss function that controls the trade-off between the filtering
efficiency and the accuracy of the cascade, and provide generalization bounds
for both accuracy and efficiency. We also extend our approach to intractable
models using tree-decomposition ensembles, and provide algorithms and theory
for this setting. We evaluate our approach on several large-scale problems,
achieving state-of-the-art performance in handwriting recognition and human
pose recognition. We find that structured prediction cascades allow tremendous
speedups and the use of previously intractable features and models in both
settings.
"
"  In this paper we study general $l_p$ regularized unconstrained minimization
problems. In particular, we derive lower bounds for nonzero entries of first-
and second-order stationary points, and hence also of local minimizers of the
$l_p$ minimization problems. We extend some existing iterative reweighted $l_1$
(IRL1) and $l_2$ (IRL2) minimization methods to solve these problems and
proposed new variants for them in which each subproblem has a closed form
solution. Also, we provide a unified convergence analysis for these methods. In
addition, we propose a novel Lipschitz continuous $\epsilon$-approximation to
$\|x\|^p_p$. Using this result, we develop new IRL1 methods for the $l_p$
minimization problems and showed that any accumulation point of the sequence
generated by these methods is a first-order stationary point, provided that the
approximation parameter $\epsilon$ is below a computable threshold value. This
is a remarkable result since all existing iterative reweighted minimization
methods require that $\epsilon$ be dynamically updated and approach zero. Our
computational results demonstrate that the new IRL1 method is generally more
stable than the existing IRL1 methods [21,18] in terms of objective function
value and CPU time.
"
"  Metric learning makes it plausible to learn distances for complex
distributions of data from labeled data. However, to date, most metric learning
methods are based on a single Mahalanobis metric, which cannot handle
heterogeneous data well. Those that learn multiple metrics throughout the space
have demonstrated superior accuracy, but at the cost of computational
efficiency. Here, we take a new angle to the metric learning problem and learn
a single metric that is able to implicitly adapt its distance function
throughout the feature space. This metric adaptation is accomplished by using a
random forest-based classifier to underpin the distance function and
incorporate both absolute pairwise position and standard relative position into
the representation. We have implemented and tested our method against state of
the art global and multi-metric methods on a variety of data sets. Overall, the
proposed method outperforms both types of methods in terms of accuracy
(consistently ranked first) and is an order of magnitude faster than state of
the art multi-metric methods (16x faster in the worst case).
"
"  In this paper we propose a novel gradient algorithm to learn a policy from an
expert's observed behavior assuming that the expert behaves optimally with
respect to some unknown reward function of a Markovian Decision Problem. The
algorithm's aim is to find a reward function such that the resulting optimal
policy matches well the expert's observed behavior. The main difficulty is that
the mapping from the parameters to policies is both nonsmooth and highly
redundant. Resorting to subdifferentials solves the first difficulty, while the
second one is over- come by computing natural gradients. We tested the proposed
method in two artificial domains and found it to be more reliable and efficient
than some previous methods.
"
"  Limited spectrum coverage is a problem in shotgun proteomics. Replicates are
generated to improve the spectrum coverage. When integrating peptide
identification results obtained from replicates, the state-of-the-art algorithm
PeptideProphet combines Peptide-Spectrum Matches (PSMs) before building the
statistical model to calculate peptide probabilities.
  In this paper, we find the connection between merging results of replicates
and Bagging, which is a standard routine to improve the power of statistical
methods. Following Bagging's philosophy, we propose to run PeptideProphet
separately on each replicate and combine the outputs to obtain the final
peptide probabilities. In our experiments, we show that the proposed routine
can improve PeptideProphet consistently on a standard protein dataset, a Human
dataset and a Yeast dataset.
"
"  The use of statistical modeling in baseball has received substantial
attention recently in both the media and academic community. We focus on a
relatively under-explored topic: the use of statistical models for the analysis
of fielding based on high-resolution data consisting of on-field location of
batted balls. We combine spatial modeling with a hierarchical Bayesian
structure in order to evaluate the performance of individual fielders while
sharing information between fielders at each position. We present results
across four seasons of MLB data (2002--2005) and compare our approach to other
fielding evaluation procedures.
"
"  Stochastic Kronecker graphs supply a parsimonious model for large sparse real
world graphs. They can specify the distribution of a large random graph using
only three or four parameters. Those parameters have however proved difficult
to choose in specific applications. This article looks at method of moments
estimators that are computationally much simpler than maximum likelihood. The
estimators are fast and in our examples, they typically yield Kronecker
parameters with expected feature counts closer to a given graph than we get
from KronFit. The improvement was especially prominent for the number of
triangles in the graph.
"
"  We present a new method for the detection of gene pathways associated with a
multivariate quantitative trait, and use it to identify causal pathways
associated with an imaging endophenotype characteristic of longitudinal
structural change in the brains of patients with Alzheimer's disease (AD). Our
method, known as pathways sparse reduced-rank regression (PsRRR), uses group
lasso penalised regression to jointly model the effects of genome-wide single
nucleotide polymorphisms (SNPs), grouped into functional pathways using prior
knowledge of gene-gene interactions. Pathways are ranked in order of importance
using a resampling strategy that exploits finite sample variability. Our
application study uses whole genome scans and MR images from 464 subjects in
the Alzheimer's Disease Neuroimaging Initiative (ADNI) database. 66,182 SNPs
are mapped to 185 gene pathways from the KEGG pathways database. Voxel-wise
imaging signatures characteristic of AD are obtained by analysing 3D patterns
of structural change at 6, 12 and 24 months relative to baseline. High-ranking,
AD endophenotype-associated pathways in our study include those describing
chemokine, Jak-stat and insulin signalling pathways, and tight junction
interactions. All of these have been previously implicated in AD biology. In a
secondary analysis, we investigate SNPs and genes that may be driving pathway
selection, and identify a number of previously validated AD genes including
CR1, APOE and TOMM40.
"
"  In this paper we review recent advances in Stable Isotope Mixing Models
(SIMMs) and place them into an over-arching Bayesian statistical framework
which allows for several useful extensions. SIMMs are used to quantify the
proportional contributions of various sources to a mixture. The most widely
used application is quantifying the diet of organisms based on the food sources
they have been observed to consume. At the centre of the multivariate
statistical model we propose is a compositional mixture of the food sources
corrected for various metabolic factors. The compositional component of our
model is based on the isometric log ratio (ilr) transform of Egozcue (2003).
Through this transform we can apply a range of time series and non-parametric
smoothing relationships. We illustrate our models with 3 case studies based on
real animal dietary behaviour.
"
"  We consider the problem of constructing optimal designs for population
pharmacokinetics which use random effect models. It is common practice in the
design of experiments in such studies to assume uncorrelated errors for each
subject. In the present paper a new approach is introduced to determine
efficient designs for nonlinear least squares estimation which addresses the
problem of correlation between observations corresponding to the same subject.
We use asymptotic arguments to derive optimal design densities, and the designs
for finite sample sizes are constructed from the quantiles of the corresponding
optimal distribution function. It is demonstrated that compared to the optimal
exact designs, whose determination is a hard numerical problem, these designs
are very efficient. Alternatively, the designs derived from asymptotic theory
could be used as starting designs for the numerical computation of exact
optimal designs. Several examples of linear and nonlinear models are presented
in order to illustrate the methodology. In particular, it is demonstrated that
naively chosen equally spaced designs may lead to less accurate estimation.
"
"  We propose a new data-augmentation strategy for fully Bayesian inference in
models with binomial likelihoods. The approach appeals to a new class of
Polya-Gamma distributions, which are constructed in detail. A variety of
examples are presented to show the versatility of the method, including
logistic regression, negative binomial regression, nonlinear mixed-effects
models, and spatial models for count data. In each case, our data-augmentation
strategy leads to simple, effective methods for posterior inference that: (1)
circumvent the need for analytic approximations, numerical integration, or
Metropolis-Hastings; and (2) outperform other known data-augmentation
strategies, both in ease of use and in computational efficiency. All methods,
including an efficient sampler for the Polya-Gamma distribution, are
implemented in the R package BayesLogit.
  In the technical supplement appended to the end of the paper, we provide
further details regarding the generation of Polya-Gamma random variables; the
empirical benchmarks reported in the main manuscript; and the extension of the
basic data-augmentation framework to contingency tables and multinomial
outcomes.
"
"  This paper has been withdrawn by the author due to some inaccurate
descriptions in the section of INTRODUCTION and CONCLUSIONS.
"
"  In public health management there is a need to produce subnational estimates
of health outcomes. Often, however, funds are not available to collect samples
large enough to produce traditional survey sample estimates for each
subnational area. Although parametric hierarchical methods have been
successfully used to derive estimates from small samples, there is a concern
that the geographic diversity of the U.S. population may be oversimplified in
these models. In this paper, a semi-parametric model is used to describe the
geographic variability component of the model. Specifically, we assume
Dirichlet process mixtures of normals for county-specific random effects.
Results are compared to a parametric model based on the base measure of the
Dirichlet process, using binary health outcomes related to mammogram usage.
"
"  It is becoming increasingly apparent that probabilistic approaches can
overcome conservatism and computational complexity of the classical worst-case
deterministic framework and may lead to designs that are actually safer. In
this paper we argue that a comprehensive probabilistic robustness analysis
requires a detailed evaluation of the robustness function and we show that such
evaluation can be performed with essentially any desired accuracy and
confidence using algorithms with complexity linear in the dimension of the
uncertainty space. Moreover, we show that the average memory requirements of
such algorithms are absolutely bounded and well within the capabilities of
today's computers.
  In addition to efficiency, our approach permits control over statistical
sampling error and the error due to discretization of the uncertainty radius.
For a specific level of tolerance of the discretization error, our techniques
provide an efficiency improvement upon conventional methods which is inversely
proportional to the accuracy level; i.e., our algorithms get better as the
demands for accuracy increase.
"
"  The perennial problem of ""how many clusters?"" remains an issue of substantial
interest in data mining and machine learning communities, and becomes
particularly salient in large data sets such as populational genomic data where
the number of clusters needs to be relatively large and open-ended. This
problem gets further complicated in a co-clustering scenario in which one needs
to solve multiple clustering problems simultaneously because of the presence of
common centroids (e.g., ancestors) shared by clusters (e.g., possible descents
from a certain ancestor) from different multiple-cluster samples (e.g.,
different human subpopulations). In this paper we present a hierarchical
nonparametric Bayesian model to address this problem in the context of
multi-population haplotype inference. Uncovering the haplotypes of single
nucleotide polymorphisms is essential for many biological and medical
applications. While it is uncommon for the genotype data to be pooled from
multiple ethnically distinct populations, few existing programs have explicitly
leveraged the individual ethnic information for haplotype inference. In this
paper we present a new haplotype inference program, Haploi, which makes use of
such information and is readily applicable to genotype sequences with thousands
of SNPs from heterogeneous populations, with competent and sometimes superior
speed and accuracy comparing to the state-of-the-art programs. Underlying
Haploi is a new haplotype distribution model based on a nonparametric Bayesian
formalism known as the hierarchical Dirichlet process, which represents a
tractable surrogate to the coalescent process. The proposed model is
exchangeable, unbounded, and capable of coupling demographic information of
different populations.
"
"  We adapt tools from information theory to analyze how an observer comes to
synchronize with the hidden states of a finitary, stationary stochastic
process. We show that synchronization is determined by both the process's
internal organization and by an observer's model of it. We analyze these
components using the convergence of state-block and block-state entropies,
comparing them to the previously known convergence properties of the Shannon
block entropy. Along the way, we introduce a hierarchy of information
quantifiers as derivatives and integrals of these entropies, which parallels a
similar hierarchy introduced for block entropy. We also draw out the duality
between synchronization properties and a process's controllability. The tools
lead to a new classification of a process's alternative representations in
terms of minimality, synchronizability, and unifilarity.
"
"  The study of fingerprint individuality aims to determine to what extent a
fingerprint uniquely identifies an individual. Recent court cases have
highlighted the need for measures of fingerprint individuality when a person is
identified based on fingerprint evidence. The main challenge in studies of
fingerprint individuality is to adequately capture the variability of
fingerprint features in a population. In this paper hierarchical mixture models
are introduced to infer the extent of individualization. Hierarchical mixtures
utilize complementary aspects of mixtures at different levels of the hierarchy.
At the first (top) level, a mixture is used to represent homogeneous groups of
fingerprints in the population, whereas at the second level, nested mixtures
are used as flexible representations of distributions of features from each
fingerprint. Inference for hierarchical mixtures is more challenging since the
number of unknown mixture components arise in both the first and second levels
of the hierarchy. A Bayesian approach based on reversible jump Markov chain
Monte Carlo methodology is developed for the inference of all unknown
parameters of hierarchical mixtures. The methodology is illustrated on
fingerprint images from the NIST database and is used to make inference on
fingerprint individuality estimates from this population.
"
"  We examine nonparametric dose-finding designs that use toxicity estimates
based on all available data at each dose allocation decision. We prove that one
such design family, called here ""interval design"", converges almost surely to
the maximum tolerated dose (MTD), if the MTD is the only dose level whose
toxicity rate falls within the pre-specified interval around the desired target
rate. Another nonparametric family, called ""point design"", has a positive
probability of not converging. In a numerical sensitivity study, a diverse
sample of dose-toxicity scenarios was randomly generated. On this sample, the
""interval design"" convergence conditions are met far more often than the
conditions for one-parameter design convergence (the Shen-O'Quigley
conditions), suggesting that the interval-design conditions are less
restrictive. Implications of these theoretical and numerical results for
small-sample behavior of the designs, and for future research, are discussed.
"
"  Thompson Sampling is one of the oldest heuristics for multi-armed bandit
problems. It is a randomized algorithm based on Bayesian ideas, and has
recently generated significant interest after several studies demonstrated it
to have better empirical performance compared to the state of the art methods.
In this paper, we provide a novel regret analysis for Thompson Sampling that
simultaneously proves both the optimal problem-dependent bound of
$(1+\epsilon)\sum_i \frac{\ln T}{\Delta_i}+O(\frac{N}{\epsilon^2})$ and the
first near-optimal problem-independent bound of $O(\sqrt{NT\ln T})$ on the
expected regret of this algorithm. Our near-optimal problem-independent bound
solves a COLT 2012 open problem of Chapelle and Li. The optimal
problem-dependent regret bound for this problem was first proven recently by
Kaufmann et al. [ALT 2012]. Our novel martingale-based analysis techniques are
conceptually simple, easily extend to distributions other than the Beta
distribution, and also extend to the more general contextual bandits setting
[Manuscript, Agrawal and Goyal, 2012].
"
"  The Baire metric induces an ultrametric on a dataset and is of linear
computational complexity, contrasted with the standard quadratic time
agglomerative hierarchical clustering algorithm. We apply the Baire distance to
spectrometric and photometric redshifts from the Sloan Digital Sky Survey
using, in this work, about half a million astronomical objects. We want to know
how well the (more cos\ tly to determine) spectrometric redshifts can predict
the (more easily obtained) photometric redshifts, i.e. we seek to regress the
spectrometric on the photometric redshifts, and we develop a clusterwise
nearest neighbor regression procedure for this.
"
"  We consider analysis of relational data (a matrix), in which the rows
correspond to subjects (e.g., people) and the columns correspond to attributes.
The elements of the matrix may be a mix of real and categorical. Each subject
and attribute is characterized by a latent binary feature vector, and an
inferred matrix maps each row-column pair of binary feature vectors to an
observed matrix element. The latent binary features of the rows are modeled via
a multivariate Gaussian distribution with low-rank covariance matrix, and the
Gaussian random variables are mapped to latent binary features via a probit
link. The same type construction is applied jointly to the columns. The model
infers latent, low-dimensional binary features associated with each row and
each column, as well correlation structure between all rows and between all
columns.
"
"  Neuroimaging produces data that are continuous in one or more dimensions.
This calls for an inference framework that can handle data that approximate
functions of space, for example, anatomical images, time--frequency maps and
distributed source reconstructions of electromagnetic recordings over time.
Statistical parametric mapping (SPM) is the standard framework for whole-brain
inference in neuroimaging: SPM uses random field theory to furnish $p$-values
that are adjusted to control family-wise error or false discovery rates, when
making topological inferences over large volumes of space. Random field theory
regards data as realizations of a continuous process in one or more dimensions.
This contrasts with classical approaches like the Bonferroni correction, which
consider images as collections of discrete samples with no continuity
properties (i.e., the probabilistic behavior at one point in the image does not
depend on other points). Here, we illustrate how random field theory can be
applied to data that vary as a function of time, space or frequency. We
emphasize how topological inference of this sort is invariant to the geometry
of the manifolds on which data are sampled. This is particularly useful in
electromagnetic studies that often deal with very smooth data on scalp or
cortical meshes. This application illustrates the versatility and simplicity of
random field theory and the seminal contributions of Keith Worsley
(1951--2009), a key architect of topological inference.
"
"  Recent research has explored the increasingly important role of social media
by examining the dynamics of individual and group behavior, characterizing
patterns of information diffusion, and identifying influential individuals. In
this paper we suggest a measure of causal relationships between nodes based on
the information-theoretic notion of transfer entropy, or information transfer.
This theoretically grounded measure is based on dynamic information, captures
fine-grain notions of influence, and admits a natural, predictive
interpretation. Causal networks inferred by transfer entropy can differ
significantly from static friendship networks because most friendship links are
not useful for predicting future dynamics. We demonstrate through analysis of
synthetic and real-world data that transfer entropy reveals meaningful hidden
network structures. In addition to altering our notion of who is influential,
transfer entropy allows us to differentiate between weak influence over large
groups and strong influence over small groups.
"
"  We provide yet another proof of the existence of calibrated forecasters; it
has two merits. First, it is valid for an arbitrary finite number of outcomes.
Second, it is short and simple and it follows from a direct application of
Blackwell's approachability theorem to carefully chosen vector-valued payoff
function and convex target set. Our proof captures the essence of existing
proofs based on approachability (e.g., the proof by Foster, 1999 in case of
binary outcomes) and highlights the intrinsic connection between
approachability and calibration.
"
"  Information theoretical measures, such as entropy, mutual information, and
various divergences, exhibit robust characteristics in image registration
applications. However, the estimation of these quantities is computationally
intensive in high dimensions. On the other hand, consistent estimation from
pairwise distances of the sample points is possible, which suits random
projection (RP) based low dimensional embeddings. We adapt the RP technique to
this task by means of a simple ensemble method. To the best of our knowledge,
this is the first distributed, RP based information theoretical image
registration approach. The efficiency of the method is demonstrated through
numerical examples.
"
"  We explore the geometrical interpretation of the PCA based clustering
algorithm Principal Direction Divisive Partitioning (PDDP). We give several
examples where this algorithm breaks down, and suggest a new method, gap
partitioning, which takes into account natural gaps in the data between
clusters. Geometric features of the PCA space are derived and illustrated and
experimental results are given which show our method is comparable on the
datasets used in the original paper on PDDP.
"
"  We propose a method to identify all the nodes that are relevant to compute
all the conditional probability distributions for a given set of nodes. Our
method is simple, effcient, consistent, and does not require learning a
Bayesian network first. Therefore, our method can be applied to
high-dimensional databases, e.g. gene expression databases.
"
"  Rank estimation is a classical model order selection problem that arises in a
variety of important statistical signal and array processing systems, yet is
addressed relatively infrequently in the extant literature. Here we present
sample covariance asymptotics stemming from random matrix theory, and bring
them to bear on the problem of optimal rank estimation in the context of the
standard array observation model with additive white Gaussian noise. The most
significant of these results demonstrates the existence of a phase transition
threshold, below which eigenvalues and associated eigenvectors of the sample
covariance fail to provide any information on population eigenvalues. We then
develop a decision-theoretic rank estimation framework that leads to a simple
ordered selection rule based on thresholding; in contrast to competing
approaches, however, it admits asymptotic minimax optimality and is free of
tuning parameters. We analyze the asymptotic performance of our rank selection
procedure and conclude with a brief simulation study demonstrating its
practical efficacy in the context of subspace tracking.
"
"  The $\ell_1$-penalized method, or the Lasso, has emerged as an important tool
for the analysis of large data sets. Many important results have been obtained
for the Lasso in linear regression which have led to a deeper understanding of
high-dimensional statistical problems. In this article, we consider a class of
weighted $\ell_1$-penalized estimators for convex loss functions of a general
form, including the generalized linear models. We study the estimation,
prediction, selection and sparsity properties of the weighted
$\ell_1$-penalized estimator in sparse, high-dimensional settings where the
number of predictors $p$ can be much larger than the sample size $n$. Adaptive
Lasso is considered as a special case. A multistage method is developed to
apply an adaptive Lasso recursively. We provide $\ell_q$ oracle inequalities, a
general selection consistency theorem, and an upper bound on the dimension of
the Lasso estimator. Important models including the linear regression, logistic
regression and log-linear models are used throughout to illustrate the
applications of the general results.
"
"  We are interested in the online prediction of the electricity load, within
the Bayesian framework of dynamic models. We offer a review of sequential Monte
Carlo methods, and provide the calculations needed for the derivation of
so-called particles filters. We also discuss the practical issues arising from
their use, and some of the variants proposed in the literature to deal with
them, giving detailed algorithms whenever possible for an easy implementation.
We propose an additional step to help make basic particle filters more robust
with regard to outlying observations. Finally we use such a particle filter to
estimate a state-space model that includes exogenous variables in order to
forecast the electricity load for the customers of the French electricity
company \'Electricit\'e de France and discuss the various results obtained.
"
"  We describe novel subgradient methods for a broad class of matrix
optimization problems involving nuclear norm regularization. Unlike existing
approaches, our method executes very cheap iterations by combining low-rank
stochastic subgradients with efficient incremental SVD updates, made possible
by highly optimized and parallelizable dense linear algebra operations on small
matrices. Our practical algorithms always maintain a low-rank factorization of
iterates that can be conveniently held in memory and efficiently multiplied to
generate predictions in matrix completion settings. Empirical comparisons
confirm that our approach is highly competitive with several recently proposed
state-of-the-art solvers for such problems.
"
"  Recent research on multiple kernel learning has lead to a number of
approaches for combining kernels in regularized risk minimization. The proposed
approaches include different formulations of objectives and varying
regularization strategies. In this paper we present a unifying general
optimization criterion for multiple kernel learning and show how existing
formulations are subsumed as special cases. We also derive the criterion's dual
representation, which is suitable for general smooth optimization algorithms.
Finally, we evaluate multiple kernel learning in this framework analytically
using a Rademacher complexity bound on the generalization error and empirically
in a set of experiments.
"
"  Self-organizing maps (SOMs) are a technique that has been used with
high-dimensional data vectors to develop an archetypal set of states (nodes)
that span, in some sense, the high-dimensional space. Noteworthy applications
include weather states as described by weather variables over a region and
speech patterns as characterized by frequencies in time. The SOM approach is
essentially a neural network model that implements a nonlinear projection from
a high-dimensional input space to a low-dimensional array of neurons. In the
process, it also becomes a clustering technique, assigning to any vector in the
high-dimensional data space the node (neuron) to which it is closest (using,
say, Euclidean distance) in the data space. The number of nodes is thus equal
to the number of clusters. However, the primary use for the SOM is as a
representation technique, that is, finding a set of nodes which
representatively span the high-dimensional space. These nodes are typically
displayed using maps to enable visualization of the continuum of the data
space. The technique does not appear to have been discussed in the statistics
literature so it is our intent here to bring it to the attention of the
community. The technique is implemented algorithmically through a training set
of vectors. However, through the introduction of stochasticity in the form of a
space--time process model, we seek to illuminate and interpret its performance
in the context of application to daily data collection. That is, the observed
daily state vectors are viewed as a time series of multivariate process
realizations which we try to understand under the dimension reduction achieved
by the SOM procedure.
"
"  We present a novel method for solving Canonical Correlation Analysis (CCA) in
a sparse convex framework using a least squares approach. The presented method
focuses on the scenario when one is interested in (or limited to) a primal
representation for the first view while having a dual representation for the
second view. Sparse CCA (SCCA) minimises the number of features used in both
the primal and dual projections while maximising the correlation between the
two views. The method is demonstrated on two paired corpuses of English-French
and English-Spanish for mate-retrieval. We are able to observe, in the
mate-retreival, that when the number of the original features is large SCCA
outperforms Kernel CCA (KCCA), learning the common semantic space from a sparse
set of features.
"
"  We assume data independently sampled from a mixture distribution on the unit
ball of the D-dimensional Euclidean space with K+1 components: the first
component is a uniform distribution on that ball representing outliers and the
other K components are uniform distributions along K d-dimensional linear
subspaces restricted to that ball. We study both the simultaneous recovery of
all K underlying subspaces and the recovery of the best l0 subspace (i.e., with
largest number of points) by minimizing the lp-averaged distances of data
points from d-dimensional subspaces of the D-dimensional space. Unlike other lp
minimization problems, this minimization is non-convex for all p>0 and thus
requires different methods for its analysis. We show that if 0<p <= 1, then
both all underlying subspaces and the best l0 subspace can be precisely
recovered by lp minimization with overwhelming probability. This result extends
to additive homoscedastic uniform noise around the subspaces (i.e., uniform
distribution in a strip around them) and near recovery with an error
proportional to the noise level. On the other hand, if K>1 and p>1, then we
show that both all underlying subspaces and the best l0 subspace cannot be
recovered and even nearly recovered. Further relaxations are also discussed. We
use the results of this paper for partially justifying recent effective
algorithms for modeling data by mixtures of multiple subspaces as well as for
discussing the effect of using variants of lp minimizations in RANSAC-type
strategies for single subspace recovery.
"
"  This paper is motivated by a Eurobarometer survey on science knowledge. As
part of the survey, respondents were asked to rank sources of science
information in order of importance. The official statistical analysis of these
data however failed to use the complete ranking information. We instead propose
a method which treats ranked data as a set of paired comparisons which places
the problem in the standard framework of generalized linear models and also
allows respondent covariates to be incorporated. An extension is proposed to
allow for heterogeneity in the ranked responses. The resulting model uses a
nonparametric formulation of the random effects structure, fitted using the EM
algorithm. Each mass point is multivalued, with a parameter for each item. The
resultant model is equivalent to a covariate latent class model, where the
latent class profiles are provided by the mass point components and the
covariates act on the class profiles. This provides an alternative
interpretation of the fitted model. The approach is also suitable for paired
comparison data.
"
"  The belief propagation (BP) algorithm is widely applied to perform
approximate inference on arbitrary graphical models, in part due to its
excellent empirical properties and performance. However, little is known
theoretically about when this algorithm will perform well. Using recent
analysis of convergence and stability properties in BP and new results on
approximations in binary systems, we derive a bound on the error in BP's
estimates for pairwise Markov random fields over discrete valued random
variables. Our bound is relatively simple to compute, and compares favorably
with a previous method of bounding the accuracy of BP.
"
"  In the popular approach of ""Bayesian variable selection"" (BVS), one uses
prior and posterior distributions to select a subset of candidate variables to
enter the model. A completely new direction will be considered here to study
BVS with a Gibbs posterior originating in statistical mechanics. The Gibbs
posterior is constructed from a risk function of practical interest (such as
the classification error) and aims at minimizing a risk function without
modeling the data probabilistically. This can improve the performance over the
usual Bayesian approach, which depends on a probability model which may be
misspecified. Conditions will be provided to achieve good risk performance,
even in the presence of high dimensionality, when the number of candidate
variables ""$K$"" can be much larger than the sample size ""$n$."" In addition, we
develop a convenient Markov chain Monte Carlo algorithm to implement BVS with
the Gibbs posterior.
"
"  To better understand the spatial structure of large panels of economic and
financial time series and provide a guideline for constructing semiparametric
models, this paper first considers estimating a large spatial covariance matrix
of the generalized $m$-dependent and $\beta$-mixing time series (with $J$
variables and $T$ observations) by hard thresholding regularization as long as
${{\log J \, \cx^*(\ct)}}/{T} = \Co(1)$ (the former scheme with some time
dependence measure $\cx^*(\ct)$) or $\log J /{T} = \Co(1)$ (the latter scheme
with some upper bounded mixing coefficient). We quantify the interplay between
the estimators' consistency rate and the time dependence level, discuss an
intuitive resampling scheme for threshold selection, and also prove a general
cross-validation result justifying this. Given a consistently estimated
covariance (correlation) matrix, by utilizing its natural links with graphical
models and semiparametrics, after ""screening"" the (explanatory) variables, we
implement a novel forward (and backward) label permutation procedure to cluster
the ""relevant"" variables and construct the corresponding semiparametric model,
which is further estimated by the groupwise dimension reduction method with
sign constraints. We call this the SCE (screen - cluster - estimate) approach
for modeling high dimensional data with complex spatial structure. Finally we
apply this method to study the spatial structure of large panels of economic
and financial time series and find the proper semiparametric structure for
estimating the consumer price index (CPI) to illustrate its superiority over
the linear models.
"
"  Recently, the market size of online game has been increasing astonishingly
fast, and so does the importance of good game design. In online games, usually
a human user competes with others, so the fairness of the game system to all
users is of great importance not to lose interests of users on the game.
Furthermore, the emergence and success of electronic sports (e-sports) and
professional gaming which specially talented gamers compete with others draws
more attention on whether they are competing in the fair environment. No matter
how fierce the debates are in the game-design community, it is rarely the case
that one employs statistical analysis to answer this question seriously. But
considering the fact that we can easily gather large amount of user behavior
data on games, it seems potentially beneficial to make use of this data to aid
making decisions on design problems of games. Actually, modern games do not aim
to perfectly design the game at once: rather, they first release the game, and
then monitor users' behavior to better balance the game. In such a scenario,
statistical analysis can be particularly helpful. Specifically, we chose to
analyze the balance of StarCraft II, which is a very successful
recently-released real-time strategy (RTS) game. It is a central icon in
current e-Sports and professional gaming community: from April 1st to 15th,
there were 18 tournaments of StarCraft II. However, there is endless debate on
whether the winner of the tournament is actually superior to others, or it is
largely due to certain design flaws of the game. In this paper, we aim to
answer such a question using traditional statistical tool, logistic regression.
"
"  This paper introduces a novel paradigm to impute missing data that combines a
decision tree with an auto-associative neural network (AANN) based model and a
principal component analysis-neural network (PCA-NN) based model. For each
model, the decision tree is used to predict search bounds for a genetic
algorithm that minimize an error function derived from the respective model.
The models' ability to impute missing data is tested and compared using HIV
sero-prevalance data. Results indicate an average increase in accuracy of 13%
with the AANN based model's average accuracy increasing from 75.8% to 86.3%
while that of the PCA-NN based model increasing from 66.1% to 81.6%.
"
"  The goal of predictive sparse coding is to learn a representation of examples
as sparse linear combinations of elements from a dictionary, such that a
learned hypothesis linear in the new representation performs well on a
predictive task. Predictive sparse coding algorithms recently have demonstrated
impressive performance on a variety of supervised tasks, but their
generalization properties have not been studied. We establish the first
generalization error bounds for predictive sparse coding, covering two
settings: 1) the overcomplete setting, where the number of features k exceeds
the original dimensionality d; and 2) the high or infinite-dimensional setting,
where only dimension-free bounds are useful. Both learning bounds intimately
depend on stability properties of the learned sparse encoder, as measured on
the training sample. Consequently, we first present a fundamental stability
result for the LASSO, a result characterizing the stability of the sparse codes
with respect to perturbations to the dictionary. In the overcomplete setting,
we present an estimation error bound that decays as \tilde{O}(sqrt(d k/m)) with
respect to d and k. In the high or infinite-dimensional setting, we show a
dimension-free bound that is \tilde{O}(sqrt(k^2 s / m)) with respect to k and
s, where s is an upper bound on the number of non-zeros in the sparse code for
any training data point.
"
"  The problem of bipartite ranking, where instances are labeled positive or
negative and the goal is to learn a scoring function that minimizes the
probability of mis-ranking a pair of positive and negative instances (or
equivalently, that maximizes the area under the ROC curve), has been widely
studied in recent years. A dominant theoretical and algorithmic framework for
the problem has been to reduce bipartite ranking to pairwise classification; in
particular, it is well known that the bipartite ranking regret can be
formulated as a pairwise classification regret, which in turn can be upper
bounded using usual regret bounds for classification problems. Recently,
Kotlowski et al. (2011) showed regret bounds for bipartite ranking in terms of
the regret associated with balanced versions of the standard (non-pairwise)
logistic and exponential losses. In this paper, we show that such
(non-pairwise) surrogate regret bounds for bipartite ranking can be obtained in
terms of a broad class of proper (composite) losses that we term as strongly
proper. Our proof technique is much simpler than that of Kotlowski et al.
(2011), and relies on properties of proper (composite) losses as elucidated
recently by Reid and Williamson (2010, 2011) and others. Our result yields
explicit surrogate bounds (with no hidden balancing terms) in terms of a
variety of strongly proper losses, including for example logistic, exponential,
squared and squared hinge losses as special cases. We also obtain tighter
surrogate bounds under certain low-noise conditions via a recent result of
Clemencon and Robbiano (2011).
"
"  We propose a simple model for a binary decision making process on a graph,
motivated by modeling social decision making with cooperative individuals. The
model is similar to a random field Ising model or fiber bundle model, but with
key differences on heterogeneous networks. For many types of disorder and
interactions between the nodes, we predict discontinuous phase transitions with
mean field theory which are largely independent of network structure. We show
how these phase transitions can also be understood by studying microscopic
avalanches, and describe how network structure enhances fluctuations in the
distribution of avalanches. We suggest theoretically the existence of a
""glassy"" spectrum of equilibria associated with a typical phase, even on
infinite graphs, so long as the first moment of the degree distribution is
finite. This behavior implies that the model is robust against noise below a
certain scale, and also that phase transitions can switch from discontinuous to
continuous on networks with too few edges. Numerical simulations suggest that
our theory is accurate.
"
"  In this paper we propose a simple yet powerful method for learning
representations in supervised learning scenarios where each original input
datapoint is described by a set of vectors and their associated outputs may be
given by soft labels indicating, for example, class probabilities. We represent
an input datapoint as a mixture of probabilities over the corresponding set of
feature vectors where each probability indicates how likely each vector is to
belong to an unknown prototype pattern. We propose a probabilistic model that
parameterizes these prototype patterns in terms of hidden variables and
therefore it can be trained with conventional approaches based on likelihood
maximization. More importantly, both the model parameters and the prototype
patterns can be learned from data in a discriminative way. We show that our
model can be seen as a probabilistic generalization of learning vector
quantization (LVQ). We apply our method to the problems of shape
classification, hyperspectral imaging classification and people's work class
categorization, showing the superior performance of our method compared to the
standard prototype-based classification approach and other competitive
benchmark methods.
"
"  Optogenetics is a new tool to study neuronal circuits that have been
genetically modified to allow stimulation by flashes of light. We study
recordings from single neurons within neural circuits under optogenetic
stimulation. The data from these experiments present a statistical challenge of
modeling a high frequency point process (neuronal spikes) while the input is
another high frequency point process (light flashes). We further develop a
generalized linear model approach to model the relationships between two point
processes, employing additive point-process response functions. The resulting
model, Point-process Responses for Optogenetics (PRO), provides explicit
nonlinear transformations to link the input point process with the output one.
Such response functions may provide important and interpretable scientific
insights into the properties of the biophysical process that governs neural
spiking in response to optogenetic stimulation. We validate and compare the PRO
model using a real dataset and simulations, and our model yields a superior
area-under-the- curve value as high as 93% for predicting every future spike.
For our experiment on the recurrent layer V circuit in the prefrontal cortex,
the PRO model provides evidence that neurons integrate their inputs in a
sophisticated manner. Another use of the model is that it enables understanding
how neural circuits are altered under various disease conditions and/or
experimental conditions by comparing the PRO parameters.
"
"  We consider statistical analysis of double couple (DC) earthquake focal
mechanism orientation. The symmetry of DC changes with its geometrical
properties, and the number of 3-D rotations one DC source can be transformed
into another depends on its symmetry. Four rotations exist in a general case of
DC with the nodal-plane ambiguity, two transformations if the fault plane is
known, and one rotation if the sides of the fault plane are known. The symmetry
of rotated objects is extensively analyzed in statistical material texture
studies, and we apply their results to analyzing DC orientation. We consider
theoretical probability distributions which can be used to approximate
observational patterns of focal mechanisms. Uniform random rotation
distributions for various DC sources are discussed, as well as two non-uniform
distributions: the rotational Cauchy and von Mises-Fisher. We discuss how
parameters of these rotations can be estimated by a statistical analysis of
earthquake source properties in global seismicity. We also show how earthquake
focal mechanism orientations can be displayed on the Rodrigues vector space.
"
"  Word matches are often used in sequence comparison methods, either as a
measure of sequence similarity or in the first search steps of algorithms such
as BLAST or BLAT. The D2 statistic is the number of matches of words of k
letters between two sequences. Recent advances have been made in the
characterisation of this statistic and in the approximation of its
distribution. Here, these results are extended to the case of approximate word
matches.
  We compute the exact value of the variance of the D2 statistic for the case
of a uniform letter distribution, and introduce a method to provide accurate
approximations of the variance in the remaining cases. This enables the
distribution of D2 to be approximated for typical situations arising in
biological research. We apply these results to the identification of
cis-regulatory modules, and show that this method detects such sequences with a
high accuracy.
  The ability to approximate the distribution of D2 for both exact and
approximate word matches will enable the use of this statistic in a more
precise manner for sequence comparison, database searches, and identification
of transcription factor binding sites.
"
"  We report new statistical time-series analysis tools providing significant
improvements in the rapid, precision extraction of discrete state dynamics from
large databases of experimental observations of molecular machines. By building
physical knowledge and statistical innovations into analysis tools, we
demonstrate new techniques for recovering discrete state transitions buried in
highly correlated molecular noise. We demonstrate the effectiveness of our
approach on simulated and real examples of step-like rotation of the bacterial
flagellar motor and the F1-ATPase enzyme. We show that our method can clearly
identify molecular steps, symmetries and cascaded processes that are too weak
for existing algorithms to detect, and can do so much faster than existing
algorithms. Our techniques represent a major advance in the drive towards
automated, precision, highthroughput studies of molecular machine dynamics.
Modular, open-source software that implements these techniques is provided at
http://www.eng.ox.ac.uk/samp/members/max/software/
"
"  Positive definite matrices abound in a dazzling variety of applications. This
ubiquity can be in part attributed to their rich geometric structure: positive
definite matrices form a self-dual convex cone whose strict interior is a
Riemannian manifold. The manifold view is endowed with a ""natural"" distance
function while the conic view is not. Nevertheless, drawing motivation from the
conic view, we introduce the S-Divergence as a ""natural"" distance-like function
on the open cone of positive definite matrices. We motivate the S-divergence
via a sequence of results that connect it to the Riemannian distance. In
particular, we show that (a) this divergence is the square of a distance; and
(b) that it has several geometric properties similar to those of the Riemannian
distance, though without being computationally as demanding. The S-divergence
is even more intriguing: although nonconvex, we can still compute matrix means
and medians using it to global optimality. We complement our results with some
numerical experiments illustrating our theorems and our optimization algorithm
for computing matrix medians.
"
"  High-throughput biological assays such as microarrays let us ask very
detailed questions about how diseases operate, and promise to let us
personalize therapy. Data processing, however, is often not described well
enough to allow for exact reproduction of the results, leading to exercises in
""forensic bioinformatics"" where aspects of raw data and reported results are
used to infer what methods must have been employed. Unfortunately, poor
documentation can shift from an inconvenience to an active danger when it
obscures not just methods but errors. In this report we examine several related
papers purporting to use microarray-based signatures of drug sensitivity
derived from cell lines to predict patient response. Patients in clinical
trials are currently being allocated to treatment arms on the basis of these
results. However, we show in five case studies that the results incorporate
several simple errors that may be putting patients at risk. One theme that
emerges is that the most common errors are simple (e.g., row or column
offsets); conversely, it is our experience that the most simple errors are
common. We then discuss steps we are taking to avoid such errors in our own
investigations.
"
"  In structural brain networks the connections of interest consist of
white-matter fibre bundles between spatially segregated brain regions. The
presence, location and orientation of these white matter tracts can be derived
using diffusion MRI in combination with probabilistic tractography.
Unfortunately, as of yet no approaches have been suggested that provide an
undisputed way of inferring brain networks from tractography. In this paper, we
provide a computational framework which we refer to as Bayesian connectomics.
Rather than applying an arbitrary threshold to obtain a single network, we
consider the posterior distribution of networks that are supported by the data,
combined with an exponential random graph (ERGM) prior that captures a priori
knowledge concerning the graph-theoretical properties of whole-brain networks.
We show that, on simulated probabilistic tractography data, our approach is
able to reconstruct whole-brain networks. In addition, our approach directly
supports multi-model data fusion and group-level network inference.
"
"  Longitudinal imaging studies are essential to understanding the neural
development of neuropsychiatric disorders, substance use disorders, and the
normal brain. The main objective of this paper is to develop a two-stage
adjusted exponentially tilted empirical likelihood (TETEL) for the spatial
analysis of neuroimaging data from longitudinal studies. The TETEL method as a
frequentist approach allows us to efficiently analyze longitudinal data without
modeling temporal correlation and to classify different time-dependent
covariate types. To account for spatial dependence, the TETEL method developed
here specifically combines all the data in the closest neighborhood of each
voxel (or pixel) on a 3-dimensional (3D) volume (or 2D surface) with
appropriate weights to calculate adaptive parameter estimates and adaptive test
statistics. Simulation studies are used to examine the finite sample
performance of the adjusted exponential tilted likelihood ratio statistic and
TETEL. We demonstrate the application of our statistical methods to the
detection of the difference in the morphological changes of the hippocampus
across time between schizophrenia patients and healthy subjects in a
longitudinal schizophrenia study.
"
"  We discuss a general technique that can be used to form a differentiable
bound on the optima of non-differentiable or discrete objective functions. We
form a unified description of these methods and consider under which
circumstances the bound is concave. In particular we consider two concrete
applications of the method, namely sparse learning and support vector
classification.
"
"  Measuring the intellectual diversity encoded in publication records as a
proxy to the degree of interdisciplinarity has recently received considerable
attention in the science mapping community. The present paper draws upon the
use of the Stirling index as a diversity measure applied to a network model
(customized science map) of research profiles, proposed by several authors. A
modified version of the index is used and compared with the previous versions
on a sample data set in order to rank top Hungarian research organizations
(HROs) according to their research performance diversity. Results, unexpected
in several respects, show that the modified index is a candidate for measuring
the degree of polarization of a research profile. The study also points towards
a possible typology of publication portfolios that instantiate different types
of diversity.
"
"  Sparse linear (or generalized linear) models combine a standard likelihood
function with a sparse prior on the unknown coefficients. These priors can
conveniently be expressed as a maximization over zero-mean Gaussians with
different variance hyperparameters. Standard MAP estimation (Type I) involves
maximizing over both the hyperparameters and coefficients, while an empirical
Bayesian alternative (Type II) first marginalizes the coefficients and then
maximizes over the hyperparameters, leading to a tractable posterior
approximation. The underlying cost functions can be related via a dual-space
framework from Wipf et al. (2011), which allows both the Type I or Type II
objectives to be expressed in either coefficient or hyperparmeter space. This
perspective is useful because some analyses or extensions are more conducive to
development in one space or the other. Herein we consider the estimation of a
trade-off parameter balancing sparsity and data fit. As this parameter is
effectively a variance, natural estimators exist by assessing the problem in
hyperparameter (variance) space, transitioning natural ideas from Type II to
solve what is much less intuitive for Type I. In contrast, for analyses of
update rules and sparsity properties of local and global solutions, as well as
extensions to more general likelihood models, we can leverage coefficient-space
techniques developed for Type I and apply them to Type II. For example, this
allows us to prove that Type II-inspired techniques can be successful
recovering sparse coefficients when unfavorable restricted isometry properties
(RIP) lead to failure of popular L1 reconstructions. It also facilitates the
analysis of Type II when non-Gaussian likelihood models lead to intractable
integrations.
"
"  Whether a dwarf spheroidal galaxy is in equilibrium or being tidally
disrupted by the Milky Way is an important question for the study of its dark
matter content and distribution. This question is investigated using 328 recent
observations from the dwarf spheroidal Leo I. For Leo I, tidal disruption is
detected, at least for stars sufficiently far from the center, but the effect
appears to be quite modest. Statistical tools include isotonic and split point
estimators, asymptotic theory, and resampling methods.
"
"  In the field of quality of health care measurement, one approach to assessing
patient sickness at admission involves a logistic regression of mortality
within 30 days of admission on a fairly large number of sickness indicators (on
the order of 100) to construct a sickness scale, employing classical variable
selection methods to find an ``optimal'' subset of 10--20 indicators. Such
``benefit-only'' methods ignore the considerable differences among the sickness
indicators in cost of data collection, an issue that is crucial when admission
sickness is used to drive programs (now implemented or under consideration in
several countries, including the U.S. and U.K.) that attempt to identify
substandard hospitals by comparing observed and expected mortality rates (given
admission sickness). When both data-collection cost and accuracy of prediction
of 30-day mortality are considered, a large variable-selection problem arises
in which costly variables that do not predict well enough should be omitted
from the final scale. In this paper (a) we develop a method for solving this
problem based on posterior model odds, arising from a prior distribution that
(1) accounts for the cost of each variable and (2) results in a set of
posterior model probabilities that corresponds to a generalized cost-adjusted
version of the Bayesian information criterion (BIC), and (b) we compare this
method with a decision-theoretic cost-benefit approach based on maximizing
expected utility. We use reversible-jump Markov chain Monte Carlo (RJMCMC)
methods to search the model space, and we check the stability of our findings
with two variants of the MCMC model composition ($\mathit{MC}^3$) algorithm.
"
"  This paper develops projection pursuit for discrete data using the discrete
Radon transform. Discrete projection pursuit is presented as an exploratory
method for finding informative low dimensional views of data such as binary
vectors, rankings, phylogenetic trees or graphs. We show that for most data
sets, most projections are close to uniform. Thus, informative summaries are
ones deviating from uniformity. Syllabic data from several of Plato's great
works is used to illustrate the methods. Along with some basic distribution
theory, an automated procedure for computing informative projections is
introduced.
"
"  A new segmented compressed sampling method for analog-to-information
conversion (AIC) is proposed. An analog signal measured by a number of parallel
branches of mixers and integrators (BMIs), each characterized by a specific
random sampling waveform, is first segmented in time into $M$ segments. Then
the sub-samples collected on different segments and different BMIs are reused
so that a larger number of samples than the number of BMIs is collected. This
technique is shown to be equivalent to extending the measurement matrix, which
consists of the BMI sampling waveforms, by adding new rows without actually
increasing the number of BMIs. We prove that the extended measurement matrix
satisfies the restricted isometry property with overwhelming probability if the
original measurement matrix of BMI sampling waveforms satisfies it. We also
show that the signal recovery performance can be improved significantly if our
segmented AIC is used for sampling instead of the conventional AIC. Simulation
results verify the effectiveness of the proposed segmented compressed sampling
method and the validity of our theoretical studies.
"
"  The problem of learning forest-structured discrete graphical models from
i.i.d. samples is considered. An algorithm based on pruning of the Chow-Liu
tree through adaptive thresholding is proposed. It is shown that this algorithm
is both structurally consistent and risk consistent and the error probability
of structure learning decays faster than any polynomial in the number of
samples under fixed model size. For the high-dimensional scenario where the
size of the model d and the number of edges k scale with the number of samples
n, sufficient conditions on (n,d,k) are given for the algorithm to satisfy
structural and risk consistencies. In addition, the extremal structures for
learning are identified; we prove that the independent (resp. tree) model is
the hardest (resp. easiest) to learn using the proposed algorithm in terms of
error rates for structure learning.
"
"  Computational approaches to transcription factor binding site identification
have been actively researched for the past decade.
  Negative examples have long been utilized in de novo motif discovery and have
been shown useful in transcription factor binding site search as well.
  However, understanding of the roles of negative examples in binding site
search is still very limited.
  We propose the 2-centroid and optimal discriminating vector methods, taking
into account negative examples. Cross-validation results on E. coli
transcription factors show that the proposed methods benefit from negative
examples, outperforming the centroid and position-specific scoring matrix
methods. We further show that our proposed methods perform better than a
state-of-the-art method. We characterize the proposed methods in the context of
the other compared methods and show that, coupled with motif subtype
identification, the proposed methods can be effectively applied to a wide range
of transcription factors. Finally, we argue that the proposed methods are
well-suited for eukaryotic transcription factors as well.
  Software tools are available at: http://biogrid.engr.uconn.edu/tfbs_search/.
"
"  Many complex disease syndromes such as asthma consist of a large number of
highly related, rather than independent, clinical phenotypes, raising a new
technical challenge in identifying genetic variations associated simultaneously
with correlated traits. In this study, we propose a new statistical framework
called graph-guided fused lasso (GFlasso) to address this issue in a principled
way. Our approach explicitly represents the dependency structure among the
quantitative traits as a network, and leverages this trait network to encode
structured regularizations in a multivariate regression model over the
genotypes and traits, so that the genetic markers that jointly influence
subgroups of highly correlated traits can be detected with high sensitivity and
specificity. While most of the traditional methods examined each phenotype
independently and combined the results afterwards, our approach analyzes all of
the traits jointly in a single statistical method, and borrow information
across correlated phenotypes to discover the genetic markers that perturbe a
subset of correlated triats jointly rather than a single trait. Using simulated
datasets based on the HapMap consortium data and an asthma dataset, we compare
the performance of our method with the single-marker analysis, and other sparse
regression methods such as the ridge regression and the lasso that do not use
any structural information in the traits. Our results show that there is a
significant advantage in detecting the true causal SNPs when we incorporate the
correlation pattern in traits using our proposed methods.
"
"  We compare a sample-free method proposed by Gargiulo et al. (2010) and a
sample-based method proposed by Ye et al. (2009) for generating a synthetic
population, organised in households, from various statistics. We generate a
reference population for a French region including 1310 municipalities and
measure how both methods approximate it from a set of statistics dervied from
this reference population. We also perform sensitivity analysis. The
sample-free method better fits the reference distributions of both individuals
and households. It is also less data demanding but it requires more
pre-processing. The quality of the results for the sample-based method is
highly dependent on the quality of the initial sample.
"
"  Kernel methods are successful approaches for different machine learning
problems. This success is mainly rooted in using feature maps and kernel
matrices. Some methods rely on the eigenvalues/eigenvectors of the kernel
matrix, while for other methods the spectral information can be used to
estimate the excess risk. An important question remains on how close the sample
eigenvalues/eigenvectors are to the population values. In this paper, we
improve earlier results on concentration bounds for eigenvalues of general
kernel matrices. For distance and inner product kernel functions, e.g. radial
basis functions, we provide new concentration bounds, which are characterized
by the eigenvalues of the sample covariance matrix. Meanwhile, the obstacles
for sharper bounds are accounted for and partially addressed. As a case study,
we derive a concentration inequality for sample kernel target-alignment.
"
"  We obtain a tight distribution-specific characterization of the sample
complexity of large-margin classification with L_2 regularization: We introduce
the \gamma-adapted-dimension, which is a simple function of the spectrum of a
distribution's covariance matrix, and show distribution-specific upper and
lower bounds on the sample complexity, both governed by the
\gamma-adapted-dimension of the source distribution. We conclude that this new
quantity tightly characterizes the true sample complexity of large-margin
classification. The bounds hold for a rich family of sub-Gaussian
distributions.
"
"  To evaluate the calibration of a disease risk prediction tool, the quantity
$E/O$, i.e., the ratio of the expected number of events to the observed number
of events, is generally computed. However, because of censoring, or more
precisely because of individuals who drop out before the termination of the
study, this quantity is generally unavailable for the complete population study
and an alternative estimate has to be computed. In this paper, we present and
compare four methods to do this. We show that two of the most commonly used
methods generally lead to biased estimates. Our arguments are first based on
some theoretic considerations. Then, we perform a simulation study to highlight
the magnitude of the previously mentioned biases. As a concluding example, we
evaluate the calibration of an existing predictive model for breast cancer on
the E3N-EPIC cohort.
"
"  In this paper we apply survival techniques to soccer data, treating a goal
scoring as the event of interest. It specifically concerns the relationship
between the time of the first goal in the game and the time of the second goal.
In order to do so, the relevant survival analysis concepts are readjusted to
fit the problem and a Cox model is developed for the hazard function.
Attributes such as time dependent covariates and a frailty term are also being
considered. We also use a reliable propensity score to summarize the pre-game
covariates. The conclusions derived from the results are that a first goal
occurrence could either expedite or impede the next goal scoring, depending on
the time it was scored. Moreover, once a goal is scored, another goal scoring
become more and more likely as the game progresses. Furthermore, the first goal
effect is the same whether the goal was scored or conceded.
"
"  This paper considers the problem of completing a matrix with many missing
entries under the assumption that the columns of the matrix belong to a union
of multiple low-rank subspaces. This generalizes the standard low-rank matrix
completion problem to situations in which the matrix rank can be quite high or
even full rank. Since the columns belong to a union of subspaces, this problem
may also be viewed as a missing-data version of the subspace clustering
problem. Let X be an n x N matrix whose (complete) columns lie in a union of at
most k subspaces, each of rank <= r < n, and assume N >> kn. The main result of
the paper shows that under mild assumptions each column of X can be perfectly
recovered with high probability from an incomplete version so long as at least
CrNlog^2(n) entries of X are observed uniformly at random, with C>1 a constant
depending on the usual incoherence conditions, the geometrical arrangement of
subspaces, and the distribution of columns over the subspaces. The result is
illustrated with numerical experiments and an application to Internet distance
matrix completion and topology identification.
"
"  The Classification Literature Automated Search Service, an annual
bibliography based on citation of one or more of a set of around 80 book or
journal publications, ran from 1972 to 2012. We analyze here the years 1994 to
2011. The Classification Society's Service, as it was termed, has been produced
by the Classification Society. In earlier decades it was distributed as a
diskette or CD with the Journal of Classification. Among our findings are the
following: an enormous increase in scholarly production post approximately
2000; a very major increase in quantity, coupled with work in different
disciplines, from approximately 2004; and a major shift also from cluster
analysis in earlier times having mathematics and psychology as disciplines of
the journals published in, and affiliations of authors, contrasted with, in
more recent times, a ""centre of gravity"" in management and engineering.
"
"  Random projections have been applied in many machine learning algorithms.
However, whether margin is preserved after random projection is non-trivial and
not well studied. In this paper we analyse margin distortion after random
projection, and give the conditions of margin preservation for binary
classification problems. We also extend our analysis to margin for multiclass
problems, and provide theoretical bounds on multiclass margin on the projected
data.
"
"  For a number of reasons, computational intelligence and machine learning
methods have been largely dismissed by the professional community. The reasons
for this are numerous and varied, but inevitably amongst the reasons given is
that the systems designed often do not perform as expected by their designers.
The reasons for this lack of performance is a direct result of mistakes that
are commonly seen in market-prediction systems. This paper examines some of the
more common mistakes, namely dataset insufficiency; inappropriate scaling;
time-series tracking; inappropriate target quantification and inappropriate
measures of performance. The rationale that leads to each of these mistakes is
examined, as well as the nature of the errors they introduce to the analysis /
design. Alternative ways of performing each task are also recommended in order
to avoid perpetuating these mistakes, and hopefully to aid in clearing the way
for the use of these powerful techniques in industry.
"
"  Spectral clustering is one of the most widely used techniques for extracting
the underlying global structure of a data set. Compressed sensing and matrix
completion have emerged as prevailing methods for efficiently recovering sparse
and partially observed signals respectively. We combine the distance preserving
measurements of compressed sensing and matrix completion with the power of
robust spectral clustering. Our analysis provides rigorous bounds on how small
errors in the affinity matrix can affect the spectral coordinates and
clusterability. This work generalizes the current perturbation results of
two-class spectral clustering to incorporate multi-class clustering with k
eigenvectors. We thoroughly track how small perturbation from using compressed
sensing and matrix completion affect the affinity matrix and in succession the
spectral coordinates. These perturbation results for multi-class clustering
require an eigengap between the kth and (k+1)th eigenvalues of the affinity
matrix, which naturally occurs in data with k well-defined clusters. Our
theoretical guarantees are complemented with numerical results along with a
number of examples of the unsupervised organization and clustering of image
data.
"
"  The Artificial Prediction Market is a recent machine learning technique for
multi-class classification, inspired from the financial markets. It involves a
number of trained market participants that bet on the possible outcomes and are
rewarded if they predict correctly. This paper generalizes the scope of the
Artificial Prediction Markets to regression, where there are uncountably many
possible outcomes and the error is usually the MSE. For that, we introduce the
reward kernel that rewards each participant based on its prediction error and
we derive the price equations. Using two reward kernels we obtain two different
learning rules, one of which is approximated using Hermite-Gauss quadrature.
The market setting makes it easy to aggregate specialized regressors that only
predict when an observation falls into their specialization domain. Experiments
show that regression markets based on the two learning rules outperform Random
Forest Regression on many UCI datasets and are rarely outperformed.
"
"  Sensor systems typically operate under resource constraints that prevent the
simultaneous use of all resources all of the time. Sensor management becomes
relevant when the sensing system has the capability of actively managing these
resources; i.e., changing its operating configuration during deployment in
reaction to previous measurements. Examples of systems in which sensor
management is currently used or is likely to be used in the near future include
autonomous robots, surveillance and reconnaissance networks, and waveform-agile
radars. This paper provides an overview of the theory, algorithms, and
applications of sensor management as it has developed over the past decades and
as it stands today.
"
"  Any limiting point process for the time normalized exceedances of high levels
by a stationary sequence is necessarily compound Poisson under appropriate long
range dependence conditions. Typically exceedances appear in clusters. The
underlying Poisson points represent the cluster positions and the
multiplicities correspond to the cluster sizes. In the present paper we
introduce estimators of the limiting cluster size probabilities, which are
constructed through a recursive algorithm. We derive estimators of the extremal
index which plays a key role in determining the intensity of cluster positions.
We study the asymptotic properties of the estimators and investigate their
finite sample behavior on simulated data.
"
"  We present a new Markov chain Monte Carlo method for estimating posterior
probabilities of structural features in Bayesian networks. The method draws
samples from the posterior distribution of partial orders on the nodes; for
each sampled partial order, the conditional probabilities of interest are
computed exactly. We give both analytical and empirical results that suggest
the superiority of the new method compared to previous methods, which sample
either directed acyclic graphs or linear orders on the nodes.
"
"  Gaussian factor models have proven widely useful for parsimoniously
characterizing dependence in multivariate data. There is a rich literature on
their extension to mixed categorical and continuous variables, using latent
Gaussian variables or through generalized latent trait models acommodating
measurements in the exponential family. However, when generalizing to
non-Gaussian measured variables the latent variables typically influence both
the dependence structure and the form of the marginal distributions,
complicating interpretation and introducing artifacts. To address this problem
we propose a novel class of Bayesian Gaussian copula factor models which
decouple the latent factors from the marginal distributions. A semiparametric
specification for the marginals based on the extended rank likelihood yields
straightforward implementation and substantial computational gains, critical
for scaling to high-dimensional applications. We provide new theoretical and
empirical justifications for using this likelihood in Bayesian inference. We
propose new default priors for the factor loadings and develop efficient
parameter-expanded Gibbs sampling for posterior computation. The methods are
evaluated through simulations and applied to a dataset in political science.
The methods in this paper are implemented in the R package bfa.
"
"  Data mining involves the systematic analysis of large data sets, and data
mining in agricultural soil datasets is exciting and modern research area. The
productive capacity of a soil depends on soil fertility. Achieving and
maintaining appropriate levels of soil fertility, is of utmost importance if
agricultural land is to remain capable of nourishing crop production. In this
research, Steps for building a predictive model of soil fertility have been
explained.
  This paper aims at predicting soil fertility class using decision tree
algorithms in data mining . Further, it focuses on performance tuning of J48
decision tree algorithm with the help of meta-techniques such as attribute
selection and boosting.
"
"  Our work is motivated by and illustrated with application of association
networks in computational biology, specifically in the context of gene/protein
regulatory networks. Association networks represent systems of interacting
elements, where a link between two different elements indicates a sufficient
level of similarity between element attributes. While in reality relational
ties between elements can be expected to be based on similarity across multiple
attributes, the vast majority of work to date on association networks involves
ties defined with respect to only a single attribute. We propose an approach
for the inference of multi-attribute association networks from measurements on
continuous attribute variables, using canonical correlation and a
hypothesis-testing strategy. Within this context, we then study the impact of
partial information on multi-attribute network inference and characterization,
when only a subset of attributes is available. We consider in detail the case
of two attributes, wherein we examine through a combination of analytical and
numerical techniques the implications of the choice and number of node
attributes on the ability to detect network links and, more generally, to
estimate higher-level network summary statistics, such as node degree,
clustering coefficients, and measures of centrality. Illustration and
applications throughout the paper are developed using gene and protein
expression measurements on human cancer cell lines from the NCI-60 database.
"
"  This article provides a brief introduction to seven papers that are included
in this special section on Statistics in Neuroscience: (1) Xiaoyan Shi, Joseph
G. Ibrahim, Jeffrey Lieberman, Martin Styner, Yimei Li and Hongtu Zhu:
Two-state empirical likelihood for longitudinal neuroimaging data (2) Vincent
Q. Vu, Pradeep Ravikumar, Thomas Naselaris, Kendrick N. Kay, Jack L. Gallant
and Bin Yu: Encoding and decoding V1 fMRI responses to natural images with
sparse nonparametric models (3) Sourabh Bhattacharya and Ranjan Maitra: A
nonstationary nonparametric Bayesian approach to dynamically modeling effective
connectivity in functional magnetic resonance imaging experiments (4)
Christopher J. Long, Patrick L. Purdon, Simona Temereanca, Neil U. Desai, Matti
S. H\""{a}m\""{a}l\""{a}inen and Emery Neal Brown: State-space solutions to the
dynamic magnetoencephalography inverse problem using high performance computing
(5) Yuriy Mishchencko, Joshua T. Vogelstein and Liam Paninski: A Bayesian
approach for inferring neuronal connectivity from calcium fluorescent imaging
data (6) Robert E. Kass, Ryan C. Kelly and Wei-Liem Loh: Assessment of
synchrony in multiple neural spike trains using loglinear point process models
(7) Sofia Olhede and Brandon Whitcher: Nonparametric tests of structure for
high angular resolution diffusion imaging in Q-space
"
"  The theoretical basis for a candidate variational principle for the
information bottleneck (IB) method is formulated within the ambit of the
generalized nonadditive statistics of Tsallis. Given a nonadditivity parameter
$ q $, the role of the \textit{additive duality} of nonadditive statistics ($
q^*=2-q $) in relating Tsallis entropies for ranges of the nonadditivity
parameter $ q < 1 $ and $ q > 1 $ is described. Defining $ X $, $ \tilde X $,
and $ Y $ to be the source alphabet, the compressed reproduction alphabet, and,
the \textit{relevance variable} respectively, it is demonstrated that
minimization of a generalized IB (gIB) Lagrangian defined in terms of the
nonadditivity parameter $ q^* $ self-consistently yields the
\textit{nonadditive effective distortion measure} to be the \textit{$ q
$-deformed} generalized Kullback-Leibler divergence: $
D_{K-L}^{q}[p(Y|X)||p(Y|\tilde X)] $. This result is achieved without enforcing
any \textit{a-priori} assumptions. Next, it is proven that the $q^*-deformed $
nonadditive free energy of the system is non-negative and convex. Finally, the
update equations for the gIB method are derived. These results generalize
critical features of the IB method to the case of Tsallis statistics.
"
"  Environmental research increasingly uses high-dimensional remote sensing and
numerical model output to help fill space-time gaps between traditional
observations. Such output is often a noisy proxy for the process of interest.
Thus one needs to separate and assess the signal and noise (often called
discrepancy) in the proxy given complicated spatio-temporal dependencies. Here
I extend a popular two-likelihood hierarchical model using a more flexible
representation for the discrepancy. I employ the little-used Markov random
field approximation to a thin plate spline, which can capture small-scale
discrepancy in a computationally efficient manner while better modeling smooth
processes than standard conditional auto-regressive models. The increased
flexibility reduces identifiability, but the lack of identifiability is
inherent in the scientific context. I model particulate matter air pollution
using satellite aerosol and atmospheric model output proxies. The estimated
discrepancies occur at a variety of spatial scales, with small-scale
discrepancy particularly important. The examples indicate little predictive
improvement over modeling the observations alone. Similarly, in simulations
with an informative proxy, the presence of discrepancy and resulting
identifiability issues prevent improvement in prediction. The results highlight
but do not resolve the critical question of how best to use proxy information
while minimizing the potential for proxy-induced error.
"
"  The generation interval is the time between the infection time of an infected
person and the infection time of his or her infector. Probability density
functions for generation intervals have been an important input for epidemic
models and epidemic data analysis. In this paper, we specify a general
stochastic SIR epidemic model and prove that the mean generation interval
decreases when susceptible persons are at risk of infectious contact from
multiple sources. The intuition behind this is that when a susceptible person
has multiple potential infectors, there is a ``race'' to infect him or her in
which only the first infectious contact leads to infection. In an epidemic, the
mean generation interval contracts as the prevalence of infection increases. We
call this global competition among potential infectors. When there is rapid
transmission within clusters of contacts, generation interval contraction can
be caused by a high local prevalence of infection even when the global
prevalence is low. We call this local competition among potential infectors.
Using simulations, we illustrate both types of competition.
  Finally, we show that hazards of infectious contact can be used instead of
generation intervals to estimate the time course of the effective reproductive
number in an epidemic. This approach leads naturally to partial likelihoods for
epidemic data that are very similar to those that arise in survival analysis,
opening a promising avenue of methodological research in infectious disease
epidemiology.
"
"  Predicting user affinity to items is an important problem in applications
like content optimization, computational advertising, and many more. While
bilinear random effect models (matrix factorization) provide state-of-the-art
performance when minimizing RMSE through a Gaussian response model on explicit
ratings data, applying it to imbalanced binary response data presents
additional challenges that we carefully study in this paper. Data in many
applications usually consist of users' implicit response that are often binary
-- clicking an item or not; the goal is to predict click rates, which is often
combined with other measures to calculate utilities to rank items at runtime of
the recommender systems. Because of the implicit nature, such data are usually
much larger than explicit rating data and often have an imbalanced distribution
with a small fraction of click events, making accurate click rate prediction
difficult. In this paper, we address two problems. First, we show previous
techniques to estimate bilinear random effect models with binary data are less
accurate compared to our new approach based on adaptive rejection sampling,
especially for imbalanced response. Second, we develop a parallel bilinear
random effect model fitting framework using Map-Reduce paradigm that scales to
massive datasets. Our parallel algorithm is based on a ""divide and conquer""
strategy coupled with an ensemble approach. Through experiments on the
benchmark MovieLens data, a small Yahoo! Front Page data set, and a large
Yahoo! Front Page data set that contains 8M users and 1B binary observations,
we show that careful handling of binary response as well as identifiability
issues are needed to achieve good performance for click rate prediction, and
that the proposed adaptive rejection sampler and the partitioning as well as
ensemble techniques significantly improve model performance.
"
"  Identifying differentially expressed (DE) genes associated with a sample
characteristic is the primary objective of many microarray studies. As more and
more studies are carried out with observational rather than well controlled
experimental samples, it becomes important to evaluate and properly control the
impact of sample heterogeneity on DE gene finding. Typical methods for
identifying DE genes require ranking all the genes according to a preselected
statistic based on a single model for two or more group comparisons, with or
without adjustment for other covariates. Such single model approaches
unavoidably result in model misspecification, which can lead to increased error
due to bias for some genes and reduced efficiency for the others. We evaluated
the impact of model misspecification from such approaches on detecting DE genes
and identified parameters that affect the magnitude of impact. To properly
control for sample heterogeneity and to provide a flexible and coherent
framework for identifying simultaneously DE genes associated with a single or
multiple sample characteristics and/or their interactions, we proposed a
Bayesian model averaging approach which corrects the model misspecification by
averaging over model space formed by all relevant covariates. An empirical
approach is suggested for specifying prior model probabilities. We demonstrated
through simulated microarray data that this approach resulted in improved
performance in DE gene identification compared to the single model approaches.
The flexibility of this approach is demonstrated through our analysis of data
from two observational microarray studies.
"
"  Synthetic indices are used in Economics to measure various aspects of
monetary inequalities. These scalar indices take as input the distribution over
a finite population, for example the population of a specific country. In this
article we consider the case of the French 2004 Wealth survey. We have at hand
a partial measurement on the distribution of interest consisting of bracketed
and sometimes missing data, over a subsample of the population of interest. We
present in this article the statistical methodology used to obtain point and
interval estimates taking into account the various uncertainties. The
inequality indices being nonlinear in the input distribution, we rely on a
simulation based approach where the model for the wealth per household is
multivariate. Using the survey data as well as matched auxiliary tax
declarations data, we have at hand a quite intricate non-rectangle
multidimensional censoring. For practical issues we use a Bayesian approach.
Inference using Monte-Carlo approximations relies on a Monte-Carlo Markov chain
algorithm namely the Gibbs sampler. The quantities interesting to the decision
maker are taken to be the various inequality indices for the French population.
Their distribution conditional on the data of the subsample are assumed to be
normal centered on the design-based estimates with variance computed through
linearization and taking into account the sample design and total nonresponse.
Exogeneous selection of the subsample, in particular the nonresponse mechanism,
is assumed and we condition on the adequate covariates.
"
"  Gaussian processes retain the linear model either as a special case, or in
the limit. We show how this relationship can be exploited when the data are at
least partially linear. However from the perspective of the Bayesian posterior,
the Gaussian processes which encode the linear model either have probability of
nearly zero or are otherwise unattainable without the explicit construction of
a prior with the limiting linear model in mind. We develop such a prior, and
show that its practical benefits extend well beyond the computational and
conceptual simplicity of the linear model. For example, linearity can be
extracted on a per-dimension basis, or can be combined with treed partition
models to yield a highly efficient nonstationary model. Our approach is
demonstrated on synthetic and real datasets of varying linearity and
dimensionality.
"
"  Different directed acyclic graphs (DAGs) may be Markov equivalent in the
sense that they entail the same conditional independence relations among the
observed variables. Meek (1995) characterizes Markov equivalence classes for
DAGs (with no latent variables) by presenting a set of orientation rules that
can correctly identify all arrow orientations shared by all DAGs in a Markov
equivalence class, given a member of that class. For DAG models with latent
variables, maximal ancestral graphs (MAGs) provide a neat representation that
facilitates model search. Earlier work (Ali et al. 2005) has identified a set
of orientation rules sufficient to construct all arrowheads common to a Markov
equivalence class of MAGs. In this paper, we provide extra rules sufficient to
construct all common tails as well. We end up with a set of orientation rules
sound and complete for identifying commonalities across a Markov equivalence
class of MAGs, which is particularly useful for causal inference.
"
"  We propose a general matrix-valued multiple kernel learning framework for
high-dimensional nonlinear multivariate regression problems. This framework
allows a broad class of mixed norm regularizers, including those that induce
sparsity, to be imposed on a dictionary of vector-valued Reproducing Kernel
Hilbert Spaces. We develop a highly scalable and eigendecomposition-free
algorithm that orchestrates two inexact solvers for simultaneously learning
both the input and output components of separable matrix-valued kernels. As a
key application enabled by our framework, we show how high-dimensional causal
inference tasks can be naturally cast as sparse function estimation problems,
leading to novel nonlinear extensions of a class of Graphical Granger Causality
techniques. Our algorithmic developments and extensive empirical studies are
complemented by theoretical analyses in terms of Rademacher generalization
bounds.
"
"  In this work we examine the properties of a recently described ordinary
differential equation that relates the age-specific prevalence of a chronic
disease with the incidence and mortalities of the diseased and healthy persons.
The equation has been used to estimate the incidence from prevalence data,
which is an inverse problem. The ill-posedness of this problem is proven, too.
"
"  LSTD is a popular algorithm for value function approximation. Whenever the
number of features is larger than the number of samples, it must be paired with
some form of regularization. In particular, L1-regularization methods tend to
perform feature selection by promoting sparsity, and thus, are well-suited for
high-dimensional problems. However, since LSTD is not a simple regression
algorithm, but it solves a fixed--point problem, its integration with
L1-regularization is not straightforward and might come with some drawbacks
(e.g., the P-matrix assumption for LASSO-TD). In this paper, we introduce a
novel algorithm obtained by integrating LSTD with the Dantzig Selector. We
investigate the performance of the proposed algorithm and its relationship with
the existing regularized approaches, and show how it addresses some of their
drawbacks.
"
"  Principal component analysis (PCA) is widely used for dimensionality
reduction, with well-documented merits in various applications involving
high-dimensional data, including computer vision, preference measurement, and
bioinformatics. In this context, the fresh look advocated here permeates
benefits from variable selection and compressive sampling, to robustify PCA
against outliers. A least-trimmed squares estimator of a low-rank bilinear
factor analysis model is shown closely related to that obtained from an
$\ell_0$-(pseudo)norm-regularized criterion encouraging sparsity in a matrix
explicitly modeling the outliers. This connection suggests robust PCA schemes
based on convex relaxation, which lead naturally to a family of robust
estimators encompassing Huber's optimal M-class as a special case. Outliers are
identified by tuning a regularization parameter, which amounts to controlling
sparsity of the outlier matrix along the whole robustification path of (group)
least-absolute shrinkage and selection operator (Lasso) solutions. Beyond its
neat ties to robust statistics, the developed outlier-aware PCA framework is
versatile to accommodate novel and scalable algorithms to: i) track the
low-rank signal subspace robustly, as new data are acquired in real time; and
ii) determine principal components robustly in (possibly) infinite-dimensional
feature spaces. Synthetic and real data tests corroborate the effectiveness of
the proposed robust PCA schemes, when used to identify aberrant responses in
personality assessment surveys, as well as unveil communities in social
networks, and intruders from video surveillance data.
"
"  In this paper we examine the effect of applying ensemble learning to the
performance of collaborative filtering methods. We present several systematic
approaches for generating an ensemble of collaborative filtering models based
on a single collaborative filtering algorithm (single-model or homogeneous
ensemble). We present an adaptation of several popular ensemble techniques in
machine learning for the collaborative filtering domain, including bagging,
boosting, fusion and randomness injection. We evaluate the proposed approach on
several types of collaborative filtering base models: k- NN, matrix
factorization and a neighborhood matrix factorization model. Empirical
evaluation shows a prediction improvement compared to all base CF algorithms.
In particular, we show that the performance of an ensemble of simple (weak) CF
models such as k-NN is competitive compared with a single strong CF model (such
as matrix factorization) while requiring an order of magnitude less
computational cost.
"
"  In mixed strategy 2\times2 population games, the realization of maximum
entropy (Maxent) is of the theoretical expectation. We evaluate this
theoretical prediction in the experimental economics game data. The data
includes 12 treatments and 108 experimental sessions in which the random match
human subjects pairs make simultaneous strategy moves repeated 200 rounds. Main
results are (1) We confirm that experimental entropy value fit the prediction
from Maxent well; and (2) In small proportion samples, distributions are
deviated from Maxent expectations; interesting is that, the deviated patterns
are significant more concentrated. These experimental findings could enhance
the understanding of social game behavior with the natural science rule ---
Maxent.
"
"  We propose robust methods for inference on the effect of a treatment variable
on a scalar outcome in the presence of very many controls. Our setting is a
partially linear model with possibly non-Gaussian and heteroscedastic
disturbances. Our analysis allows the number of controls to be much larger than
the sample size. To make informative inference feasible, we require the model
to be approximately sparse; that is, we require that the effect of confounding
factors can be controlled for up to a small approximation error by conditioning
on a relatively small number of controls whose identities are unknown. The
latter condition makes it possible to estimate the treatment effect by
selecting approximately the right set of controls. We develop a novel
estimation and uniformly valid inference method for the treatment effect in
this setting, called the ""post-double-selection"" method. Our results apply to
Lasso-type methods used for covariate selection as well as to any other model
selection method that is able to find a sparse model with good approximation
properties.
  The main attractive feature of our method is that it allows for imperfect
selection of the controls and provides confidence intervals that are valid
uniformly across a large class of models. In contrast, standard post-model
selection estimators fail to provide uniform inference even in simple cases
with a small, fixed number of controls. Thus our method resolves the problem of
uniform inference after model selection for a large, interesting class of
models. We illustrate the use of the developed methods with numerical
simulations and an application to the effect of abortion on crime rates.
"
"  We address discrete-marks survival analysis, also known as categorical sieve
analysis, for a setting of a randomized placebo-controlled treatment
intervention to prevent infection by a pathogen to which multiple exposures are
possible, with a finite number of types of ""failure"". In particular, we address
the case of interventions that are partially efficacious due to a combination
of failure-type-dependent efficacy and subject-dependent efficacy, for an
intervention that is ""non-leaky"" (where ""leaky"" interventions are those for
which each exposure event has a chance of resulting in a ""failure"" outcome, so
multiple exposures to pathogens of a single type increase the chance of
failure). We introduce the notion of some-or-none interventions, which are
completely effective only against some of the failure types, and are completely
ineffective against the others. Under conditions of no intervention-induced
failures, we introduce a framework and Bayesian and frequentist methods to
detect and quantify the extent to which an intervention's partial efficacy is
attributable to uneven efficacy across the failure types rather than to
incomplete ""take"" of the intervention. These new methods provide more power
than existing methods to detect sieve effects when the conditions hold. We
demonstrate the new framework and methods with simulation results and new
analyses of genomic signatures of HIV-1 vaccine effects in the STEP and RV144
vaccine efficacy trials.
"
"  This paper deals with phase II, univariate, statistical process control when
a set of in-control data is available, and when both the in-control and
out-of-control distributions of the process are unknown. Existing process
control techniques typically require substantial knowledge about the in-control
and out-of-control distributions of the process, which is often difficult to
obtain in practice. We propose (a) using a sequence of control limits for the
cumulative sum (CUSUM) control charts, where the control limits are determined
by the conditional distribution of the CUSUM statistic given the last time it
was zero, and (b) estimating the control limits by bootstrap. Traditionally,
the CUSUM control chart uses a single control limit, which is obtained under
the assumption that the in-control and out-of-control distributions of the
process are Normal. When the normality assumption is not valid, which is often
true in applications, the actual in-control average run length, defined to be
the expected time duration before the control chart signals a process change,
is quite different from the nominal in-control average run length. This
limitation is mostly eliminated in the proposed procedure, which is
distribution-free and robust against different choices of the in-control and
out-of-control distributions.
"
"  Importance sampling is a promising variance reduction technique for Monte
Carlo simulation based derivative pricing. Existing importance sampling methods
are based on a parametric choice of the proposal. This article proposes an
algorithm that estimates the optimal proposal nonparametrically using a
multivariate frequency polygon estimator. In contrast to parametric methods,
nonparametric estimation allows for close approximation of the optimal
proposal. Standard nonparametric importance sampling is inefficient for
high-dimensional problems. We solve this issue by applying the procedure to a
low-dimensional subspace, which is identified through principal component
analysis and the concept of the effective dimension. The mean square error
properties of the algorithm are investigated and its asymptotic optimality is
shown. Quasi-Monte Carlo is used for further improvement of the method. It is
easy to implement, particularly it does not require any analytical computation,
and it is computationally very efficient. We demonstrate through path-dependent
and multi-asset option pricing problems that the algorithm leads to significant
efficiency gains compared to other algorithms in the literature.
"
"  Exponential family extensions of principal component analysis (EPCA) have
received a considerable amount of attention in recent years, demonstrating the
growing need for basic modeling tools that do not assume the squared loss or
Gaussian distribution. We extend the EPCA model toolbox by presenting the first
exponential family multi-view learning methods of the partial least squares and
canonical correlation analysis, based on a unified representation of EPCA as
matrix factorization of the natural parameters of exponential family. The
models are based on a new family of priors that are generally usable for all
such factorizations. We also introduce new inference strategies, and
demonstrate how the methods outperform earlier ones when the Gaussianity
assumption does not hold.
"
"  Statistical methodology is proposed for comparing unlabeled marked point
sets, with an application to aligning steroid molecules in chemoinformatics.
Methods from statistical shape analysis are combined with techniques for
predicting random fields in spatial statistics in order to define a suitable
measure of similarity between two marked point sets. Bayesian modeling of the
predicted field overlap between pairs of point sets is proposed, and posterior
inference of the alignment is carried out using Markov chain Monte Carlo
simulation. By representing the fields in reproducing kernel Hilbert spaces,
the degree of overlap can be computed without expensive numerical integration.
Superimposing entire fields rather than the configuration matrices of point
coordinates thereby avoids the problem that there is usually no clear
one-to-one correspondence between the points. In addition, mask parameters are
introduced in the model, so that partial matching of the marked point sets can
be carried out. We also propose an adaptation of the generalized Procrustes
analysis algorithm for the simultaneous alignment of multiple point sets. The
methodology is illustrated with a simulation study and then applied to a data
set of 31 steroid molecules, where the relationship between shape and binding
activity to the corticosteroid binding globulin receptor is explored.
"
"  A prototypical blind signal separation problem is the so-called cocktail
party problem, with n people talking simultaneously and n different microphones
within a room. The goal is to recover each speech signal from the microphone
inputs. Mathematically this can be modeled by assuming that we are given
samples from an n-dimensional random variable X=AS, where S is a vector whose
coordinates are independent random variables corresponding to each speaker. The
objective is to recover the matrix A^{-1} given random samples from X. A range
of techniques collectively known as Independent Component Analysis (ICA) have
been proposed to address this problem in the signal processing and machine
learning literature. Many of these techniques are based on using the kurtosis
or other cumulants to recover the components.
  In this paper we propose a new algorithm for solving the blind signal
separation problem in the presence of additive Gaussian noise, when we are
given samples from X=AS+\eta, where \eta is drawn from an unknown, not
necessarily spherical n-dimensional Gaussian distribution. Our approach is
based on a method for decorrelating a sample with additive Gaussian noise under
the assumption that the underlying distribution is a linear transformation of a
distribution with independent components. Our decorrelation routine is based on
the properties of cumulant tensors and can be combined with any standard
cumulant-based method for ICA to get an algorithm that is provably robust in
the presence of Gaussian noise. We derive polynomial bounds for the sample
complexity and error propagation of our method.
"
"  Properties of data are frequently seen to vary depending on the sampled
situations, which usually changes along a time evolution or owing to
environmental effects. One way to analyze such data is to find invariances, or
representative features kept constant over changes. The aim of this paper is to
identify one such feature, namely interactions or dependencies among variables
that are common across multiple datasets collected under different conditions.
To that end, we propose a common substructure learning (CSSL) framework based
on a graphical Gaussian model. We further present a simple learning algorithm
based on the Dual Augmented Lagrangian and the Alternating Direction Method of
Multipliers. We confirm the performance of CSSL over other existing techniques
in finding unchanging dependency structures in multiple datasets through
numerical simulations on synthetic data and through a real world application to
anomaly detection in automobile sensors.
"
"  We study the problem of reconstructing an unknown matrix M of rank r and
dimension d using O(rd poly log d) Pauli measurements. This has applications in
quantum state tomography, and is a non-commutative analogue of a well-known
problem in compressed sensing: recovering a sparse vector from a few of its
Fourier coefficients.
  We show that almost all sets of O(rd log^6 d) Pauli measurements satisfy the
rank-r restricted isometry property (RIP). This implies that M can be recovered
from a fixed (""universal"") set of Pauli measurements, using nuclear-norm
minimization (e.g., the matrix Lasso), with nearly-optimal bounds on the error.
A similar result holds for any class of measurements that use an orthonormal
operator basis whose elements have small operator norm. Our proof uses Dudley's
inequality for Gaussian processes, together with bounds on covering numbers
obtained via entropy duality.
"
"  Missing data is an important challenge when dealing with high dimensional
data arranged in the form of an array. In this paper, we propose methods for
estimation of the parameters of array variate normal probability model from
partially observed multiway data. The methods developed here are useful for
missing data imputation, estimation of mean and covariance parameters for
multiway data. A multiway semi-parametric mixed effects model that allows
separation of multiway covariance effects is also defined and an efficient
algorithm for estimation is recommended. We provide simulation results along
with real life data from genetics to demonstrate these methods.
"
"  A new type of statistical analysis of the science and technical information
(STI) in the Web context is produced. We propose a set of indicators about Web
users, visualized bibliographic records, and e-commercial transactions. In
addition, we introduce two Web usage factors. Finally, we give an overview of
the co-usage analysis. For these tasks, we introduce a computer based system,
called Miri@d, which produces descriptive statistical information about the Web
users' searching behaviour, and what is effectively used from a free access
digital bibliographical database. The system is conceived as a server of
statistical data which are carried out beforehand, and as an interactive server
for online statistical work. The results will be made available to analysts,
who can use this descriptive statistical information as raw data for their
indicator design tasks, and as input for multivariate data analysis, clustering
analysis, and mapping. Managers also can exploit the results in order to
improve management and decision-making.
"
"  The main goal of this paper is an application of Bayesian inference in
testing the relation between risk and return on the financial instruments. On
the basis of the Intertemporal CAPM model we built a general sampling model
suitable in analysing such a relationship. The most important feature of our
assumptions is that the skewness of the conditional distribution of returns is
used as an alternative source of relation between risk and return. This general
specification relates to GARCH-In-Mean model. In order to make conditional
distribution of financial returns skewed we considered a constructive approach
based on the inverse probability integral transformation. In particular, we
apply the hidden truncation mechanism, two equivalent approaches of the inverse
scale factors, order statistics concept, Beta and Bernstein distribution
transformations, and also the constructive method. Based on the daily excess
returns on the Warsaw Stock Exchange Index we checked the empirical importance
of the conditional skewness assumption on the relation between risk and return
on the Warsaw Stock Market. We present posterior probabilities of all competing
specifications as well as the posterior analysis of positive sign of the tested
relationship.
"
"  In the series of our earlier papers on the subject, we proposed a novel
statistical hypothesis testing method for detection of objects in noisy images.
The method uses results from percolation theory and random graph theory. We
developed algorithms that allowed to detect objects of unknown shapes in the
presence of nonparametric noise of unknown level and of unknown distribution.
No boundary shape constraints were imposed on the objects, only a weak bulk
condition for the object's interior was required. Our algorithms have linear
complexity and exponential accuracy. In the present paper, we describe an
implementation of our nonparametric hypothesis testing method. We provide a
program that can be used for statistical experiments in image processing. This
program is written in the statistical programming language R.
"
"  The compound models of clutter statistics are found suitable to describe the
nonstationary nature of radar backscattering from high-resolution observations.
In this letter, we show that the properties of Mellin transform can be utilized
to generate higher order moments of simple and compound models of clutter
statistics in a compact manner
"
"  We refine a stimulating study by Sarvotham et al. [2005] which highlighted
the influence of peak transmission rate on network burstiness. From TCP packet
headers, we amalgamate packets into sessions where each session is
characterized by a 5-tuple (S, D, R, Peak R, Initiation T)=(total payload,
duration, average transmission rate, peak transmission rate, initiation time).
After careful consideration, a new definition of peak rate is required. Unlike
Sarvotham et al. [2005] who segmented sessions into two groups labelled alpha
and beta, we segment into 10 sessions according to the empirical quantiles of
the peak rate variable as a demonstration that the beta group is far from
homogeneous. Our more refined segmentation reveals additional structure that is
missed by segmentation into two groups. In each segment, we study the
dependence structure of (S, D, R) and find that it varies across the groups.
Furthermore, within each segment, session initiation times are well
approximated by a Poisson process whereas this property does not hold for the
data set taken as a whole. Therefore, we conclude that the peak rate level is
important for understanding structure and for constructing accurate simulations
of data in the wild. We outline a simple method of simulating network traffic
based on our findings.
"
"  Fisher's linear discriminant analysis (FLDA) is an important dimension
reduction method in statistical pattern recognition. It has been shown that
FLDA is asymptotically Bayes optimal under the homoscedastic Gaussian
assumption. However, this classical result has the following two major
limitations: 1) it holds only for a fixed dimensionality $D$, and thus does not
apply when $D$ and the training sample size $N$ are proportionally large; 2) it
does not provide a quantitative description on how the generalization ability
of FLDA is affected by $D$ and $N$. In this paper, we present an asymptotic
generalization analysis of FLDA based on random matrix theory, in a setting
where both $D$ and $N$ increase and $D/N\longrightarrow\gamma\in[0,1)$. The
obtained lower bound of the generalization discrimination power overcomes both
limitations of the classical result, i.e., it is applicable when $D$ and $N$
are proportionally large and provides a quantitative description of the
generalization ability of FLDA in terms of the ratio $\gamma=D/N$ and the
population discrimination power. Besides, the discrimination power bound also
leads to an upper bound on the generalization error of binary-classification
with FLDA.
"
"  The entanglement between two arbitrary subsystems of random pure states is
studied via properties of the density matrix's partial transpose,
$\rho_{12}^{T_2}$. The density of states of $\rho_{12}^{T_2}$ is close to the
semicircle law when both subsystems have dimensions which are not too small and
are of the same order. A simple random matrix model for the partial transpose
is found to capture the entanglement properties well, including a transition
across a critical dimension. Log-negativity is used to quantify entanglement
between subsystems and analytic formulas for this are derived based on the
simple model. The skewness of the eigenvalue density of $\rho_{12}^{T_2}$ is
derived analytically, using the average of the third moment over the ensemble
of random pure states. The third moment after partial transpose is also shown
to be related to a generalization of the Kempe invariant. The smallest
eigenvalue after partial transpose is found to follow the extreme value
statistics of random matrices, namely the Tracy-Widom distribution. This
distribution, with relevant parameters obtained from the model, is found to be
useful in calculating the fraction of entangled states at critical dimensions.
These results are tested in a quantum dynamical system of three coupled
standard maps, where one finds that if the parameters represent a strongly
chaotic system, the results are close to those of random states, although there
are some systematic deviations at critical dimensions.
"
"  Social advertising uses information about consumers' peers, including peer
affiliations with a brand, product, organization, etc., to target ads and
contextualize their display. This approach can increase ad efficacy for two
main reasons: peers' affiliations reflect unobserved consumer characteristics,
which are correlated along the social network; and the inclusion of social cues
(i.e., peers' association with a brand) alongside ads affect responses via
social influence processes. For these reasons, responses may be increased when
multiple social signals are presented with ads, and when ads are affiliated
with peers who are strong, rather than weak, ties.
  We conduct two very large field experiments that identify the effect of
social cues on consumer responses to ads, measured in terms of ad clicks and
the formation of connections with the advertised entity. In the first
experiment, we randomize the number of social cues present in word-of-mouth
advertising, and measure how responses increase as a function of the number of
cues. The second experiment examines the effect of augmenting traditional ad
units with a minimal social cue (i.e., displaying a peer's affiliation below an
ad in light grey text). On average, this cue causes significant increases in ad
performance. Using a measurement of tie strength based on the total amount of
communication between subjects and their peers, we show that these influence
effects are greatest for strong ties. Our work has implications for ad
optimization, user interface design, and central questions in social science
research.
"
"  This work provides a computationally efficient and statistically consistent
moment-based estimator for mixtures of spherical Gaussians. Under the condition
that component means are in general position, a simple spectral decomposition
technique yields consistent parameter estimates from low-order observable
moments, without additional minimum separation assumptions needed by previous
computationally efficient estimation procedures. Thus computational and
information-theoretic barriers to efficient estimation in mixture models are
precluded when the mixture components have means in general position and
spherical covariances. Some connections are made to estimation problems related
to independent component analysis.
"
"  In the paper, we consider the problem of link prediction in time-evolving
graphs. We assume that certain graph features, such as the node degree, follow
a vector autoregressive (VAR) model and we propose to use this information to
improve the accuracy of prediction. Our strategy involves a joint optimization
procedure over the space of adjacency matrices and VAR matrices which takes
into account both sparsity and low rank properties of the matrices. Oracle
inequalities are derived and illustrate the trade-offs in the choice of
smoothing parameters when modeling the joint effect of sparsity and low rank
property. The estimate is computed efficiently using proximal methods through a
generalized forward-backward agorithm.
"
"  Modern time-domain surveys continuously monitor large swaths of the sky to
look for astronomical variability. Astrophysical discovery in such data sets is
complicated by the fact that detections of real transient and variable sources
are highly outnumbered by bogus detections caused by imperfect subtractions,
atmospheric effects and detector artefacts. In this work we present a machine
learning (ML) framework for discovery of variability in time-domain imaging
surveys. Our ML methods provide probabilistic statements, in near real time,
about the degree to which each newly observed source is astrophysically
relevant source of variable brightness. We provide details about each of the
analysis steps involved, including compilation of the training and testing
sets, construction of descriptive image-based and contextual features, and
optimization of the feature subset and model tuning parameters. Using a
validation set of nearly 30,000 objects from the Palomar Transient Factory, we
demonstrate a missed detection rate of at most 7.7% at our chosen
false-positive rate of 1% for an optimized ML classifier of 23 features,
selected to avoid feature correlation and over-fitting from an initial library
of 42 attributes. Importantly, we show that our classification methodology is
insensitive to mis-labelled training data up to a contamination of nearly 10%,
making it easier to compile sufficient training sets for accurate performance
in future surveys. This ML framework, if so adopted, should enable the
maximization of scientific gain from future synoptic survey and enable fast
follow-up decisions on the vast amounts of streaming data produced by such
experiments.
"
"  Some astronomy projects require a blind search through a vast number of
hypotheses to detect objects of interest. The number of hypotheses to test can
be in the billions. A naive blind search over every single hypothesis would be
far too costly computationally. We propose a hierarchical scheme for blind
search, using various ""resolution"" levels. At lower resolution levels,
""regions"" of interest in the search space are singled out with a low
computational cost. These regions are refined at intermediate resolution levels
and only the most promising candidates are finally tested at the original fine
resolution. The optimal search strategy is found by dynamic programming. We
demonstrate the procedure for pulsar search from satellite gamma-ray
observations and show that the power of the naive blind search can almost be
matched with the hierarchical scheme while reducing the computational burden by
more than three orders of magnitude.
"
"  In a recent presentation at the 17th International Conference on Science and
Technology Indicators, Schneider (2012) criticised the proposal of Bornmann, de
Moya Anegon, and Leydesdorff (2012) and Leydesdorff and Bornmann (2012) to use
statistical tests in order to evaluate research assessments and university
rankings. We agree with Schneider's proposal to add statistical power analysis
and effect size measures to research evaluations, but disagree that these
procedures would replace significance testing. Accordingly, effect size
measures were added to the Excel sheets that we bring online for testing
performance differences between institutions in the Leiden Ranking and the
SCImago Institutions Ranking.
"
"  We give an overview of several aspects arising in the statistical analysis of
extreme risks with actuarial applications in view. In particular it is
demonstrated that empirical process theory is a very powerful tool, both for
the asymptotic analysis of extreme value estimators and to devise tools for the
validation of the underlying model assumptions. While the focus of the paper is
on univariate tail risk analysis, the basic ideas of the analysis of the
extremal dependence between different risks are also outlined. Here we
emphasize some of the limitation of classical multivariate extreme value theory
and sketch how a different model proposed by Ledford and Tawn can help to avoid
pitfalls. Finally, these theoretical results are used to analyze a data set of
large claim sizes from health insurance.
"
"  Following an article by Muller and Pflug, we study the adjustment coefficient
of ruin theory in a context of temporal dependency. We provide a consistent
estimator of this coefficient, and perform some simulations.
"
"  A new framework based on the theory of copulas is proposed to address semi-
supervised domain adaptation problems. The presented method factorizes any
multivariate density into a product of marginal distributions and bivariate
cop- ula functions. Therefore, changes in each of these factors can be detected
and corrected to adapt a density model accross different learning domains.
Impor- tantly, we introduce a novel vine copula model, which allows for this
factorization in a non-parametric manner. Experimental results on regression
problems with real-world data illustrate the efficacy of the proposed approach
when compared to state-of-the-art techniques.
"
"  As a person learns a new skill, distinct synapses, brain regions, and
circuits are engaged and change over time. In this paper, we develop methods to
examine patterns of correlated activity across a large set of brain regions.
Our goal is to identify properties that enable robust learning of a motor
skill. We measure brain activity during motor sequencing and characterize
network properties based on coherent activity between brain regions. Using
recently developed algorithms to detect time-evolving communities, we find that
the complex reconfiguration patterns of the brain's putative functional modules
that control learning can be described parsimoniously by the combined presence
of a relatively stiff temporal core that is composed primarily of sensorimotor
and visual regions whose connectivity changes little in time and a flexible
temporal periphery that is composed primarily of multimodal association regions
whose connectivity changes frequently. The separation between temporal core and
periphery changes over the course of training and, importantly, is a good
predictor of individual differences in learning success. The core of
dynamically stiff regions exhibits dense connectivity, which is consistent with
notions of core-periphery organization established previously in social
networks. Our results demonstrate that core-periphery organization provides an
insightful way to understand how putative functional modules are linked. This,
in turn, enables the prediction of fundamental human capacities, including the
production of complex goal-directed behavior.
"
"  Overview of inference methods in PLoM.
"
"  In this paper we relate the partition function to the max-statistics of
random variables. In particular, we provide a novel framework for approximating
and bounding the partition function using MAP inference on randomly perturbed
models. As a result, we can use efficient MAP solvers such as graph-cuts to
evaluate the corresponding partition function. We show that our method excels
in the typical ""high signal - high coupling"" regime that results in ragged
energy landscapes difficult for alternative approaches.
"
"  Penalized regression is an attractive framework for variable selection
problems. Often, variables possess a grouping structure, and the relevant
selection problem is that of selecting groups, not individual variables. The
group lasso has been proposed as a way of extending the ideas of the lasso to
the problem of group selection. Nonconvex penalties such as SCAD and MCP have
been proposed and shown to have several advantages over the lasso; these
penalties may also be extended to the group selection problem, giving rise to
group SCAD and group MCP methods. Here, we describe algorithms for fitting
these models stably and efficiently. In addition, we present simulation results
and real data examples comparing and contrasting the statistical properties of
these methods.
"
"  In this paper we present a family of algorithms that can simultaneously align
and cluster sets of multidimensional curves measured on a discrete time grid.
Our approach is based on a generative mixture model that allows non-linear time
warping of the observed curves relative to the mean curves within the clusters.
We also allow for arbitrary discrete-valued translation of the time axis,
random real-valued offsets of the measured curves, and additive measurement
noise. The resulting model can be viewed as a dynamic Bayesian network with a
special transition structure that allows effective inference and learning. The
Expectation-Maximization (EM) algorithm can be used to simultaneously recover
both the curve models for each cluster, and the most likely time warping,
translation, offset, and cluster membership for each curve. We demonstrate how
Bayesian estimation methods improve the results for smaller sample sizes by
enforcing smoothness in the cluster mean curves. We evaluate the methodology on
two real-world data sets, and show that the DBN models provide systematic
improvements in predictive power over competing approaches.
"
"  The problem of active diagnosis arises in several applications such as
disease diagnosis, and fault diagnosis in computer networks, where the goal is
to rapidly identify the binary states of a set of objects (e.g., faulty or
working) by sequentially selecting, and observing, (noisy) responses to binary
valued queries. Current algorithms in this area rely on loopy belief
propagation for active query selection. These algorithms have an exponential
time complexity, making them slow and even intractable in large networks. We
propose a rank-based greedy algorithm that sequentially chooses queries such
that the area under the ROC curve of the rank-based output is maximized. The
AUC criterion allows us to make a simplifying assumption that significantly
reduces the complexity of active query selection (from exponential to near
quadratic), with little or no compromise on the performance quality.
"
"  The statistical tests that are commonly used for detecting mean or median
treatment effects suffer from low power when the two distribution functions
differ only in the upper (or lower) tail, as in the assessment of the Total
Sharp Score (TSS) under different treatments for rheumatoid arthritis. In this
article, we propose a more powerful test that detects treatment effects through
the expected shortfalls. We show how the expected shortfall can be adjusted for
covariates, and demonstrate that the proposed test can achieve a substantial
sample size reduction over the conventional tests on the mean effects.
"
"  The Burning Index (BI) produced daily by the United States government's
National Fire Danger Rating System is commonly used in forecasting the hazard
of wildfire activity in the United States. However, recent evaluations have
shown the BI to be less effective at predicting wildfires in Los Angeles
County, compared to simple point process models incorporating similar
meteorological information. Here, we explore the forecasting power of a suite
of more complex point process models that use seasonal wildfire trends, daily
and lagged weather variables, and historical spatial burn patterns as
covariates, and that interpolate the records from different weather stations.
Results are compared with models using only the BI. The performance of each
model is compared by Akaike Information Criterion (AIC), as well as by the
power in predicting wildfires in the historical data set and residual analysis.
We find that multiplicative models that directly use weather variables offer
substantial improvement in fit compared to models using only the BI, and, in
particular, models where a distinct spatial bandwidth parameter is estimated
for each weather station appear to offer substantially improved fit.
"
"  Networks or graphs can easily represent a diverse set of data sources that
are characterized by interacting units or actors. Social networks, representing
people who communicate with each other, are one example. Communities or
clusters of highly connected actors form an essential feature in the structure
of several empirical networks. Spectral clustering is a popular and
computationally feasible method to discover these communities. The stochastic
blockmodel [Social Networks 5 (1983) 109--137] is a social network model with
well-defined communities; each node is a member of one community. For a network
generated from the Stochastic Blockmodel, we bound the number of nodes
""misclustered"" by spectral clustering. The asymptotic results in this paper are
the first clustering results that allow the number of clusters in the model to
grow with the number of nodes, hence the name high-dimensional. In order to
study spectral clustering under the stochastic blockmodel, we first show that
under the more general latent space model, the eigenvectors of the normalized
graph Laplacian asymptotically converge to the eigenvectors of a ""population""
normalized graph Laplacian. Aside from the implication for spectral clustering,
this provides insight into a graph visualization technique. Our method of
studying the eigenvectors of random matrices is original.
"
"  We analyze convergence rates of stochastic optimization procedures for
non-smooth convex optimization problems. By combining randomized smoothing
techniques with accelerated gradient methods, we obtain convergence rates of
stochastic optimization procedures, both in expectation and with high
probability, that have optimal dependence on the variance of the gradient
estimates. To the best of our knowledge, these are the first variance-based
rates for non-smooth optimization. We give several applications of our results
to statistical estimation problems, and provide experimental results that
demonstrate the effectiveness of the proposed algorithms. We also describe how
a combination of our algorithm with recent work on decentralized optimization
yields a distributed stochastic optimization algorithm that is order-optimal.
"
"  This technical note considers the problems of blind sparse learning and
inference of electrogram (EGM) signals under atrial fibrillation (AF)
conditions. First of all we introduce a mathematical model for the observed
signals that takes into account the multiple foci typically appearing inside
the heart during AF. Then we propose a reconstruction model based on a fixed
dictionary and discuss several alternatives for choosing the dictionary. In
order to obtain a sparse solution that takes into account the biological
restrictions of the problem, a first alternative is using LASSO regularization
followed by a post-processing stage that removes low amplitude coefficients
violating the refractory period characteristic of cardiac cells. As an
alternative we propose a novel regularization term, called cross products LASSO
(CP-LASSO), that is able to incorporate the biological constraints directly
into the optimization problem. Unfortunately, the resulting problem is
non-convex, but we show how it can be solved efficiently in an approximated way
making use of successive convex approximations (SCA). Finally, spectral
analysis is performed on the clean activation sequence obtained from the sparse
learning stage in order to estimate the number of latent foci and their
frequencies. Simulations on synthetic and real data are provided to validate
the proposed approach.
"
"  Multiclass prediction is the problem of classifying an object into a relevant
target class. We consider the problem of learning a multiclass predictor that
uses only few features, and in particular, the number of used features should
increase sub-linearly with the number of possible classes. This implies that
features should be shared by several classes. We describe and analyze the
ShareBoost algorithm for learning a multiclass predictor that uses few shared
features. We prove that ShareBoost efficiently finds a predictor that uses few
shared features (if such a predictor exists) and that it has a small
generalization error. We also describe how to use ShareBoost for learning a
non-linear predictor that has a fast evaluation time. In a series of
experiments with natural data sets we demonstrate the benefits of ShareBoost
and evaluate its success relatively to other state-of-the-art approaches.
"
"  A number of problems in a variety of fields are characterised by target
distributions with a multimodal structure in which the presence of several
isolated local maxima dramatically reduces the efficiency of Markov Chain Monte
Carlo sampling algorithms. Several solutions, such as simulated tempering or
the use of parallel chains, have been proposed to facilitate the exploration of
the relevant parameter space. They provide effective strategies in the cases in
which the dimension of the parameter space is small and/or the computational
costs are not a limiting factor. These approaches fail however in the case of
high-dimensional spaces where the multimodal structure is induced by
degeneracies between regions of the parameter space. In this paper we present a
fully Markovian way to efficiently sample this kind of distribution based on
the general Delayed Rejection scheme with an arbitrary number of steps, and
provide details for an efficient numerical implementation of the algorithm.
"
"  Visual reranking is effective to improve the performance of the text-based
video search. However, existing reranking algorithms can only achieve limited
improvement because of the well-known semantic gap between low level visual
features and high level semantic concepts. In this paper, we adopt interactive
video search reranking to bridge the semantic gap by introducing user's
labeling effort. We propose a novel dimension reduction tool, termed sparse
transfer learning (STL), to effectively and efficiently encode user's labeling
information. STL is particularly designed for interactive video search
reranking. Technically, it a) considers the pair-wise discriminative
information to maximally separate labeled query relevant samples from labeled
query irrelevant ones, b) achieves a sparse representation for the subspace to
encodes user's intention by applying the elastic net penalty, and c) propagates
user's labeling information from labeled samples to unlabeled samples by using
the data distribution knowledge. We conducted extensive experiments on the
TRECVID 2005, 2006 and 2007 benchmark datasets and compared STL with popular
dimension reduction algorithms. We report superior performance by using the
proposed STL based interactive video search reranking.
"
"  This paper introduces a privacy-aware Bayesian approach that combines
ensembles of classifiers and clusterers to perform semi-supervised and
transductive learning. We consider scenarios where instances and their
classification/clustering results are distributed across different data sites
and have sharing restrictions. As a special case, the privacy aware computation
of the model when instances of the target data are distributed across different
data sites, is also discussed. Experimental results show that the proposed
approach can provide good classification accuracies while adhering to the
data/model sharing constraints.
"
"  DNA Copy number variation (CNV) has recently gained considerable interest as
a source of genetic variation that likely influences phenotypic differences.
Many statistical and computational methods have been proposed and applied to
detect CNVs based on data that generated by genome analysis platforms. However,
most algorithms are computationally intensive with complexity at least
$O(n^2)$, where $n$ is the number of probes in the experiments. Moreover, the
theoretical properties of those existing methods are not well understood. A
faster and better characterized algorithm is desirable for the ultra high
throughput data. In this study, we propose the Screening and Ranking algorithm
(SaRa) which can detect CNVs fast and accurately with complexity down to O(n).
In addition, we characterize theoretical properties and present numerical
analysis for our algorithm.
"
"  Many businesses are using recommender systems for marketing outreach.
Recommendation algorithms can be either based on content or driven by
collaborative filtering. We study different ways to incorporate content
information directly into the matrix factorization approach of collaborative
filtering. These content-boosted matrix factorization algorithms not only
improve recommendation accuracy, but also provide useful insights about the
contents, as well as make recommendations more easily interpretable.
"
"  Previous work on recommender systems mainly focus on fitting the ratings
provided by users. However, the response patterns, i.e., some items are rated
while others not, are generally ignored. We argue that failing to observe such
response patterns can lead to biased parameter estimation and sub-optimal model
performance. Although several pieces of work have tried to model users'
response patterns, they miss the effectiveness and interpretability of the
successful matrix factorization collaborative filtering approaches. To bridge
the gap, in this paper, we unify explicit response models and PMF to establish
the Response Aware Probabilistic Matrix Factorization (RAPMF) framework. We
show that RAPMF subsumes PMF as a special case. Empirically we demonstrate the
merits of RAPMF from various aspects.
"
"  We attempt to set a mathematical foundation of immunology and amino acid
chains. To measure the similarities of these chains, a kernel on strings is
defined using only the sequence of the chains and a good amino acid
substitution matrix (e.g. BLOSUM62). The kernel is used in learning machines to
predict binding affinities of peptides to human leukocyte antigens DR (HLA-DR)
molecules. On both fixed allele (Nielsen and Lund 2009) and pan-allele (Nielsen
et.al. 2010) benchmark databases, our algorithm achieves the state-of-the-art
performance. The kernel is also used to define a distance on an HLA-DR allele
set based on which a clustering analysis precisely recovers the serotype
classifications assigned by WHO (Nielsen and Lund 2009, and Marsh et.al. 2010).
These results suggest that our kernel relates well the chain structure of both
peptides and HLA-DR molecules to their biological functions, and that it offers
a simple, powerful and promising methodology to immunology and amino acid chain
studies.
"
"  This paper considers the problem of robust subspace recovery: given a set of
$N$ points in $\mathbb{R}^D$, if many lie in a $d$-dimensional subspace, then
can we recover the underlying subspace? We show that Tyler's M-estimator can be
used to recover the underlying subspace, if the percentage of the inliers is
larger than $d/D$ and the data points lie in general position. Empirically,
Tyler's M-estimator compares favorably with other convex subspace recovery
algorithms in both simulations and experiments on real data sets.
"
"  Bob predicts a future observation based on a sample of size one. Alice can
draw a sample of any size before issuing her prediction. How much better can
she do than Bob? Perhaps surprisingly, under a large class of loss functions,
which we refer to as the Cover-Hart family, the best Alice can do is to halve
Bob's risk. In this sense, half the information in an infinite sample is
contained in a sample of size one. The Cover-Hart family is a convex cone that
includes metrics and negative definite functions, subject to slight regularity
conditions. These results may help explain the small relative differences in
empirical performance measures in applied classification and forecasting
problems, as well as the success of reasoning and learning by analogy in
general, and nearest neighbor techniques in particular.
"
"  Understanding the factors driving innovation in energy technologies is of
critical importance to mitigating climate change and addressing other
energy-related global challenges. Low levels of innovation, measured in terms
of energy patent filings, were noted in the 1980s and 90s as an issue of
concern and were attributed to low investment in public and private research
and development (R&D). Here we build a comprehensive global database of energy
patents covering the period 1970-2009 which is unique in its temporal and
geographical scope. Analysis of the data reveals a recent, marked departure
from historical trends. A sharp increase in rates of patenting has occurred
over the last decade, particularly in renewable technologies, despite continued
low levels of R&D funding. To solve the puzzle of fast innovation despite
modest R&D increases we develop a model that explains the nonlinear response
observed in the empirical data of technological innovation to various types of
investment. The model reveals a regular relationship between patents, R&D
funding, and growing markets across technologies, and accurately predicts
patenting rates at different stages of technological maturity and market
development. We show quantitatively how growing markets have formed a vital
complement to public R&D in driving innovative activity; these two forms of
investment have each leveraged the effect of the other in driving patenting
trends over long periods of time.
"
"  The key limiting factor in graphical model inference and learning is the
complexity of the partition function. We thus ask the question: what are
general conditions under which the partition function is tractable? The answer
leads to a new kind of deep architecture, which we call sum-product networks
(SPNs). SPNs are directed acyclic graphs with variables as leaves, sums and
products as internal nodes, and weighted edges. We show that if an SPN is
complete and consistent it represents the partition function and all marginals
of some graphical model, and give semantics to its nodes. Essentially all
tractable graphical models can be cast as SPNs, but SPNs are also strictly more
general. We then propose learning algorithms for SPNs, based on backpropagation
and EM. Experiments show that inference and learning with SPNs can be both
faster and more accurate than with standard deep networks. For example, SPNs
perform image completion better than state-of-the-art deep networks for this
task. SPNs also have intriguing potential connections to the architecture of
the cortex.
"
"  We present a probabilistic model for natural images which is based on
Gaussian scale mixtures and a simple multiscale representation. In contrast to
the dominant approach to modeling whole images focusing on Markov random
fields, we formulate our model in terms of a directed graphical model. We show
that it is able to generate images with interesting higher-order correlations
when trained on natural images or samples from an occlusion based model. More
importantly, the directed model enables us to perform a principled evaluation.
While it is easy to generate visually appealing images, we demonstrate that our
model also yields the best performance reported to date when evaluated with
respect to the cross-entropy rate, a measure tightly linked to the average
log-likelihood.
"
"  The majority of real-world networks are dynamic and extremely large (e.g.,
Internet Traffic, Twitter, Facebook, ...). To understand the structural
behavior of nodes in these large dynamic networks, it may be necessary to model
the dynamics of behavioral roles representing the main connectivity patterns
over time. In this paper, we propose a dynamic behavioral mixed-membership
model (DBMM) that captures the roles of nodes in the graph and how they evolve
over time. Unlike other node-centric models, our model is scalable for
analyzing large dynamic networks. In addition, DBMM is flexible,
parameter-free, has no functional form or parameterization, and is
interpretable (identifies explainable patterns). The performance results
indicate our approach can be applied to very large networks while the
experimental results show that our model uncovers interesting patterns
underlying the dynamics of these networks.
"
"  Local Linear embedding (LLE) is a popular dimension reduction method. In this
paper, we first show LLE with nonnegative constraint is equivalent to the
widely used Laplacian embedding. We further propose to iterate the two steps in
LLE repeatedly to improve the results. Thirdly, we relax the kNN constraint of
LLE and present a sparse similarity learning algorithm. The final Iterative LLE
combines these three improvements. Extensive experiment results show that
iterative LLE algorithm significantly improve both classification and
clustering results.
"
"  Conformal predictors, introduced by Vovk et al. (2005), serve to build
prediction intervals by exploiting a notion of conformity of the new data point
with previously observed data. In the present paper, we propose a novel method
for constructing prediction intervals for the response variable in multivariate
linear models. The main emphasis is on sparse linear models, where only few of
the covariates have significant influence on the response variable even if
their number is very large. Our approach is based on combining the principle of
conformal prediction with the $\ell_1$ penalized least squares estimator
(LASSO). The resulting confidence set depends on a parameter $\epsilon>0$ and
has a coverage probability larger than or equal to $1-\epsilon$. The numerical
experiments reported in the paper show that the length of the confidence set is
small. Furthermore, as a by-product of the proposed approach, we provide a
data-driven procedure for choosing the LASSO penalty. The selection power of
the method is illustrated on simulated data.
"
"  Polarimetric Synthetic Aperture Radar (PolSAR) images are establishing as an
important source of information in remote sensing applications. The most
complete format this type of imaging produces consists of complex-valued
Hermitian matrices in every image coordinate and, as such, their visualization
is challenging. They also suffer from speckle noise which reduces the
signal-to-noise ratio. Smoothing techniques have been proposed in the
literature aiming at preserving different features and, analogously,
projections from the cone of Hermitian positive matrices to different color
representation spaces are used for enhancing certain characteristics. In this
work we propose the use of stochastic distances between models that describe
this type of data in a Nagao-Matsuyama-type of smoothing technique. The
resulting images are shown to present good visualization properties (noise
reduction with preservation of fine details) in all the considered
visualization spaces.
"
"  Developing the ability to comprehensively study infections in small
populations enables us to improve epidemic models and better advise individuals
about potential risks to their health. We currently have a limited
understanding of how infections spread within a small population because it has
been difficult to closely track an infection within a complete community. The
paper presents data closely tracking the spread of an infection centered on a
student dormitory, collected by leveraging the residents' use of cellular
phones. The data are based on daily symptom surveys taken over a period of four
months and proximity tracking through cellular phones. We demonstrate that
using a Bayesian, discrete-time multi-agent model of infection to model
real-world symptom reports and proximity tracking records gives us important
insights about infec-tions in small populations.
"
"  Covariance is used as an inner product on a formal vector space built on n
random variables to define measures of correlation Md across a set of vectors
in a d-dimensional space. For d = 1, one has the diameter; for d = 2, one has
an area. These concepts are directly applied to correlation studies in climate
science.
"
"  Recent theory work has found that a special type of spatial partition tree -
called a random projection tree - is adaptive to the intrinsic dimension of the
data from which it is built. Here we examine this same question, with a
combination of theory and experiments, for a broader class of trees that
includes k-d trees, dyadic trees, and PCA trees. Our motivation is to get a
feel for (i) the kind of intrinsic low dimensional structure that can be
empirically verified, (ii) the extent to which a spatial partition can exploit
such structure, and (iii) the implications for standard statistical tasks such
as regression, vector quantization, and nearest neighbor search.
"
"  Beta-binomial/Poisson models have been used by many authors to model
multivariate count data. Lora and Singer (Statistics in Medicine, 2008)
extended such models to accommodate repeated multivariate count data with
overdipersion in the binomial component. To overcome some of the limitations of
that model, we consider a beta-binomial/gamma-Poisson alternative that also
allows for both overdispersion and different covariances between the Poisson
counts. We obtain maximum likelihood estimates for the parameters using a
Newton-Raphson algorithm and compare both models in a practical example.
"
"  Belief propagation and its variants are popular methods for approximate
inference, but their running time and even their convergence depend greatly on
the schedule used to send the messages. Recently, dynamic update schedules have
been shown to converge much faster on hard networks than static schedules,
namely the residual BP schedule of Elidan et al. [2006]. But that RBP algorithm
wastes message updates: many messages are computed solely to determine their
priority, and are never actually performed. In this paper, we show that
estimating the residual, rather than calculating it directly, leads to
significant decreases in the number of messages required for convergence, and
in the total running time. The residual is estimated using an upper bound based
on recent work on message errors in BP. On both synthetic and real-world
networks, this dramatically decreases the running time of BP, in some cases by
a factor of five, without affecting the quality of the solution.
"
"  Non-invasive marks, including pigmentation patterns, acquired scars,and
genetic mark- ers, are often used to identify individuals in mark-recapture
experiments. If animals in a population can be identified from multiple,
non-invasive marks then some individuals may be counted twice in the observed
data. Analyzing the observed histories without accounting for these errors will
provide incorrect inference about the population dynamics. Previous approaches
to this problem include modeling data from only one mark and combining
estimators obtained from each mark separately assuming that they are
independent. Motivated by the analysis of data from the ECOCEAN online whale
shark (Rhincodon typus) catalog, we describe a Bayesian method to analyze data
from multiple, non-invasive marks that is based on the latent-multinomial model
of Link et al. (2010). Further to this, we describe a simplification of the
Markov chain Monte Carlo algorithm of Link et al. (2010) that leads to more
efficient computation. We present results from the analysis of the ECOCEAN
whale shark data and from simulation studies comparing our method with the
previous approaches.
"
"  This comment reexamines Simard et al.'s work in [D. Simard, L. Nadeau, H.
Kroger, Phys. Lett. A 336 (2005) 8-15]. We found that Simard et al. calculated
mistakenly the local connectivity lengths Dlocal of networks. The right results
of Dlocal are presented and the supervised learning performance of feedforward
neural networks (FNNs) with different rewirings are re-investigated in this
comment. This comment discredits Simard et al's work by two conclusions: 1)
Rewiring connections of FNNs cannot generate networks with small-world
connectivity; 2) For different training sets, there do not exist networks with
a certain number of rewirings generating reduced learning errors than networks
with other numbers of rewiring.
"
"  The natural gradient allows for more efficient gradient descent by removing
dependencies and biases inherent in a function's parameterization. Several
papers present the topic thoroughly and precisely. It remains a very difficult
idea to get your head around however. The intent of this note is to provide
simple intuition for the natural gradient and its use. We review how an ill
conditioned parameter space can undermine learning, introduce the natural
gradient by analogy to the more widely understood concept of signal whitening,
and present tricks and specific prescriptions for applying the natural gradient
to learning problems.
"
"  We develop a new statistical method for estimating functional connectivity
between neurophysiological signals represented by a multivariate time series.
We use partial coherence as the measure of functional connectivity. Partial
coherence identifies the frequency bands that drive the direct linear
association between any pair of channels. To estimate partial coherence, one
would first need an estimate of the spectral density matrix of the multivariate
time series. Parametric estimators of the spectral density matrix provide good
frequency resolution but could be sensitive when the parametric model is
misspecified. Smoothing-based nonparametric estimators are robust to model
misspecification and are consistent but may have poor frequency resolution. In
this work, we develop the generalized shrinkage estimator, which is a weighted
average of a parametric estimator and a nonparametric estimator. The optimal
weights are frequency-specific and derived under the quadratic risk criterion
so that the estimator, either the parametric estimator or the nonparametric
estimator, that performs better at a particular frequency receives heavier
weight. We validate the proposed estimator in a simulation study and apply it
on electroencephalogram recordings from a visual-motor experiment.
"
"  In order to maintain consistent quality of service, computer network
engineers face the task of monitoring the traffic fluctuations on the
individual links making up the network. However, due to resource constraints
and limited access, it is not possible to directly measure all the links.
Starting with a physically interpretable probabilistic model of network-wide
traffic, we demonstrate how an expensively obtained set of measurements may be
used to develop a network-specific model of the traffic across the network.
This model may then be used in conjunction with easily obtainable measurements
to provide more accurate prediction than is possible with only the inexpensive
measurements. We show that the model, once learned may be used for the same
network for many different periods of traffic. Finally, we show an application
of the prediction technique to create relevant control charts for detection and
isolation of shifts in network traffic.
"
"  In this paper, we present a partial survey of the tools borrowed from tensor
algebra, which have been utilized recently in Statistics and Signal Processing.
It is shown why the decompositions well known in linear algebra can hardly be
extended to tensors. The concept of rank is itself difficult to define, and its
calculation raises difficulties. Numerical algorithms have nevertheless been
developed, and some are reported here, but their limitations are emphasized.
These reports hopefully open research perspectives for enterprising readers.
"
"  Kernel method is a very powerful tool in machine learning. The trick of
kernel has been effectively and extensively applied in many areas of machine
learning, such as support vector machine (SVM) and kernel principal component
analysis (kernel PCA). Kernel trick is to define a kernel function which relies
on the inner-product of data in the feature space without knowing these feature
space data. In this paper, the kernel trick will be employed to extend the
algorithm of spectrum sensing with leading eigenvector under the framework of
PCA to a higher dimensional feature space. Namely, the leading eigenvector of
the sample covariance matrix in the feature space is used for spectrum sensing
without knowing the leading eigenvector explicitly. Spectrum sensing with
leading eigenvector under the framework of kernel PCA is proposed with the
inner-product as a measure of similarity. A modified kernel GLRT algorithm
based on matched subspace model will be the first time applied to spectrum
sensing. The experimental results on simulated sinusoidal signal show that
spectrum sensing with kernel PCA is about 4 dB better than PCA, besides, kernel
GLRT is also better than GLRT. The proposed algorithms are also tested on the
measured DTV signal. The simulation results show that kernel methods are 4 dB
better than the corresponding linear methods. The leading eigenvector of the
sample covariance matrix learned by kernel PCA is more stable than that learned
by PCA for different segments of DTV signal.
"
"  Hawaiian monk seals (Monachus schauinslandi) are endemic to the Hawaiian
Islands and are the most endangered species of marine mammal that lives
entirely within the jurisdiction of the United States. The species numbers
around 1300 and has been declining owing, among other things, to poor juvenile
survival which is evidently related to poor foraging success. Consequently,
data have been collected recently on the foraging habitats, movements, and
behaviors of monk seals throughout the Northwestern and main Hawaiian Islands.
Our work here is directed to exploring a data set located in a relatively
shallow offshore submerged bank (Penguin Bank) in our search of a model for a
seal's journey. The work ends by fitting a stochastic differential equation
(SDE) that mimics some aspects of the behavior of seals by working with
location data collected for one seal. The SDE is found by developing a time
varying potential function with two points of attraction. The times of location
are irregularly spaced and not close together geographically, leading to some
difficulties of interpretation. Synthetic plots generated using the model are
employed to assess its reasonableness spatially and temporally. One aspect is
that the animal stays mainly southwest of Molokai. The work led to the
estimation of the lengths and locations of the seal's foraging trips.
"
"  In this paper, we tackle the problem of online semi-supervised learning
(SSL). When data arrive in a stream, the dual problems of computation and data
storage arise for any SSL method. We propose a fast approximate online SSL
algorithm that solves for the harmonic solution on an approximate graph. We
show, both empirically and theoretically, that good behavior can be achieved by
collapsing nearby points into a set of local ""representative points"" that
minimize distortion. Moreover, we regularize the harmonic solution to achieve
better stability properties. We apply our algorithm to face recognition and
optical character recognition applications to show that we can take advantage
of the manifold structure to outperform the previous methods. Unlike previous
heuristic approaches, we show that our method yields provable performance
bounds.
"
"  A randomized clinical trial comparing an experimental new treatment to a
standard therapy for a life-threatening medical condition should be stopped
early on ethical grounds, in either of the following scenarios: (1) it has
become overwhelmingly clear that the new treatment is better than the standard;
(2) it has become overwhelmingly clear that the trial is not going to show that
the new treatment is any better than the standard. The trial is continued in
the third scenario: (3) there is a reasonable chance that the new treatment
will finally turn out to be better than the standard, but we aren't sure yet.
  However, the (blinded) data monitoring committee in the ""PROPATRIA"" trial of
an experimental probiotica treatment for patients with acute pancreatitis
allowed the trial to continue at the half way interim analysis, in effect
because there was still a good chance of proving that the probiotica treatment
was very harmful to their patients. The committee did not know whether
treatment A was the probiotica treatment or the placebo. In itself this should
not have caused a problem, since it could easily have determined the
appropriate decision under both scenarios. Were the decisions in the two
scenarios different, then the data would have to be de-blinded, in order to
determine the appropriate decision. The committee mis-read the output of SPSS,
which reports the smaller of two one-sided p-values, without informing the user
what it is doing. It seems that about 5 lives were sacrificed to the chance of
getting a significant result that the probiotica treatment was bad for the
patients in the trial.
"
"  Driven by a large number of potential applications in areas like
bioinformatics, information retrieval and social network analysis, the problem
setting of inferring relations between pairs of data objects has recently been
investigated quite intensively in the machine learning community. To this end,
current approaches typically consider datasets containing crisp relations, so
that standard classification methods can be adopted. However, relations between
objects like similarities and preferences are often expressed in a graded
manner in real-world applications. A general kernel-based framework for
learning relations from data is introduced here. It extends existing approaches
because both crisp and graded relations are considered, and it unifies existing
approaches because different types of graded relations can be modeled,
including symmetric and reciprocal relations. This framework establishes
important links between recent developments in fuzzy set theory and machine
learning. Its usefulness is demonstrated through various experiments on
synthetic and real-world data.
"
"  The paper deals with the study of labor market dynamics, and aims to
characterize its equilibriums and possible trajectories. The theoretical
background is the theory of the segmented labor market. The main idea is that
this theory is well adapted to interpret the observed trajectories, due to the
heterogeneity of the work situations.
"
"  The present contribution suggests the use of a multidimensional scaling (MDS)
algorithm as a visualization tool for manifold-valued elements. A visualization
tool of this kind is useful in signal processing and machine learning whenever
learning/adaptation algorithms insist on high-dimensional parameter manifolds.
"
"  This note gives a simple analysis of a randomized approximation scheme for
matrix multiplication proposed by Sarlos (2006) based on a random rotation
followed by uniform column sampling. The result follows from a matrix version
of Bernstein's inequality and a tail inequality for quadratic forms in
subgaussian random vectors.
"
"  We study the property of the Fused Lasso Signal Approximator (FLSA) for
estimating a blocky signal sequence with additive noise. We transform the FLSA
to an ordinary Lasso problem. By studying the property of the design matrix in
the transformed Lasso problem, we find that the irrepresentable condition might
not hold, in which case we show that the FLSA might not be able to recover the
signal pattern. We then apply the newly developed preconditioning method --
Puffer Transformation [Jia and Rohe, 2012] on the transformed Lasso problem. We
call the new method the preconditioned fused Lasso and we give non-asymptotic
results for this method. Results show that when the signal jump strength
(signal difference between two neighboring groups) is big and the noise level
is small, our preconditioned fused Lasso estimator gives the correct pattern
with high probability. Theoretical results give insight on what controls the
signal pattern recovery ability -- it is the noise level {instead of} the
length of the sequence. Simulations confirm our theorems and show significant
improvement of the preconditioned fused Lasso estimator over the vanilla FLSA.
"
"  In this paper histograms of user ratings for movies (1,...,10) are analysed.
The evolving stabilised shapes of histograms follow the rule that all are
either double- or triple-peaked. Moreover, at most one peak can be on the
central bins 2,...,9 and the distribution in these bins looks smooth
`Gaussian-like' while changes at the extremes (1 and 10) often look abrupt. It
is shown that this is well approximated under the assumption that histograms
are confined and discretised probability density functions of L\'evy skew
alpha-stable distributions. These distributions are the only stable
distributions which could emerge due to a generalized central limit theorem
from averaging of various independent random avriables as which one can see the
initial opinions of users. Averaging is also an appropriate assumption about
the social process which underlies the process of continuous opinion formation.
Surprisingly, not the normal distribution achieves the best fit over histograms
obseved on the web, but distributions with fat tails which decay as power-laws
with exponent -(1+alpha) (alpha=4/3). The scale and skewness parameters of the
Levy skew alpha-stable distributions seem to depend on the deviation from an
average movie (with mean about 7.6). The histogram of such an average movie has
no skewness and is the most narrow one. If a movie deviates from average the
distribution gets broader and skew. The skewness pronounces the deviation. This
is used to construct a one parameter fit which gives some evidence of
universality in processes of continuous opinion dynamics about taste.
"
"  When dealing with very large datasets of functional data, survey sampling
approaches are useful in order to obtain estimators of simple functional
quantities, without being obliged to store all the data. We propose here a
Horvitz--Thompson estimator of the mean trajectory. In the context of a
superpopulation framework, we prove under mild regularity conditions that we
obtain uniformly consistent estimators of the mean function and of its variance
function. With additional assumptions on the sampling design we state a
functional Central Limit Theorem and deduce asymptotic confidence bands.
Stratified sampling is studied in detail, and we also obtain a functional
version of the usual optimal allocation rule considering a mean variance
criterion. These techniques are illustrated by means of a test population of
N=18902 electricity meters for which we have individual electricity consumption
measures every 30 minutes over one week. We show that stratification can
substantially improve both the accuracy of the estimators and reduce the width
of the global confidence bands compared to simple random sampling without
replacement.
"
"  Resonance energy transfer methods are in wide use for evaluating
protein-protein interactions and protein conformational changes in living
cells. Fluorescence resonance energy transfer (FRET) measures energy transfer
as a function of the acceptor:donor ratio, generating FRET saturation curves.
Modeling these curves by Michaelis-Menten kinetics allows characterization by
two parameters, which serve to evaluate apparent affinity between two proteins
and to compare this affinity in different experimental conditions. To reduce
the effect of sampling variability, several statistical samples of the
saturation curve are generated in the same biological conditions. Here we study
three procedures to determine whether statistical samples in a collection are
homogeneous, in the sense that they are extracted from the same regression
model. From the hypothesis testing viewpoint, we considered an F test and a
procedure based on bootstrap resampling. The third method analyzed the problem
from the model selection viewpoint, and used the Akaike information criterion
(AIC). Although we only considered the Michaelis-Menten model, all statistical
procedures would be applicable to any other nonlinear regression model. We
compared the performance of the homogeneity testing methods in a Monte Carlo
study and through analysis in living cells of FRET saturation curves for
dimeric complexes of CXCR4, a seven-transmembrane receptor of the G
protein-coupled receptor family. We show that the F test, the bootstrap
procedure and the model selection method lead in general to similar
conclusions, although AIC gave the best results when sample sizes were small,
whereas the F test and the bootstrap method were more appropriate for large
samples. In practice, all three methods are easy to use simultaneously and show
consistency, facilitating conclusions on sample homogeneity.
"
"  In the United States electoral system, a candidate is elected indirectly by
winning a majority of electoral votes cast by individual states, the election
usually being decided by the votes cast by a small number of ""swing states""
where the two candidates historically have roughly equal probabilities of
winning. The effective value of a swing state in deciding the election is
determined not only by the number of its electoral votes but by the frequency
of its appearance in the set of winning partitions of the electoral college.
Since the electoral vote values of swing states are not identical, the presence
or absence of a state in a winning partition is generally correlated with the
frequency of appearance of other states and, hence, their effective values. We
quantify the effective value of states by an {\sl electoral susceptibility},
$\chi_j$, the variation of the winning probability with the ""cost"" of changing
the probability of winning state $j$. We study $\chi_j$ for realistic data
accumulated for the 2012 U.S. presidential election and for a simple model with
a Zipf's law type distribution of electoral votes. In the latter model we show
that the susceptibility for small states is largest in ""one-sided"" electoral
contests and smallest in close contests. We draw an analogy to models of
entropically driven interactions in poly-disperse colloidal solutions.
"
"  We address the asymptotic and approximate distributions of a large class of
test statistics with quadratic forms used in association studies. The
statistics of interest do not necessarily follow a chi-square distribution and
take the general form $D=X^T A X$, where $X$ follows the multivariate normal
distribution, and $A$ is a general similarity matrix which may or may not be
positive semi-definite. We show that $D$ can be written as a linear combination
of independent chi-square random variables, whose distribution can be
approximated by a chi-square or the difference of two chi-square distributions.
In the setting of association testing, our methods are especially useful in two
situations. First, for a genome screen, the required significance level is much
smaller than 0.05 due to multiple comparisons, and estimation of p-values using
permutation procedures is particularly challenging. An efficient and accurate
estimation procedure would therefore be useful. Second, in a candidate gene
study based on haplotypes when phase is unknown a computationally expensive
method-the EM algorithm-is usually required to infer haplotype frequencies.
Because the EM algorithm is needed for each permutation, this results in a
substantial computational burden, which can be eliminated with our mathematical
solution. We assess the practical utility of our method using extensive
simulation studies based on two example statistics and apply it to find the
sample size needed for a typical candidate gene association study when phase
information is not available. Our method can be applied to any quadratic form
statistic and therefore should be of general interest.
"
"  We analyze a class of estimators based on convex relaxation for solving
high-dimensional matrix decomposition problems. The observations are noisy
realizations of a linear transformation $\mathfrak{X}$ of the sum of an
approximately) low rank matrix $\Theta^\star$ with a second matrix
$\Gamma^\star$ endowed with a complementary form of low-dimensional structure;
this set-up includes many statistical models of interest, including factor
analysis, multi-task regression, and robust covariance estimation. We derive a
general theorem that bounds the Frobenius norm error for an estimate of the
pair $(\Theta^\star, \Gamma^\star)$ obtained by solving a convex optimization
problem that combines the nuclear norm with a general decomposable regularizer.
Our results utilize a ""spikiness"" condition that is related to but milder than
singular vector incoherence. We specialize our general result to two cases that
have been studied in past work: low rank plus an entrywise sparse matrix, and
low rank plus a columnwise sparse matrix. For both models, our theory yields
non-asymptotic Frobenius error bounds for both deterministic and stochastic
noise matrices, and applies to matrices $\Theta^\star$ that can be exactly or
approximately low rank, and matrices $\Gamma^\star$ that can be exactly or
approximately sparse. Moreover, for the case of stochastic noise matrices and
the identity observation operator, we establish matching lower bounds on the
minimax error. The sharpness of our predictions is confirmed by numerical
simulations.
"
"  In recent years, the spectral analysis of appropriately defined kernel
matrices has emerged as a principled way to extract the low-dimensional
structure often prevalent in high-dimensional data. Here we provide an
introduction to spectral methods for linear and nonlinear dimension reduction,
emphasizing ways to overcome the computational limitations currently faced by
practitioners with massive datasets. In particular, a data subsampling or
landmark selection process is often employed to construct a kernel based on
partial information, followed by an approximate spectral analysis termed the
Nystrom extension. We provide a quantitative framework to analyse this
procedure, and use it to demonstrate algorithmic performance bounds on a range
of practical approaches designed to optimize the landmark selection process. We
compare the practical implications of these bounds by way of real-world
examples drawn from the field of computer vision, whereby low-dimensional
manifold structure is shown to emerge from high-dimensional video data streams.
"
"  We report a method for estimating people's achievement based on their fame.
Earlier we discovered (cond-mat/0310049) that fame of fighter pilot aces
(measured as number of Google hits) grows exponentially with their achievement
(number of victories). We hypothesize that the same functional relation between
achievement and fame holds for other professions. This allows us to estimate
achievement for professions where an unquestionable and universally accepted
measure of achievement does not exist. We apply the method to Nobel Prize
winners in Physics. For example, we obtain that Paul Dirac, who is hundred
times less famous than Einstein contributed to physics only two times less. We
compare our results with Landau's ranking.
"
"  We present a Hamiltonian Monte Carlo algorithm to sample from multivariate
Gaussian distributions in which the target space is constrained by linear and
quadratic inequalities or products thereof. The Hamiltonian equations of motion
can be integrated exactly and there are no parameters to tune. The algorithm
mixes faster and is more efficient than Gibbs sampling. The runtime depends on
the number and shape of the constraints but the algorithm is highly
parallelizable. In many cases, we can exploit special structure in the
covariance matrices of the untruncated Gaussian to further speed up the
runtime. A simple extension of the algorithm permits sampling from
distributions whose log-density is piecewise quadratic, as in the ""Bayesian
Lasso"" model.
"
"  The measurement of biallelic pair-wise association called linkage
disequilibrium (LD) is an important issue in order to understand the genomic
architecture. A large variety of such measures of association have been
proposed in the literature. We propose and justify six biometrical postulates
which should be fulfilled by a canonical measure of LD. In short, LD measures
are defined as a mapping of probability tables to the set of real numbers. They
should be zero in case of independence and extremal if one of the entries
approaches zero while the marginals are positively bounded. They should reflect
the symmetry group of the tables and be invariant under certain transformations
of the marginals (selection invariance). There scale should have maximum
entropy relative to a calibrating symmetric distribution. None of the
established measures fulfil all of these properties in general. We prove that
there is a unique canonical measure of LD for each choice of a calibrating
distribution. We compa- re the canonical LD measures with other candidates from
the literature. We recommend the canonical measure derived from Jeffreys'
non-informative prior distribution when assessing linkage disequilibrium of SNP
array data. In a second part, we consider various estimators for the
theoretical LD measures discussed and compare them in a simulation study.
"
"  Quantitative genetic studies that model complex, multivariate phenotypes are
important for both evolutionary prediction and artificial selection. For
example, changes in gene expression can provide insight into developmental and
physiological mechanisms that link genotype and phenotype. However, classical
analytical techniques are poorly suited to quantitative genetic studies of gene
expression where the number of traits assayed per individual can reach many
thousand. Here, we derive a Bayesian genetic sparse factor model for estimating
the genetic covariance matrix (G-matrix) of high-dimensional traits, such as
gene expression, in a mixed effects model. The key idea of our model is that we
need only consider G-matrices that are biologically plausible. An organism's
entire phenotype is the result of processes that are modular and have limited
complexity. This implies that the G-matrix will be highly structured. In
particular, we assume that a limited number of intermediate traits (or factors,
e.g., variations in development or physiology) control the variation in the
high-dimensional phenotype, and that each of these intermediate traits is
sparse -- affecting only a few observed traits. The advantages of this approach
are two-fold. First, sparse factors are interpretable and provide biological
insight into mechanisms underlying the genetic architecture. Second, enforcing
sparsity helps prevent sampling errors from swamping out the true signal in
high-dimensional data. We demonstrate the advantages of our model on simulated
data and in an analysis of a published Drosophila melanogaster gene expression
data set.
"
"  This volume is our tribute to David A. Freedman, whom we regard as one of the
great statisticians of our time. He received his B.Sc. degree from McGill
University and his Ph.D. from Princeton, and joined the Department of
Statistics of the University of California, Berkeley, in 1962, where, apart
from sabbaticals, he has been ever since. In a career of over 45 years, David
has made many fine contributions to probability and statistical theory, and to
the application of statistics. His early research was on Markov chains and
martingales, and two topics with which he has had a lifelong fascination:
exchangeability and De Finetti's theorem, and the consistency of Bayes
estimates. His asymptotic theory for the bootstrap was also highly influential.
David was elected to the American Academy of Arts and Sciences in 1991, and in
2003 he received the John J. Carty Award for the Advancement of Science from
the U.S. National Academy of Sciences. In addition to his purely academic
research, David has extensive experience as a consultant, including working for
the Carnegie Commission, the City of San Francisco, and the Federal Reserve, as
well as several Departments of the U.S. Government--Energy, Treasury, Justice,
and Commerce. He has testified as an expert witness on statistics in a number
of law cases, including Piva v. Xerox (employment discrimination), Garza v.
County of Los Angeles (voting rights), and New York v. Department of Commerce
(census adjustment). Lastly, he is an exceptionally good writer and teacher,
and his many books and review articles are arguably his most important
contribution to our subject.
"
"  Distance correlation is a new class of multivariate dependence coefficients
applicable to random vectors of arbitrary and not necessarily equal dimension.
Distance covariance and distance correlation are analogous to product-moment
covariance and correlation, but generalize and extend these classical bivariate
measures of dependence. Distance correlation characterizes independence: it is
zero if and only if the random vectors are independent. The notion of
covariance with respect to a stochastic process is introduced, and it is shown
that population distance covariance coincides with the covariance with respect
to Brownian motion; thus, both can be called Brownian distance covariance. In
the bivariate case, Brownian covariance is the natural extension of
product-moment covariance, as we obtain Pearson product-moment covariance by
replacing the Brownian motion in the definition with identity. The
corresponding statistic has an elegantly simple computing formula. Advantages
of applying Brownian covariance and correlation vs the classical Pearson
covariance and correlation are discussed and illustrated.
"
"  We study the problem of estimating multiple predictive functions from a
dictionary of basis functions in the nonparametric regression setting. Our
estimation scheme assumes that each predictive function can be estimated in the
form of a linear combination of the basis functions. By assuming that the
coefficient matrix admits a sparse low-rank structure, we formulate the
function estimation problem as a convex program regularized by the trace norm
and the $\ell_1$-norm simultaneously. We propose to solve the convex program
using the accelerated gradient (AG) method and the alternating direction method
of multipliers (ADMM) respectively; we also develop efficient algorithms to
solve the key components in both AG and ADMM. In addition, we conduct
theoretical analysis on the proposed function estimation scheme: we derive a
key property of the optimal solution to the convex program; based on an
assumption on the basis functions, we establish a performance bound of the
proposed function estimation scheme (via the composite regularization).
Simulation studies demonstrate the effectiveness and efficiency of the proposed
algorithms.
"
"  The stochastic multi-armed bandit problem is well understood when the reward
distributions are sub-Gaussian. In this paper we examine the bandit problem
under the weaker assumption that the distributions have moments of order
1+\epsilon, for some $\epsilon \in (0,1]$. Surprisingly, moments of order 2
(i.e., finite variance) are sufficient to obtain regret bounds of the same
order as under sub-Gaussian reward distributions. In order to achieve such
regret, we define sampling strategies based on refined estimators of the mean
such as the truncated empirical mean, Catoni's M-estimator, and the
median-of-means estimator. We also derive matching lower bounds that also show
that the best achievable regret deteriorates when \epsilon <1.
"
"  The efficacy of family-based approaches to mixture model-based clustering and
classification depends on the selection of parsimonious models. Current wisdom
suggests the Bayesian information criterion (BIC) for mixture model selection.
However, the BIC has well-known limitations, including a tendency to
overestimate the number of components as well as a proclivity for, often
drastically, underestimating the number of components in higher dimensions.
While the former problem might be soluble through merging components, the
latter is impossible to mitigate in clustering and classification applications.
In this paper, a LASSO-penalized BIC (LPBIC) is introduced to overcome this
problem. This approach is illustrated based on applications of extensions of
mixtures of factor analyzers, where the LPBIC is used to select both the number
of components and the number of latent factors. The LPBIC is shown to match or
outperform the BIC in several situations.
"
"  The past decade has seen substantial work on the use of non-negative matrix
factorization and its probabilistic counterparts for audio source separation.
Although able to capture audio spectral structure well, these models neglect
the non-stationarity and temporal dynamics that are important properties of
audio. The recently proposed non-negative factorial hidden Markov model
(N-FHMM) introduces a temporal dimension and improves source separation
performance. However, the factorial nature of this model makes the complexity
of inference exponential in the number of sound sources. Here, we present a
Bayesian variant of the N-FHMM suited to an efficient variational inference
algorithm, whose complexity is linear in the number of sound sources. Our
algorithm performs comparably to exact inference in the original N-FHMM but is
significantly faster. In typical configurations of the N-FHMM, our method
achieves around a 30x increase in speed.
"
"  Frequently, acquiring training data has an associated cost. We consider the
situation where the learner may purchase data during training, subject TO a
budget. IN particular, we examine the CASE WHERE each feature label has an
associated cost, AND the total cost OF ALL feature labels acquired during
training must NOT exceed the budget.This paper compares methods FOR choosing
which feature label TO purchase next, given the budget AND the CURRENT belief
state OF naive Bayes model parameters.Whereas active learning has traditionally
focused ON myopic(greedy) strategies FOR query selection, this paper presents a
tractable method FOR incorporating knowledge OF the budget INTO the decision
making process, which improves performance.
"
"  We consider the problem of sparse estimation in a factor analysis model. A
traditional estimation procedure in use is the following two-step approach: the
model is estimated by maximum likelihood method and then a rotation technique
is utilized to find sparse factor loadings. However, the maximum likelihood
estimates cannot be obtained when the number of variables is much larger than
the number of observations. Furthermore, even if the maximum likelihood
estimates are available, the rotation technique does not often produce a
sufficiently sparse solution. In order to handle these problems, this paper
introduces a penalized likelihood procedure that imposes a nonconvex penalty on
the factor loadings. We show that the penalized likelihood procedure can be
viewed as a generalization of the traditional two-step approach, and the
proposed methodology can produce sparser solutions than the rotation technique.
A new algorithm via the EM algorithm along with coordinate descent is
introduced to compute the entire solution path, which permits the application
to a wide variety of convex and nonconvex penalties. Monte Carlo simulations
are conducted to investigate the performance of our modeling strategy. A real
data example is also given to illustrate our procedure.
"
"  Rich and complex time-series data, such as those generated from engineering
systems, financial markets, videos or neural recordings, are now a common
feature of modern data analysis. Explaining the phenomena underlying these
diverse data sets requires flexible and accurate models. In this paper, we
promote Gaussian process dynamical systems (GPDS) as a rich model class that is
appropriate for such analysis. In particular, we present a message passing
algorithm for approximate inference in GPDSs based on expectation propagation.
By posing inference as a general message passing problem, we iterate
forward-backward smoothing. Thus, we obtain more accurate posterior
distributions over latent structures, resulting in improved predictive
performance compared to state-of-the-art GPDS smoothers, which are special
cases of our general message passing algorithm. Hence, we provide a unifying
approach within which to contextualize message passing in GPDSs.
"
"  Many models of interest in the natural and social sciences have no
closed-form likelihood function, which means that they cannot be treated using
the usual techniques of statistical inference. In the case where such models
can be efficiently simulated, Bayesian inference is still possible thanks to
the Approximate Bayesian Computation (ABC) algorithm. Although many refinements
have been suggested, ABC inference is still far from routine. ABC is often
excruciatingly slow due to very low acceptance rates. In addition, ABC requires
introducing a vector of ""summary statistics"", the choice of which is relatively
arbitrary, and often require some trial and error, making the whole process
quite laborious for the user.
  We introduce in this work the EP-ABC algorithm, which is an adaptation to the
likelihood-free context of the variational approximation algorithm known as
Expectation Propagation (Minka, 2001). The main advantage of EP-ABC is that it
is faster by a few orders of magnitude than standard algorithms, while
producing an overall approximation error which is typically negligible. A
second advantage of EP-ABC is that it replaces the usual global ABC constraint
on the vector of summary statistics computed on the whole dataset, by n local
constraints of the form that apply separately to each data-point. As a
consequence, it is often possible to do away with summary statistics entirely.
In that case, EP-ABC approximates directly the evidence (marginal likelihood)
of the model.
  Comparisons are performed in three real-world applications which are typical
of likelihood-free inference, including one application in neuroscience which
is novel, and possibly too challenging for standard ABC techniques.
"
"  Our work has focused on support for film or television scriptwriting. Since
this involves potentially varied story-lines, we note the implicit or latent
support for interactivity. Furthermore the film, television, games, publishing
and other sectors are converging, so that cross-over and re-use of one form of
product in another of these sectors is ever more common. Technically our work
has been largely based on mathematical algorithms for data clustering and
display. Operationally, we also discuss how our algorithms can support
collective, distributed problem-solving.
"
"  We propose a novel technique to assess functional brain connectivity in
EEG/MEG signals. Our method, called Sparsely-Connected Sources Analysis (SCSA),
can overcome the problem of volume conduction by modeling neural data
innovatively with the following ingredients: (a) the EEG is assumed to be a
linear mixture of correlated sources following a multivariate autoregressive
(MVAR) model, (b) the demixing is estimated jointly with the source MVAR
parameters, (c) overfitting is avoided by using the Group Lasso penalty. This
approach allows to extract the appropriate level cross-talk between the
extracted sources and in this manner we obtain a sparse data-driven model of
functional connectivity. We demonstrate the usefulness of SCSA with simulated
data, and compare to a number of existing algorithms with excellent results.
"
"  Conditional Restricted Boltzmann Machines (CRBMs) are rich probabilistic
models that have recently been applied to a wide range of problems, including
collaborative filtering, classification, and modeling motion capture data.
While much progress has been made in training non-conditional RBMs, these
algorithms are not applicable to conditional models and there has been almost
no work on training and generating predictions from conditional RBMs for
structured output problems. We first argue that standard Contrastive
Divergence-based learning may not be suitable for training CRBMs. We then
identify two distinct types of structured output prediction problems and
propose an improved learning algorithm for each. The first problem type is one
where the output space has arbitrary structure but the set of likely output
configurations is relatively small, such as in multi-label classification. The
second problem is one where the output space is arbitrarily structured but
where the output space variability is much greater, such as in image denoising
or pixel labeling. We show that the new learning algorithms can work much
better than Contrastive Divergence on both types of problems.
"
"  We consider the problem of subspace estimation in a Bayesian setting. Since
we are operating in the Grassmann manifold, the usual approach which consists
of minimizing the mean square error (MSE) between the true subspace $U$ and its
estimate $\hat{U}$ may not be adequate as the MSE is not the natural metric in
the Grassmann manifold. As an alternative, we propose to carry out subspace
estimation by minimizing the mean square distance (MSD) between $U$ and its
estimate, where the considered distance is a natural metric in the Grassmann
manifold, viz. the distance between the projection matrices. We show that the
resulting estimator is no longer the posterior mean of $U$ but entails
computing the principal eigenvectors of the posterior mean of $U U^{T}$.
Derivation of the MMSD estimator is carried out in a few illustrative examples
including a linear Gaussian model for the data and a Bingham or von Mises
Fisher prior distribution for $U$. In all scenarios, posterior distributions
are derived and the MMSD estimator is obtained either analytically or
implemented via a Markov chain Monte Carlo simulation method. The method is
shown to provide accurate estimates even when the number of samples is lower
than the dimension of $U$. An application to hyperspectral imagery is finally
investigated.
"
"  In this paper, we explore salient questions about user interests,
conversations and friendships in the Facebook social network, using a novel
latent space model that integrates several data types. A key challenge of
studying Facebook's data is the wide range of data modalities such as text,
network links, and categorical labels. Our latent space model seamlessly
combines all three data modalities over millions of users, allowing us to study
the interplay between user friendships, interests, and higher-order
network-wide social trends on Facebook. The recovered insights not only answer
our initial questions, but also reveal surprising facts about user interests in
the context of Facebook's ecosystem. We also confirm that our results are
significant with respect to evidential information from the study subjects.
"
"  Background: We study the statistical properties of fragment coverage in
genome sequencing experiments. In an extension of the classic Lander-Waterman
model, we consider the effect of the length distribution of fragments. We also
introduce the notion of the shape of a coverage function, which can be used to
detect abberations in coverage. The probability theory underlying these
problems is essential for constructing models of current high-throughput
sequencing experiments, where both sample preparation protocols and sequencing
technology particulars can affect fragment length distributions.
  Results: We show that regardless of fragment length distribution and under
the mild assumption that fragment start sites are Poisson distributed, the
fragments produced in a sequencing experiment can be viewed as resulting from a
two-dimensional spatial Poisson process. We then study the jump skeleton of the
the coverage function, and show that the induced trees are Galton-Watson trees
whose parameters can be computed.
  Conclusions: Our results extend standard analyses of shotgun sequencing that
focus on coverage statistics at individual sites, and provide a null model for
detecting deviations from random coverage in high-throughput sequence census
based experiments. By focusing on fragments, we are also led to a new approach
for visualizing sequencing data that should be of independent interest.
"
"  We introduce new definitions of universal and superuniversal computable
codes, which are based on a code's ability to approximate Kolmogorov complexity
within the prescribed margin for all individual sequences from a given set.
Such sets of sequences may be singled out almost surely with respect to certain
probability measures. Consider a measure parameterized with a real parameter
and put an arbitrary prior on the parameter. The Bayesian measure is the
expectation of the parameterized measure with respect to the prior. It appears
that a modified Shannon-Fano code for any computable Bayesian measure, which we
call the Bayesian code, is superuniversal on a set of parameterized
measure-almost all sequences for prior-almost every parameter. According to
this result, in the typical setting of mathematical statistics no computable
code enjoys redundancy which is ultimately much less than that of the Bayesian
code. Thus we introduce another characteristic of computable codes: The
catch-up time is the length of data for which the code length drops below the
Kolmogorov complexity plus the prescribed margin. Some codes may have smaller
catch-up times than Bayesian codes.
"
"  We apply the spike-and-slab Restricted Boltzmann Machine (ssRBM) to texture
modeling. The ssRBM with tiled-convolution weight sharing (TssRBM) achieves or
surpasses the state-of-the-art on texture synthesis and inpainting by
parametric models. We also develop a novel RBM model with a spike-and-slab
visible layer and binary variables in the hidden layer. This model is designed
to be stacked on top of the TssRBM. We show the resulting deep belief network
(DBN) is a powerful generative model that improves on single-layer models and
is capable of modeling not only single high-resolution and challenging textures
but also multiple textures.
"
"  In recent years, advances in high throughput sequencing technology have led
to a need for specialized methods for the analysis of digital gene expression
data. While gene expression data measured on a microarray take on continuous
values and can be modeled using the normal distribution, RNA sequencing data
involve nonnegative counts and are more appropriately modeled using a discrete
count distribution, such as the Poisson or the negative binomial. Consequently,
analytic tools that assume a Gaussian distribution (such as classification
methods based on linear discriminant analysis and clustering methods that use
Euclidean distance) may not perform as well for sequencing data as methods that
are based upon a more appropriate distribution. Here, we propose new approaches
for performing classification and clustering of observations on the basis of
sequencing data. Using a Poisson log linear model, we develop an analog of
diagonal linear discriminant analysis that is appropriate for sequencing data.
We also propose an approach for clustering sequencing data using a new
dissimilarity measure that is based upon the Poisson model. We demonstrate the
performances of these approaches in a simulation study, on three publicly
available RNA sequencing data sets, and on a publicly available chromatin
immunoprecipitation sequencing data set.
"
"  Pneumonia is one of the serious illnesses, which involves lung infection
specifically alveoli. Nearly 40,000 to 70,000 people die each year in United
State because of pneumonia. Therefore, it is not a surprise that pneumonia is
one of the most critical illnesses for children under 12 years old in many
parts of the world, including Malaysia and particularly in Tawau, Sabah,
Malaysia. The objectives of this study are: to develop a summary on the
prevalence of pneumonia in Tawau General Hospital, to analyze the best practice
to prevent this illness and lastly to determine an overview of which area that
is widely affected by pneumonia. The results can assist doctors and the
government to take major precautions and preventive measures efficiently to the
full extent. This paper presents a descriptive analysis of the data, which are
retrieved from the medical reports at the Tawau General Hospital. Through the
findings, pneumonia is widely spread among young children under 12 years old.
There are more than one major factor that leads to this critical illness, such
as family background, genetic and environment. Therefore, the government,
doctors and parents should take major steps to prevent children suffering from
pneumonia.
"
"  Item neighbourhood methods for collaborative filtering learn a weighted graph
over the set of items, where each item is connected to those it is most similar
to. The prediction of a user's rating on an item is then given by that rating
of neighbouring items, weighted by their similarity. This paper presents a new
neighbourhood approach which we call item fields, whereby an undirected
graphical model is formed over the item graph. The resulting prediction rule is
a simple generalization of the classical approaches, which takes into account
non-local information in the graph, allowing its best results to be obtained
when using drastically fewer edges than other neighbourhood approaches. A fast
approximate maximum entropy training method based on the Bethe approximation is
presented, which uses a simple gradient ascent procedure. When using
precomputed sufficient statistics on the Movielens datasets, our method is
faster than maximum likelihood approaches by two orders of magnitude.
"
"  A trivariate Weibull survival model using competing risks concept is applied
on studying recidivism of committing 3 types of crimes - sex, violent and
others. The assumption of independence of time to commit each type of crimes is
relaxed so that the association of the time to recidivism between any two types
of crimes can be evaluated. We found that the correlation of time to recidivism
between sex crimes and violent crimes are more correlated than other pairs.
Probability of experiencing a charged arrest of other crimes is greater than a
charged arrest of violent crimes followed by a charged arrest of sex crimes for
an individual after release.
"
"  We consider the problem of sequential sampling from a finite number of
independent statistical populations to maximize the expected infinite horizon
average outcome per period, under a constraint that the expected average
sampling cost does not exceed an upper bound. The outcome distributions are not
known. We construct a class of consistent adaptive policies, under which the
average outcome converges with probability 1 to the true value under complete
information for all distributions with finite means. We also compare the rate
of convergence for various policies in this class using simulation.
"
"  In this paper, we study the target tracking problem in wireless sensor
networks (WSNs) using quantized sensor measurements under limited bandwidth
availability. At each time step of tracking, the available bandwidth $R$ needs
to be distributed among the $N$ sensors in the WSN for the next time step. The
optimal solution for the bandwidth allocation problem can be obtained by using
a combinatorial search which may become computationally prohibitive for large
$N$ and $R$. Therefore, we develop two new computationally efficient suboptimal
bandwidth distribution algorithms which are based on convex relaxation and
approximate dynamic programming (A-DP). We compare the mean squared error (MSE)
and computational complexity performances of convex relaxation and A-DP with
other existing suboptimal bandwidth distribution schemes based on generalized
Breiman, Friedman, Olshen, and Stone (GBFOS) algorithm and greedy search.
Simulation results show that, A-DP, convex optimization and GBFOS yield similar
MSE performance, which is very close to that based on the optimal exhaustive
search approach and they outperform greedy search and nearest neighbor based
bandwidth allocation approaches significantly. Computationally, A-DP is more
efficient than the bandwidth allocation schemes based on convex relaxation and
GBFOS, especially for a large sensor network.
"
"  In this paper, we consider formal series associated with events, profiles
derived from events, and statistical models that make predictions about events.
We prove theorems about realizations for these formal series using the language
and tools of Hopf algebras.
"
"  The Minimum Description Length (MDL) principle states that the optimal model
for a given data set is that which compresses it best. Due to practial
limitations the model can be restricted to a class such as linear regression
models, which we address in this study. As in other formulations such as the
LASSO and forward step-wise regression we are interested in sparsifying the
feature set while preserving generalization ability. We derive a
well-principled set of codes for both parameters and error residuals along with
smooth approximations to lengths of these codes as to allow gradient descent
optimization of description length, and go on to show that sparsification and
feature selection using our approach is faster than the LASSO on several
datasets from the UCI and StatLib repositories, with favorable generalization
accuracy, while being fully automatic, requiring neither cross-validation nor
tuning of regularization hyper-parameters, allowing even for a nonlinear
expansion of the feature set followed by sparsification.
"
"  We study clustering algorithms based on neighborhood graphs on a random
sample of data points. The question we ask is how such a graph should be
constructed in order to obtain optimal clustering results. Which type of
neighborhood graph should one choose, mutual k-nearest neighbor or symmetric
k-nearest neighbor? What is the optimal parameter k? In our setting, clusters
are defined as connected components of the t-level set of the underlying
probability distribution. Clusters are said to be identified in the
neighborhood graph if connected components in the graph correspond to the true
underlying clusters. Using techniques from random geometric graph theory, we
prove bounds on the probability that clusters are identified successfully, both
in a noise-free and in a noisy setting. Those bounds lead to several
conclusions. First, k has to be chosen surprisingly high (rather of the order n
than of the order log n) to maximize the probability of cluster identification.
Secondly, the major difference between the mutual and the symmetric k-nearest
neighbor graph occurs when one attempts to detect the most significant cluster
only.
"
"  Adaptive sampling results in dramatic improvements in the recovery of sparse
signals in white Gaussian noise. A sequential adaptive sampling-and-refinement
procedure called Distilled Sensing (DS) is proposed and analyzed. DS is a form
of multi-stage experimental design and testing. Because of the adaptive nature
of the data collection, DS can detect and localize far weaker signals than
possible from non-adaptive measurements. In particular, reliable detection and
localization (support estimation) using non-adaptive samples is possible only
if the signal amplitudes grow logarithmically with the problem dimension. Here
it is shown that using adaptive sampling, reliable detection is possible
provided the amplitude exceeds a constant, and localization is possible when
the amplitude exceeds any arbitrarily slowly growing function of the dimension.
"
"  Causal inference approaches in systems genetics exploit quantitative trait
loci (QTL) genotypes to infer causal relationships among phenotypes. The
genetic architecture of each phenotype may be complex, and poorly estimated
genetic architectures may compromise the inference of causal relationships
among phenotypes. Existing methods assume QTLs are known or inferred without
regard to the phenotype network structure. In this paper we develop a
QTL-driven phenotype network method (QTLnet) to jointly infer a causal
phenotype network and associated genetic architecture for sets of correlated
phenotypes. Randomization of alleles during meiosis and the unidirectional
influence of genotype on phenotype allow the inference of QTLs causal to
phenotypes. Causal relationships among phenotypes can be inferred using these
QTL nodes, enabling us to distinguish among phenotype networks that would
otherwise be distribution equivalent. We jointly model phenotypes and QTLs
using homogeneous conditional Gaussian regression models, and we derive a
graphical criterion for distribution equivalence. We validate the QTLnet
approach in a simulation study. Finally, we illustrate with simulated data and
a real example how QTLnet can be used to infer both direct and indirect effects
of QTLs and phenotypes that co-map to a genomic region.
"
"  If people at risk of HIV infection are tested annually and started on
treatment as soon as they are found to be HIV-positive it should be possible to
reduce the case reproduction number for HIV to less than one, eliminate
transmission and end the epidemic. If this is to be done it is essential to
know if it would be affordable, and cost effective. Here we show that in all
but eleven countries of the world it is affordable by those countries, that in
these eleven countries it is affordable for the international community, and in
all countries it is highly cost-effective.
"
"  A topological approach to stratification learning is developed for point
cloud data drawn from a stratified space. Given such data, our objective is to
infer which points belong to the same strata. First we define a multi-scale
notion of a stratified space, giving a stratification for each radius level. We
then use methods derived from kernel and cokernel persistent homology to
cluster the data points into different strata, and we prove a result which
guarantees the correctness of our clustering, given certain topological
conditions; some geometric intuition for these topological conditions is also
provided. Our correctness result is then given a probabilistic flavor: we give
bounds on the minimum number of sample points required to infer, with
probability, which points belong to the same strata. Finally, we give an
explicit algorithm for the clustering, prove its correctness, and apply it to
some simulated data.
"
"  We propose a new statistics for the detection of differentially expressed
genes, when the genes are activated only in a subset of the samples. Statistics
designed for this unconventional circumstance has proved to be valuable for
most cancer studies, where oncogenes are activated for a small number of
disease samples. Previous efforts made in this direction include COPA, OS and
ORT. We propose a new statistics called maximum ordered subset t-statistics
(MOST) which seems to be natural when the number of activated samples is
unknown. We compare MOST to other statistics and find the proposed method often
has more power then its competitors.
"
"  A collection of races in a single election can be audited as a group by
auditing a random sample of batches of ballots and combining observed
discrepancies in the races represented in those batches in a particular way:
the maximum across-race relative overstatement of pairwise margins (MARROP). A
risk-limiting audit for the entire collection of races can be built on this
ballot-based auditing using a variety of probability sampling schemes. The
audit controls the familywise error rate (the chance that one or more incorrect
outcomes fails to be corrected by a full hand count) at a cost that can be
lower than that of controlling the per-comparison error rate with independent
audits. The approach is particularly efficient if batches are drawn with
probability proportional to a bound on the MARROP (PPEB sampling).
"
"  We present a graphical criterion for reading dependencies from the minimal
directed independence map G of a graphoid p when G is a polytree and p
satisfies composition and weak transitivity. We prove that the criterion is
sound and complete. We argue that assuming composition and weak transitivity is
not too restrictive.
"
"  Gaussian process models -also called Kriging models- are often used as
mathematical approximations of expensive experiments. However, the number of
observation required for building an emulator becomes unrealistic when using
classical covariance kernels when the dimension of input increases. In oder to
get round the curse of dimensionality, a popular approach is to consider
simplified models such as additive models. The ambition of the present work is
to give an insight into covariance kernels that are well suited for building
additive Kriging models and to describe some properties of the resulting
models.
"
"  In this article we show the relationship between the Pareto distribution and
the gamma distribution. This shows that the second one, appropriately extended,
explains some anomalies that arise in the practical use of extreme value
theory. The results are useful to certain phenomena that are fitted by the
Pareto distribution but, at the same time, they present a deviation from this
law for very large values. Two examples of data analysis with the new model are
provided. The first one is on the influence of climate variability on the
occurrence of tropical cyclones. The second one on the analysis of aggregate
loss distributions associated to operational risk management.
"
"  In many applications of time series models, such as climate analysis and
social media analysis, we are often interested in extreme events, such as
heatwave, wind gust, and burst of topics. These time series data usually
exhibit a heavy-tailed distribution rather than a Gaussian distribution. This
poses great challenges to existing approaches due to the significantly
different assumptions on the data distributions and the lack of sufficient past
data on extreme events. In this paper, we propose the Sparse-GEV model, a
latent state model based on the theory of extreme value modeling to
automatically learn sparse temporal dependence and make predictions. Our model
is theoretically significant because it is among the first models to learn
sparse temporal dependencies among multivariate extreme value time series. We
demonstrate the superior performance of our algorithm to the state-of-art
methods, including Granger causality, copula approach, and transfer entropy, on
one synthetic dataset, one climate dataset and two Twitter datasets.
"
"  Group-based brain connectivity networks have great appeal for researchers
interested in gaining further insight into complex brain function and how it
changes across different mental states and disease conditions. Accurately
constructing these networks presents a daunting challenge given the
difficulties associated with accounting for inter-subject topological
variability. Viable approaches to this task must engender networks that capture
the constitutive topological properties of the group of subjects' networks that
it is aiming to represent. The conventional approach has been to use a mean or
median correlation network (Achard et al., 2006; Song et al., 2009) to embody a
group of networks. However, the degree to which their topological properties
conform with those of the groups that they are purported to represent has yet
to be explored. Here we investigate the performance of these mean and median
correlation networks. We also propose an alternative approach based on an
exponential random graph modeling framework and compare its performance to that
of the aforementioned conventional approach. Simpson et al. (2010) illustrated
the utility of exponential random graph models (ERGMs) for creating brain
networks that capture the topological characteristics of a single subject's
brain network. However, their advantageousness in the context of producing a
brain network that ""represents"" a group of brain networks has yet to be
examined. Here we show that our proposed ERGM approach outperforms the
conventional mean and median correlation based approaches and provides an
accurate and flexible method for constructing group-based representative brain
networks.
"
"  Statistical multispecies models of multiarea marine ecosystems use a variety
of data sources to estimate parameters using composite or weighted likelihood
functions with associated weighting issues and questions on how to obtain
variance estimates. Regardless of the method used to obtain point estimates, a
method is needed for variance estimation. A bootstrap technique is introduced
for the evaluation of uncertainty in such models, taking into account inherent
spatial and temporal correlations in the data sets thus avoiding many
model--specification issues, which are commonly transferred as assumptions from
a likelihood estimation procedure into Hessian--based variance estimation
procedures. The technique is demonstrated on a real data set and used to look
for estimation bias and the effects of different aggregation levels in
population dynamics models.
"
"  Statistical dependencies among wavelet coefficients are commonly represented
by graphical models such as hidden Markov trees(HMTs). However, in linear
inverse problems such as deconvolution, tomography, and compressed sensing, the
presence of a sensing or observation matrix produces a linear mixing of the
simple Markovian dependency structure. This leads to reconstruction problems
that are non-convex optimizations. Past work has dealt with this issue by
resorting to greedy or suboptimal iterative reconstruction methods. In this
paper, we propose new modeling approaches based on group-sparsity penalties
that leads to convex optimizations that can be solved exactly and efficiently.
We show that the methods we develop perform significantly better in
deconvolution and compressed sensing applications, while being as
computationally efficient as standard coefficient-wise approaches such as
lasso.
"
"  Understanding space usage and resource selection is a primary focus of many
studies of animal populations. Usually, such studies are based on location data
obtained from telemetry, and resource selection functions (RSF) are used for
inference. Another important focus of wildlife research is estimation and
modeling population size and density. Recently developed spatial
capture-recapture (SCR) models accomplish this objective using individual
encounter history data with auxiliary spatial information on location of
capture. SCR models include encounter probability functions that are
intuitively related to RSFs, but to date, no one has extended SCR models to
allow for explicit inference about space usage and resource selection. We
develop a statistical framework for jointly modeling space usage, resource
selection, and population density by integrating SCR data, such as from camera
traps, mist-nets, or conventional catch-traps, with resource selection data
from telemetered individuals. We provide a framework for estimation based on
marginal likelihood, wherein we estimate simultaneously the parameters of the
SCR and RSF models.
  Our method leads to increases in precision for estimating population density
and parameters of ordinary SCR models. Importantly, we also find that SCR
models alone can estimate parameters of resource selection functions and, as
such, SCR methods can be used as the sole source for studying space-usage;
however, precision will be higher when telemetry data are available. Finally,
we find that SCR models using standard symmetric and stationary encounter
probability models produce biased estimates of density when animal space usage
is related to a landscape covariate. Therefore, it is important that space
usage be taken into consideration, if possible, in studies focused on
estimating density using capture-recapture methods.
"
"  We consider the problem of using a factor model we call {\em spike-and-slab
sparse coding} (S3C) to learn features for a classification task. The S3C model
resembles both the spike-and-slab RBM and sparse coding. Since exact inference
in this model is intractable, we derive a structured variational inference
procedure and employ a variational EM training algorithm. Prior work on
approximate inference for this model has not prioritized the ability to exploit
parallel architectures and scale to enormous problem sizes. We present an
inference procedure appropriate for use with GPUs which allows us to
dramatically increase both the training set size and the amount of latent
factors.
  We demonstrate that this approach improves upon the supervised learning
capabilities of both sparse coding and the ssRBM on the CIFAR-10 dataset. We
evaluate our approach's potential for semi-supervised learning on subsets of
CIFAR-10. We demonstrate state-of-the art self-taught learning performance on
the STL-10 dataset and use our method to win the NIPS 2011 Workshop on
Challenges In Learning Hierarchical Models' Transfer Learning Challenge.
"
"  All gamma-ray telescopes suffer from source confusion due to their inability
to focus incident high-energy radiation, and the resulting background
contamination can obscure the periodic emission from faint pulsars. In the
context of the Fermi Large Area Telescope, we outline enhanced statistical
tests for pulsation in which each photon is weighted by its probability to have
originated from the candidate pulsar. The probabilities are calculated using
the instrument response function and a full spectral model, enabling powerful
background rejection. With Monte Carlo methods, we demonstrate that the new
tests increase the sensitivity to pulsars by more than 50% under a wide range
of conditions. This improvement may appreciably increase the completeness of
the sample of radio-loud gamma-ray pulsars. Finally, we derive the asymptotic
null distribution for the H-test, expanding its domain of validity to
arbitrarily complex light curves.
"
"  With the advent of kernel methods, automating the task of specifying a
suitable kernel has become increasingly important. In this context, the
Multiple Kernel Learning (MKL) problem of finding a combination of
pre-specified base kernels that is suitable for the task at hand has received
significant attention from researchers. In this paper we show that Multiple
Kernel Learning can be framed as a standard binary classification problem with
additional constraints that ensure the positive definiteness of the learned
kernel. Framing MKL in this way has the distinct advantage that it makes it
easy to leverage the extensive research in binary classification to develop
better performing and more scalable MKL algorithms that are conceptually
simpler, and, arguably, more accessible to practitioners. Experiments on nine
data sets from different domains show that, despite its simplicity, the
proposed technique compares favorably with current leading MKL approaches.
"
"  In this paper, we propose two algorithms for solving linear inverse problems
when the observations are corrupted by Poisson noise. A proper data fidelity
term (log-likelihood) is introduced to reflect the Poisson statistics of the
noise. On the other hand, as a prior, the images to restore are assumed to be
positive and sparsely represented in a dictionary of waveforms. Piecing
together the data fidelity and the prior terms, the solution to the inverse
problem is cast as the minimization of a non-smooth convex functional. We
establish the well-posedness of the optimization problem, characterize the
corresponding minimizers, and solve it by means of primal and primal-dual
proximal splitting algorithms originating from the field of non-smooth convex
optimization theory. Experimental results on deconvolution and comparison to
prior methods are also reported.
"
"  Mirror descent with an entropic regularizer is known to achieve shifting
regret bounds that are logarithmic in the dimension. This is done using either
a carefully designed projection or by a weight sharing technique. Via a novel
unified analysis, we show that these two approaches deliver essentially
equivalent bounds on a notion of regret generalizing shifting, adaptive,
discounted, and other related regrets. Our analysis also captures and extends
the generalized weight sharing technique of Bousquet and Warmuth, and can be
refined in several ways, including improvements for small losses and adaptive
tuning of parameters.
"
"  In this paper we propose a class of prior distributions on decomposable
graphs, allowing for improved modeling flexibility. While existing methods
solely penalize the number of edges, the proposed work empowers practitioners
to control clustering, level of separation, and other features of the graph.
Emphasis is placed on a particular prior distribution which derives its
motivation from the class of product partition models; the properties of this
prior relative to existing priors is examined through theory and simulation. We
then demonstrate the use of graphical models in the field of agriculture,
showing how the proposed prior distribution alleviates the inflexibility of
previous approaches in properly modeling the interactions between the yield of
different crop varieties.
"
"  Following Hartigan, a cluster is defined as a connected component of the
t-level set of the underlying density, i.e., the set of points for which the
density is greater than t. A clustering algorithm which combines a density
estimate with spectral clustering techniques is proposed. Our algorithm is
composed of two steps. First, a nonparametric density estimate is used to
extract the data points for which the estimated density takes a value greater
than t. Next, the extracted points are clustered based on the eigenvectors of a
graph Laplacian matrix. Under mild assumptions, we prove the almost sure
convergence in operator norm of the empirical graph Laplacian operator
associated with the algorithm. Furthermore, we give the typical behavior of the
representation of the dataset into the feature space, which establishes the
strong consistency of our proposed algorithm.
"
"  It has been argued by Daryl Bem in his 2011 paper that 8 out of 9 experiments
yielded statistically significant results in favour of the psi effect. It is
pointed out in this short communication that many of the results in the above
mentioned paper could be explained by using well known concepts in statistics
such as Confidence Level and Standard Error of the Sample Mean. This short
communication also discusses implied confidence level and confidence intervals
in polling results.
"
"  Chromosomal DNA is characterized by variation between individuals at the
level of entire chromosomes (e.g., aneuploidy in which the chromosome copy
number is altered), segmental changes (including insertions, deletions,
inversions, and translocations), and changes to small genomic regions
(including single nucleotide polymorphisms). A variety of alterations that
occur in chromosomal DNA, many of which can be detected using high density
single nucleotide polymorphism (SNP) microarrays, are linked to normal
variation as well as disease and are therefore of particular interest. These
include changes in copy number (deletions and duplications) and genotype (e.g.,
the occurrence of regions of homozygosity). Hidden Markov models (HMM) are
particularly useful for detecting such alterations, modeling the spatial
dependence between neighboring SNPs. Here, we improve previous approaches that
utilize HMM frameworks for inference in high throughput SNP arrays by
integrating copy number, genotype calls, and the corresponding measures of
uncertainty when available. Using simulated and experimental data, we, in
particular, demonstrate how confidence scores control smoothing in a
probabilistic framework. Software for fitting HMMs to SNP array data is
available in the R package VanillaICE.
"
"  We study the problem of estimating multiple linear regression equations for
the purpose of both prediction and variable selection. Following recent work on
multi-task learning Argyriou et al. [2008], we assume that the regression
vectors share the same sparsity pattern. This means that the set of relevant
predictor variables is the same across the different equations. This assumption
leads us to consider the Group Lasso as a candidate estimation method. We show
that this estimator enjoys nice sparsity oracle inequalities and variable
selection properties. The results hold under a certain restricted eigenvalue
condition and a coherence condition on the design matrix, which naturally
extend recent work in Bickel et al. [2007], Lounici [2008]. In particular, in
the multi-task learning scenario, in which the number of tasks can grow, we are
able to remove completely the effect of the number of predictor variables in
the bounds. Finally, we show how our results can be extended to more general
noise distributions, of which we only require the variance to be finite.
"
"  Markovian memory embedded in a binary system is shaping its evolution on the
basis of its current state and introduces either clustering or dispersion of
binary states. The consequence is directly observed in the lengthening or
shortening of the runs of the same binary state and also in the way the
proportion of a state within a sequence of state measurements scatters about
its true average, which is quantifiable through the Markovian self-transition
probabilities. It is shown that the Markovian memory can even imitate the
evolution of a random process, regarding the long-term behavior of the
frequencies of its binary states. This situation occurs when the associated
binary state self-transition probabilities are balanced. To exemplify the
behavior of Markovian memory, two natural processes are selected. The first
example is studying the preferences of nonhuman troglodytes regarding
handedness. The Markovian model in this case assesses the extent of influence
two contiguous individuals may have on each other. The other example studies
the hindering of the quantum state transitions that rapid state measurements
introduce, known as the Quantum Zeno effect (QZE). Based on the current
methodology, simulations of the experimentally observed clustering of states
allowed for the estimation of the two self-transition probabilities in this
quantum system. Through these, one can appreciate how the particular hindering
of the evolution of a quantum state may have originated. The aim of this work
is to illustrate as merits of the current mathematical approach, its wide range
applicability and its potential to provide a variety of information regarding
the dynamics of the studied process.
"
"  We prove a novel result wherein the density function of the
gradients---corresponding to density function of the derivatives in one
dimension---of a thrice differentiable function S (obtained via a random
variable transformation of a uniformly distributed random variable) defined on
a closed, bounded interval \Omega \subset R is accurately approximated by the
normalized power spectrum of \phi=exp(iS/\tau) as the free parameter \tau-->0.
The result is shown using the well known stationary phase approximation and
standard integration techniques and requires proper ordering of limits.
Experimental results provide anecdotal visual evidence corroborating the
result.
"
"  We address the problem of the joint statistical inference of phylogenetic
trees and multiple sequence alignments from unaligned molecular sequences. This
problem is generally formulated in terms of string-valued evolutionary
processes along the branches of a phylogenetic tree. The classical evolutionary
process, the TKF91 model, is a continuous-time Markov chain model comprised of
insertion, deletion and substitution events. Unfortunately this model gives
rise to an intractable computational problem---the computation of the marginal
likelihood under the TKF91 model is exponential in the number of taxa. In this
work, we present a new stochastic process, the Poisson Indel Process (PIP), in
which the complexity of this computation is reduced to linear. The new model is
closely related to the TKF91 model, differing only in its treatment of
insertions, but the new model has a global characterization as a Poisson
process on the phylogeny. Standard results for Poisson processes allow key
computations to be decoupled, which yields the favorable computational profile
of inference under the PIP model. We present illustrative experiments in which
Bayesian inference under the PIP model is compared to separate inference of
phylogenies and alignments.
"
"  Estimating the dependences between random variables, and ranking them
accordingly, is a prevalent problem in machine learning. Pursuing frequentist
and information-theoretic approaches, we first show that the p-value and the
mutual information can fail even in simplistic situations. We then propose two
conditions for regularizing an estimator of dependence, which leads to a simple
yet effective new measure. We discuss its advantages and compare it to
well-established model-selection criteria. Apart from that, we derive a simple
constraint for regularizing parameter estimates in a graphical model. This
results in an analytical approximation for the optimal value of the equivalent
sample size, which agrees very well with the more involved Bayesian approach in
our experiments.
"
"  The goal of decentralized optimization over a network is to optimize a global
objective formed by a sum of local (possibly nonsmooth) convex functions using
only local computation and communication. It arises in various application
domains, including distributed tracking and localization, multi-agent
co-ordination, estimation in sensor networks, and large-scale optimization in
machine learning. We develop and analyze distributed algorithms based on dual
averaging of subgradients, and we provide sharp bounds on their convergence
rates as a function of the network size and topology. Our method of analysis
allows for a clear separation between the convergence of the optimization
algorithm itself and the effects of communication constraints arising from the
network structure. In particular, we show that the number of iterations
required by our algorithm scales inversely in the spectral gap of the network.
The sharpness of this prediction is confirmed both by theoretical lower bounds
and simulations for various networks. Our approach includes both the cases of
deterministic optimization and communication, as well as problems with
stochastic optimization and/or communication.
"
"  Probability models are proposed for passage time data collected in
experiments with a device designed to measure particle flow during aerial
application of fertilizer. Maximum likelihood estimation of flow intensity is
reviewed for the simple linear Boolean model, which arises with the assumption
that each particle requires the same known passage time. M-estimation is
developed for a generalization of the model in which passage times behave as a
random sample from a distribution with a known mean. The generalized model
improves fit in these experiments. An estimator of total particle flow is
constructed by conditioning on lengths of multi-particle clumps.
"
"  We consider the problem of finding the graph on which an epidemic cascade
spreads, given only the times when each node gets infected. While this is a
problem of importance in several contexts -- offline and online social
networks, e-commerce, epidemiology, vulnerabilities in infrastructure networks
-- there has been very little work, analytical or empirical, on finding the
graph. Clearly, it is impossible to do so from just one cascade; our interest
is in learning the graph from a small number of cascades.
  For the classic and popular ""independent cascade"" SIR epidemics, we
analytically establish the number of cascades required by both the global
maximum-likelihood (ML) estimator, and a natural greedy algorithm. Both results
are based on a key observation: the global graph learning problem decouples
into $n$ local problems -- one for each node. For a node of degree $d$, we show
that its neighborhood can be reliably found once it has been infected $O(d^2
\log n)$ times (for ML on general graphs) or $O(d\log n)$ times (for greedy on
trees). We also provide a corresponding information-theoretic lower bound of
$\Omega(d\log n)$; thus our bounds are essentially tight. Furthermore, if we
are given side-information in the form of a super-graph of the actual graph (as
is often the case), then the number of cascade samples required -- in all cases
-- becomes independent of the network size $n$.
  Finally, we show that for a very general SIR epidemic cascade model, the
Markov graph of infection times is obtained via the moralization of the network
graph.
"
"  Distinguishing between uniform and non-uniform sample distributions is a
common problem in directional data analysis; however for many tests,
non-uniform distributions exist that fail uniformity rejection. By merging
directional statistics with frame theory, we find that probabilistic tight
frames yield non-uniform distributions that minimize directional potentials,
leading to failure of uniformity rejection for the Bingham test. Finally, we
apply our results to model patterns found in granular rod experiments.
"
"  We consider forecasting the latent rate profiles of a time series of
inhomogeneous Poisson processes. The work is motivated by operations management
of queueing systems, in particular, telephone call centers, where accurate
forecasting of call arrival rates is a crucial primitive for efficient staffing
of such centers. Our forecasting approach utilizes dimension reduction through
a factor analysis of Poisson variables, followed by time series modeling of
factor score series. Time series forecasts of factor scores are combined with
factor loadings to yield forecasts of future Poisson rate profiles. Penalized
Poisson regressions on factor loadings guided by time series forecasts of
factor scores are used to generate dynamic within-process rate updating.
Methods are also developed to obtain distributional forecasts. Our methods are
illustrated using simulation and real data. The empirical results demonstrate
how forecasting and dynamic updating of call arrival rates can affect the
accuracy of call center staffing.
"
"  Acute infectious diseases are transmitted over networks of social contacts.
Epidemic models are used to predict the spread of emergent pathogens and
compare intervention strategies. Many of these models assume equal probability
of contact within mixing groups (homes, schools, etc.), but little work has
inferred the actual contact network, which may influence epidemic estimates. We
develop a penalized likelihood method to infer contact networks within
households, a key area for disease transmission. Using egocentric surveys of
contact behavior in Belgium, we estimate within-household contact networks for
six different age compositions. Our estimates show dependency in contact
behavior and vary substantively by age composition, with fewer contacts
occurring in older households. Our results are relevant for epidemic models
used to make policy recommendations.
"
"  Graphs provide an efficient tool for object representation in various
computer vision applications. Once graph-based representations are constructed,
an important question is how to compare graphs. This problem is often
formulated as a graph matching problem where one seeks a mapping between
vertices of two graphs which optimally aligns their structure. In the classical
formulation of graph matching, only one-to-one correspondences between vertices
are considered. However, in many applications, graphs cannot be matched
perfectly and it is more interesting to consider many-to-many correspondences
where clusters of vertices in one graph are matched to clusters of vertices in
the other graph. In this paper, we formulate the many-to-many graph matching
problem as a discrete optimization problem and propose an approximate algorithm
based on a continuous relaxation of the combinatorial problem. We compare our
method with other existing methods on several benchmark computer vision
datasets.
"
"  We introduce a procedure to automatically count and locate the fluorescent
particles in a microscopy image. Our procedure employs an approximate
likelihood estimator derived from a Poisson random field model for photon
emission. Estimates of standard errors are generated for each image along with
the parameter estimates, and the number of particles in the image is determined
using an information criterion and likelihood ratio tests. Realistic
simulations show that our procedure is robust and that it leads to accurate
estimates, both of parameters and of standard errors. This approach improves on
previous ad hoc least squares procedures by giving a more explicit stochastic
model for certain fluorescence images and by employing a consistent framework
for analysis.
"
"  Unlabeled shape analysis is a rapidly emerging and challenging area of
statistics. This has been driven by various novel applications in
bioinformatics. We consider here the situation where two configurations are
matched under various constraints, namely, the configurations have a subset of
manually located ""markers"" with high probability of matching each other while a
larger subset consists of unlabeled points. We consider a plausible model and
give an implementation using the EM algorithm. The work is motivated by a real
experiment of gels for renal cancer and our approach allows for the possibility
of missing and misallocated markers. The methodology is successfully used to
automatically locate and remove a grossly misallocated marker within the given
data set.
"
"  The spatial interaction between two or more classes (or species) has
important consequences in many fields and might cause multivariate clustering
patterns such as segregation or association. The spatial pattern of segregation
occurs when members of a class tend to be found near members of the same class
(i.e., conspecifics), while association occurs when members of a class tend to
be found near members of the other class or classes. These patterns can be
tested using a nearest neighbor contingency table (NNCT). The null hypothesis
is randomness in the nearest neighbor (NN) structure, which may result from --
among other patterns -- random labeling (RL) or complete spatial randomness
(CSR) of points from two or more classes (which is called the CSR independence,
henceforth). In this article, we consider Dixon's class-specific tests of
segregation and introduce a new class-specific test, which is a new
decomposition of Dixon's overall chi-square segregation statistic. We
demonstrate that the tests we consider provide information on different aspects
of the spatial interaction between the classes and they are conditional under
the CSR independence pattern, but not under the RL pattern. We analyze the
distributional properties and prove the consistency of these tests; compare the
empirical significant levels (Type I error rates) and empirical power estimates
of the tests using Monte Carlo simulations. We demonstrate that the new
class-specific tests also have comparable performance with the currently
available tests based on NNCTs in terms of Type I error and power estimates.
For illustrative purposes, we use three example data sets. We also provide
guidelines for using these tests.
"
"  We study the problem of supervised linear dimensionality reduction, taking an
information-theoretic viewpoint. The linear projection matrix is designed by
maximizing the mutual information between the projected signal and the class
label (based on a Shannon entropy measure). By harnessing a recent theoretical
result on the gradient of mutual information, the above optimization problem
can be solved directly using gradient descent, without requiring simplification
of the objective function. Theoretical analysis and empirical comparison are
made between the proposed method and two closely related methods (Linear
Discriminant Analysis and Information Discriminant Analysis), and comparisons
are also made with a method in which Renyi entropy is used to define the mutual
information (in this case the gradient may be computed simply, under a special
parameter setting). Relative to these alternative approaches, the proposed
method achieves promising results on real datasets.
"
"  Demixing problems in many areas such as hyperspectral imaging and
differential optical absorption spectroscopy (DOAS) often require finding
sparse nonnegative linear combinations of dictionary elements that match
observed data. We show how aspects of these problems, such as misalignment of
DOAS references and uncertainty in hyperspectral endmembers, can be modeled by
expanding the dictionary with grouped elements and imposing a structured
sparsity assumption that the combinations within each group should be sparse or
even 1-sparse. If the dictionary is highly coherent, it is difficult to obtain
good solutions using convex or greedy methods, such as non-negative least
squares (NNLS) or orthogonal matching pursuit. We use penalties related to the
Hoyer measure, which is the ratio of the $l_1$ and $l_2$ norms, as sparsity
penalties to be added to the objective in NNLS-type models. For solving the
resulting nonconvex models, we propose a scaled gradient projection algorithm
that requires solving a sequence of strongly convex quadratic programs. We
discuss its close connections to convex splitting methods and difference of
convex programming. We also present promising numerical results for example
DOAS analysis and hyperspectral demixing problems.
"
"  In this paper, we develop the continuous time dynamic topic model (cDTM). The
cDTM is a dynamic topic model that uses Brownian motion to model the latent
topics through a sequential collection of documents, where a ""topic"" is a
pattern of word use that we expect to evolve over the course of the collection.
We derive an efficient variational approximate inference algorithm that takes
advantage of the sparsity of observations in text, a property that lets us
easily handle many time points. In contrast to the cDTM, the original
discrete-time dynamic topic model (dDTM) requires that time be discretized.
Moreover, the complexity of variational inference for the dDTM grows quickly as
time granularity increases, a drawback which limits fine-grained
discretization. We demonstrate the cDTM on two news corpora, reporting both
predictive perplexity and the novel task of time stamp prediction.
"
"  The Random Projection Tree structures proposed in [Freund-Dasgupta STOC08]
are space partitioning data structures that automatically adapt to various
notions of intrinsic dimensionality of data. We prove new results for both the
RPTreeMax and the RPTreeMean data structures. Our result for RPTreeMax gives a
near-optimal bound on the number of levels required by this data structure to
reduce the size of its cells by a factor $s \geq 2$. We also prove a packing
lemma for this data structure. Our final result shows that low-dimensional
manifolds have bounded Local Covariance Dimension. As a consequence we show
that RPTreeMean adapts to manifold dimension as well.
"
"  In 1994, I came to Berkeley and was fortunate to stay there three years,
first as a postdoctoral researcher and then as Neyman Visiting Assistant
Professor. For me, this period was a unique opportunity to see other aspects
and learn many more things about statistics: the Department of Statistics at
Berkeley was much bigger and hence broader than my home at ETH Z\""urich and I
enjoyed very much that the science was perhaps a bit more speculative. As soon
as I settled in the department, I tried to get in touch with the local faculty.
Leo Breiman started a reading group on topics in machine learning and I didn't
hesitate to participate together with other Ph.D. students. Leo spread a
tremendous amount of enthusiasm, telling us about the vast opportunity we now
had by taking advantage of computational power. Hearing his views and opinions
and listening to his thoughts and ideas has been very exciting, stimulating and
entertaining as well. This was my first occasion to get to know Leo. And there
was, at least a bit, a vice-versa implication: now, Leo knew my name and who I
am. Whenever we saw each other on the 4th floor in Evans Hall, I got a very
gentle smile and ""hello"" from Leo. And in fact, this happened quite often: I
often walked around while thinking about a problem, and it seemed to me, that
Leo had a similar habit.
"
"  In this paper we study the applicability of the bootstrap to do inference on
Manski's maximum score estimator under the full generality of the model. We
propose three new, model-based bootstrap procedures for this problem and show
their consistency. Simulation experiments are carried out to evaluate their
performance and to compare them with subsampling methods. Additionally, we
prove a uniform convergence theorem for triangular arrays of random variables
coming from binary choice models, which may be of independent interest.
"
"  We propose an efficient algorithm for sparse signal reconstruction problems.
The proposed algorithm is an augmented Lagrangian method based on the dual
sparse reconstruction problem. It is efficient when the number of unknown
variables is much larger than the number of observations because of the dual
formulation. Moreover, the primal variable is explicitly updated and the
sparsity in the solution is exploited. Numerical comparison with the
state-of-the-art algorithms shows that the proposed algorithm is favorable when
the design matrix is poorly conditioned or dense and very large.
"
"  Our goal in this paper is to propose an alternative risk measure which takes
into account the fluctuations of losses and possible correlations between
random variables. This new notion of risk measures, that we call Copula
Conditional Tail Expectation describes the expected amount of risk that can be
experienced given that a potential bivariate risk exceeds a bivariate threshold
value, and provides an important measure for right-tail risk. An application to
real financial data is given.
"
"  With the growing interest on Network Analysis, Relational Data Mining is
becoming an emphasized domain of Data Mining. This paper addresses the problem
of extracting representative elements from a relational dataset. After defining
the notion of degree of representativeness, computed using the Borda
aggregation procedure, we present the extraction of exemplars which are the
representative elements of the dataset. We use these concepts to build a
network on the dataset. We expose the main properties of these notions and we
propose two typical applications of our framework. The first application
consists in resuming and structuring a set of binary images and the second in
mining co-authoring relation in a research team.
"
"  In this work we study numerical construction of optimal clinical diagnostic
tests for detecting sporadic Creutzfeldt-Jakob disease (sCJD). A cerebrospinal
fluid sample (CSF) from a suspected sCJD patient is subjected to a process
which initiates the aggregation of a protein present only in cases of sCJD.
This aggregation is indirectly observed in real-time at regular intervals, so
that a longitudinal set of data is constructed that is then analysed for
evidence of this aggregation. The best existing test is based solely on the
final value of this set of data, which is compared against a threshold to
conclude whether or not aggregation, and thus sCJD, is present. This test
criterion was decided upon by analysing data from a total of 108 sCJD and
non-sCJD samples, but this was done subjectively and there is no supporting
mathematical analysis declaring this criterion to be exploiting the available
data optimally. This paper addresses this deficiency, seeking to validate or
improve the test primarily via support vector machine (SVM) classification.
Besides this, we address a number of additional issues such as i) early
stopping of the measurement process, ii) the possibility of detecting the
particular type of sCJD and iii) the incorporation of additional patient data
such as age, sex, disease duration and timing of CSF sampling into the
construction of the test.
"
"  How can we model networks with a mathematically tractable model that allows
for rigorous analysis of network properties? Networks exhibit a long list of
surprising properties: heavy tails for the degree distribution; small
diameters; and densification and shrinking diameters over time. Most present
network models either fail to match several of the above properties, are
complicated to analyze mathematically, or both. In this paper we propose a
generative model for networks that is both mathematically tractable and can
generate networks that have the above mentioned properties. Our main idea is to
use the Kronecker product to generate graphs that we refer to as ""Kronecker
graphs"".
  First, we prove that Kronecker graphs naturally obey common network
properties. We also provide empirical evidence showing that Kronecker graphs
can effectively model the structure of real networks.
  We then present KronFit, a fast and scalable algorithm for fitting the
Kronecker graph generation model to large real networks. A naive approach to
fitting would take super- exponential time. In contrast, KronFit takes linear
time, by exploiting the structure of Kronecker matrix multiplication and by
using statistical simulation techniques.
  Experiments on large real and synthetic networks show that KronFit finds
accurate parameters that indeed very well mimic the properties of target
networks. Once fitted, the model parameters can be used to gain insights about
the network structure, and the resulting synthetic graphs can be used for null-
models, anonymization, extrapolations, and graph summarization.
"
"  Rejoinder to ""Brownian distance covariance"" by G\'abor J. Sz\'ekely and Maria
L. Rizzo [arXiv:1010.0297]
"
"  Biclustering, the process of simultaneously clustering the rows and columns
of a data matrix, is a popular and effective tool for finding structure in a
high-dimensional dataset. Many biclustering procedures appear to work well in
practice, but most do not have associated consistency guarantees. To address
this shortcoming, we propose a new biclustering procedure based on profile
likelihood. The procedure applies to a broad range of data modalities,
including binary, count, and continuous observations. We prove that the
procedure recovers the true row and column classes when the dimensions of the
data matrix tend to infinity, even if the functional form of the data
distribution is misspecified. The procedure requires computing a combinatorial
search, which can be expensive in practice. Rather than performing this search
directly, we propose a new heuristic optimization procedure based on the
Kernighan-Lin heuristic, which has nice computational properties and performs
well in simulations. We demonstrate our procedure with applications to
congressional voting records, and microarray analysis.
"
"  We study statistical detection of grayscale objects in noisy images. The
object of interest is of unknown shape and has an unknown intensity, that can
be varying over the object and can be negative. No boundary shape constraints
are imposed on the object, only a weak bulk condition for the object's interior
is required. We propose an algorithm that can be used to detect grayscale
objects of unknown shapes in the presence of nonparametric noise of unknown
level. Our algorithm is based on a nonparametric multiple testing procedure. We
establish the limit of applicability of our method via an explicit,
closed-form, non-asymptotic and nonparametric consistency bound. This bound is
valid for a wide class of nonparametric noise distributions. We achieve this by
proving an uncertainty principle for percolation on finite lattices.
"
"  In this paper, we review and apply several approaches to model selection for
analysis of variance models which are used in a credibility and insurance
context. The reversible jump algorithm is employed for model selection, where
posterior model probabilities are computed. We then apply this method to
insurance data from workers' compensation insurance schemes. The reversible
jump results are compared with the Deviance Information Criterion, and are
shown to be consistent.
"
"  We develop a unified approach for classification and regression support
vector machines for data subject to right censoring. We provide finite sample
bounds on the generalization error of the algorithm, prove risk consistency for
a wide class of probability measures, and study the associated learning rates.
We apply the general methodology to estimation of the (truncated) mean, median,
quantiles, and for classification problems. We present a simulation study that
demonstrates the performance of the proposed approach.
"
"  In this paper, we consider the problem of forming machine cell in cellular
manufacturing (CM). The major problem in the design of a CM system is to
identify the part families and machine groups and consequently to form
manufacturing cells. The aim of this article is to formulate a multivariate
approach based on a correlation analysis for solving cell formation problem.
The proposed approach is carried out in two phases. In the first phase, the
correlation matrix is used as an original similarity coefficient matrix. In the
second phase, Principal Component Analysis (PCA) is applied to find the
eigenvalues and eigenvectors on the correlation similarity matrix. A scatter
plot analysis as a cluster analysis is applied to make machine groups while
maximizing correlation between elements. A numerical example for the design of
cell structures is provided in order to illustrate the proposed approach. The
results of a comparative study based on multiple performance criteria show that
the present approach is very effective, efficient and practical
"
"  Classical statistical process control often relies on univariate
characteristics. In many contemporary applications, however, the quality of
products must be characterized by some functional relation between a response
variable and its explanatory variables. Monitoring such functional profiles has
been a rapidly growing field due to increasing demands. This paper develops a
novel nonparametric $L$-1 location-scale model to screen the shapes of
profiles. The model is built on three basic elements: location shifts, local
shape distortions, and overall shape deviations, which are quantified by three
individual metrics. The proposed approach is applied to the previously analyzed
vertical density profile data, leading to some interesting insights.
"
"  Correlations in the signal observed via functional Magnetic Resonance Imaging
(fMRI), are expected to reveal the interactions in the underlying neural
populations through hemodynamic response. In particular, they highlight
distributed set of mutually correlated regions that correspond to brain
networks related to different cognitive functions. Yet graph-theoretical
studies of neural connections give a different picture: that of a highly
integrated system with small-world properties: local clustering but with short
pathways across the complete structure. We examine the conditional independence
properties of the fMRI signal, i.e. its Markov structure, to find realistic
assumptions on the connectivity structure that are required to explain the
observed functional connectivity. In particular we seek a decomposition of the
Markov structure into segregated functional networks using decomposable graphs:
a set of strongly-connected and partially overlapping cliques. We introduce a
new method to efficiently extract such cliques on a large, strongly-connected
graph. We compare methods learning different graph structures from functional
connectivity by testing the goodness of fit of the model they learn on new
data. We find that summarizing the structure as strongly-connected networks can
give a good description only for very large and overlapping networks. These
results highlight that Markov models are good tools to identify the structure
of brain connectivity from fMRI signals, but for this purpose they must reflect
the small-world properties of the underlying neural systems.
"
"  The total measurable level of a pathogen is due to many sources, which
produce a variety of pulses, overlapping in time, that rise suddenly and then
decay. What is measured is the level of the total contribution of the sources
at a given time. But since we are only capable of measuring the total level
above some threshold $x_0$, we would like to predict the distribution below
this level. Our principal model assumption is that of the asymptotic
exponential decay of all pulses. We show that this implies a power law
distribution for the frequencies of low amplitude observations. As a
consequence, there is a simple extrapolation procedure for carrying the data to
the region below $x_0$.
"
"  In the Bayesian approach to sequential decision making, exact calculation of
the (subjective) utility is intractable. This extends to most special cases of
interest, such as reinforcement learning problems. While utility bounds are
known to exist for this problem, so far none of them were particularly tight.
In this paper, we show how to efficiently calculate a lower bound, which
corresponds to the utility of a near-optimal memoryless policy for the decision
problem, which is generally different from both the Bayes-optimal policy and
the policy which is optimal for the expected MDP under the current belief. We
then show how these can be applied to obtain robust exploration policies in a
Bayesian reinforcement learning setting.
"
"  Besides serving as prediction models, classification trees are useful for
finding important predictor variables and identifying interesting subgroups in
the data. These functions can be compromised by weak split selection algorithms
that have variable selection biases or that fail to search beyond local main
effects at each node of the tree. The resulting models may include many
irrelevant variables or select too few of the important ones. Either
eventuality can lead to erroneous conclusions. Four techniques to improve the
precision of the models are proposed and their effectiveness compared with that
of other algorithms, including tree ensembles, on real and simulated data sets.
"
"  A factor effect study was conducted on a set of observations at the
contingency of a series of plant species and bacteria species regarding the
antibacterial activity of essential oil extracts. The study reveals a very good
agreement between the observations and the hypothesis of independent and
multiplicative effect of plant and bacteria species factors on the
antibacterial activity. Shaping of the observable to a Negative Binomial
distribution allowed the separation of two convoluted Gamma distributions in
the observable further assigned to the distribution of factors. Statistics of
the Gamma distribution allowed estimating the ratio between diversity of plants
factors and bacteria factors in the antibacterial activity of essential oils
extracts.
"
"  This article introduces both a new algorithm for reconstructing
epsilon-machines from data, as well as the decisional states. These are defined
as the internal states of a system that lead to the same decision, based on a
user-provided utility or pay-off function. The utility function encodes some a
priori knowledge external to the system, it quantifies how bad it is to make
mistakes. The intrinsic underlying structure of the system is modeled by an
epsilon-machine and its causal states. The decisional states form a partition
of the lower-level causal states that is defined according to the higher-level
user's knowledge. In a complex systems perspective, the decisional states are
thus the ""emerging"" patterns corresponding to the utility function. The
transitions between these decisional states correspond to events that lead to a
change of decision. The new REMAPF algorithm estimates both the epsilon-machine
and the decisional states from data. Application examples are given for hidden
model reconstruction, cellular automata filtering, and edge detection in
images.
"
"  Shape-based regularization has proven to be a useful method for delineating
objects within noisy images where one has prior knowledge of the shape of the
targeted object. When a collection of possible shapes is available, the
specification of a shape prior using kernel density estimation is a natural
technique. Unfortunately, energy functionals arising from kernel density
estimation are of a form that makes them impossible to directly minimize using
efficient optimization algorithms such as graph cuts. Our main contribution is
to show how one may recast the energy functional into a form that is
minimizable iteratively and efficiently using graph cuts.
"
"  This paper deals with variable selection in the regression and binary
classification frameworks. It proposes an automatic and exhaustive procedure
which relies on the use of the CART algorithm and on model selection via
penalization. This work, of theoretical nature, aims at determining adequate
penalties, i.e. penalties which allow to get oracle type inequalities
justifying the performance of the proposed procedure. Since the exhaustive
procedure can not be executed when the number of variables is too big, a more
practical procedure is also proposed and still theoretically validated. A
simulation study completes the theoretical results.
"
"  We have developed a method to obtain robust quantitative bibliometric
indicators for several thousand scientists. This allows us to study the
dependence of bibliometric indicators (such as number of publications, number
of citations, Hirsch index...) on the age, position, etc. of CNRS scientists.
Our data suggests that the normalized h index (h divided by the career length)
is not constant for scientists with the same productivity but differents ages.
We also compare the predictions of several bibliometric indicators on the
promotions of about 600 CNRS researchers. Contrary to previous publications,
our study encompasses most disciplines, and shows that no single indicator is
the best predictor for all disciplines. Overall, however, the Hirsch index h
provides the least bad correlations, followed by the number of papers
published. It is important to realize however that even h is able to recover
only half of the actual promotions. The number of citations or the mean number
of citations per paper are definitely not good predictors of promotion.
"
"  Thompson Sampling is one of the oldest heuristics for multi-armed bandit
problems. It is a randomized algorithm based on Bayesian ideas, and has
recently generated significant interest after several studies demonstrated it
to have better empirical performance compared to the state-of-the-art methods.
However, many questions regarding its theoretical performance remained open. In
this paper, we design and analyze a generalization of Thompson Sampling
algorithm for the stochastic contextual multi-armed bandit problem with linear
payoff functions, when the contexts are provided by an adaptive adversary. This
is among the most important and widely studied versions of the contextual
bandits problem. We provide the first theoretical guarantees for the contextual
version of Thompson Sampling. We prove a high probability regret bound of
$\tilde{O}(d^{3/2}\sqrt{T})$ (or $\tilde{O}(d\sqrt{T \log(N)})$), which is the
best regret bound achieved by any computationally efficient algorithm available
for this problem in the current literature, and is within a factor of
$\sqrt{d}$ (or $\sqrt{\log(N)}$) of the information-theoretic lower bound for
this problem.
"
"  After an elementary derivation of the ""time transformation"", mapping a
counting process onto a homogeneous Poisson process with rate one, a brief
review of Ogata's goodness of fit tests is presented and a new test, the
""Wiener process test"", is proposed. This test is based on a straightforward
application of Donsker's Theorem to the intervals of time transformed counting
processes. The finite sample properties of the test are studied by Monte Carlo
simulations. Performances on simulated as well as on real data are presented.
It is argued that due to its good finite sample properties, the new test is
both a simple and a useful complement to Ogata's tests. Warnings are moreover
given against the use of a single goodness of fit test.
"
"  Flow cytometry is a technology that rapidly measures antigen-based markers
associated to cells in a cell population. Although analysis of flow cytometry
data has traditionally considered one or two markers at a time, there has been
increasing interest in multidimensional analysis. However, flow cytometers are
limited in the number of markers they can jointly observe, which is typically a
fraction of the number of markers of interest. For this reason, practitioners
often perform multiple assays based on different, overlapping combinations of
markers. In this paper, we address the challenge of imputing the high
dimensional jointly distributed values of marker attributes based on
overlapping marginal observations. We show that simple nearest neighbor based
imputation can lead to spurious subpopulations in the imputed data, and
introduce an alternative approach based on nearest neighbor imputation
restricted to a cell's subpopulation. This requires us to perform clustering
with missing data, which we address with a mixture model approach and novel EM
algorithm. Since mixture model fitting may be ill-posed, we also develop
techniques to initialize the EM algorithm using domain knowledge. We
demonstrate our approach on real flow cytometry data.
"
"  We generalize Newton-type methods for minimizing smooth functions to handle a
sum of two convex functions: a smooth function and a nonsmooth function with a
simple proximal mapping. We show that the resulting proximal Newton-type
methods inherit the desirable convergence behavior of Newton-type methods for
minimizing smooth functions, even when search directions are computed
inexactly. Many popular methods tailored to problems arising in bioinformatics,
signal processing, and statistical learning are special cases of proximal
Newton-type methods, and our analysis yields new convergence results for some
of these methods.
"
"  The receiver operating characteristic (ROC) curve is a very useful tool for
analyzing the diagnostic/classification power of instruments/classification
schemes as long as a binary-scale gold standard is available. When the gold
standard is continuous and there is no confirmative threshold, ROC curve
becomes less useful. Hence, there are several extensions proposed for
evaluating the diagnostic potential of variables of interest. However, due to
the computational difficulties of these nonparametric based extensions, they
are not easy to be used for finding the optimal combination of variables to
improve the individual diagnostic power. Therefore, we propose a new measure,
which extends the AUC index for identifying variables with good potential to be
used in a diagnostic scheme. In addition, we propose a threshold gradient
descent based algorithm for finding the best linear combination of variables
that maximizes this new measure, which is applicable even when the number of
variables is huge. The estimate of the proposed index and its asymptotic
property are studied. The performance of the proposed method is illustrated
using both synthesized and real data sets.
"
"  Often, high dimensional data lie close to a low-dimensional submanifold and
it is of interest to understand the geometry of these submanifolds. The
homology groups of a manifold are important topological invariants that provide
an algebraic summary of the manifold. These groups contain rich topological
information, for instance, about the connected components, holes, tunnels and
sometimes the dimension of the manifold. In this paper, we consider the
statistical problem of estimating the homology of a manifold from noisy samples
under several different noise models. We derive upper and lower bounds on the
minimax risk for this problem. Our upper bounds are based on estimators which
are constructed from a union of balls of appropriate radius around carefully
selected points. In each case we establish complementary lower bounds using Le
Cam's lemma.
"
"  Direct quantile regression involves estimating a given quantile of a response
variable as a function of input variables. We present a new framework for
direct quantile regression where a Gaussian process model is learned,
minimising the expected tilted loss function. The integration required in
learning is not analytically tractable so to speed up the learning we employ
the Expectation Propagation algorithm. We describe how this work relates to
other quantile regression methods and apply the method on both synthetic and
real data sets. The method is shown to be competitive with state of the art
methods whilst allowing for the leverage of the full Gaussian process
probabilistic framework.
"
"  Applied researchers often find themselves making statistical inferences in
settings that would seem to require multiple comparisons adjustments. We
challenge the Type I error paradigm that underlies these corrections. Moreover
we posit that the problem of multiple comparisons can disappear entirely when
viewed from a hierarchical Bayesian perspective. We propose building multilevel
models in the settings where multiple comparisons arise.
  Multilevel models perform partial pooling (shifting estimates toward each
other), whereas classical procedures typically keep the centers of intervals
stationary, adjusting for multiple comparisons by making the intervals wider
(or, equivalently, adjusting the $p$-values corresponding to intervals of fixed
width). Thus, multilevel models address the multiple comparisons problem and
also yield more efficient estimates, especially in settings with low
group-level variation, which is where multiple comparisons are a particular
concern.
"
"  Dozens of research centers, foundations, international organizations and
scientific societies, including the Institute of Mathematical Statistics, have
joined forces to celebrate 2013 as a special year for the Mathematics of Planet
Earth. In its five-year history, the Annals of Applied Statistics has been
publishing cutting edge research in this area, including geophysical,
biological and socio-economic aspects of planet Earth, with the special section
on statistics in the atmospheric sciences edited by Fuentes, Guttorp and Stein
(2008) and the discussion paper by McShane and Wyner (2011) on paleoclimate
reconstructions [Stein (2011)] having been highlights.
"
"  The aim of this work is to establish the personal income distribution from
the elementary constituents of a free market; products of a representative good
and agents forming the economic network. The economy is treated as a
self-organized system. Based on the idea that the dynamics of an economy is
governed by slow modes, the model suggests that for short time intervals a
fixed ratio of total labour income (capital income) to net income exists
(Cobb-Douglas relation). Explicitly derived is Gibrat's law from an
evolutionary market dynamics of short term fluctuations. The total private
income distribution is shown to consist of four main parts. From capital income
of private firms the income distribution contains a lognormal distribution for
small and a Pareto tail for large incomes. Labour income contributes an
exponential distribution. Also included is the income from a social insurance
system, approximated by a Gaussian peak. The evolutionary model is able to
reproduce the stylized facts of the income distribution, shown by a comparison
with empirical data of a high resolution income distribution. The theory
suggests that in a free market competition between products is ultimately the
origin of the uneven income distribution.
"
"  Rejoinder to ""A statistical analysis of multiple temperature proxies: Are
reconstructions of surface temperatures over the last 1000 years reliable?"" by
B.B. McShane and A.J. Wyner [arXiv:1104.4002]
"
"  The article proposes an expert system for detection, and subsequent
investigation, of groups of collaborating automobile insurance fraudsters. The
system is described and examined in great detail, several technical
difficulties in detecting fraud are also considered, for it to be applicable in
practice. Opposed to many other approaches, the system uses networks for
representation of data. Networks are the most natural representation of such a
relational domain, allowing formulation and analysis of complex relations
between entities. Fraudulent entities are found by employing a novel assessment
algorithm, \textit{Iterative Assessment Algorithm} (\textit{IAA}), also
presented in the article. Besides intrinsic attributes of entities, the
algorithm explores also the relations between entities. The prototype was
evaluated and rigorously analyzed on real world data. Results show that
automobile insurance fraud can be efficiently detected with the proposed system
and that appropriate data representation is vital.
"
"  Undirected graphs are often used to describe high dimensional distributions.
Under sparsity conditions, the graph can be estimated using $\ell_1$
penalization methods. However, current methods assume that the data are
independent and identically distributed. If the distribution, and hence the
graph, evolves over time then the data are not longer identically distributed.
In this paper, we show how to estimate the sequence of graphs for
non-identically distributed data, where the distribution evolves over time.
"
"  To perform a queuing analysis or design in a communications context, we need
to estimate the values of the input parameters, specifically the mean of the
arrival rate and service time. In this paper, we propose an approach for
estimating the arrival rate of Poisson processes and the average service time
for servers under the assumption that the service time is exponential. In
particular, we derive sample size (i.e., the number of i.i.d. observations)
required to obtain an estimate satisfying a pre-specified relative accuracy
with a given confidence level. A remarkable feature of this approach is that no
a priori information about the parameter is needed. In contrast to conventional
methods such as, standard error estimation and confidence interval
construction, which only provides post-experimental evaluations of the
estimate, this approach allows experimenters to rigorously control the error of
estimation.
"
"  Exponential random graph models are extremely difficult models to handle from
a statistical viewpoint, since their normalising constant, which depends on
model parameters, is available only in very trivial cases. We show how
inference can be carried out in a Bayesian framework using a MCMC algorithm,
which circumvents the need to calculate the normalising constants. We use a
population MCMC approach which accelerates convergence and improves mixing of
the Markov chain. This approach improves performance with respect to the Monte
Carlo maximum likelihood method of Geyer and Thompson (1992).
"
"  A method is given for calculating the strict minimum message length (SMML)
estimator for 1-dimensional exponential families with continuous sufficient
statistics. A set of $n$ equations are found that the $n$ cut-points of the
SMML estimator must satisfy. These equations can be solved using Newton's
method and this approach is used to produce new results and to replicate
results that C. S. Wallace obtained using his boundary rules for the SMML
estimator. A rigorous proof is also given that, despite being composed of step
functions, the posterior probability corresponding to the SMML estimator is a
continuous function of the data.
"
"  We present a streaming model for large-scale classification (in the context
of $\ell_2$-SVM) by leveraging connections between learning and computational
geometry. The streaming model imposes the constraint that only a single pass
over the data is allowed. The $\ell_2$-SVM is known to have an equivalent
formulation in terms of the minimum enclosing ball (MEB) problem, and an
efficient algorithm based on the idea of \emph{core sets} exists (Core Vector
Machine, CVM). CVM learns a $(1+\varepsilon)$-approximate MEB for a set of
points and yields an approximate solution to corresponding SVM instance.
However CVM works in batch mode requiring multiple passes over the data. This
paper presents a single-pass SVM which is based on the minimum enclosing ball
of streaming data. We show that the MEB updates for the streaming case can be
easily adapted to learn the SVM weight vector in a way similar to using online
stochastic gradient updates. Our algorithm performs polylogarithmic computation
at each example, and requires very small and constant storage. Experimental
results show that, even in such restrictive settings, we can learn efficiently
in just one pass and get accuracies comparable to other state-of-the-art SVM
solvers (batch and online). We also give an analysis of the algorithm, and
discuss some open issues and possible extensions.
"
"  Approximate Bayesian inference on the basis of summary statistics is
well-suited to complex problems for which the likelihood is either
mathematically or computationally intractable. However the methods that use
rejection suffer from the curse of dimensionality when the number of summary
statistics is increased. Here we propose a machine-learning approach to the
estimation of the posterior density by introducing two innovations. The new
method fits a nonlinear conditional heteroscedastic regression of the parameter
on the summary statistics, and then adaptively improves estimation using
importance sampling. The new algorithm is compared to the state-of-the-art
approximate Bayesian methods, and achieves considerable reduction of the
computational burden in two examples of inference in statistical genetics and
in a queueing model.
"
"  Most accurate predictions are typically obtained by learning machines with
complex feature spaces (as e.g. induced by kernels). Unfortunately, such
decision rules are hardly accessible to humans and cannot easily be used to
gain insights about the application domain. Therefore, one often resorts to
linear models in combination with variable selection, thereby sacrificing some
predictive power for presumptive interpretability. Here, we introduce the
Feature Importance Ranking Measure (FIRM), which by retrospective analysis of
arbitrary learning machines allows to achieve both excellent predictive
performance and superior interpretation. In contrast to standard raw feature
weighting, FIRM takes the underlying correlation structure of the features into
account. Thereby, it is able to discover the most relevant features, even if
their appearance in the training data is entirely prevented by noise. The
desirable properties of FIRM are investigated analytically and illustrated in
simulations.
"
"  We revisit the Kolmogorov-Smirnov and Cram\'er-von Mises goodness-of-fit
(GoF) tests and propose a generalisation to identically distributed, but
dependent univariate random variables. We show that the dependence leads to a
reduction of the ""effective"" number of independent observations. The
generalised GoF tests are not distribution-free but rather depend on all the
lagged bivariate copulas. These objects, that we call ""self-copulas"", encode
all the non-linear temporal dependences. We introduce a specific, log-normal
model for these self-copulas, for which a number of analytical results are
derived. An application to financial time series is provided. As is well known,
the dependence is to be long-ranged in this case, a finding that we confirm
using self-copulas. As a consequence, the acceptance rates for GoF tests are
substantially higher than if the returns were iid random variables.
"
"  We present a second iteration of a machine learning approach to static code
analysis and fingerprinting for weaknesses related to security, software
engineering, and others using the open-source MARF framework and the MARFCAT
application based on it for the NIST's SATE IV static analysis tool exposition
workshop's data sets that include additional test cases, including new large
synthetic cases. To aid detection of weak or vulnerable code, including source
or binary on different platforms the machine learning approach proved to be
fast and accurate to for such tasks where other tools are either much slower or
have much smaller recall of known vulnerabilities. We use signal and NLP
processing techniques in our approach to accomplish the identification and
classification tasks. MARFCAT's design from the beginning in 2010 made is
independent of the language being analyzed, source code, bytecode, or binary.
In this follow up work with explore some preliminary results in this area. We
evaluated also additional algorithms that were used to process the data.
"
"  We study the stability vis a vis adversarial noise of matrix factorization
algorithm for matrix completion. In particular, our results include: (I) we
bound the gap between the solution matrix of the factorization method and the
ground truth in terms of root mean square error; (II) we treat the matrix
factorization as a subspace fitting problem and analyze the difference between
the solution subspace and the ground truth; (III) we analyze the prediction
error of individual users based on the subspace stability. We apply these
results to the problem of collaborative filtering under manipulator attack,
which leads to useful insights and guidelines for collaborative filtering
system design.
"
"  In 2004, the UK Government's Department for Environment, Food and Rural
Affairs commissioned research with the aim of developing a scheme for assessing
the risks posed to species, habitats and ecosystems in the UK by non-native
organisms. The outcome was the UK Non-Native Organism Risk Assessment Scheme.
Unfortunately, the mathematical basis of the procedure for summarising risks
deployed in the Risk Assessment Scheme, as outlined in Baker et al. (2008) and
described in more detail in the Risk Assessment Scheme's User Manual, contains
several analytical errors. These errors are outlined in the notes that follow.
"
"  In sparse regression modeling via regularization such as the lasso, it is
important to select appropriate values of tuning parameters including
regularization parameters. The choice of tuning parameters can be viewed as a
model selection and evaluation problem. Mallows' $C_p$ type criteria may be
used as a tuning parameter selection tool in lasso-type regularization methods,
for which the concept of degrees of freedom plays a key role. In the present
paper, we propose an efficient algorithm that computes the degrees of freedom
by extending the generalized path seeking algorithm. Our procedure allows us to
construct model selection criteria for evaluating models estimated by
regularization with a wide variety of convex and non-convex penalties. Monte
Carlo simulations demonstrate that our methodology performs well in various
situations. A real data example is also given to illustrate our procedure.
"
"  Telemonitoring of electroencephalogram (EEG) through wireless body-area
networks is an evolving direction in personalized medicine. Among various
constraints in designing such a system, three important constraints are energy
consumption, data compression, and device cost. Conventional data compression
methodologies, although effective in data compression, consumes significant
energy and cannot reduce device cost. Compressed sensing (CS), as an emerging
data compression methodology, is promising in catering to these constraints.
However, EEG is non-sparse in the time domain and also non-sparse in
transformed domains (such as the wavelet domain). Therefore, it is extremely
difficult for current CS algorithms to recover EEG with the quality that
satisfies the requirements of clinical diagnosis and engineering applications.
Recently, Block Sparse Bayesian Learning (BSBL) was proposed as a new method to
the CS problem. This study introduces the technique to the telemonitoring of
EEG. Experimental results show that its recovery quality is better than
state-of-the-art CS algorithms, and sufficient for practical use. These results
suggest that BSBL is very promising for telemonitoring of EEG and other
non-sparse physiological signals.
"
"  Notwithstanding the popularity of conventional clustering algorithms such as
K-means and probabilistic clustering, their clustering results are sensitive to
the presence of outliers in the data. Even a few outliers can compromise the
ability of these algorithms to identify meaningful hidden structures rendering
their outcome unreliable. This paper develops robust clustering algorithms that
not only aim to cluster the data, but also to identify the outliers. The novel
approaches rely on the infrequent presence of outliers in the data which
translates to sparsity in a judiciously chosen domain. Capitalizing on the
sparsity in the outlier domain, outlier-aware robust K-means and probabilistic
clustering approaches are proposed. Their novelty lies on identifying outliers
while effecting sparsity in the outlier domain through carefully chosen
regularization. A block coordinate descent approach is developed to obtain
iterative algorithms with convergence guarantees and small excess computational
complexity with respect to their non-robust counterparts. Kernelized versions
of the robust clustering algorithms are also developed to efficiently handle
high-dimensional data, identify nonlinearly separable clusters, or even cluster
objects that are not represented by vectors. Numerical tests on both synthetic
and real datasets validate the performance and applicability of the novel
algorithms.
"
"  We improve existing results in the field of compressed sensing and matrix
completion when sampled data may be grossly corrupted. We introduce three new
theorems. 1) In compressed sensing, we show that if the m \times n sensing
matrix has independent Gaussian entries, then one can recover a sparse signal x
exactly by tractable \ell1 minimimization even if a positive fraction of the
measurements are arbitrarily corrupted, provided the number of nonzero entries
in x is O(m/(log(n/m) + 1)). 2) In the very general sensing model introduced in
""A probabilistic and RIPless theory of compressed sensing"" by Candes and Plan,
and assuming a positive fraction of corrupted measurements, exact recovery
still holds if the signal now has O(m/(log^2 n)) nonzero entries. 3) Finally,
we prove that one can recover an n \times n low-rank matrix from m corrupted
sampled entries by tractable optimization provided the rank is on the order of
O(m/(n log^2 n)); again, this holds when there is a positive fraction of
corrupted samples.
"
"  Gaussian processes (GP) are powerful tools for probabilistic modeling
purposes. They can be used to define prior distributions over latent functions
in hierarchical Bayesian models. The prior over functions is defined implicitly
by the mean and covariance function, which determine the smoothness and
variability of the function. The inference can then be conducted directly in
the function space by evaluating or approximating the posterior process.
Despite their attractive theoretical properties GPs provide practical
challenges in their implementation. GPstuff is a versatile collection of
computational tools for GP models compatible with Linux and Windows MATLAB and
Octave. It includes, among others, various inference methods, sparse
approximations and tools for model assessment. In this work, we review these
tools and demonstrate the use of GPstuff in several models.
"
"  Meaning can be generated when information is related at a systemic level.
Such a system can be an observer, but also a discourse, for example,
operationalized as a set of documents. The measurement of semantics as
similarity in patterns (correlations) and latent variables (factor analysis)
has been enhanced by computer techniques and the use of statistics; for
example, in ""Latent Semantic Analysis"". This communication provides an
introduction, an example, pointers to relevant software, and summarizes the
choices that can be made by the analyst. Visualization (""semantic mapping"") is
thus made more accessible.
"
"  Several approaches have been developed for forecasting mortality using the
stochastic model. In particular, the Lee-Carter model has become widely used
and there have been various extensions and modifications proposed to attain a
broader interpretation and to capture the main features of the dynamics of the
mortality intensity. Hyndman-Ullah show a particular version of the Lee-Carter
methodology, the so-called Functional Demographic Model, which is one of the
most accurate approaches as regards some mortality data, particularly for
longer forecast horizons where the benefit of a damped trend forecast is
greater. The paper objective is properly to single out the most suitable model
between the basic Lee-Carter and the Functional Demographic Model to the
Italian mortality data. A comparative assessment is made and the empirical
results are presented using a range of graphical analyses.
"
"  A new microeconomic model is presented that aims at a description of the
long-term unit sales and price evolution of homogeneous non-durable goods in
polypoly markets. It merges the product lifecycle approach with the price
dispersion dynamics of homogeneous goods. The model predicts a minimum critical
lifetime of non-durables in order to survive. Under the condition that the
supply side of the market evolves much faster than the demand side the theory
suggests that unsatisfied demands are present in the first stages of the
lifecycle. With the growth of production capacities these demands disappear
accompanied with a logistic decrease of the mean price of the good. The model
is applied to electricity as a non-durable satisfying the model condition. The
presented theory allows a deeper understanding of the sales and price dynamics
of non-durables.
"
"  Fractals are self-similar and scale-invariant patterns found ubiquitously in
nature. A lot of evidences implying fractal properties such as 1/f power
spectrums have been also observed in resting state fMRI time series. To explain
the fractal behavior in rs-fMRI, we have proposed the fractal-based model of
resting state hemodynamic response function (rs-HRF) whose properties can be
summarized by a fractal exponent. Here we show, through a simulation studies,
that the fractal behavior of cerebral hemodynamics may cause significant
distortion of network properties between neuronal activities and BOLD signals.
We simulated neuronal population activities based on the stochastic neural
field model from the Macaque brain network, and then obtained their
corresponding BOLD signals by convolving them with the rs-HRF filter. The
precision of centrality estimated in each node was deteriorated overall in
three networks based on transfer entropy, mutual information, and Pearson
correlation; particularly the distortion of transfer entropy was more sensitive
to the standard deviation of fractal exponents. A node with high centrality was
resilient to desynchronized fractal dynamics over all frequencies while a node
with small centrality exhibited huge distortion of both wavelet correlation and
centrality over low frequencies. This theoretical expectation indicates that
the difference of fractal exponents between brain regions leads to discrepancy
of statistical network properties, especially at nodes with small centrality,
between neuronal activities and BOLD signals, and that the traditional
definitions of resting state functional connectivity may not effectively
reflect the dynamics of spontaneous neuronal activities.
"
"  We prove the statistical consistency of kernel Partial Least Squares
Regression applied to a bounded regression learning problem on a reproducing
kernel Hilbert space. Partial Least Squares stands out of well-known classical
approaches as e.g. Ridge Regression or Principal Components Regression, as it
is not defined as the solution of a global cost minimization procedure over a
fixed model nor is it a linear estimator. Instead, approximate solutions are
constructed by projections onto a nested set of data-dependent subspaces. To
prove consistency, we exploit the known fact that Partial Least Squares is
equivalent to the conjugate gradient algorithm in combination with early
stopping. The choice of the stopping rule (number of iterations) is a crucial
point. We study two empirical stopping rules. The first one monitors the
estimation error in each iteration step of Partial Least Squares, and the
second one estimates the empirical complexity in terms of a condition number.
Both stopping rules lead to universally consistent estimators provided the
kernel is universal.
"
"  One popular approach for nonstructural economic and financial forecasting is
to include a large number of economic and financial variables, which has been
shown to lead to significant improvements for forecasting, for example, by the
dynamic factor models. A challenging issue is to determine which variables and
(their) lags are relevant, especially when there is a mixture of serial
correlation (temporal dynamics), high dimensional (spatial) dependence
structure and moderate sample size (relative to dimensionality and lags). To
this end, an \textit{integrated} solution that addresses these three challenges
simultaneously is appealing. We study the large vector auto regressions here
with three types of estimates. We treat each variable's own lags different from
other variables' lags, distinguish various lags over time, and is able to
select the variables and lags simultaneously. We first show the consequences of
using Lasso type estimate directly for time series without considering the
temporal dependence. In contrast, our proposed method can still produce an
estimate as efficient as an \textit{oracle} under such scenarios. The tuning
parameters are chosen via a data driven ""rolling scheme"" method to optimize the
forecasting performance. A macroeconomic and financial forecasting problem is
considered to illustrate its superiority over existing estimators.
"
"  Using the InCites tool of Thomson Reuters, this study compares normalized
citation impact values calculated for China, Japan, France, Germany, United
States, and the UK throughout the time period from 1981 to 2010. The citation
impact values are normalized to four subject areas: natural sciences;
engineering and technology; medical and health sciences; and agricultural
sciences. The results show an increasing trend in citation impact values for
France, the UK and especially for Germany across the last thirty years in all
subject areas. The citation impact of papers from China is still at a
relatively low level (mostly below the world average), but the country follows
an increasing trend line. The USA exhibits a relatively stable pattern of high
citation impact values across the years. With small impact differences between
the publication years, the US trend is increasing in engineering and technology
but decreasing in medical and health sciences as well as in agricultural
sciences. Similar to the USA, Japan follows increasing as well as decreasing
trends in different subject areas, but the variability across the years is
small. In most of the years, papers from Japan perform below or approximately
at the world average in each subject area.
"
"  Affinity propagation is an exemplar-based clustering algorithm that finds a
set of data-points that best exemplify the data, and associates each datapoint
with one exemplar. We extend affinity propagation in a principled way to solve
the hierarchical clustering problem, which arises in a variety of domains
including biology, sensor networks and decision making in operational research.
We derive an inference algorithm that operates by propagating information up
and down the hierarchy, and is efficient despite the high-order potentials
required for the graphical model formulation. We demonstrate that our method
outperforms greedy techniques that cluster one layer at a time. We show that on
an artificial dataset designed to mimic the HIV-strain mutation dynamics, our
method outperforms related methods. For real HIV sequences, where the ground
truth is not available, we show our method achieves better results, in terms of
the underlying objective function, and show the results correspond meaningfully
to geographical location and strain subtypes. Finally we report results on
using the method for the analysis of mass spectra, showing it performs
favorably compared to state-of-the-art methods.
"
"  Nonparametric Bayesian approaches to clustering, information retrieval,
language modeling and object recognition have recently shown great promise as a
new paradigm for unsupervised data analysis. Most contributions have focused on
the Dirichlet process mixture models or extensions thereof for which efficient
Gibbs samplers exist. In this paper we explore Gibbs samplers for infinite
complexity mixture models in the stick breaking representation. The advantage
of this representation is improved modeling flexibility. For instance, one can
design the prior distribution over cluster sizes or couple multiple infinite
mixture models (e.g. over time) at the level of their parameters (i.e. the
dependent Dirichlet process model). However, Gibbs samplers for infinite
mixture models (as recently introduced in the statistics literature) seem to
mix poorly over cluster labels. Among others issues, this can have the adverse
effect that labels for the same cluster in coupled mixture models are mixed up.
We introduce additional moves in these samplers to improve mixing over cluster
labels and to bring clusters into correspondence. An application to modeling of
storm trajectories is used to illustrate these ideas.
"
"  The vector autoregressive (VAR) model has been widely used for modeling
temporal dependence in a multivariate time series. For large (and even
moderate) dimensions, the number of AR coefficients can be prohibitively large,
resulting in noisy estimates, unstable predictions and difficult-to-interpret
temporal dependence. To overcome such drawbacks, we propose a 2-stage approach
for fitting sparse VAR (sVAR) models in which many of the AR coefficients are
zero. The first stage selects non-zero AR coefficients based on an estimate of
the partial spectral coherence (PSC) together with the use of BIC. The PSC is
useful for quantifying the conditional relationship between marginal series in
a multivariate process. A refinement second stage is then applied to further
reduce the number of parameters. The performance of this 2-stage approach is
illustrated with simulation results. The 2-stage approach is also applied to
two real data examples: the first is the Google Flu Trends data and the second
is a time series of concentration levels of air pollutants.
"
"  There are many sources of error in counting votes: the apparent winner might
not be the rightful winner. Hand tallies of the votes in a random sample of
precincts can be used to test the hypothesis that a full manual recount would
find a different outcome. This paper develops a conservative sequential test
based on the vote-counting errors found in a hand tally of a simple or
stratified random sample of precincts. The procedure includes a natural
escalation: If the hypothesis that the apparent outcome is incorrect is not
rejected at stage $s$, more precincts are audited. Eventually, either the
hypothesis is rejected--and the apparent outcome is confirmed--or all precincts
have been audited and the true outcome is known. The test uses a priori bounds
on the overstatement of the margin that could result from error in each
precinct. Such bounds can be derived from the reported counts in each precinct
and upper bounds on the number of votes cast in each precinct. The test allows
errors in different precincts to be treated differently to reflect voting
technology or precinct sizes. It is not optimal, but it is conservative: the
chance of erroneously confirming the outcome of a contest if a full manual
recount would show a different outcome is no larger than the nominal
significance level. The approach also gives a conservative $P$-value for the
hypothesis that a full manual recount would find a different outcome, given the
errors found in a fixed size sample. This is illustrated with two contests from
November, 2006: the U.S. Senate race in Minnesota and a school board race for
the Sausalito Marin City School District in California, a small contest in
which voters could vote for up to three candidates.
"
"  Parameter estimation in multidimensional diffusion models with only one
coordinate observed is highly relevant in many biological applications, but a
statistically difficult problem. In neuroscience, the membrane potential
evolution in single neurons can be measured at high frequency, but biophysical
realistic models have to include the unobserved dynamics of ion channels. One
such model is the stochastic Morris-Lecar model, defined by a nonlinear
two-dimensional stochastic differential equation. The coordinates are coupled,
that is, the unobserved coordinate is nonautonomous, the model exhibits
oscillations to mimic the spiking behavior, which means it is not of
gradient-type, and the measurement noise from intracellular recordings is
typically negligible. Therefore, the hidden Markov model framework is
degenerate, and available methods break down. The main contributions of this
paper are an approach to estimate in this ill-posed situation and nonasymptotic
convergence results for the method. Specifically, we propose a sequential Monte
Carlo particle filter algorithm to impute the unobserved coordinate, and then
estimate parameters maximizing a pseudo-likelihood through a stochastic version
of the Expectation-Maximization algorithm. It turns out that even the rate
scaling parameter governing the opening and closing of ion channels of the
unobserved coordinate can be reasonably estimated. An experimental data set of
intracellular recordings of the membrane potential of a spinal motoneuron of a
red-eared turtle is analyzed, and the performance is further evaluated in a
simulation study.
"
"  In this paper, we discuss an extension of the Split Hamiltonian Monte Carlo
(Split HMC) method for Gaussian process model (GPM). This method is based on
splitting the Hamiltonian in a way that allows much of the movement around the
state space to be done at low computational cost. To this end, we approximate
the negative log density (i.e., the energy function) of the distribution of
interest by a quadratic function U0 for which Hamiltonian dynamics can be
solved analytically. The overall energy function U is then written as U0 + U1,
where U1 is the approximation error. The Hamiltonian is then split into two
parts; one part is based on U0 is handled analytically, the other part is based
on U1 for which we approximate Hamiltonian's equations by discretizing time. We
use simulated and real data to compare the performance of our method to the
standard HMC. We find that splitting the Hamiltonian for GP models could lead
to substantial improvement (up to 10 folds) of sampling efficiency, which is
measured in terms of the amount of time required for producing an independent
sample with high acceptance probability from posterior distributions.
"
"  With the availability of the huge amounts of data produced by current and
future large multi-band photometric surveys, photometric redshifts have become
a crucial tool for extragalactic astronomy and cosmology. In this paper we
present a novel method, called Weak Gated Experts (WGE), which allows to derive
photometric redshifts through a combination of data mining techniques.
\noindent The WGE, like many other machine learning techniques, is based on the
exploitation of a spectroscopic knowledge base composed by sources for which a
spectroscopic value of the redshift is available. This method achieves a
variance \sigma^2(\Delta z)=2.3x10^{-4} (\sigma^2(\Delta z) =0.08), where
\Delta z = z_{phot} - z_{spec}) for the reconstruction of the photometric
redshifts for the optical galaxies from the SDSS and for the optical quasars
respectively, while the Root Mean Square (RMS) of the \Delta z variable
distributions for the two experiments is respectively equal to 0.021 and 0.35.
The WGE provides also a mechanism for the estimation of the accuracy of each
photometric redshift. We also present and discuss the catalogs obtained for the
optical SDSS galaxies, for the optical candidate quasars extracted from the DR7
SDSS photometric dataset {The sample of SDSS sources on which the accuracy of
the reconstruction has been assessed is composed of bright sources, for a
subset of which spectroscopic redshifts have been measured.}, and for optical
SDSS candidate quasars observed by GALEX in the UV range. The WGE method
exploits the new technological paradigm provided by the Virtual Observatory and
the emerging field of Astroinformatics.
"
"  Elucidating the genetic basis of human diseases is a central goal of genetics
and molecular biology. While traditional linkage analysis and modern
high-throughput techniques often provide long lists of tens or hundreds of
disease gene candidates, the identification of disease genes among the
candidates remains time-consuming and expensive. Efficient computational
methods are therefore needed to prioritize genes within the list of candidates,
by exploiting the wealth of information available about the genes in various
databases. Here we propose ProDiGe, a novel algorithm for Prioritization of
Disease Genes. ProDiGe implements a novel machine learning strategy based on
learning from positive and unlabeled examples, which allows to integrate
various sources of information about the genes, to share information about
known disease genes across diseases, and to perform genome-wide searches for
new disease genes. Experiments on real data show that ProDiGe outperforms
state-of-the-art methods for the prioritization of genes in human diseases.
"
"  The development of statistical methods and numerical algorithms for model
choice is vital to many real-world applications. In practice, the ABC approach
can be instrumental for sequential model design; however, the theoretical basis
of its use has been questioned. We present a measure-theoretic framework for
using the ABC error towards model choice and describe how easily existing
rejection, Metropolis-Hastings and sequential importance sampling ABC
algorithms are extended for the purpose of model checking. Considering a panel
of applications from evolutionary biology to dynamic systems, we discuss the
choice of summaries which differs from standard ABC approaches. The methods and
algorithms presented here may provide the workhorse machinery for an
exploratory approach to ABC model choice, particularly as the application of
standard Bayesian tools can prove impossible.
"
"  Iterative reweighted algorithms, as a class of algorithms for sparse signal
recovery, have been found to have better performance than their non-reweighted
counterparts. However, for solving the problem of multiple measurement vectors
(MMVs), all the existing reweighted algorithms do not account for temporal
correlation among source vectors and thus their performance degrades
significantly in the presence of correlation. In this work we propose an
iterative reweighted sparse Bayesian learning (SBL) algorithm exploiting the
temporal correlation, and motivated by it, we propose a strategy to improve
existing reweighted $\ell_2$ algorithms for the MMV problem, i.e. replacing
their row norms with Mahalanobis distance measure. Simulations show that the
proposed reweighted SBL algorithm has superior performance, and the proposed
improvement strategy is effective for existing reweighted $\ell_2$ algorithms.
"
"  We develop stochastic variational inference, a scalable algorithm for
approximating posterior distributions. We develop this technique for a large
class of probabilistic models and we demonstrate it with two probabilistic
topic models, latent Dirichlet allocation and the hierarchical Dirichlet
process topic model. Using stochastic variational inference, we analyze several
large collections of documents: 300K articles from Nature, 1.8M articles from
The New York Times, and 3.8M articles from Wikipedia. Stochastic inference can
easily handle data sets of this size and outperforms traditional variational
inference, which can only handle a smaller subset. (We also show that the
Bayesian nonparametric topic model outperforms its parametric counterpart.)
Stochastic variational inference lets us apply complex Bayesian models to
massive data sets.
"
"  Motivated by a challenging problem in financial trading we are presented with
a mixture of regressions with variable selection problem. In this regard, one
is faced with data which possess outliers, skewness and, simultaneously, due to
the nature of financial trading, one would like to be able to construct
clusters with specific predictors that are fairly sparse. We develop a Bayesian
mixture of lasso regressions with $t-$errors to reflect these specific demands.
The resulting model is necessarily complex and to fit the model to real data,
we develop a state-of-the-art Particle Markov chain Monte Carlo (PMCMC)
algorithm based upon sequential Monte Carlo (SMC) methods. The model and
algorithm are investigated on both simulated and real data.
"
"  In light of the burgeoning interest in network analysis in the new millenium,
we bring to the attention of contemporary network theorists, a two-stage
double-standarization and hierarchical clustering (single-linkage-like)
procedure devised in 1974. In its many applications over the next
decade--primarily to the migration flows between geographic subdivisions within
nations--the presence was often revealed of ``hubs''. These are, typically,
``cosmopolitan/non-provincial'' areas--such as the French capital, Paris--which
send and receive people relatively broadly across their respective nations.
Additionally, this two-stage procedure--which ``might very well be the most
successful application of cluster analysis'' (R. C. Dubes)--has detected many
(physically or socially) isolated groups (regions) of areas, such as those
forming the southern islands, Shikoku and Kyushu, of Japan, the Italian islands
of Sardinia and Sicily, and the New England region of the United States.
Further, we discuss a (complementary) approach developed in 1976, involving the
application of the max-flow/min-cut theorem to raw/non-standardized flows.
"
"  Computerized touchscreen ""Direct Recording Electronic"" DRE voting systems
have been used by over 1/3 of American voters in recent elections. In many
places, insufficient DRE numbers in combination with lengthy ballots and high
voter traffic have caused long lines and disenfranchised voters who left
without voting. We have applied computer queuing simulation to the voting
process and conclude that far more DREs, at great expense, would be needed to
keep waiting times low. Alternatively, paper ballot-optical scan systems can be
easily and economically scaled to prevent long lines and meet unexpected
contingencies.
"
"  This paper studies prototypical strategies to sequentially aggregate
independent decisions. We consider a collection of agents, each performing
binary hypothesis testing and each obtaining a decision over time. We assume
the agents are identical and receive independent information. Individual
decisions are sequentially aggregated via a threshold-based rule. In other
words, a collective decision is taken as soon as a specified number of agents
report a concordant decision (simultaneous discordant decisions and no-decision
outcomes are also handled).
  We obtain the following results. First, we characterize the probabilities of
correct and wrong decisions as a function of time, group size and decision
threshold. The computational requirements of our approach are linear in the
group size. Second, we consider the so-called fastest and majority rules,
corresponding to specific decision thresholds. For these rules, we provide a
comprehensive scalability analysis of both accuracy and decision time. In the
limit of large group sizes, we show that the decision time for the fastest rule
converges to the earliest possible individual time, and that the decision
accuracy for the majority rule shows an exponential improvement over the
individual accuracy. Additionally, via a theoretical and numerical analysis, we
characterize various speed/accuracy tradeoffs. Finally, we relate our results
to some recent observations reported in the cognitive information processing
literature.
"
"  A clustering algorithm partitions a set of data points into smaller sets
(clusters) such that each subset is more tightly packed than the whole. Many
approaches to clustering translate the vector data into a graph with edges
reflecting a distance or similarity metric on the points, then look for highly
connected subgraphs. We introduce such an algorithm based on ideas borrowed
from the topological notion of thin position for knots and 3-dimensional
manifolds.
"
"  We consider perfect simulation algorithms for locally stable point processes
based on dominated coupling from the past. A version of the algorithm is
developed which is feasible for processes which are neither purely attractive
nor purely repulsive. Such processes include multiscale area-interaction
processes, which are capable of modelling point patterns whose clustering
structure varies across scales. We prove correctness of the algorithm and
existence of these processes. An application to the redwood seedlings data is
discussed.
"
"  This paper establishes information-theoretic limits in estimating a finite
field low-rank matrix given random linear measurements of it. These linear
measurements are obtained by taking inner products of the low-rank matrix with
random sensing matrices. Necessary and sufficient conditions on the number of
measurements required are provided. It is shown that these conditions are sharp
and the minimum-rank decoder is asymptotically optimal. The reliability
function of this decoder is also derived by appealing to de Caen's lower bound
on the probability of a union. The sufficient condition also holds when the
sensing matrices are sparse - a scenario that may be amenable to efficient
decoding. More precisely, it is shown that if the n\times n-sensing matrices
contain, on average, \Omega(nlog n) entries, the number of measurements
required is the same as that when the sensing matrices are dense and contain
entries drawn uniformly at random from the field. Analogies are drawn between
the above results and rank-metric codes in the coding theory literature. In
fact, we are also strongly motivated by understanding when minimum rank
distance decoding of random rank-metric codes succeeds. To this end, we derive
distance properties of equiprobable and sparse rank-metric codes. These
distance properties provide a precise geometric interpretation of the fact that
the sparse ensemble requires as few measurements as the dense one. Finally, we
provide a non-exhaustive procedure to search for the unknown low-rank matrix.
"
"  We design iterative receiver schemes for a generic wireless communication
system by treating channel estimation and information decoding as an inference
problem in graphical models. We introduce a recently proposed inference
framework that combines belief propagation (BP) and the mean field (MF)
approximation and includes these algorithms as special cases. We also show that
the expectation propagation and expectation maximization algorithms can be
embedded in the BP-MF framework with slight modifications. By applying the
considered inference algorithms to our probabilistic model, we derive four
different message-passing receiver schemes. Our numerical evaluation
demonstrates that the receiver based on the BP-MF framework and its variant
based on BP-EM yield the best compromise between performance, computational
complexity and numerical stability among all candidate algorithms.
"
"  A spatial stochastic model is developed which describes the 3D nanomorphology
of composite materials, being blends of two different (organic and inorganic)
solid phases. Such materials are used, for example, in photoactive layers of
hybrid polymer zinc oxide solar cells. The model is based on ideas from
stochastic geometry and spatial statistics. Its parameters are fitted to image
data gained by electron tomography (ET), where adaptive thresholding and
stochastic segmentation have been used to represent morphological features of
the considered ET data by unions of overlapping spheres. Their midpoints are
modeled by a stack of 2D point processes with a suitably chosen correlation
structure, whereas a moving-average procedure is used to add the radii of
spheres. The model is validated by comparing physically relevant
characteristics of real and simulated data, like the efficiency of exciton
quenching, which is important for the generation of charges and their transport
toward the electrodes.
"
"  Classical peaks over threshold analysis is widely used for statistical
modeling of sample extremes, and can be supplemented by a model for the sizes
of clusters of exceedances. Under mild conditions a compound Poisson process
model allows the estimation of the marginal distribution of threshold
exceedances and of the mean cluster size, but requires the choice of a
threshold and of a run parameter, $K$, that determines how exceedances are
declustered. We extend a class of estimators of the reciprocal mean cluster
size, known as the extremal index, establish consistency and asymptotic
normality, and use the compound Poisson process to derive misspecification
tests of model validity and of the choice of run parameter and threshold.
Simulated examples and real data on temperatures and rainfall illustrate the
ideas, both for estimating the extremal index in nonstandard situations and for
assessing the validity of extremal models.
"
"  This paper reports on our analysis of the 2011 CAMRa Challenge dataset (Track
2) for context-aware movie recommendation systems. The train dataset comprises
4,536,891 ratings provided by 171,670 users on 23,974$ movies, as well as the
household groupings of a subset of the users. The test dataset comprises 5,450
ratings for which the user label is missing, but the household label is
provided. The challenge required to identify the user labels for the ratings in
the test set. Our main finding is that temporal information (time labels of the
ratings) is significantly more useful for achieving this objective than the
user preferences (the actual ratings). Using a model that leverages on this
fact, we are able to identify users within a known household with an accuracy
of approximately 96% (i.e. misclassification rate around 4%).
"
"  Bayesian Networks (BNs) are useful tools giving a natural and compact
representation of joint probability distributions. In many applications one
needs to learn a Bayesian Network (BN) from data. In this context, it is
important to understand the number of samples needed in order to guarantee a
successful learning. Previous work have studied BNs sample complexity, yet it
mainly focused on the requirement that the learned distribution will be close
to the original distribution which generated the data. In this work, we study a
different aspect of the learning, namely the number of samples needed in order
to learn the correct structure of the network. We give both asymptotic results,
valid in the large sample limit, and experimental results, demonstrating the
learning behavior for feasible sample sizes. We show that structure learning is
a more difficult task, compared to approximating the correct distribution, in
the sense that it requires a much larger number of samples, regardless of the
computational power available for the learner.
"
"  Conventional multiclass conditional probability estimation methods, such as
Fisher's discriminate analysis and logistic regression, often require
restrictive distributional model assumption. In this paper, a model-free
estimation method is proposed to estimate multiclass conditional probability
through a series of conditional quantile regression functions. Specifically,
the conditional class probability is formulated as difference of corresponding
cumulative distribution functions, where the cumulative distribution functions
can be converted from the estimated conditional quantile regression functions.
The proposed estimation method is also efficient as its computation cost does
not increase exponentially with the number of classes. The theoretical and
numerical studies demonstrate that the proposed estimation method is highly
competitive against the existing competitors, especially when the number of
classes is relatively large.
"
"  We present a simple, yet effective, approach to Semi-Supervised Learning. Our
approach is based on estimating density-based distances (DBD) using a shortest
path calculation on a graph. These Graph-DBD estimates can then be used in any
distance-based supervised learning method, such as Nearest Neighbor methods and
SVMs with RBF kernels. In order to apply the method to very large data sets, we
also present a novel algorithm which integrates nearest neighbor computations
into the shortest path search and can find exact shortest paths even in
extremely large dense graphs. Significant runtime improvement over the commonly
used Laplacian regularization method is then shown on a large scale dataset.
"
"  We present a general method for deriving collapsed variational inference
algo- rithms for probabilistic models in the conjugate exponential family. Our
method unifies many existing approaches to collapsed variational inference. Our
collapsed variational inference leads to a new lower bound on the marginal
likelihood. We exploit the information geometry of the bound to derive much
faster optimization methods based on conjugate gradients for these models. Our
approach is very general and is easily applied to any model where the mean
field update equations have been derived. Empirically we show significant
speed-ups for probabilistic models optimized using our bound.
"
"  The two-parameter distribution known as exponential-Poisson (EP)
distribution, which has decreasing failure rate, was introduced by Kus (2007).
In this paper we generalize the EP distribution and show that the failure rate
of the new distribution can be decreasing or increasing. The failure rate can
also be upside-down bathtub shaped. A comprehensive mathematical treatment of
the new distribution is provided. We provide closed-form expressions for the
density, cumulative distribution, survival and failure rate functions; we also
obtain the density of the $i$th order statistic. We derive the $r$th raw moment
of the new distribution and also the moments of order statistics. Moreover, we
discuss estimation by maximum likelihood and obtain an expression for Fisher's
information matrix. Furthermore, expressions for the R\'enyi and Shannon
entropies are given and estimation of the stress-strength parameter is
discussed. Applications using two real data sets are presented.
"
"  Bayesian inference is developed for matrix-variate dynamic linear models
(MV-DLMs), in order to allow missing observation analysis, of any sub-vector or
sub-matrix of the observation time series matrix. We propose modifications of
the inverted Wishart and matrix $t$ distributions, replacing the scalar degrees
of freedom by a diagonal matrix of degrees of freedom. The MV-DLM is then
re-defined and modifications of the updating algorithm for missing observations
are suggested.
"
"  Positive definite operator-valued kernels generalize the well-known notion of
reproducing kernels, and are naturally adapted to multi-output learning
situations. This paper addresses the problem of learning a finite linear
combination of infinite-dimensional operator-valued kernels which are suitable
for extending functional data analysis methods to nonlinear contexts. We study
this problem in the case of kernel ridge regression for functional responses
with an lr-norm constraint on the combination coefficients. The resulting
optimization problem is more involved than those of multiple scalar-valued
kernel learning since operator-valued kernels pose more technical and
theoretical issues. We propose a multiple operator-valued kernel learning
algorithm based on solving a system of linear operator equations by using a
block coordinatedescent procedure. We experimentally validate our approach on a
functional regression task in the context of finger movement prediction in
brain-computer interfaces.
"
"  Several different types of statistical interaction are defined and
distinguished, primarily on the basis of the nature of the factors defining the
interaction. Illustrative examples, mostly epidemiological, are given. The
emphasis is primarily on interpretation rather than on methods for detecting
interactions.
"
"  Undirected graphs are often used to describe high dimensional distributions.
Under sparsity conditions, the graph can be estimated using
$\ell_1$-penalization methods. We propose and study the following method. We
combine a multiple regression approach with ideas of thresholding and
refitting: first we infer a sparse undirected graphical model structure via
thresholding of each among many $\ell_1$-norm penalized regression functions;
we then estimate the covariance matrix and its inverse using the maximum
likelihood estimator. We show that under suitable conditions, this approach
yields consistent estimation in terms of graphical structure and fast
convergence rates with respect to the operator and Frobenius norm for the
covariance matrix and its inverse. We also derive an explicit bound for the
Kullback Leibler divergence.
"
"  The versatility of exponential families, along with their attendant convexity
properties, make them a popular and effective statistical model. A central
issue is learning these models in high-dimensions, such as when there is some
sparsity pattern of the optimal parameter. This work characterizes a certain
strong convexity property of general exponential families, which allow their
generalization ability to be quantified. In particular, we show how this
property can be used to analyze generic exponential families under L_1
regularization.
"
"  Motivation: Illumina BeadArray technology includes negative control features
that allow a precise estimation of the background noise. As an alternative to
the background subtraction proposed in BeadStudio which leads to an important
loss of information by generating negative values, a background correction
method modeling the observed intensities as the sum of the exponentially
distributed signal and normally distributed noise has been developed.
Nevertheless, Wang and Ye (2011) display a kernel-based estimator of the signal
distribution on Illumina BeadArrays and suggest that a gamma distribution would
represent a better modeling of the signal density. Hence, the
normal-exponential modeling may not be appropriate for Illumina data and
background corrections derived from this model may lead to wrong estimation.
Results: We propose a more flexible modeling based on a gamma distributed
signal and a normal distributed background noise and develop the associated
background correction. Our model proves to be markedly more accurate to model
Illumina BeadArrays: on the one hand, this model offers a more correct fit of
the observed intensities. On the other hand, the comparison of the operating
characteristics of several background correction procedures on spike-in and on
normal-gamma simulated data shows high similarities, reinforcing the validation
of the normal-gamma modeling. The performance of the background corrections
based on the normal-gamma and normal-exponential models are compared on two
dilution data sets. Surprisingly, we observe that the implementation of a more
accurate parametrisation in the model-based background correction does not
increase the sensitivity. These results may be explained by the operating
characteristics of the estimators: the normal-gamma background correction
offers an improvement in terms of bias, but at the cost of a loss in precision.
"
"  Using the $\ell_1$-norm to regularize the estimation of the parameter vector
of a linear model leads to an unstable estimator when covariates are highly
correlated. In this paper, we introduce a new penalty function which takes into
account the correlation of the design matrix to stabilize the estimation. This
norm, called the trace Lasso, uses the trace norm, which is a convex surrogate
of the rank, of the selected covariates as the criterion of model complexity.
We analyze the properties of our norm, describe an optimization algorithm based
on reweighted least-squares, and illustrate the behavior of this norm on
synthetic data, showing that it is more adapted to strong correlations than
competing methods such as the elastic net.
"
"  In Natural Language Processing (NLP) tasks, data often has the following two
properties: First, data can be chopped into multi-views which has been
successfully used for dimension reduction purposes. For example, in topic
classification, every paper can be chopped into the title, the main text and
the references. However, it is common that some of the views are less noisier
than other views for supervised learning problems. Second, unlabeled data are
easy to obtain while labeled data are relatively rare. For example, articles
occurred on New York Times in recent 10 years are easy to grab but having them
classified as 'Politics', 'Finance' or 'Sports' need human labor. Hence less
noisy features are preferred before running supervised learning methods. In
this paper we propose an unsupervised algorithm which optimally weights
features from different views when these views are generated from a low
dimensional hidden state, which occurs in widely used models like Mixture
Gaussian Model, Hidden Markov Model (HMM) and Latent Dirichlet Allocation
(LDA).
"
"  Macro-economic models describe the dynamics of economic quantities. The
estimations and forecasts produced by such models play a substantial role for
financial and political decisions. In this contribution we describe an approach
based on genetic programming and symbolic regression to identify variable
interactions in large datasets. In the proposed approach multiple symbolic
regression runs are executed for each variable of the dataset to find
potentially interesting models. The result is a variable interaction network
that describes which variables are most relevant for the approximation of each
variable of the dataset. This approach is applied to a macro-economic dataset
with monthly observations of important economic indicators in order to identify
potentially interesting dependencies of these indicators. The resulting
interaction network of macro-economic indicators is briefly discussed and two
of the identified models are presented in detail. The two models approximate
the help wanted index and the CPI inflation in the US.
"
"  Multi-step ahead forecasting is still an open challenge in time series
forecasting. Several approaches that deal with this complex problem have been
proposed in the literature but an extensive comparison on a large number of
tasks is still missing. This paper aims to fill this gap by reviewing existing
strategies for multi-step ahead forecasting and comparing them in theoretical
and practical terms. To attain such an objective, we performed a large scale
comparison of these different strategies using a large experimental benchmark
(namely the 111 series from the NN5 forecasting competition). In addition, we
considered the effects of deseasonalization, input variable selection, and
forecast combination on these strategies and on multi-step ahead forecasting at
large. The following three findings appear to be consistently supported by the
experimental results: Multiple-Output strategies are the best performing
approaches, deseasonalization leads to uniformly improved forecast accuracy,
and input selection is more effective when performed in conjunction with
deseasonalization.
"
"  Computer experiments are often performed to allow modeling of a response
surface of a physical experiment that can be too costly or difficult to run
except using a simulator. Running the experiment over a dense grid can be
prohibitively expensive, yet running over a sparse design chosen in advance can
result in obtaining insufficient information in parts of the space,
particularly when the surface calls for a nonstationary model. We propose an
approach that automatically explores the space while simultaneously fitting the
response surface, using predictive uncertainty to guide subsequent experimental
runs. The newly developed Bayesian treed Gaussian process is used as the
surrogate model, and a fully Bayesian approach allows explicit measures of
uncertainty. We develop an adaptive sequential design framework to cope with an
asynchronous, random, agent--based supercomputing environment, by using a
hybrid approach that melds optimal strategies from the statistics literature
with flexible strategies from the active learning literature. The merits of
this approach are borne out in several examples, including the motivating
computational fluid dynamics simulation of a rocket booster.
"
"  Hierarchical models are a powerful tool for high-throughput data with a small
to moderate number of replicates, as they allow sharing information across
units of information, for example, genes. We propose two such models and show
its increased sensitivity in microarray differential expression applications.
We build on the gamma--gamma hierarchical model introduced by Kendziorski et
al. [Statist. Med. 22 (2003) 3899--3914] and Newton et al. [Biostatistics 5
(2004) 155--176], by addressing important limitations that may have hampered
its performance and its more widespread use. The models parsimoniously describe
the expression of thousands of genes with a small number of hyper-parameters.
This makes them easy to interpret and analytically tractable. The first model
is a simple extension that improves the fit substantially with almost no
increase in complexity. We propose a second extension that uses a mixture of
gamma distributions to further improve the fit, at the expense of increased
computational burden. We derive several approximations that significantly
reduce the computational cost. We find that our models outperform the original
formulation of the model, as well as some other popular methods for
differential expression analysis. The improved performance is specially
noticeable for the small sample sizes commonly encountered in high-throughput
experiments. Our methods are implemented in the freely available Bioconductor
gaga package.
"
"  The Women's Health Initiative randomized clinical trial of hormone therapy
found no benefit of hormones in preventive cardiovascular disease, a finding in
striking contrast with a large body of observational research. Understanding
whether better methodology and/or statistical adjustment might have prevented
the erroneous conclusions of observational research is important. This is a
re-analysis of data from a case-control study examining the relationship of
postmenopausal hormone therapy and the risks of myocardial infarction (MI) and
ischemic stroke in which we reported no overall increase or decrease in the
risk of either event. Variables measuring health behavior/lifestyle that are
not likely to be causally with the risks of MI and stroke (e.g., sunscreen use)
were included in multivariate analysis along with traditional confounders (age,
hypertension, diabetes, smoking, body mass index, ethnicity, education, prior
coronary heart disease for MI and prior stroke/TIA for stroke) to determine
whether adjustment for the health behavior/lifestyle variables could reproduce
or bring the results closer to the findings in a large and definitive
randomized clinical trial of hormone therapy, the Women's Health Initiative.
For both MI and stroke, measures of health behavior/lifestyle were associated
with odds ratios (ORs) less than 1.0. Adjustment for traditional cardiovascular
disease confounders did not alter the magnitude of the ORs for MI or stroke.
Addition of a subset of these variables selected using stepwise regression to
the final MI or stroke models along with the traditional cardiovascular disease
confounders moved the ORs for estrogen and estrogen/progestin use closer to
values observed in the Women Health Initiative clinical trial, but did not
reliably reproduce the clinical trial results for these two endpoints.
"
"  In the setting of additive regression model for continuous time process, we
establish the optimal uniform convergence rates and optimal asymptotic
quadratic error of additive regression. To build our estimate, we use the
marginal integration method.
"
"  In this paper we describe a statistical procedure to account for differences
in grading practices from one course to another. The goal is to define a course
""inflatedness"" and a student ""aptitude"" that best captures one's intuitive
notions of these concepts.
"
"  Much of current machine learning (ML) research has lost its connection to
problems of import to the larger world of science and society. From this
perspective, there exist glaring limitations in the data sets we investigate,
the metrics we employ for evaluation, and the degree to which results are
communicated back to their originating domains. What changes are needed to how
we conduct research to increase the impact that ML has? We present six Impact
Challenges to explicitly focus the field?s energy and attention, and we discuss
existing obstacles that must be addressed. We aim to inspire ongoing discussion
and focus on ML that matters.
"
"  A nonparametric approach for policy learning for POMDPs is proposed. The
approach represents distributions over the states, observations, and actions as
embeddings in feature spaces, which are reproducing kernel Hilbert spaces.
Distributions over states given the observations are obtained by applying the
kernel Bayes' rule to these distribution embeddings. Policies and value
functions are defined on the feature space over states, which leads to a
feature space expression for the Bellman equation. Value iteration may then be
used to estimate the optimal value function and associated policy. Experimental
results confirm that the correct policy is learned using the feature space
representation.
"
"  Most existing distance metric learning methods assume perfect side
information that is usually given in pairwise or triplet constraints. Instead,
in many real-world applications, the constraints are derived from side
information, such as users' implicit feedbacks and citations among articles. As
a result, these constraints are usually noisy and contain many mistakes. In
this work, we aim to learn a distance metric from noisy constraints by robust
optimization in a worst-case scenario, to which we refer as robust metric
learning. We formulate the learning task initially as a combinatorial
optimization problem, and show that it can be elegantly transformed to a convex
programming problem. We present an efficient learning algorithm based on smooth
optimization [7]. It has a worst-case convergence rate of
O(1/{\surd}{\varepsilon}) for smooth optimization problems, where {\varepsilon}
is the desired error of the approximate solution. Finally, our empirical study
with UCI data sets demonstrate the effectiveness of the proposed method in
comparison to state-of-the-art methods.
"
"  With the rapid development of economy and the accelerated globalization
process, the aviation industry plays more and more critical role in today's
world, in both developed and developing countries. As the infrastructure of
aviation industry, the airport network is one of the most important indicators
of economic growth. In this paper, we investigate the evolution of Chinese
airport network (CAN) via complex network theory. It is found that although the
topology of CAN remains steady during the past several years, there are many
dynamic switchings inside the network, which changes the relative relevance of
airports and airlines. Moreover, we investigate the evolution of traffic flow
(passengers and cargoes) on CAN. It is found that the traffic keeps growing in
an exponential form and it has evident seasonal fluctuations. We also found
that cargo traffic and passenger traffic are positively related but the
correlations are quite different for different kinds of cities.
"
"  Sparse linear regression -- finding an unknown vector from linear
measurements -- is now known to be possible with fewer samples than variables,
via methods like the LASSO. We consider the multiple sparse linear regression
problem, where several related vectors -- with partially shared support sets --
have to be recovered. A natural question in this setting is whether one can use
the sharing to further decrease the overall number of samples required. A line
of recent research has studied the use of \ell_1/\ell_q norm
block-regularizations with q>1 for such problems; however these could actually
perform worse in sample complexity -- vis a vis solving each problem separately
ignoring sharing -- depending on the level of sharing.
  We present a new method for multiple sparse linear regression that can
leverage support and parameter overlap when it exists, but not pay a penalty
when it does not. A very simple idea: we decompose the parameters into two
components and regularize these differently. We show both theoretically and
empirically, our method strictly and noticeably outperforms both \ell_1 or
\ell_1/\ell_q methods, over the entire range of possible overlaps (except at
boundary cases, where we match the best method). We also provide theoretical
guarantees that the method performs well under high-dimensional scaling.
"
"  A fundamental operation in many vision tasks, including motion understanding,
stereopsis, visual odometry, or invariant recognition, is establishing
correspondences between images or between images and data from other
modalities. We present an analysis of the role that multiplicative interactions
play in learning such correspondences, and we show how learning and inferring
relationships between images can be viewed as detecting rotations in the
eigenspaces shared among a set of orthogonal matrices. We review a variety of
recent multiplicative sparse coding methods in light of this observation. We
also review how the squaring operation performed by energy models and by models
of complex cells can be thought of as a way to implement multiplicative
interactions. This suggests that the main utility of including complex cells in
computational models of vision may be that they can encode relations not
invariances.
"
"  Constructing an efficient parameterization of a large, noisy data set of
points lying close to a smooth manifold in high dimension remains a fundamental
problem. One approach consists in recovering a local parameterization using the
local tangent plane. Principal component analysis (PCA) is often the tool of
choice, as it returns an optimal basis in the case of noise-free samples from a
linear subspace. To process noisy data samples from a nonlinear manifold, PCA
must be applied locally, at a scale small enough such that the manifold is
approximately linear, but at a scale large enough such that structure may be
discerned from noise. Using eigenspace perturbation theory and non-asymptotic
random matrix theory, we study the stability of the subspace estimated by PCA
as a function of scale, and bound (with high probability) the angle it forms
with the true tangent space. By adaptively selecting the scale that minimizes
this bound, our analysis reveals an appropriate scale for local tangent plane
recovery. We also introduce a geometric uncertainty principle quantifying the
limits of noise-curvature perturbation for stable recovery. With the purpose of
providing perturbation bounds that can be used in practice, we propose plug-in
estimates that make it possible to directly apply the theoretical results to
real data sets.
"
"  A new procedure for learning cost-sensitive SVM(CS-SVM) classifiers is
proposed. The SVM hinge loss is extended to the cost sensitive setting, and the
CS-SVM is derived as the minimizer of the associated risk. The extension of the
hinge loss draws on recent connections between risk minimization and
probability elicitation. These connections are generalized to cost-sensitive
classification, in a manner that guarantees consistency with the cost-sensitive
Bayes risk, and associated Bayes decision rule. This ensures that optimal
decision rules, under the new hinge loss, implement the Bayes-optimal
cost-sensitive classification boundary. Minimization of the new hinge loss is
shown to be a generalization of the classic SVM optimization problem, and can
be solved by identical procedures. The dual problem of CS-SVM is carefully
scrutinized by means of regularization theory and sensitivity analysis and the
CS-SVM algorithm is substantiated. The proposed algorithm is also extended to
cost-sensitive learning with example dependent costs. The minimum cost
sensitive risk is proposed as the performance measure and is connected to ROC
analysis through vector optimization. The resulting algorithm avoids the
shortcomings of previous approaches to cost-sensitive SVM design, and is shown
to have superior experimental performance on a large number of cost sensitive
and imbalanced datasets.
"
"  Bayesian model averaging, model selection and its approximations such as BIC
are generally statistically consistent, but sometimes achieve slower rates og
convergence than other methods such as AIC and leave-one-out cross-validation.
On the other hand, these other methods can br inconsistent. We identify the
""catch-up phenomenon"" as a novel explanation for the slow convergence of
Bayesian methods. Based on this analysis we define the switch distribution, a
modification of the Bayesian marginal distribution. We show that, under broad
conditions,model selection and prediction based on the switch distribution is
both consistent and achieves optimal convergence rates, thereby resolving the
AIC-BIC dilemma. The method is practical; we give an efficient implementation.
The switch distribution has a data compression interpretation, and can thus be
viewed as a ""prequential"" or MDL method; yet it is different from the MDL
methods that are usually considered in the literature. We compare the switch
distribution to Bayes factor model selection and leave-one-out
cross-validation.
"
"  The web application presented in this paper allows for an analysis to reveal
centres of excellence in different fields worldwide using publication and
citation data. Only specific aspects of institutional performance are taken
into account and other aspects such as teaching performance or societal impact
of research are not considered. Based on data gathered from Scopus,
field-specific excellence can be identified in institutions where highly-cited
papers have been frequently published. The web application combines both a list
of institutions ordered by different indicator values and a map with circles
visualizing indicator values for geocoded institutions. Compared to the mapping
and ranking approaches introduced hitherto, our underlying statistics
(multi-level models) are analytically oriented by allowing (1) the estimation
of values for the number of excellent papers for an institution which are
statistically more appropriate than the observed values; (2) the calculation of
confidence intervals as measures of accuracy for the institutional citation
impact; (3) the comparison of a single institution with an ""average""
institution in a subject area, and (4) the direct comparison of at least two
institutions.
"
"  Volterra and polynomial regression models play a major role in nonlinear
system identification and inference tasks. Exciting applications ranging from
neuroscience to genome-wide association analysis build on these models with the
additional requirement of parsimony. This requirement has high interpretative
value, but unfortunately cannot be met by least-squares based or kernel
regression methods. To this end, compressed sampling (CS) approaches, already
successful in linear regression settings, can offer a viable alternative. The
viability of CS for sparse Volterra and polynomial models is the core theme of
this work. A common sparse regression task is initially posed for the two
models. Building on (weighted) Lasso-based schemes, an adaptive RLS-type
algorithm is developed for sparse polynomial regressions. The identifiability
of polynomial models is critically challenged by dimensionality. However,
following the CS principle, when these models are sparse, they could be
recovered by far fewer measurements. To quantify the sufficient number of
measurements for a given level of sparsity, restricted isometry properties
(RIP) are investigated in commonly met polynomial regression settings,
generalizing known results for their linear counterparts. The merits of the
novel (weighted) adaptive CS algorithms to sparse polynomial modeling are
verified through synthetic as well as real data tests for genotype-phenotype
analysis.
"
"  In this paper we study the performance of the Projected Gradient Descent(PGD)
algorithm for $\ell_{p}$-constrained least squares problems that arise in the
framework of Compressed Sensing. Relying on the Restricted Isometry Property,
we provide convergence guarantees for this algorithm for the entire range of
$0\leq p\leq1$, that include and generalize the existing results for the
Iterative Hard Thresholding algorithm and provide a new accuracy guarantee for
the Iterative Soft Thresholding algorithm as special cases. Our results suggest
that in this group of algorithms, as $p$ increases from zero to one, conditions
required to guarantee accuracy become stricter and robustness to noise
deteriorates.
"
"  This paper proposes a novel multiscale estimator for the integrated
volatility of an Ito process, in the presence of market microstructure noise
(observation error). The multiscale structure of the observed process is
represented frequency-by-frequency and the concept of the multiscale ratio is
introduced to quantify the bias in the realized integrated volatility due to
the observation error. The multiscale ratio is estimated from a single sample
path, and a frequency-by-frequency bias correction procedure is proposed, which
simultaneously reduces variance. We extend the method to include correlated
observation errors and provide the implied time domain form of the estimation
procedure. The new method is implemented to estimate the integrated volatility
for the Heston and other models, and the improved performance of our method
over existing methods is illustrated by simulation studies.
"
"  It is common and convenient to treat distributed physical parameters as
Gaussian random fields and model them in an ""inverse procedure"" using
measurements of various properties of the fields. This article presents a
general method for this problem based on a flexible parameterization device
called ""anchors"", which captures local or global features of the fields. A
classification of all relevant data into two categories closely cooperates with
the anchor concept to enable systematic use of datasets of different sources
and disciplinary natures. In particular, nonlinearity in the ""forward models""
is handled automatically. Treatment of measurement and model errors is
systematic and integral in the method; however the method is also suitable in
the usual setting where one does not have reliable information about these
errors. Compared to a state-space approach, the anchor parameterization renders
the task in a parameter space of radically reduced dimension; consequently,
easier and more rigorous statistical inference, interpretation, and sampling
are possible. A procedure for deriving the posterior distribution of model
parameters is presented. Based on Monte Carlo sampling and normal mixture
approximation to high-dimensional densities, the procedure has generality and
efficiency features that provide a basis for practical implementations of this
computationally demanding inverse procedure. We emphasize distinguishing
features of the method compared to state-space approaches and
optimization-based ideas. Connections with existing methods in stochastic
hydrogeology are discussed. The work is illustrated by a one-dimensional
synthetic problem.
  Key words: anchored inversion, Gaussian process, ill-posedness, model error,
state space, pilot point method, stochastic hydrogeology.
"
"  We consider the problem of learning the optimal action-value function in the
discounted-reward Markov decision processes (MDPs). We prove a new PAC bound on
the sample-complexity of model-based value iteration algorithm in the presence
of the generative model, which indicates that for an MDP with N state-action
pairs and the discount factor \gamma\in[0,1) only
O(N\log(N/\delta)/((1-\gamma)^3\epsilon^2)) samples are required to find an
\epsilon-optimal estimation of the action-value function with the probability
1-\delta. We also prove a matching lower bound of \Theta
(N\log(N/\delta)/((1-\gamma)^3\epsilon^2)) on the sample complexity of
estimating the optimal action-value function by every RL algorithm. To the best
of our knowledge, this is the first matching result on the sample complexity of
estimating the optimal (action-) value function in which the upper bound
matches the lower bound of RL in terms of N, \epsilon, \delta and 1/(1-\gamma).
Also, both our lower bound and our upper bound significantly improve on the
state-of-the-art in terms of 1/(1-\gamma).
"
"  Rapid identification of object from radar cross section (RCS) signals is
important for many space and military applications. This identification is a
problem in pattern recognition which either neural networks or support vector
machines should prove to be high-speed. Bayesian networks would also provide
value but require significant preprocessing of the signals. In this paper, we
describe the use of a support vector machine for object identification from
synthesized RCS data. Our best results are from data fusion of X-band and
S-band signals, where we obtained 99.4%, 95.3%, 100% and 95.6% correct
identification for cylinders, frusta, spheres, and polygons, respectively. We
also compare our results with a Bayesian approach and show that the SVM is
three orders of magnitude faster, as measured by the number of floating point
operations.
"
"  We introduce a method that defines the species (representatives) of inorganic
compounds, and studied the statistical distribution of the defined species
among space groups (distribution of space groups), by using ICSD (Inorganic
Crystal Structure Database). Here we show that the number of formula units in a
unit cell gives a natural classification to understand the statistical
distribution of crystallographic groups.
"
"  Shrinking methods in regression analysis are usually designed for metric
predictors. In this article, however, shrinkage methods for categorial
predictors are proposed. As an application we consider data from the Munich
rent standard, where, for example, urban districts are treated as a categorial
predictor. If independent variables are categorial, some modifications to usual
shrinking procedures are necessary. Two $L_1$-penalty based methods for factor
selection and clustering of categories are presented and investigated. The
first approach is designed for nominal scale levels, the second one for ordinal
predictors. Besides applying them to the Munich rent standard, methods are
illustrated and compared in simulation studies.
"
"  We show how to compute lower bounds for the supremum Bayes error if the
class-conditional distributions must satisfy moment constraints, where the
supremum is with respect to the unknown class-conditional distributions. Our
approach makes use of Curto and Fialkow's solutions for the truncated moment
problem. The lower bound shows that the popular Gaussian assumption is not
robust in this regard. We also construct an upper bound for the supremum Bayes
error by constraining the decision boundary to be linear.
"
"  We consider unsupervised estimation of mixtures of discrete graphical models,
where the class variable corresponding to the mixture components is hidden and
each mixture component over the observed variables can have a potentially
different Markov graph structure and parameters. We propose a novel approach
for estimating the mixture components, and our output is a tree-mixture model
which serves as a good approximation to the underlying graphical model mixture.
Our method is efficient when the union graph, which is the union of the Markov
graphs of the mixture components, has sparse vertex separators between any pair
of observed variables. This includes tree mixtures and mixtures of bounded
degree graphs. For such models, we prove that our method correctly recovers the
union graph structure and the tree structures corresponding to
maximum-likelihood tree approximations of the mixture components. The sample
and computational complexities of our method scale as $\poly(p, r)$, for an
$r$-component mixture of $p$-variate graphical models. We further extend our
results to the case when the union graph has sparse local separators between
any pair of observed variables, such as mixtures of locally tree-like graphs,
and the mixture components are in the regime of correlation decay.
"
"  We present a hybrid algorithm for Bayesian topic models that combines the
efficiency of sparse Gibbs sampling with the scalability of online stochastic
inference. We used our algorithm to analyze a corpus of 1.2 million books (33
billion words) with thousands of topics. Our approach reduces the bias of
variational inference and generalizes to many Bayesian hidden-variable models.
"
"  A determinantal point process (DPP) is a random process useful for modeling
the combinatorial problem of subset selection. In particular, DPPs encourage a
random subset Y to contain a diverse set of items selected from a base set Y.
For example, we might use a DPP to display a set of news headlines that are
relevant to a user's interests while covering a variety of topics. Suppose,
however, that we are asked to sequentially select multiple diverse sets of
items, for example, displaying new headlines day-by-day. We might want these
sets to be diverse not just individually but also through time, offering
headlines today that are unlike the ones shown yesterday. In this paper, we
construct a Markov DPP (M-DPP) that models a sequence of random sets {Yt}. The
proposed M-DPP defines a stationary process that maintains DPP margins.
Crucially, the induced union process Zt = Yt u Yt-1 is also marginally
DPP-distributed. Jointly, these properties imply that the sequence of random
sets are encouraged to be diverse both at a given time step as well as across
time steps. We describe an exact, efficient sampling procedure, and a method
for incrementally learning a quality measure over items in the base set Y based
on external preferences. We apply the M-DPP to the task of sequentially
displaying diverse and relevant news articles to a user with topic preferences.
"
"  Greedy Pursuits are very popular in Compressed Sensing for sparse signal
recovery. Though many of the Greedy Pursuits possess elegant theoretical
guarantees for performance, it is well known that their performance depends on
the statistical distribution of the non-zero elements in the sparse signal. In
practice, the distribution of the sparse signal may not be known a priori. It
is also observed that performance of Greedy Pursuits degrades as the number of
available measurements decreases from a threshold value which is method
dependent. To improve the performance in these situations, we introduce a novel
fusion framework for Greedy Pursuits and also propose two algorithms for sparse
recovery. Through Monte Carlo simulations we show that the proposed schemes
improve sparse signal recovery in clean as well as noisy measurement cases.
"
"  In this paper, we propose an unifying view of several recently proposed
structured sparsity-inducing norms. We consider the situation of a model
simultaneously (a) penalized by a set- function de ned on the support of the
unknown parameter vector which represents prior knowledge on supports, and (b)
regularized in Lp-norm. We show that the natural combinatorial optimization
problems obtained may be relaxed into convex optimization problems and
introduce a notion, the lower combinatorial envelope of a set-function, that
characterizes the tightness of our relaxations. We moreover establish links
with norms based on latent representations including the latent group Lasso and
block-coding, and with norms obtained from submodular functions.
"
"  Recently there has been an increasing interest in methods that deal with
multiple outputs. This has been motivated partly by frameworks like multitask
learning, multisensor networks or structured output data. From a Gaussian
processes perspective, the problem reduces to specifying an appropriate
covariance function that, whilst being positive semi-definite, captures the
dependencies between all the data points and across all the outputs. One
approach to account for non-trivial correlations between outputs employs
convolution processes. Under a latent function interpretation of the
convolution transform we establish dependencies between output variables. The
main drawbacks of this approach are the associated computational and storage
demands. In this paper we address these issues. We present different sparse
approximations for dependent output Gaussian processes constructed through the
convolution formalism. We exploit the conditional independencies present
naturally in the model. This leads to a form of the covariance similar in
spirit to the so called PITC and FITC approximations for a single output. We
show experimental results with synthetic and real data, in particular, we show
results in pollution prediction, school exams score prediction and gene
expression data.
"
"  To gain insight into how characteristics of an establishment are associated
with nonresponse, a recursive partitioning algorithm is applied to the
Occupational Employment Statistics May 2006 survey data to build a regression
tree. The tree models an establishment's propensity to respond to the survey
given certain establishment characteristics. It provides mutually exclusive
cells based on the characteristics with homogeneous response propensities. This
makes it easy to identify interpretable associations between the characteristic
variables and an establishment's propensity to respond, something not easily
done using a logistic regression propensity model. We test the model obtained
using the May data against data from the November 2006 Occupational Employment
Statistics survey. Testing the model on a disjoint set of establishment data
with a very large sample size $(n=179,360)$ offers evidence that the regression
tree model accurately describes the association between the establishment
characteristics and the response propensity for the OES survey. The accuracy of
this modeling approach is compared to that of logistic regression through
simulation. This representation is then used along with frame-level
administrative wage data linked to sample data to investigate the possibility
of nonresponse bias. We show that without proper adjustments the nonresponse
does pose a risk of bias and is possibly nonignorable.
"
"  Bayesian Model Calibration is used to revisit the problem of scaling factor
calibration for semi-empirical correction of ab initio calculations. A
particular attention is devoted to uncertainty evaluation for scaling factors,
and to their effect on prediction of observables involving scaled properties.
We argue that linear models used for calibration of scaling factors are
generally not statistically valid, in the sense that they are not able to fit
calibration data within their uncertainty limits. Uncertainty evaluation and
uncertainty propagation by statistical methods from such invalid models are
doomed to failure. To relieve this problem, a stochastic function is included
in the model to account for model inadequacy, according to the Bayesian Model
Calibration approach. In this framework, we demonstrate that standard
calibration summary statistics, as optimal scaling factor and root mean square,
can be safely used for uncertainty propagation only when large calibration sets
of precise data are used. For small datasets containing a few dozens of data, a
more accurate formula is provided which involves scaling factor calibration
uncertainty. For measurement uncertainties larger than model inadequacy, the
problem can be reduced to a weighted least squares analysis. For intermediate
cases, no analytical estimators were found, and numerical Bayesian estimation
of parameters has to be used.
"
"  In mixture model-based clustering applications, it is common to fit several
models from a family and report clustering results from only the `best' one. In
such circumstances, selection of this best model is achieved using a model
selection criterion, most often the Bayesian information criterion. Rather than
throw away all but the best model, we average multiple models that are in some
sense close to the best one, thereby producing a weighted average of clustering
results. Two (weighted) averaging approaches are considered: averaging the
component membership probabilities and averaging models. In both cases, Occam's
window is used to determine closeness to the best model and weights are
computed within a Bayesian model averaging paradigm. In some cases, we need to
merge components before averaging; we introduce a method for merging mixture
components based on the adjusted Rand index. The effectiveness of our
model-based clustering averaging approaches is illustrated using a family of
Gaussian mixture models on real and simulated data.
"
"  Sparse methods for supervised learning aim at finding good linear predictors
from as few variables as possible, i.e., with small cardinality of their
supports. This combinatorial selection problem is often turned into a convex
optimization problem by replacing the cardinality function by its convex
envelope (tightest convex lower bound), in this case the L1-norm. In this
paper, we investigate more general set-functions than the cardinality, that may
incorporate prior knowledge or structural constraints which are common in many
applications: namely, we show that for nondecreasing submodular set-functions,
the corresponding convex envelope can be obtained from its \lova extension, a
common tool in submodular analysis. This defines a family of polyhedral norms,
for which we provide generic algorithmic tools (subgradients and proximal
operators) and theoretical results (conditions for support recovery or
high-dimensional inference). By selecting specific submodular functions, we can
give a new interpretation to known norms, such as those based on
rank-statistics or grouped norms with potentially overlapping groups; we also
define new norms, in particular ones that can be used as non-factorial priors
for supervised learning.
"
"  Nonsingular estimation of high dimensional covariance matrices is an
important step in many statistical procedures like classification, clustering,
variable selection an future extraction. After a review of the essential
background material, this paper introduces a technique we call slicing for
obtaining a nonsingular covariance matrix of high dimensional data. Slicing is
essentially assuming that the data has Kronecker delta covariance structure.
Finally, we discuss the implications of the results in this paper and provide
an example of classification for high dimensional gene expression data.
"
"  Art historians and archaeologists have long grappled with the regional
classification of ancient Near Eastern ivory carvings. Based on the visual
similarity of sculptures, individuals within these fields have proposed object
assemblages linked to hypothesized regional production centers. Using
quantitative rather than visual methods, we here approach this classification
task by exploiting computational methods from machine learning currently used
with success in a variety of statistical problems in science and engineering.
We first construct a prediction function using 66 categorical features as
inputs and regional style as output. The model assigns regional style group
(RSG), with 98 percent prediction accuracy. We then rank these features by
their mutual information with RSG, quantifying single-feature predictive power.
Using the highest- ranking features in combination with nomographic
visualization, we have found previously unknown relationships that may aid in
the regional classification of these ivories and their interpretation in art
historical context.
"
"  The problem of recovering a signal from the magnitude of its short-time
Fourier transform (STFT) is a longstanding one in audio signal processing.
Existing approaches rely on heuristics that often perform poorly because of the
nonconvexity of the problem. We introduce a formulation of the problem that
lends itself to a tractable convex program. We observe that our method yields
better reconstructions than the standard Griffin-Lim algorithm. We provide an
algorithm and discuss practical implementation details, including how the
method can be scaled up to larger examples.
"
"  We develop a model of issue-specific voting behavior. This model can be used
to explore lawmakers' personal voting patterns of voting by issue area,
providing an exploratory window into how the language of the law is correlated
with political support. We derive approximate posterior inference algorithms
based on variational methods. Across 12 years of legislative data, we
demonstrate both improvement in heldout prediction performance and the model's
utility in interpreting an inherently multi-dimensional space.
"
"  We investigate the relationship between the structure of a discrete graphical
model and the support of the inverse of a generalized covariance matrix. We
show that for certain graph structures, the support of the inverse covariance
matrix of indicator variables on the vertices of a graph reflects the
conditional independence structure of the graph. Our work extends results that
have previously been established only in the context of multivariate Gaussian
graphical models, thereby addressing an open question about the significance of
the inverse covariance matrix of a non-Gaussian distribution. The proof
exploits a combination of ideas from the geometry of exponential families,
junction tree theory and convex analysis. These population-level results have
various consequences for graph selection methods, both known and novel,
including a novel method for structure estimation for missing or corrupted
observations. We provide nonasymptotic guarantees for such methods and
illustrate the sharpness of these predictions via simulations.
"
"  New advances in nano sciences open the door for scientists to study
biological processes on a microscopic molecule-by-molecule basis. Recent
single-molecule biophysical experiments on enzyme systems, in particular,
reveal that enzyme molecules behave fundamentally differently from what
classical model predicts. A stochastic network model was previously proposed to
explain the experimental discovery. This paper conducts detailed theoretical
and data analyses of the stochastic network model, focusing on the correlation
structure of the successive reaction times of a single enzyme molecule. We
investigate the correlation of experimental fluorescence intensity and the
correlation of enzymatic reaction times, and examine the role of substrate
concentration in enzymatic reactions. Our study shows that the stochastic
network model is capable of explaining the experimental data in depth.
"
"  This paper treats the problem of screening for variables with high
correlations in high dimensional data in which there can be many fewer samples
than variables. We focus on threshold-based correlation screening methods for
three related applications: screening for variables with large correlations
within a single treatment (autocorrelation screening); screening for variables
with large cross-correlations over two treatments (cross-correlation
screening); screening for variables that have persistently large
auto-correlations over two treatments (persistent-correlation screening). The
novelty of correlation screening is that it identifies a smaller number of
variables which are highly correlated with others, as compared to identifying a
number of correlation parameters. Correlation screening suffers from a phase
transition phenomenon: as the correlation threshold decreases the number of
discoveries increases abruptly. We obtain asymptotic expressions for the mean
number of discoveries and the phase transition thresholds as a function of the
number of samples, the number of variables, and the joint sample distribution.
We also show that under a weak dependency condition the number of discoveries
is dominated by a Poisson random variable giving an asymptotic expression for
the false positive rate. The correlation screening approach bears tremendous
dividends in terms of the type and strength of the asymptotic results that can
be obtained. It also overcomes some of the major hurdles faced by existing
methods in the literature as correlation screening is naturally scalable to
high dimension. Numerical results strongly validate the theory that is
presented in this paper. We illustrate the application of the correlation
screening methodology on a large scale gene-expression dataset, revealing a few
influential variables that exhibit a significant amount of correlation over
multiple treatments.
"
"  A linear non-Gaussian structural equation model called LiNGAM is an
identifiable model for exploratory causal analysis. Previous methods estimate a
causal ordering of variables and their connection strengths based on a single
dataset. However, in many application domains, data are obtained under
different conditions, that is, multiple datasets are obtained rather than a
single dataset. In this paper, we present a new method to jointly estimate
multiple LiNGAMs under the assumption that the models share a causal ordering
but may have different connection strengths and differently distributed
variables. In simulations, the new method estimates the models more accurately
than estimating them separately.
"
"  We consider apprenticeship learning, i.e., having an agent learn a task by
observing an expert demonstrating the task in a partially observable
environment when the model of the environment is uncertain. This setting is
useful in applications where the explicit modeling of the environment is
difficult, such as a dialogue system. We show that we can extract information
about the environment model by inferring action selection process behind the
demonstration, under the assumption that the expert is choosing optimal actions
based on knowledge of the true model of the target environment. Proposed
algorithms can achieve more accurate estimates of POMDP parameters and better
policies from a short demonstration, compared to methods that learns only from
the reaction from the environment.
"
"  This monograph deals with adaptive supervised classification, using tools
borrowed from statistical mechanics and information theory, stemming from the
PACBayesian approach pioneered by David McAllester and applied to a conception
of statistical learning theory forged by Vladimir Vapnik. Using convex analysis
on the set of posterior probability measures, we show how to get local measures
of the complexity of the classification model involving the relative entropy of
posterior distributions with respect to Gibbs posterior measures. We then
discuss relative bounds, comparing the generalization error of two
classification rules, showing how the margin assumption of Mammen and Tsybakov
can be replaced with some empirical measure of the covariance structure of the
classification model.We show how to associate to any posterior distribution an
effective temperature relating it to the Gibbs prior distribution with the same
level of expected error rate, and how to estimate this effective temperature
from data, resulting in an estimator whose expected error rate converges
according to the best possible power of the sample size adaptively under any
margin and parametric complexity assumptions. We describe and study an
alternative selection scheme based on relative bounds between estimators, and
present a two step localization technique which can handle the selection of a
parametric model from a family of those. We show how to extend systematically
all the results obtained in the inductive setting to transductive learning, and
use this to improve Vapnik's generalization bounds, extending them to the case
when the sample is made of independent non-identically distributed pairs of
patterns and labels. Finally we review briefly the construction of Support
Vector Machines and show how to derive generalization bounds for them,
measuring the complexity either through the number of support vectors or
through the value of the transductive or inductive margin.
"
"  This paper addresses the estimation of parameters of a Bayesian network from
incomplete data. The task is usually tackled by running the
Expectation-Maximization (EM) algorithm several times in order to obtain a high
log-likelihood estimate. We argue that choosing the maximum log-likelihood
estimate (as well as the maximum penalized log-likelihood and the maximum a
posteriori estimate) has severe drawbacks, being affected both by overfitting
and model uncertainty. Two ideas are discussed to overcome these issues: a
maximum entropy approach and a Bayesian model averaging approach. Both ideas
can be easily applied on top of EM, while the entropy idea can be also
implemented in a more sophisticated way, through a dedicated non-linear solver.
A vast set of experiments shows that these ideas produce significantly better
estimates and inferences than the traditional and widely used maximum
(penalized) log-likelihood and maximum a posteriori estimates. In particular,
if EM is adopted as optimization engine, the model averaging approach is the
best performing one; its performance is matched by the entropy approach when
implemented using the non-linear solver. The results suggest that the
applicability of these ideas is immediate (they are easy to implement and to
integrate in currently available inference engines) and that they constitute a
better way to learn Bayesian network parameters.
"
"  We study the problem of allocating stocks to dark pools. We propose and
analyze an optimal approach for allocations, if continuous-valued allocations
are allowed. We also propose a modification for the case when only
integer-valued allocations are possible. We extend the previous work on this
problem to adversarial scenarios, while also improving on their results in the
iid setup. The resulting algorithms are efficient, and perform well in
simulations under stochastic and adversarial inputs.
"
"  This work is an extension in Arch models of the theorem of S.Y. Hwang and
I.V. Basawa Hwang and Basawa (2001) which was used before in nonlinear time
series contiguous to AR(1) processes. Our results are established under some
general assumptions and stationarity and ergodicity conditions. Local
asymptotic normality (LAN) for the log likelihood ratio was established.An
optimal test was constructed when the parameter is assumed known. Also the
optimality of our test was proved when the parameter is unspecified. The method
is based on the introducing of a new estimator.
"
"  We consider the maximum likelihood (Viterbi) alignment of a hidden Markov
model (HMM). In an HMM, the underlying Markov chain is usually hidden and the
Viterbi alignment is often used as the estimate of it. This approach will be
referred to as the Viterbi segmentation. The goodness of the Viterbi
segmentation can be measured by several risks. In this paper, we prove the
existence of asymptotic risks. Being independent of data, the asymptotic risks
can be considered as the characteristics of the model that illustrate the
long-run behavior of the Viterbi segmentation.
"
"  We consider the least-square linear regression problem with regularization by
the $\ell^1$-norm, a problem usually referred to as the Lasso. In this paper,
we first present a detailed asymptotic analysis of model consistency of the
Lasso in low-dimensional settings. For various decays of the regularization
parameter, we compute asymptotic equivalents of the probability of correct
model selection. For a specific rate decay, we show that the Lasso selects all
the variables that should enter the model with probability tending to one
exponentially fast, while it selects all other variables with strictly positive
probability. We show that this property implies that if we run the Lasso for
several bootstrapped replications of a given sample, then intersecting the
supports of the Lasso bootstrap estimates leads to consistent model selection.
This novel variable selection procedure, referred to as the Bolasso, is
extended to high-dimensional settings by a provably consistent two-step
procedure.
"
"  Early detection of person-to-person transmission of emerging infectious
diseases such as avian influenza is crucial for containing pandemics. We
developed a simple permutation test and its refined version for this purpose. A
simulation study shows that the refined permutation test is as powerful as or
outcompetes the conventional test built on asymptotic theory, especially when
the sample size is small. In addition, our resampling methods can be applied to
a broad range of problems where an asymptotic test is not available or fails.
We also found that decent statistical power could be attained with just a small
number of cases, if the disease is moderately transmissible between humans.
"
"  We built a model of the daily temperature based on a diffusion process and
address to extreme values not taken into account in the literature on this kind
of models. We first study, using non parametric tools, the trends on mean and
variance. In a second step we estimate a stationary model first non
parametrically and then using likelihood methods. Extreme values are taken into
account in the estimation of model and to obtain a definitive estimation we use
in a specific framework extreme theory for diffusions. A test of suitable model
by simulation is done.
"
"  We present a Dempster--Shafer (DS) approach to estimating limits from Poisson
counting data with nuisance parameters. Dempster--Shafer is a statistical
framework that generalizes Bayesian statistics. DS calculus augments
traditional probability by allowing mass to be distributed over power sets of
the event space. This eliminates the Bayesian dependence on prior distributions
while allowing the incorporation of prior information when it is available. We
use the Poisson Dempster--Shafer model (DSM) to derive a posterior DSM for the
``Banff upper limits challenge'' three-Poisson model. The results compare
favorably with other approaches, demonstrating the utility of the approach. We
argue that the reduced dependence on priors afforded by the Dempster--Shafer
framework is both practically and theoretically desirable.
"
"  In computational biology, gene expression datasets are characterized by very
few individual samples compared to a large number of measurements per sample.
Thus, it is appealing to merge these datasets in order to increase the number
of observations and diversify the data, allowing a more reliable selection of
genes relevant to the biological problem. Besides, the increased size of a
merged dataset facilitates its re-splitting into training and validation sets.
This necessitates the introduction of the dataset as a random effect. In this
context, extending a work of Lee et al. (2003), a method is proposed to select
relevant variables among tens of thousands in a probit mixed regression model,
considered as part of a larger hierarchical Bayesian model. Latent variables
are used to identify subsets of selected variables and the grouping (or
blocking) technique of Liu (1994) is combined with a Metropolis-within-Gibbs
algorithm (Robert and Casella 2004). The method is applied to a merged dataset
made of three individual gene expression datasets, in which tens of thousands
of measurements are available for each of several hundred human breast cancer
samples. Even for this large dataset comprised of around 20000 predictors, the
method is shown to be efficient and feasible. As an illustration, it is used to
select the most important genes that characterize the estrogen receptor status
of patients with breast cancer.
"
"  We introduce a new type of graphical model called a ""cumulative distribution
network"" (CDN), which expresses a joint cumulative distribution as a product of
local functions. Each local function can be viewed as providing evidence about
possible orderings, or rankings, of variables. Interestingly, we find that the
conditional independence properties of CDNs are quite different from other
graphical models. We also describe a messagepassing algorithm that efficiently
computes conditional cumulative distributions. Due to the unique independence
properties of the CDN, these messages do not in general have a one-to-one
correspondence with messages exchanged in standard algorithms, such as belief
propagation. We demonstrate the application of CDNs for structured ranking
learning using a previously-studied multi-player gaming dataset.
"
"  Performance period determination and bad definition for credit scorecard has
been a mix of fortune for the typical data modeler. The lack of literature on
these matters led to a proliferation of approaches and techniques to solve the
problems. However, the most commonly accepted approach involves subjective
interpretations of the performance period and bad definition as well as being
chicken and egg problem. These complications result in poorly developed credit
scorecard with minimal benefits to the banks. In this paper, we will be
recommending a simple and effective approach to resolve these issues.
"
"  Probabilistic matrix factorization (PMF) is a powerful method for modeling
data associated with pairwise relationships, finding use in collaborative
filtering, computational biology, and document analysis, among other areas. In
many domains, there is additional information that can assist in prediction.
For example, when modeling movie ratings, we might know when the rating
occurred, where the user lives, or what actors appear in the movie. It is
difficult, however, to incorporate this side information into the PMF model. We
propose a framework for incorporating side information by coupling together
multiple PMF problems via Gaussian process priors. We replace scalar latent
features with functions that vary over the space of side information. The GP
priors on these functions require them to vary smoothly and share information.
We successfully use this new method to predict the scores of professional
basketball games, where side information about the venue and date of the game
are relevant for the outcome.
"
"  this article illustrates the use of linear and bilinear random effects models
to represent statistical dependencies that often characterize dyadic data such
as international relations. In particular, we show how to estimate models for
dyadic data that simultaneously take into account: regressor variables and
third-order dependencies, such as transitivity, clustering, and balance. We
apply this new approach to the relations among ph.d. of university in Iran over
the period from 1991-2005, illustrating the presence and strength of second and
third-order statistical dependencies in these data.
"
"  We propose a novel probabilistic method for detection of objects in noisy
images. The method uses results from percolation and random graph theories. We
present an algorithm that allows to detect objects of unknown shapes in the
presence of random noise. Our procedure substantially differs from
wavelets-based algorithms. The algorithm has linear complexity and exponential
accuracy and is appropriate for real-time systems. We prove results on
consistency and algorithmic complexity of our procedure.
"
"  Existing approaches to analyzing the asymptotics of graph Laplacians
typically assume a well-behaved kernel function with smoothness assumptions. We
remove the smoothness assumption and generalize the analysis of graph
Laplacians to include previously unstudied graphs including kNN graphs. We also
introduce a kernel-free framework to analyze graph constructions with shrinking
neighborhoods in general and apply it to analyze locally linear embedding
(LLE). We also describe how for a given limiting Laplacian operator desirable
properties such as a convergent spectrum and sparseness can be achieved
choosing the appropriate graph construction.
"
"  Discussion of ""A statistical analysis of multiple temperature proxies: Are
reconstructions of surface temperatures over the last 1000 years reliable?"" by
B.B. McShane and A.J. Wyner [arXiv:1104.4002]
"
"  Artificial neural networks are simple and efficient machine learning tools.
Defined originally in the traditional setting of simple vector data, neural
network models have evolved to address more and more difficulties of complex
real world problems, ranging from time evolving data to sophisticated data
structures such as graphs and functions. This paper summarizes advances on
those themes from the last decade, with a focus on results obtained by members
of the SAMM team of Universit\'e Paris 1
"
"  Temporal-difference (TD) networks are a class of predictive state
representations that use well-established TD methods to learn models of
partially observable dynamical systems. Previous research with TD networks has
dealt only with dynamical systems with finite sets of observations and actions.
We present an algorithm for learning TD network representations of dynamical
systems with continuous observations and actions. Our results show that the
algorithm is capable of learning accurate and robust models of several noisy
continuous dynamical systems. The algorithm presented here is the first fully
incremental method for learning a predictive representation of a continuous
dynamical system.
"
"  Among the many ways to model signals, a recent approach that draws
considerable attention is sparse representation modeling. In this model, the
signal is assumed to be generated as a random linear combination of a few atoms
from a pre-specified dictionary. In this work we analyze two Bayesian denoising
algorithms -- the Maximum-Aposteriori Probability (MAP) and the
Minimum-Mean-Squared-Error (MMSE) estimators, under the assumption that the
dictionary is unitary. It is well known that both these estimators lead to a
scalar shrinkage on the transformed coefficients, albeit with a different
response curve. In this work we start by deriving closed-form expressions for
these shrinkage curves and then analyze their performance. Upper bounds on the
MAP and the MMSE estimation errors are derived. We tie these to the error
obtained by a so-called oracle estimator, where the support is given,
establishing a worst-case gain-factor between the MAP/MMSE estimation errors
and the oracle's performance. These denoising algorithms are demonstrated on
synthetic signals and on true data (images).
"
"  We propose a novel application of the Simultaneous Orthogonal Matching
Pursuit (S-OMP) procedure for sparsistant variable selection in ultra-high
dimensional multi-task regression problems. Screening of variables, as
introduced in \cite{fan08sis}, is an efficient and highly scalable way to
remove many irrelevant variables from the set of all variables, while retaining
all the relevant variables. S-OMP can be applied to problems with hundreds of
thousands of variables and once the number of variables is reduced to a
manageable size, a more computationally demanding procedure can be used to
identify the relevant variables for each of the regression outputs. To our
knowledge, this is the first attempt to utilize relatedness of multiple outputs
to perform fast screening of relevant variables. As our main theoretical
contribution, we prove that, asymptotically, S-OMP is guaranteed to reduce an
ultra-high number of variables to below the sample size without losing true
relevant variables. We also provide formal evidence that a modified Bayesian
information criterion (BIC) can be used to efficiently determine the number of
iterations in S-OMP. We further provide empirical evidence on the benefit of
variable selection using multiple regression outputs jointly, as opposed to
performing variable selection for each output separately. The finite sample
performance of S-OMP is demonstrated on extensive simulation studies, and on a
genetic association mapping problem. $Keywords$ Adaptive Lasso; Greedy forward
regression; Orthogonal matching pursuit; Multi-output regression; Multi-task
learning; Simultaneous orthogonal matching pursuit; Sure screening; Variable
selection
"
"  In this article we derive an unbiased expression for the expected
mean-squared error associated with continuously differentiable estimators of
the noncentrality parameter of a chi-square random variable. We then consider
the task of denoising squared-magnitude magnetic resonance image data, which
are well modeled as independent noncentral chi-square random variables on two
degrees of freedom. We consider two broad classes of linearly parameterized
shrinkage estimators that can be optimized using our risk estimate, one in the
general context of undecimated filterbank transforms, and another in the
specific case of the unnormalized Haar wavelet transform. The resultant
algorithms are computationally tractable and improve upon state-of-the-art
methods for both simulated and actual magnetic resonance image data.
"
"  In Passive POMDPs actions do not affect the world state, but still incur
costs. When the agent is bounded by information-processing constraints, it can
only keep an approximation of the belief. We present a variational principle
for the problem of maintaining the information which is most useful for
minimizing the cost, and introduce an efficient and simple algorithm for
finding an optimum.
"
"  Much of the trading activity in Equity markets is directed to brokerage
houses. In exchange they provide so-called ""soft dollars,"" which basically are
amounts spent in ""research"" for identifying profitable trading opportunities.
Soft dollars represent about USD 1 out of every USD 10 paid in commissions.
Obviously they are costly, and it is interesting for an institutional investor
to determine whether soft dollar inputs are worth being used (and indirectly
paid for) or not, from a statistical point of view. To address this question,
we develop association measures between what broker--dealers predict and what
markets realize. Our data are ordinal predictions by two broker--dealers and
realized values on several markets, on the same ordinal scale. We develop a
structural equation model with latent variables in an ordinal setting which
allows us to test broker--dealer predictive ability of financial market
movements. We use a multivariate logit model in a latent factor framework,
develop a tractable estimator based on a Laplace approximation, and show its
consistency and asymptotic normality. Monte Carlo experiments reveal that both
the estimation method and the testing procedure perform well in small samples.
The method is then used to analyze our dataset.
"
"  This work discusses the problem of sparse signal recovery when there is
correlation among the values of non-zero entries. We examine intra-vector
correlation in the context of the block sparse model and inter-vector
correlation in the context of the multiple measurement vector model, as well as
their combination. Algorithms based on the sparse Bayesian learning are
presented and the benefits of incorporating correlation at the algorithm level
are discussed. The impact of correlation on the limits of support recovery is
also discussed highlighting the different impact intra-vector and inter-vector
correlations have on such limits.
"
"  Chia and Nakano (2009) introduced the concept of M-decomposability of
probability densities in one-dimension. In this paper, we generalize
M-decomposability to any dimension. We prove that all elliptical unimodal
densities are M-undecomposable. We also derive an inequality to show that it is
better to represent an M-decomposable density via a mixture of unimodal
densities. Finally, we demonstrate the application of M-decomposability to
clustering and kernel density estimation, using real and simulated data. Our
results show that M-decomposability can be used as a non-parametric criterion
to locate modes in probability densities.
"
"  This paper evaluates heterogeneous information fusion using multi-task
Gaussian processes in the context of geological resource modeling.
Specifically, it empirically demonstrates that information integration across
heterogeneous information sources leads to superior estimates of all the
quantities being modeled, compared to modeling them individually. Multi-task
Gaussian processes provide a powerful approach for simultaneous modeling of
multiple quantities of interest while taking correlations between these
quantities into consideration. Experiments are performed on large scale real
sensor data.
"
"  We propose a new approach to analyze data that naturally lie on manifolds. We
focus on a special class of manifolds, called direct product manifolds, whose
intrinsic dimension could be very high. Our method finds a low-dimensional
representation of the manifold that can be used to find and visualize the
principal modes of variation of the data, as Principal Component Analysis (PCA)
does in linear spaces. The proposed method improves upon earlier manifold
extensions of PCA by more concisely capturing important nonlinear modes. For
the special case of data on a sphere, variation following nongeodesic arcs is
captured in a single mode, compared to the two modes needed by previous
methods. Several computational and statistical challenges are resolved. The
development on spheres forms the basis of principal arc analysis on more
complicated manifolds. The benefits of the method are illustrated by a data
example using medial representations in image analysis.
"
"  Colon and rectum cancer share many risk factors, and are often tabulated
together as ``colorectal cancer'' in published summaries. However, recent work
indicating that exercise, diet, and family history may have differential
impacts on the two cancers encourages analyzing them separately, so that
corresponding public health interventions can be more efficiently targeted. We
analyze colon and rectum cancer data from the Minnesota Cancer Surveillance
System from 1998--2002 over the 16-county Twin Cities (Minneapolis--St. Paul)
metro and exurban area. The data consist of two marked point patterns, meaning
that any statistical model must account for randomness in the observed
locations, and expected positive association between the two cancer patterns.
Our model extends marked spatial point pattern analysis in the context of a log
Gaussian Cox process to accommodate spatially referenced covariates (local
poverty rate and location within the metro area), individual-level risk factors
(patient age and cancer stage), and related interactions. We obtain smoothed
maps of marginal log-relative intensity surfaces for colon and rectum cancer,
and uncover significant age and stage differences between the two groups. This
encourages more aggressive colon cancer screening in the inner Twin Cities and
their southern and western exurbs, where our model indicates higher colon
cancer relative intensity.
"
"  This paper analyzes the problem of Gaussian process (GP) bandits with
deterministic observations. The analysis uses a branch and bound algorithm that
is related to the UCB algorithm of (Srinivas et al, 2010). For GPs with
Gaussian observation noise, with variance strictly greater than zero, Srinivas
et al proved that the regret vanishes at the approximate rate of
$O(1/\sqrt{t})$, where t is the number of observations. To complement their
result, we attack the deterministic case and attain a much faster exponential
convergence rate. Under some regularity assumptions, we show that the regret
decreases asymptotically according to $O(e^{-\frac{\tau t}{(\ln t)^{d/4}}})$
with high probability. Here, d is the dimension of the search space and tau is
a constant that depends on the behaviour of the objective function near its
global maximum.
"
"  We propose a set-valued framework for the well-posedness of birth-and-growth
process. Our birth-and-growth model is rigorously defined as a suitable
combination, involving Minkowski sum and Aumann integral, of two very general
set-valued processes representing nucleation and growth respectively. The
simplicity of the used geometrical approach leads us to avoid problems arising
by an analytical definition of the front growth such as boundary regularities.
In this framework, growth is generally anisotropic and, according to a
mesoscale point of view, it is not local, i.e. for a fixed time instant, growth
is the same at each space point.
"
"  Our article presents a robust and flexible statistical modeling for the
growth curve associated to the age-length relationship of Cardinalfish
(Epigonus Crassicaudus). Specifically, we consider a non-linear regression
model, in which the error distribution allows heteroscedasticity and belongs to
the family of scale mixture of the skewnormal (SMSN) distributions, thus
eliminating the need to transform the dependent variable into many data sets.
The SMSN is a tractable and flexible class of asymmetric heavy-tailed
distributions that are useful for robust inference when the normality
assumption for error distribution is questionable. Two well-known important
members of this class are the proper skew-normal and skew-t distributions. In
this work emphasis is given to the skew-t model. However, the proposed
methodology can be adapted for each of the SMSN models with some basic changes.
The present work is motivated by previous analysis about of Cardinalfish age,
in which a maximum age of 15 years has been determined. Therefore, in this
study we carry out the mentioned methodology over a data set that include a
long-range of ages based on an otolith sample where the determined longevity is
higher than 54 years.
"
"  We information-theoretically reformulate two measures of capacity from
statistical learning theory: empirical VC-entropy and empirical Rademacher
complexity. We show these capacity measures count the number of hypotheses
about a dataset that a learning algorithm falsifies when it finds the
classifier in its repertoire minimizing empirical risk. It then follows from
that the future performance of predictors on unseen data is controlled in part
by how many hypotheses the learner falsifies. As a corollary we show that
empirical VC-entropy quantifies the message length of the true hypothesis in
the optimal code of a particular probability distribution, the so-called actual
repertoire.
"
"  The classic N p chart gives a signal if the number of successes in a sequence
of inde- pendent binary variables exceeds a control limit. Motivated by
engineering applications in industrial image processing and, to some extent,
financial statistics, we study a simple modification of this chart, which uses
only the most recent observations. Our aim is to construct a control chart for
detecting a shift of an unknown size, allowing for an unknown distribution of
the error terms. Simulation studies indicate that the proposed chart is su-
perior in terms of out-of-control average run length, when one is interest in
the detection of very small shifts. We provide a (functional) central limit
theorem under a change-point model with local alternatives which explains that
unexpected and interesting behavior. Since real observations are often not
independent, the question arises whether these re- sults still hold true for
the dependent case. Indeed, our asymptotic results work under the fairly
general condition that the observations form a martingale difference array.
This enlarges the applicability of our results considerably, firstly, to a
large class time series models, and, secondly, to locally dependent image data,
as we demonstrate by an example.
"
"  The Luria-Delbr\""uck distribution is a classical model of mutations in cell
kinetics. It is obtained as a limit when the probability of mutation tends to
zero and the number of divisions to infinity. It can be interpreted as a
compound Poisson distribution (for the number of mutations) of exponential
mixtures (for the developing time of mutant clones) of geometric distributions
(for the number of cells produced by a mutant clone in a given time). The
probabilistic interpretation, and a rigourous proof of convergence in the
general case, are deduced from classical results on Bellman-Harris branching
processes. The two parameters of the Luria-Delbr\""uck distribution are the
expected number of mutations, which is the parameter of interest, and the
relative fitness of normal cells compared to mutants, which is the heavy tail
exponent. Both can be simultaneously estimated by the maximum likehood method.
However, the computation becomes numerically unstable as soon as the maximal
value of the sample is large, which occurs frequently due to the heavy tail
property. Based on the empirical generating function, robust estimators are
proposed and their asymptotic variance is given. They are comparable in
precision to maximum likelihood estimators, with a much broader range of
calculability, a better numerical stability, and a negligible computing time.
"
"  We present a flexible stochastic model for a class of cooperative wireless
relay networks, in which the relay processing functionality is not known at the
destination. In addressing this problem we develop efficient algorithms to
perform relay identification in a wireless relay network. We first construct a
statistical model based on a representation of the system using Gaussian
Processes in a non-standard manner due to the way we treat the imperfect
channel state information. We then formulate the estimation problem to perform
system identification, taking into account complexity and computational
efficiency. Next we develop a set of three algorithms to solve the
identification problem each of decreasing complexity, trading-off the
estimation bias for computational efficiency. The joint optimisation problem is
tackled via a Bayesian framework using the Iterated Conditioning on the Modes
methodology. We develop a lower bound and several sub-optimal computationally
efficient solutions to the identification problem, for comparison. We
illustrate the estimation performance of our methodology for a range of widely
used relay functionalities. The relative total error attained by our algorithm
when compared to the lower bound is found to be at worst 9% for low SNR values
under all functions considered. The effect of the relay functional estimation
error is also studied via BER simulations and is shown to be less than 2dB
worse than the lower bound.
"
"  A mixture of Gaussians fit to a single curved or heavy-tailed cluster will
report that the data contains many clusters. To produce more appropriate
clusterings, we introduce a model which warps a latent mixture of Gaussians to
produce nonparametric cluster shapes. The possibly low-dimensional latent
mixture model allows us to summarize the properties of the high-dimensional
clusters (or density manifolds) describing the data. The number of manifolds,
as well as the shape and dimension of each manifold is automatically inferred.
We derive a simple inference scheme for this model which analytically
integrates out both the mixture parameters and the warping function. We show
that our model is effective for density estimation, performs better than
infinite Gaussian mixture models at recovering the true number of clusters, and
produces interpretable summaries of high-dimensional datasets.
"
"  Latent variable models are an elegant framework for capturing rich
probabilistic dependencies in many applications. However, current approaches
typically parametrize these models using conditional probability tables, and
learning relies predominantly on local search heuristics such as Expectation
Maximization. Using tensor algebra, we propose an alternative parameterization
of latent variable models (where the model structures are junction trees) that
still allows for computation of marginals among observed variables. While this
novel representation leads to a moderate increase in the number of parameters
for junction trees of low treewidth, it lets us design a local-minimum-free
algorithm for learning this parameterization. The main computation of the
algorithm involves only tensor operations and SVDs which can be orders of
magnitude faster than EM algorithms for large datasets. To our knowledge, this
is the first provably consistent parameter learning technique for a large class
of low-treewidth latent graphical models beyond trees. We demonstrate the
advantages of our method on synthetic and real datasets.
"
"  The problem of content search through comparisons has recently received
considerable attention. In short, a user searching for a target object
navigates through a database in the following manner: the user is asked to
select the object most similar to her target from a small list of objects. A
new object list is then presented to the user based on her earlier selection.
This process is repeated until the target is included in the list presented, at
which point the search terminates. This problem is known to be strongly related
to the small-world network design problem.
  However, contrary to prior work, which focuses on cases where objects in the
database are equally popular, we consider here the case where the demand for
objects may be heterogeneous. We show that, under heterogeneous demand, the
small-world network design problem is NP-hard. Given the above negative result,
we propose a novel mechanism for small-world design and provide an upper bound
on its performance under heterogeneous demand. The above mechanism has a
natural equivalent in the context of content search through comparisons, and we
establish both an upper bound and a lower bound for the performance of this
mechanism. These bounds are intuitively appealing, as they depend on the
entropy of the demand as well as its doubling constant, a quantity capturing
the topology of the set of target objects. They also illustrate interesting
connections between comparison-based search to classic results from information
theory. Finally, we propose an adaptive learning algorithm for content search
that meets the performance guarantees achieved by the above mechanisms.
"
"  We develop a new approach to robust adaptive beamforming in the presence of
signal steering vector errors. Since the signal steering vector is known
imprecisely, its presumed (prior) value is used to find a more accurate
estimate of the actual steering vector, which then is used for obtaining the
optimal beamforming weight vector. The objective for finding such an estimate
of the actual signal steering vector is the maximization of the beamformer
output power, while the constraints are the normalization condition and the
requirement that the estimate of the steering vector does not converge to an
interference steering vector. Our objective and constraints are free of any
design parameters of non-unique choice. The resulting optimization problem is a
non-convex quadratically constrained quadratic program, which is NP hard in
general. However, for our problem we show that an efficient solution can be
found using the semi-definite relaxation technique. Moreover, the strong
duality holds for the proposed problem and can also be used for finding the
optimal solution efficiently and at low complexity. In some special cases, the
solution can be even found in closed-form. Our simulation results demonstrate
the superiority of the proposed method over other previously developed robust
adaptive beamforming methods for several frequently encountered types of signal
steering vector errors.
"
"  A nonlinear channel estimator using complex Least Square Support Vector
Machines (LS-SVM) is proposed for pilot-aided OFDM system and applied to Long
Term Evolution (LTE) downlink under high mobility conditions. The estimation
algorithm makes use of the reference signals to estimate the total frequency
response of the highly selective multipath channel in the presence of
non-Gaussian impulse noise interfering with pilot signals. Thus, the algorithm
maps trained data into a high dimensional feature space and uses the structural
risk minimization (SRM) principle to carry out the regression estimation for
the frequency response function of the highly selective channel. The
simulations show the effectiveness of the proposed method which has good
performance and high precision to track the variations of the fading channels
compared to the conventional LS method and it is robust at high speed mobility.
"
"  This article discusses a partially adapted particle filter for estimating the
likelihood of a nonlinear structural econometric state space models whose state
transition density cannot be expressed in closed form. The filter generates the
disturbances in the state transition equation and allows for multiple modes in
the conditional disturbance distribution. The particle filter produces an
unbiased estimate of the likelihood and so can be used to carry out Bayesian
inference in a particle Markov chain Monte Carlo framework. We show empirically
that when the signal to noise ratio is high, the new filter can be much more
efficient than the standard particle filter, in the sense that it requires far
fewer particles to give the same accuracy. The new filter is applied to several
simulated and real examples and in particular to a dynamic stochastic general
equilibrium model.
"
"  In this paper, auto-associative models are proposed as candidates to the
generalization of Principal Component Analysis. We show that these models are
dedicated to the approximation of the dataset by a manifold. Here, the word
""manifold"" refers to the topology properties of the structure. The
approximating manifold is built by a projection pursuit algorithm. At each step
of the algorithm, the dimension of the manifold is incremented. Some
theoretical properties are provided. In particular, we can show that, at each
step of the algorithm, the mean residuals norm is not increased. Moreover, it
is also established that the algorithm converges in a finite number of steps.
Some particular auto-associative models are exhibited and compared to the
classical PCA and some neural networks models. Implementation aspects are
discussed. We show that, in numerous cases, no optimization procedure is
required. Some illustrations on simulated and real data are presented.
"
"  A number of recent emerging applications call for studying data streams,
potentially infinite flows of information updated in real-time. When multiple
co-evolving data streams are observed, an important task is to determine how
these streams depend on each other, accounting for dynamic dependence patterns
without imposing any restrictive probabilistic law governing this dependence.
In this paper we argue that flexible least squares (FLS), a penalized version
of ordinary least squares that accommodates for time-varying regression
coefficients, can be deployed successfully in this context. Our motivating
application is statistical arbitrage, an investment strategy that exploits
patterns detected in financial data streams. We demonstrate that FLS is
algebraically equivalent to the well-known Kalman filter equations, and take
advantage of this equivalence to gain a better understanding of FLS and suggest
a more efficient algorithm. Promising experimental results obtained from a
FLS-based algorithmic trading system for the S&P 500 Futures Index are
reported.
"
"  To elucidate allometric scaling in complex systems, we investigated the
underlying scaling relationships between typical three-scale indicators for
approximately 500,000 Japanese firms; namely, annual sales, number of
employees, and number of business partners. First, new scaling relations
including the distributions of fluctuations were discovered by systematically
analyzing conditional statistics. Second, we introduced simple probabilistic
models that reproduce all these scaling relations, and we derived relations
between scaling exponents and the magnitude of fluctuations.
"
"  High throughput genetic sequencing arrays with thousands of measurements per
sample and a great amount of related censored clinical data have increased
demanding need for better measurement specific model selection. In this paper
we establish strong oracle properties of nonconcave penalized methods for
nonpolynomial (NP) dimensional data with censoring in the framework of Cox's
proportional hazards model. A class of folded-concave penalties are employed
and both LASSO and SCAD are discussed specifically. We unveil the question
under which dimensionality and correlation restrictions can an oracle estimator
be constructed and grasped. It is demonstrated that nonconcave penalties lead
to significant reduction of the ""irrepresentable condition"" needed for LASSO
model selection consistency. The large deviation result for martingales,
bearing interests of its own, is developed for characterizing the strong oracle
property. Moreover, the nonconcave regularized estimator, is shown to achieve
asymptotically the information bound of the oracle estimator. A coordinate-wise
algorithm is developed for finding the grid of solution paths for penalized
hazard regression problems, and its performance is evaluated on simulated and
gene association study examples.
"
"  Under the Basel II standards, the Operational Risk (OpRisk) advanced
measurement approach allows a provision for reduction of capital as a result of
insurance mitigation of up to 20%. This paper studies the behaviour of
different insurance policies in the context of capital reduction for a range of
possible extreme loss models and insurance policy scenarios in a multi-period,
multiple risk settings. A Loss Distributional Approach (LDA) for modelling of
the annual loss process, involving homogeneous compound Poisson processes for
the annual losses, with heavy tailed severity models comprised of alpha-stable
severities is considered. There has been little analysis of such models to date
and it is believed, insurance models will play more of a role in OpRisk
mitigation and capital reduction in future. The first question of interest is
when would it be equitable for a bank or financial institution to purchase
insurance for heavy tailed OpRisk losses under different insurance policy
scenarios? The second question then pertains to Solvency II and addresses what
the insurers capital would be for such operational risk scenarios under
different policy offerings. In addition we consider the insurers perspective
with respect to fair premium as a percentage above the expected annual claim
for each insurance policy. The intention being to address questions related to
VaR reduction under Basel II, SCR under Solvency II and fair insurance premiums
in OpRisk for different extreme loss scenarios. In the process we provide
closed form solutions for the distribution of loss process and claims process
in an LDA structure as well as closed form analytic solutions for the Expected
Shortfall, SCR and MCR under Basel II and Solvency II. We also provide closed
form analytic solutions for the annual loss distribution of multiple risks
including insurance mitigation.
"
"  A significant theoretical advantage of search-and-score methods for learning
Bayesian Networks is that they can accept informative prior beliefs for each
possible network, thus complementing the data. In this paper, a method is
presented for assigning priors based on beliefs on the presence or absence of
certain paths in the true network. Such beliefs correspond to knowledge about
the possible causal and associative relations between pairs of variables. This
type of knowledge naturally arises from prior experimental and observational
data, among others. In addition, a novel search-operator is proposed to take
advantage of such prior knowledge. Experiments show that, using path beliefs
improves the learning of the skeleton, as well as the edge directions in the
network.
"
"  Radio interferometry probes astrophysical signals through incomplete and
noisy Fourier measurements. The theory of compressed sensing demonstrates that
such measurements may actually suffice for accurate reconstruction of sparse or
compressible signals. We propose new generic imaging techniques based on convex
optimization for global minimization problems defined in this context. The
versatility of the framework notably allows introduction of specific prior
information on the signals, which offers the possibility of significant
improvements of reconstruction relative to the standard local matching pursuit
algorithm CLEAN used in radio astronomy. We illustrate the potential of the
approach by studying reconstruction performances on simulations of two
different kinds of signals observed with very generic interferometric
configurations. The first kind is an intensity field of compact astrophysical
objects. The second kind is the imprint of cosmic strings in the temperature
field of the cosmic microwave background radiation, of particular interest for
cosmology.
"
"  Structural equation models and Bayesian networks have been widely used to
analyze causal relations between continuous variables. In such frameworks,
linear acyclic models are typically used to model the data-generating process
of variables. Recently, it was shown that use of non-Gaussianity identifies the
full structure of a linear acyclic model, i.e., a causal ordering of variables
and their connection strengths, without using any prior knowledge on the
network structure, which is not the case with conventional methods. However,
existing estimation methods are based on iterative search algorithms and may
not converge to a correct solution in a finite number of steps. In this paper,
we propose a new direct method to estimate a causal ordering and connection
strengths based on non-Gaussianity.
  In contrast to the previous methods, our algorithm requires no algorithmic
parameters and is guaranteed to converge to the right solution within a small
fixed number of steps if the data strictly follows the model.
"
"  In geostatistics, it is common to model spatially distributed phenomena
through an underlying stationary and isotropic spatial process. However, these
assumptions are often untenable in practice because of the influence of local
effects in the correlation structure. Therefore, it has been of prolonged
interest in the literature to provide flexible and effective ways to model
nonstationarity in the spatial effects. Arguably, due to the local nature of
the problem, we might envision that the correlation structure would be highly
dependent on local characteristics of the domain of study, namely, the
latitude, longitude and altitude of the observation sites, as well as other
locally defined covariate information. In this work, we provide a flexible and
computationally feasible way for allowing the correlation structure of the
underlying processes to depend on local covariate information. We discuss the
properties of the induced covariance functions and methods to assess its
dependence on local covariate information. The proposed method is used to
analyze daily ozone in the southeast United States.
"
"  Smoothing methods and SiZer (SIgnificant ZERo crossing of the derivatives)
are useful tools for exploring significant underlying structures in data
samples. An extension of SiZer to circular data, namely CircSiZer, is
introduced. Based on scale-space ideas, CircSiZer presents a graphical device
to assess which observed features are statistically significant, both for
density and regression analysis with circular data. The method is intended for
analyzing the behavior of wind direction in the atlantic coast of Galicia (NW
Spain) and how it has an influence over wind speed. The performance of
CircSiZer is also checked with some simulated examples.
"
"  This paper tackles the problem of selecting among several linear estimators
in non-parametric regression; this includes model selection for linear
regression, the choice of a regularization parameter in kernel ridge
regression, spline smoothing or locally weighted regression, and the choice of
a kernel in multiple kernel learning. We propose a new algorithm which first
estimates consistently the variance of the noise, based upon the concept of
minimal penalty, which was previously introduced in the context of model
selection. Then, plugging our variance estimate in Mallows' $C_L$ penalty is
proved to lead to an algorithm satisfying an oracle inequality. Simulation
experiments with kernel ridge regression and multiple kernel learning show that
the proposed algorithm often improves significantly existing calibration
procedures such as generalized cross-validation.
"
"  Analysis of multivariate data sets from e.g. microarray studies frequently
results in lists of genes which are associated with some response of interest.
The biological interpretation is often complicated by the statistical
instability of the obtained gene lists with respect to sampling variations,
which may partly be due to the functional redundancy among genes, implying that
multiple genes can play exchangeable roles in the cell. In this paper we use
the concept of exchangeability of random variables to model this functional
redundancy and thereby account for the instability attributable to sampling
variations. We present a flexible framework to incorporate the exchangeability
into the representation of lists. The proposed framework supports
straightforward robust comparison between any two lists. It can also be used to
generate new, more stable gene rankings incorporating more information from the
experimental data. Using a microarray data set from lung cancer patients we
show that the proposed method provides more robust gene rankings than existing
methods with respect to sampling variations, without compromising the
biological significance.
"
"  The statistical analysis of measurement data has become a key component of
many quantum engineering experiments. As standard full state tomography becomes
unfeasible for large dimensional quantum systems, one needs to exploit prior
information and the ""sparsity"" properties of the experimental state in order to
reduce the dimensionality of the estimation problem. In this paper we propose
model selection as a general principle for finding the simplest, or most
parsimonious explanation of the data, by fitting different models and choosing
the estimator with the best trade-off between likelihood fit and model
complexity. We apply two well established model selection methods -- the Akaike
information criterion (AIC) and the Bayesian information criterion (BIC) -- to
models consising of states of fixed rank and datasets such as are currently
produced in multiple ions experiments. We test the performance of AIC and BIC
on randomly chosen low rank states of 4 ions, and study the dependence of the
selected rank with the number of measurement repetitions for one ion states. We
then apply the methods to real data from a 4 ions experiment aimed at creating
a Smolin state of rank 4. The two methods indicate that the optimal model for
describing the data lies between ranks 6 and 9, and the Pearson $\chi^{2}$ test
is applied to validate this conclusion. Additionally we find that the mean
square error of the maximum likelihood estimator for pure states is close to
that of the optimal over all possible measurements.
"
"  The problem of supervised classification (or discrimination) with functional
data is considered, with a special interest on the popular k-nearest neighbors
(k-NN) classifier. First, relying on a recent result by Cerou and Guyader
(2006), we prove the consistency of the k-NN classifier for functional data
whose distribution belongs to a broad family of Gaussian processes with
triangular covariance functions. Second, on a more practical side, we check the
behavior of the k-NN method when compared with a few other functional
classifiers. This is carried out through a small simulation study and the
analysis of several real functional data sets. While no global ""uniform"" winner
emerges from such comparisons, the overall performance of the k-NN method,
together with its sound intuitive motivation and relative simplicity, suggests
that it could represent a reasonable benchmark for the classification problem
with functional data.
"
"  Case vs control comparisons have been the classical approach to the study of
neurological diseases. However, most patients will not fall cleanly into either
group. Instead, clinicians will typically find patients that cannot be
classified as having clearly progressed into the disease state. For those
subjects, very little can be said about their brain function on the basis of
analyses of group differences. To describe the intermediate brain function
requires models that interpolate between the disease states. We have chosen
Gaussian Processes (GP) regression to obtain a continuous spectrum of brain
activation and to extract the unknown disease progression profile. Our models
incorporate spatial distribution of measures of activation, e.g. the
correlation of an fMRI trace with an input stimulus, and so constitute
ultra-high multi-variate GP regressors. We applied GPs to model fMRI image
phenotypes across Alzheimer's Disease (AD) behavioural measures, e.g. MMSE, ACE
etc. scores, and obtained predictions at non-observed MMSE/ACE values. The
overall model confirmed the known reduction in the spatial extent of activity
in response to reading versus false-font stimulation. The predictive
uncertainty indicated the worsening confidence intervals at behavioural scores
distance from those used for GP training. Thus, the model indicated the type of
patient (what behavioural score) that would need to included in the training
data to improve models predictions.
"
"  We introduce fully nonparametric two-sample tests for testing the null
hypothesis that the samples come from the same distribution if the values are
only indirectly given via current status censoring. The tests are based on the
likelihood ratio principle and allow the observation distributions to be
different for the two samples, in contrast with earlier proposals for this
situation. A bootstrap method is given for determining critical values and
asymptotic theory is developed. A simulation study, using Weibull
distributions, is presented to compare the power behavior of the tests with the
power of other nonparametric tests in this situation.
"
"  We present the first tree-based regressor whose convergence rate depends only
on the intrinsic dimension of the data, namely its Assouad dimension. The
regressor uses the RPtree partitioning procedure, a simple randomized variant
of k-d trees.
"
"  This paper illustrates novel methods for nonstationary time series modeling
along with their applications to selected problems in neuroscience. These
methods are semi-parametric in that inferences are derived by combining
sequential Bayesian updating with a non-parametric change-point test. As a test
statistic, we propose a Kullback--Leibler (KL) divergence between posterior
distributions arising from different sets of data. A closed form expression of
this statistic is derived for exponential family models, whereas standard
Markov chain Monte Carlo output is used to approximate its value and its
critical region for more general models. The behavior of one-step ahead
predictive distributions under our semi-parametric framework is described
analytically for a dynamic linear time series model. Conditions under which our
approach reduces to fully parametric state-space modeling are also illustrated.
We apply our methods to estimating the functional dynamics of a wide range of
neural data, including multi-channel electroencephalogram recordings,
longitudinal behavioral experiments and in-vivo multiple spike trains
recordings. The estimated dynamics are related to the presentation of visual
stimuli, to the evaluation of a learning performance and to changes in the
functional connections between neurons over a sequence of experiments.
"
"  Extending the work of Connolly and Rendleman (2008), we document the
dominance of Tiger Woods during the 1998-2001 PGA Tour seasons. We show that by
playing ""average,"" Woods could have won some tournaments and placed no worse
than fourth in the tournaments in which he participated in year 2000, his best
on the PGA Tour. No other PGA Tour player in our sample could have come close
to such a feat. We also are able to quantify the intimidation factor associated
with playing with Woods. On average, players who were paired with Woods during
the 1998-2001 period scored 0.462 strokes per round worse than normal. Although
we find that Woods' presence in a tournament may have had a small, but
statistically significant adverse impact on the entire field, this effect was
swamped by the apparent intimidation factor associated with having to play with
Tiger side-by-side.
  We also demonstrate that Phil Mickelson's performance in major golf
championships over the 1998-2001 period was not nearly as bad as was frequently
mentioned in the golf press. Although Mickelson won no majors during this
period, he played sufficiently well to have won one or two majors under normal
circumstances. Moreover, his overall performance in majors, relative to his
estimated skill level, was comparable to that of Tiger Woods, who won five of
16 major golf championships during our four-year sample period. Thus, the
general characterization of Woods as golf's dominant player over the 1998-2001
period was accurate, but the frequent characterization of Phil Mickelson
choking in majors was not.
"
"  The large number of spectral variables in most data sets encountered in
spectral chemometrics often renders the prediction of a dependent variable
uneasy. The number of variables hopefully can be reduced, by using either
projection techniques or selection methods; the latter allow for the
interpretation of the selected variables. Since the optimal approach of testing
all possible subsets of variables with the prediction model is intractable, an
incremental selection approach using a nonparametric statistics is a good
option, as it avoids the computationally intensive use of the model itself. It
has two drawbacks however: the number of groups of variables to test is still
huge, and colinearities can make the results unstable. To overcome these
limitations, this paper presents a method to select groups of spectral
variables. It consists in a forward-backward procedure applied to the
coefficients of a B-Spline representation of the spectra. The criterion used in
the forward-backward procedure is the mutual information, allowing to find
nonlinear dependencies between variables, on the contrary of the generally used
correlation. The spline representation is used to get interpretability of the
results, as groups of consecutive spectral variables will be selected. The
experiments conducted on NIR spectra from fescue grass and diesel fuels show
that the method provides clearly identified groups of selected variables,
making interpretation easy, while keeping a low computational load. The
prediction performances obtained using the selected coefficients are higher
than those obtained by the same method applied directly to the original
variables and similar to those obtained using traditional models, although
using significantly less spectral variables.
"
"  The most commonly used relative abundance index in stock assessments of
longline fisheries is catch per unit effort (CPUE), here defined as the number
of fish of the targeted species caught per hook and minute of soak time.
Longline CPUE can be affected by interspecific competition and the retrieval of
unbaited or empty hooks, and interannual variation in these can lead to biases
in the apparent abundance trends in the CPUE. Interspecific competition on
longlines has been previously studied but the return of empty hooks is ignored
in all current treatments of longline CPUE. In this work we propose some
different methods to build indices to address the interspecific competition
that relates to empty hooks. We show that in the absence of information about
empty hooks, the relative abundance estimates have constant biases with respect
to fish density and this is typically not problematic for stock assessment. The
simple CPUE index behaves poorly in every scenario. Understanding the reasons
for empty hooks allows selection of the appropriate index. A scientific
longline survey is conducted every two years in the Strait of Georgia, British
Columbia by Fisheries and Oceans Canada. The above methods are applied to build
the time-series of indices from 2003 to 2009 for quillback rockfish (Sebastes
maliger). Due to variation in the incidence of non-target species, the index
trend obtained is moderately sensitive to the choice of the estimator.
"
"  Remote sensors are becoming the standard for observing and recording
ecological data in the field. Such sensors can record data at fine temporal
resolutions, and they can operate under extreme conditions prohibitive to human
access. Unfortunately, sensor data streams exhibit many kinds of errors ranging
from corrupt communications to partial or total sensor failures. This means
that the raw data stream must be cleaned before it can be used by domain
scientists. In our application environment|the H.J. Andrews Experimental
Forest|this data cleaning is performed manually. This paper introduces a
Dynamic Bayesian Network model for analyzing sensor observations and
distinguishing sensor failures from valid data for the case of air temperature
measured at 15 minute time resolution. The model combines an accurate
distribution of long-term and short-term temperature variations with a single
generalized fault model. Experiments with historical data show that the
precision and recall of the method is comparable to that of the domain expert.
The system is currently being deployed to perform real-time automated data
cleaning.
"
"  In science and engineering, intelligent processing of complex signals such as
images, sound or language is often performed by a parameterized hierarchy of
nonlinear processing layers, sometimes biologically inspired. Hierarchical
systems (or, more generally, nested systems) offer a way to generate complex
mappings using simple stages. Each layer performs a different operation and
achieves an ever more sophisticated representation of the input, as, for
example, in an deep artificial neural network, an object recognition cascade in
computer vision or a speech front-end processing. Joint estimation of the
parameters of all the layers and selection of an optimal architecture is widely
considered to be a difficult numerical nonconvex optimization problem,
difficult to parallelize for execution in a distributed computation
environment, and requiring significant human expert effort, which leads to
suboptimal systems in practice. We describe a general mathematical strategy to
learn the parameters and, to some extent, the architecture of nested systems,
called the method of auxiliary coordinates (MAC). This replaces the original
problem involving a deeply nested function with a constrained problem involving
a different function in an augmented space without nesting. The constrained
problem may be solved with penalty-based methods using alternating optimization
over the parameters and the auxiliary coordinates. MAC has provable
convergence, is easy to implement reusing existing algorithms for single
layers, can be parallelized trivially and massively, applies even when
parameter derivatives are not available or not desirable, and is competitive
with state-of-the-art nonlinear optimizers even in the serial computation
setting, often providing reasonable models within a few iterations.
"
"  Exact Gaussian Process (GP) regression has O(N^3) runtime for data size N,
making it intractable for large N. Many algorithms for improving GP scaling
approximate the covariance with lower rank matrices. Other work has exploited
structure inherent in particular covariance functions, including GPs with
implied Markov structure, and equispaced inputs (both enable O(N) runtime).
However, these GP advances have not been extended to the multidimensional input
setting, despite the preponderance of multidimensional applications. This paper
introduces and tests novel extensions of structured GPs to multidimensional
inputs. We present new methods for additive GPs, showing a novel connection
between the classic backfitting method and the Bayesian framework. To achieve
optimal accuracy-complexity tradeoff, we extend this model with a novel variant
of projection pursuit regression. Our primary result -- projection pursuit
Gaussian Process Regression -- shows orders of magnitude speedup while
preserving high accuracy. The natural second and third steps include
non-Gaussian observations and higher dimensional equispaced grid methods. We
introduce novel techniques to address both of these necessary directions. We
thoroughly illustrate the power of these three advances on several datasets,
achieving close performance to the naive Full GP at orders of magnitude less
cost.
"
"  That physiological oscillations of various frequencies are present in fMRI
signals is the rule, not the exception. Herein, we propose a novel theoretical
framework, spatio-temporal Granger causality, which allows us to more reliably
and precisely estimate the Granger causality from experimental datasets
possessing time-varying properties caused by physiological oscillations. Within
this framework, Granger causality is redefined as a global index measuring the
directed information flow between two time series with time-varying properties.
Both theoretical analyses and numerical examples demonstrate that Granger
causality is a monotonically increasing function of the temporal resolution
used in the estimation. This is consistent with the general principle of coarse
graining, which causes information loss by smoothing out very fine-scale
details in time and space. Our results confirm that the Granger causality at
the finer spatio-temporal scales considerably outperforms the traditional
approach in terms of an improved consistency between two resting-state scans of
the same subject. To optimally estimate the Granger causality, the proposed
theoretical framework is implemented through a combination of several
approaches, such as dividing the optimal time window and estimating the
parameters at the fine temporal and spatial scales. Taken together, our
approach provides a novel and robust framework for estimating the Granger
causality from fMRI, EEG, and other related data.
"
"  We introduce new online and batch algorithms that are robust to data with
missing features, a situation that arises in many practical applications. In
the online setup, we allow for the comparison hypothesis to change as a
function of the subset of features that is observed on any given round,
extending the standard setting where the comparison hypothesis is fixed
throughout. In the batch setup, we present a convex relation of a non-convex
problem to jointly estimate an imputation function, used to fill in the values
of missing features, along with the classification hypothesis. We prove regret
bounds in the online setting and Rademacher complexity bounds for the batch
i.i.d. setting. The algorithms are tested on several UCI datasets, showing
superior performance over baselines.
"
"  Given a nonlinear model, a probabilistic forecast may be obtained by Monte
Carlo simulations. At a given forecast horizon, Monte Carlo simulations yield
sets of discrete forecasts, which can be converted to density forecasts. The
resulting density forecasts will inevitably be downgraded by model
mis-specification. In order to enhance the quality of the density forecasts,
one can mix them with the unconditional density. This paper examines the value
of combining conditional density forecasts with the unconditional density. The
findings have positive implications for issuing early warnings in different
disciplines including economics and meteorology, but UK inflation forecasts are
considered as an example.
"
"  Motivation: Algorithms for differential analysis of microarray data are vital
to modern biomedical research. Their accuracy strongly depends on effective
treatment of inter-gene correlation. Correlation is ordinarily accounted for in
terms of its effect on significance cut-offs. In this paper it is shown that
correlation can, in fact, be exploited {to share information across tests},
which, in turn, can increase statistical power.
  Results: Vastly and demonstrably improved differential analysis approaches
are the result of combining identifiability (the fact that in most microarray
data sets, a large proportion of genes can be identified a priori as
non-differential) with optimization criteria that incorporate correlation. As a
special case, we develop a method which builds upon the widely used two-sample
t-statistic based approach and uses the Mahalanobis distance as an optimality
criterion. Results on the prostate cancer data of Singh et al. (2002) suggest
that the proposed method outperforms all published approaches in terms of
statistical power.
  Availability: The proposed algorithm is implemented in MATLAB and in R. The
software, called Tellipsoid, and relevant data sets are available at
http://www.egr.msu.edu/~desaikey
"
"  The paper presents results of a comprehensive study of ground motions
recorded during the strong earthquakes (moment magnitude Mw > 6) generated
during last 34 years by the seismic source of Vrancea, Romania. By analyzing
over 300 accelerograms, the capacity of different expressions in the literature
to estimate the predominant period of a ground motion is compared. The
correlation between the values obtained from different evaluations is assessed
as well. The dependence of the predominant period of different factors of
influence is analysed. Comparisons are made between the parameters determined
for the same seismic event at different stations, as well as for ground motions
recorded on the same site at successive earthquakes. The results are
interpreted in correlation with the information provided by frequency bandwidth
parameters. Considerations are made on the measure in which the influence on
the frequency content of the source and of local geological conditions can be
separated, for seismic motions recorded on different locations.
"
"  We study the tracking problem, namely, estimating the hidden state of an
object over time, from unreliable and noisy measurements. The standard
framework for the tracking problem is the generative framework, which is the
basis of solutions such as the Bayesian algorithm and its approximation, the
particle filters. However, these solutions can be very sensitive to model
mismatches. In this paper, motivated by online learning, we introduce a new
framework for tracking. We provide an efficient tracking algorithm for this
framework. We provide experimental results comparing our algorithm to the
Bayesian algorithm on simulated data. Our experiments show that when there are
slight model mismatches, our algorithm outperforms the Bayesian algorithm.
"
"  When dealing with time series with complex non-stationarities, low
retrospective regret on individual realizations is a more appropriate goal than
low prospective risk in expectation. Online learning algorithms provide
powerful guarantees of this form, and have often been proposed for use with
non-stationary processes because of their ability to switch between different
forecasters or ``experts''. However, existing methods assume that the set of
experts whose forecasts are to be combined are all given at the start, which is
not plausible when dealing with a genuinely historical or evolutionary system.
We show how to modify the ``fixed shares'' algorithm for tracking the best
expert to cope with a steadily growing set of experts, obtained by fitting new
models to new data as it becomes available, and obtain regret bounds for the
growing ensemble.
"
"  Lung cancer is among the most common cancers in the United States, in terms
of incidence and mortality. In 2009, it is estimated that more than 150,000
deaths will result from lung cancer alone. Genetic information is an extremely
valuable data source in characterizing the personal nature of cancer. Over the
past several years, investigators have conducted numerous association studies
where intensive genetic data is collected on relatively few patients compared
to the numbers of gene predictors, with one scientific goal being to identify
genetic features associated with cancer recurrence or survival. In this note,
we propose high-dimensional survival analysis through a new application of
boosting, a powerful tool in machine learning. Our approach is based on an
accelerated lifetime model and minimizing the sum of pairwise differences in
residuals. We apply our method to a recent microarray study of lung
adenocarcinoma and find that our ensemble is composed of 19 genes, while a
proportional hazards (PH) ensemble is composed of nine genes, a proper subset
of the 19-gene panel. In one of our simulation scenarios, we demonstrate that
PH boosting in a misspecified model tends to underfit and ignore
moderately-sized covariate effects, on average. Diagnostic analyses suggest
that the PH assumption is not satisfied in the microarray data and may explain,
in part, the discrepancy in the sets of active coefficients. Our simulation
studies and comparative data analyses demonstrate how statistical learning by
PH models alone is insufficient.
"
"  We consider the two problems of predicting links in a dynamic graph sequence
and predicting functions defined at each node of the graph. In many
applications, the solution of one problem is useful for solving the other.
Indeed, if these functions reflect node features, then they are related through
the graph structure. In this paper, we formulate a hybrid approach that
simultaneously learns the structure of the graph and predicts the values of the
node-related functions. Our approach is based on the optimization of a joint
regularization objective. We empirically test the benefits of the proposed
method with both synthetic and real data. The results indicate that joint
regularization improves prediction performance over the graph evolution and the
node features.
"
"  There has been a substantial amount of research on the relationship between
hippocampal neurogenesis and behaviour over the past fifteen years, but the
causal role that new neurons have on cognitive and affective behavioural tasks
is still far from clear. This is partly due to the difficulty of manipulating
levels of neurogenesis without inducing off-target effects, which might also
influence behaviour. In addition, the analytical methods typically used do not
directly test whether neurogenesis mediates the effect of an intervention on
behaviour. Previous studies may have incorrectly attributed changes in
behavioural performance to neurogenesis because the role of known (or unknown)
neurogenesis-independent mechanisms were not formally taken into consideration
during the analysis. Causal models can tease apart complex causal relationships
and were used to demonstrate that the effect of exercise on pattern separation
is via neurogenesis-independent mechanisms. Many studies in the neurogenesis
literature would benefit from the use of statistical methods that can separate
neurogenesis-dependent from neurogenesis-independent effects on behaviour.
"
"  Longitudinal studies of a binary outcome are common in the health, social,
and behavioral sciences. In general, a feature of random effects logistic
regression models for longitudinal binary data is that the marginal functional
form, when integrated over the distribution of the random effects, is no longer
of logistic form. Recently, Wang and Louis [Biometrika 90 (2003) 765--775]
proposed a random intercept model in the clustered binary data setting where
the marginal model has a logistic form. An acknowledged limitation of their
model is that it allows only a single random effect that varies from cluster to
cluster. In this paper we propose a modification of their model to handle
longitudinal data, allowing separate, but correlated, random intercepts at each
measurement occasion. The proposed model allows for a flexible correlation
structure among the random intercepts, where the correlations can be
interpreted in terms of Kendall's $\tau$. For example, the marginal
correlations among the repeated binary outcomes can decline with increasing
time separation, while the model retains the property of having matching
conditional and marginal logit link functions. Finally, the proposed method is
used to analyze data from a longitudinal study designed to monitor cardiac
abnormalities in children born to HIV-infected women.
"
"  The last decade has seen the advent and consolidation of ontology based tools
for the identification and biological interpretation of classes of genes, such
as the Gene Ontology. The information accumulated time-by-time and included in
the GO is encoded in the definition of terms and in the setting up of semantic
relations amongst terms. This approach might be usefully complemented by a
bottom-up approach based on the knowledge of relationships amongst genes. To
this end, we investigate the Gene Ontology from a complex network perspective.
We consider the semantic network of terms naturally associated with the
semantic relationships provided by the Gene Ontology consortium and a
gene-based weighted network in which the nodes are the terms and a link between
any two terms is set up whenever genes are annotated in both terms. One aim of
the present paper is to understand whether the semantic and the gene-based
network share the same structural properties or not. We then consider network
communities. The identification of communities in the SVNs network can
therefore be the basis of a simple protocol aiming at fully exploiting the
possible relationships amongst terms, thus improving the knowledge of the
semantic structure of GO. This is also important from a biomedical point of
view, as it might reveal how genes over-expressed in a certain term also affect
other biological functions not directly linked by the GO semantics. As a
by-product, we present a simple methodology that allows to have a first glance
insight about the biological characterization of groups of GO terms.
"
"  Compensating changes between a subjects' training and testing session in
Brain Computer Interfacing (BCI) is challenging but of great importance for a
robust BCI operation. We show that such changes are very similar between
subjects, thus can be reliably estimated using data from other users and
utilized to construct an invariant feature space. This novel approach to
learning from other subjects aims to reduce the adverse effects of common
non-stationarities, but does not transfer discriminative information. This is
an important conceptual difference to standard multi-subject methods that e.g.
improve the covariance matrix estimation by shrinking it towards the average of
other users or construct a global feature space. These methods do not reduces
the shift between training and test data and may produce poor results when
subjects have very different signal characteristics. In this paper we compare
our approach to two state-of-the-art multi-subject methods on toy data and two
data sets of EEG recordings from subjects performing motor imagery. We show
that it can not only achieve a significant increase in performance, but also
that the extracted change patterns allow for a neurophysiologically meaningful
interpretation.
"
"  We present a theoretical analysis of Maximum a Posteriori (MAP) sequence
estimation for binary symmetric hidden Markov processes. We reduce the MAP
estimation to the energy minimization of an appropriately defined Ising spin
model, and focus on the performance of MAP as characterized by its accuracy and
the number of solutions corresponding to a typical observed sequence. It is
shown that for a finite range of sufficiently low noise levels, the solution is
uniquely related to the observed sequence, while the accuracy degrades linearly
with increasing the noise strength. For intermediate noise values, the accuracy
is nearly noise-independent, but now there are exponentially many solutions to
the estimation problem, which is reflected in non-zero ground-state entropy for
the Ising model. Finally, for even larger noise intensities, the number of
solutions reduces again, but the accuracy is poor. It is shown that these
regimes are different thermodynamic phases of the Ising model that are related
to each other via first-order phase transitions.
"
"  We propose a semiparametric model for autonomous nonlinear dynamical systems
and devise an estimation procedure for model fitting. This model incorporates
subject-specific effects and can be viewed as a nonlinear semiparametric mixed
effects model. We also propose a computationally efficient model selection
procedure. We show by simulation studies that the proposed estimation as well
as model selection procedures can efficiently handle sparse and noisy
measurements. Finally, we apply the proposed method to a plant growth data used
to study growth displacement rates within meristems of maize roots under two
different experimental conditions.
"
"  We have designed and carried out a statistical study to determine the optimal
cartridge dimensions for a Savage 10FLP law enforcement grade rifle. Optimal
performance is defined as minimal group diameter. A full factorial block design
with two main factors and one blocking factor was used. The two main factors
were bullet seating depth and powder charge. The experimental units were
individual shots taken from a bench-rest position and fired into separate
targets. Additionally, thirteen covariates describing various cartridge
dimensions were recorded. The data analysis includes ANOVA and ANCOVA. We will
describe the experiment, the analysis, and some results.
"
"  Pearl has provided the back door criterion, the front door criterion and the
conditional instrumental variable (IV) method as identifiability criteria for
total effects. In some situations, these three criteria can be applied to
identifying total effects simultaneously. For the purpose of increasing
estimating accuracy, this paper compares the three ways of identifying total
effects in terms of the asymptotic variance, and concludes that in some
situations the superior of them can be recognized directly from the graph
structure.
"
"  Techniques for data-mining, latent semantic analysis, contextual search of
databases, etc. have long ago been developed by computer scientists working on
information retrieval (IR). Experimental scientists, from all disciplines,
having to analyse large collections of raw experimental data (astronomical,
physical, biological, etc.) have developed powerful methods for their
statistical analysis and for clustering, categorising, and classifying objects.
Finally, physicists have developed a theory of quantum measurement, unifying
the logical, algebraic, and probabilistic aspects of queries into a single
formalism. The purpose of this paper is twofold: first to show that when
formulated at an abstract level, problems from IR, from statistical data
analysis, and from physical measurement theories are very similar and hence can
profitably be cross-fertilised, and, secondly, to propose a novel method of
fuzzy hierarchical clustering, termed \textit{semantic distillation} --
strongly inspired from the theory of quantum measurement --, we developed to
analyse raw data coming from various types of experiments on DNA arrays. We
illustrate the method by analysing DNA arrays experiments and clustering the
genes of the array according to their specificity.
"
"  A frequently faced task in experimental physics is to measure the probability
distribution of some quantity. Often this quantity to be measured is smeared by
a non-ideal detector response or by some physical process. The procedure of
removing this smearing effect from the measured distribution is called
unfolding, and is a delicate problem in signal processing, due to the
well-known numerical ill behavior of this task. Various methods were invented
which, given some assumptions on the initial probability distribution, try to
regularize the unfolding problem. Most of these methods definitely introduce
bias into the estimate of the initial probability distribution. We propose a
linear iterative method, which has the advantage that no assumptions on the
initial probability distribution is needed, and the only regularization
parameter is the stopping order of the iteration, which can be used to choose
the best compromise between the introduced bias and the propagated statistical
and systematic errors. The method is consistent: ""binwise"" convergence to the
initial probability distribution is proved in absence of measurement errors
under a quite general condition on the response function. This condition holds
for practical applications such as convolutions, calorimeter response
functions, momentum reconstruction response functions based on tracking in
magnetic field etc. In presence of measurement errors, explicit formulae for
the propagation of the three important error terms is provided: bias error,
statistical error, and systematic error. A trade-off between these three error
terms can be used to define an optimal iteration stopping criterion, and the
errors can be estimated there. We provide a numerical C library for the
implementation of the method, which incorporates automatic statistical error
propagation as well.
"
"  The next generation of telescopes will acquire terabytes of image data on a
nightly basis. Collectively, these large images will contain billions of
interesting objects, which astronomers call sources. The astronomers' task is
to construct a catalog detailing the coordinates and other properties of the
sources. The source catalog is the primary data product for most telescopes and
is an important input for testing new astrophysical theories, but to construct
the catalog one must first detect the sources. Existing algorithms for catalog
creation are effective at detecting sources, but do not have rigorous
statistical error control. At the same time, there are several multiple testing
procedures that provide rigorous error control, but they are not designed to
detect sources that are aggregated over several pixels. In this paper, we
propose a technique that does both, by providing rigorous statistical error
control on the aggregate objects themselves rather than the pixels. We
demonstrate the effectiveness of this approach on data from the Chandra X-ray
Observatory Satellite. Our technique effectively controls the rate of false
sources, yet still detects almost all of the sources detected by procedures
that do not have such rigorous error control and have the advantage of
additional data in the form of follow up observations, which will not be
available for upcoming large telescopes. In fact, we even detect a new source
that was missed by previous studies. The statistical methods developed in this
paper can be extended to problems beyond Astronomy, as we will illustrate with
an example from Neuroimaging.
"
"  Since their emergence in the 1990's, the support vector machine and the
AdaBoost algorithm have spawned a wave of research in statistical machine
learning. Much of this new research falls into one of two broad categories:
kernel methods and ensemble methods. In this expository article, I discuss the
main ideas behind these two types of methods, namely how to transform linear
algorithms into nonlinear ones by using kernel functions, and how to make
predictions with an ensemble or a collection of models rather than a single
model. I also share my personal perspectives on how these ideas have influenced
and shaped my own research. In particular, I present two recent algorithms that
I have invented with my collaborators: LAGO, a fast kernel algorithm for
unbalanced classification and rare target detection; and Darwinian evolution in
parallel universes, an ensemble method for variable selection.
"
"  Hodge theory is a beautiful synthesis of geometry, topology, and analysis,
which has been developed in the setting of Riemannian manifolds. On the other
hand, spaces of images, which are important in the mathematical foundations of
vision and pattern recognition, do not fit this framework. This motivates us to
develop a version of Hodge theory on metric spaces with a probability measure.
We believe that this constitutes a step towards understanding the geometry of
vision.
  The appendix by Anthony Baker provides a separable, compact metric space with
infinite dimensional \alpha-scale homology.
"
"  Wang, Li and Konig have recently compared the cluster-theoretic properties of
bi-stochasticized symmetric data similarity (e. g. kernel) matrices, produced
by minimizing two different forms of Bregman divergences. We extend their
investigation to non-symmetric matrices, specifically studying the 1995-2000 U.
S. 3,107 x 3,107 intercounty migration matrix. A particular bi-stochastized
form of it had been obtained (arXiv:1207.0437), using the well-established
Sinkhorn-Knopp (SK) (biproportional) algorithm--which minimizes the
Kullback-Leibler form of the divergence. This matrix has but a single entry
equal to (the maximal possible value of) 1. Highly contrastingly, the
bi-stochastic matrix obtained here, implementing the Wang-Li-Konig-algorithm
for the minimum of the alternative, squared-norm form of the divergence, has
2,707 such unit entries. The corresponding 3,107-vertex, 2,707-link directed
graph has 2,352 strong components. These consist of 1,659 single/isolated
counties, 654 doublets (thirty-one interstate in nature), 22 triplets (one
being interstate), 13 quartets (one being interstate), three quintets and one
septet. Not manifest in these graph-theoretic results, however, are the
five-county states of Hawaii and Rhode Island and the eight-county state of
Connecticut. These--among other regional configurations--appealingly emerged as
well-defined entities in the SK-based strong-component hierarchical clustering.
"
"  Discovering causal relations among observed variables in a given data set is
a main topic in studies of statistics and artificial intelligence. Recently,
some techniques to discover an identifiable causal structure have been explored
based on non-Gaussianity of the observed data distribution. However, most of
these are limited to continuous data. In this paper, we present a novel causal
model for binary data and propose a new approach to derive an identifiable
causal structure governing the data based on skew Bernoulli distributions of
external noise. Experimental evaluation shows excellent performance for both
artificial and real world data sets.
"
"  Most classification methods are based on the assumption that data conforms to
a stationary distribution. The machine learning domain currently suffers from a
lack of classification techniques that are able to detect the occurrence of a
change in the underlying data distribution. Ignoring possible changes in the
underlying concept, also known as concept drift, may degrade the performance of
the classification model. Often these changes make the model inconsistent and
regular updatings become necessary. Taking the temporal dimension into account
during the analysis of Web usage data is a necessity, since the way a site is
visited may indeed evolve due to modifications in the structure and content of
the site, or even due to changes in the behavior of certain user groups. One
solution to this problem, proposed in this article, is to update models using
summaries obtained by means of an evolutionary approach based on an intelligent
clustering approach. We carry out various clustering strategies that are
applied on time sub-periods. To validate our approach we apply two external
evaluation criteria which compare different partitions from the same data set.
Our experiments show that the proposed approach is efficient to detect the
occurrence of changes.
"
"  We propose a novel algebraic framework for treating probability distributions
represented by their cumulants such as the mean and covariance matrix. As an
example, we consider the unsupervised learning problem of finding the subspace
on which several probability distributions agree. Instead of minimizing an
objective function involving the estimated cumulants, we show that by treating
the cumulants as elements of the polynomial ring we can directly solve the
problem, at a lower computational cost and with higher accuracy. Moreover, the
algebraic viewpoint on probability distributions allows us to invoke the theory
of Algebraic Geometry, which we demonstrate in a compact proof for an
identifiability criterion.
"
"  In many networks, vertices have hidden attributes, or types, that are
correlated with the networks topology. If the topology is known but these
attributes are not, and if learning the attributes is costly, we need a method
for choosing which vertex to query in order to learn as much as possible about
the attributes of the other vertices. We assume the network is generated by a
stochastic block model, but we make no assumptions about its assortativity or
disassortativity. We choose which vertex to query using two methods: 1)
maximizing the mutual information between its attributes and those of the
others (a well-known approach in active learning) and 2) maximizing the average
agreement between two independent samples of the conditional Gibbs
distribution. Experimental results show that both these methods do much better
than simple heuristics. They also consistently identify certain vertices as
important by querying them early on.
"
"  Nonparametric methods are widely applicable to statistical inference
problems, since they rely on a few modeling assumptions. In this context, the
fresh look advocated here permeates benefits from variable selection and
compressive sampling, to robustify nonparametric regression against outliers -
that is, data markedly deviating from the postulated models. A variational
counterpart to least-trimmed squares regression is shown closely related to an
L0-(pseudo)norm-regularized estimator, that encourages sparsity in a vector
explicitly modeling the outliers. This connection suggests efficient solvers
based on convex relaxation, which lead naturally to a variational M-type
estimator equivalent to the least-absolute shrinkage and selection operator
(Lasso). Outliers are identified by judiciously tuning regularization
parameters, which amounts to controlling the sparsity of the outlier vector
along the whole robustification path of Lasso solutions. Reduced bias and
enhanced generalization capability are attractive features of an improved
estimator obtained after replacing the L0-(pseudo)norm with a nonconvex
surrogate. The novel robust spline-based smoother is adopted to cleanse load
curve data, a key task aiding operational decisions in the envisioned smart
grid system. Computer simulations and tests on real load curve data corroborate
the effectiveness of the novel sparsity-controlling robust estimators.
"
"  We investigate the problem of modeling symbolic sequences of polyphonic music
in a completely general piano-roll representation. We introduce a probabilistic
model based on distribution estimators conditioned on a recurrent neural
network that is able to discover temporal dependencies in high-dimensional
sequences. Our approach outperforms many traditional models of polyphonic music
on a variety of realistic datasets. We show how our musical language model can
serve as a symbolic prior to improve the accuracy of polyphonic transcription.
"
"  Much work has been done refining and characterizing the receptive fields
learned by deep learning algorithms. A lot of this work has focused on the
development of Gabor-like filters learned when enforcing sparsity constraints
on a natural image dataset. Little work however has investigated how these
filters might expand to the temporal domain, namely through training on natural
movies. Here we investigate exactly this problem in established temporal deep
learning algorithms as well as a new learning paradigm suggested here, the
Temporal Autoencoding Restricted Boltzmann Machine (TARBM).
"
"  In recent years, total variation (TV) and Euler's elastica (EE) have been
successfully applied to image processing tasks such as denoising and
inpainting. This paper investigates how to extend TV and EE to the supervised
learning settings on high dimensional data. The supervised learning problem can
be formulated as an energy functional minimization under Tikhonov
regularization scheme, where the energy is composed of a squared loss and a
total variation smoothing (or Euler's elastica smoothing). Its solution via
variational principles leads to an Euler-Lagrange PDE. However, the PDE is
always high-dimensional and cannot be directly solved by common methods.
Instead, radial basis functions are utilized to approximate the target
function, reducing the problem to finding the linear coefficients of basis
functions. We apply the proposed methods to supervised learning tasks
(including binary classification, multi-class classification, and regression)
on benchmark data sets. Extensive experiments have demonstrated promising
results of the proposed methods.
"
"  We propose a generalized extreme shock model with a possibly increasing
failure threshold. While standard models assume that the crucial threshold for
the system may only decrease over time, because of weakening shocks and
obsolescence, we assume that, especially at the beginning of the system's life,
some strengthening shocks may increase the system tolerance to large shock.
This is for example the case of turbines' running-in in the field of
engineering. On the basis of parametric assumptions, we provide theoretical
results and derive some exact and asymptotic univariate and multivariate
distributions for the model. In the last part of the paper we show how to link
this new model to some nonparametric approaches proposed in the literature.
"
"  A critically challenging problem facing statisticians is the identification
of a suitable framework which consolidates data of various types, from
different sources, and across different time frames or scales (many of which
can be missing), and from which appropriate analysis and subsequent inference
can proceed.
"
"  We show how an interactive graph visualization method based on maximal
modularity clustering can be used to explore a large epidemic network. The
visual representation is used to display statistical tests results that expose
the relations between the propagation of HIV in a sexual contact network and
the sexual orientation of the patients.
"
"  The notion that natural disasters can be controlled is, of course, farcical;
history is permeated with examples of countless failed attempts at this
pointless task; it is synonymous with trying to build a perpetual motion
machine. Nonetheless, there are ways to reduce their impact on human
communities, particularly by looking away from the normal hypothesis.
"
"  This article reformulates a common illness-death model in terms of a new
system of stochastical differential equations (SDEs). The SDEs are used to
estimate epidemiological characteristics and burden of systemic lupus
erythematosus in England and Wales in 1995.
"
"  In order to efficiently study the characteristics of network domains and
support development of network systems (e.g. algorithms, protocols that operate
on networks), it is often necessary to sample a representative subgraph from a
large complex network. Although recent subgraph sampling methods have been
shown to work well, they focus on sampling from memory-resident graphs and
assume that the sampling algorithm can access the entire graph in order to
decide which nodes/edges to select. Many large-scale network datasets, however,
are too large and/or dynamic to be processed using main memory (e.g., email,
tweets, wall posts). In this work, we formulate the problem of sampling from
large graph streams. We propose a streaming graph sampling algorithm that
dynamically maintains a representative sample in a reservoir based setting. We
evaluate the efficacy of our proposed methods empirically using several
real-world data sets. Across all datasets, we found that our method produce
samples that preserve better the original graph distributions.
"
"  Ancestral graph models, introduced by Richardson and Spirtes (2002),
generalize both Markov random fields and Bayesian networks to a class of graphs
with a global Markov property that is closed under conditioning and
marginalization. By design, ancestral graphs encode precisely the conditional
independence structures that can arise from Bayesian networks with selection
and unobserved (hidden/latent) variables. Thus, ancestral graph models provide
a potentially very useful framework for exploratory model selection when
unobserved variables might be involved in the data-generating process but no
particular hidden structure can be specified. In this paper, we present the
Iterative Conditional Fitting (ICF) algorithm for maximum likelihood estimation
in Gaussian ancestral graph models. The name reflects that in each step of the
procedure a conditional distribution is estimated, subject to constraints,
while a marginal distribution is held fixed. This approach is in duality to the
well-known Iterative Proportional Fitting algorithm, in which marginal
distributions are fitted while conditional distributions are held fixed.
"
"  In information retrieval, a fundamental goal is to transform a document into
concepts that are representative of its content. The term ""representative"" is
in itself challenging to define, and various tasks require different
granularities of concepts. In this paper, we aim to model concepts that are
sparse over the vocabulary, and that flexibly adapt their content based on
other relevant semantic information such as textual structure or associated
image features. We explore a Bayesian nonparametric model based on nested beta
processes that allows for inferring an unknown number of strictly sparse
concepts. The resulting model provides an inherently different representation
of concepts than a standard LDA (or HDP) based topic model, and allows for
direct incorporation of semantic features. We demonstrate the utility of this
representation on multilingual blog data and the Congressional Record.
"
"  The hidden Markov model (HMM) is a generative model that treats sequential
data under the assumption that each observation is conditioned on the state of
a discrete hidden variable that evolves in time as a Markov chain. In this
paper, we derive a novel algorithm to cluster HMMs through their probability
distributions. We propose a hierarchical EM algorithm that i) clusters a given
collection of HMMs into groups of HMMs that are similar, in terms of the
distributions they represent, and ii) characterizes each group by a ""cluster
center"", i.e., a novel HMM that is representative for the group. We present
several empirical studies that illustrate the benefits of the proposed
algorithm.
"
"  Monte-Carlo Tree Search (MCTS) methods are drawing great interest after
yielding breakthrough results in computer Go. This paper proposes a Bayesian
approach to MCTS that is inspired by distributionfree approaches such as UCT
[13], yet significantly differs in important respects. The Bayesian framework
allows potentially much more accurate (Bayes-optimal) estimation of node values
and node uncertainties from a limited number of simulation trials. We further
propose propagating inference in the tree via fast analytic Gaussian
approximation methods: this can make the overhead of Bayesian inference
manageable in domains such as Go, while preserving high accuracy of
expected-value estimates. We find substantial empirical outperformance of UCT
in an idealized bandit-tree test environment, where we can obtain valuable
insights by comparing with known ground truth. Additionally we rigorously prove
on-policy and off-policy convergence of the proposed methods.
"
"  Computing the partition function and the marginals of a global probability
distribution are two important issues in any probabilistic inference problem.
In a previous work, we presented sub-tree based upper and lower bounds on the
partition function of a given probabilistic inference problem. Using the
entropies of the sub-trees we proved an inequality that compares the lower
bounds obtained from different sub-trees. In this paper we investigate the
properties of one specific lower bound, namely the lower bound computed by the
minimum entropy sub-tree. We also investigate the relationship between the
minimum entropy sub-tree and the sub-tree that gives the best lower bound.
"
"  Much of scientific data is collected as randomized experiments intervening on
some and observing other variables of interest. Quite often, a given phenomenon
is investigated in several studies, and different sets of variables are
involved in each study. In this article we consider the problem of integrating
such knowledge, inferring as much as possible concerning the underlying causal
structure with respect to the union of observed variables from such
experimental or passive observational overlapping data sets. We do not assume
acyclicity or joint causal sufficiency of the underlying data generating model,
but we do restrict the causal relationships to be linear and use only second
order statistics of the data. We derive conditions for full model
identifiability in the most generic case, and provide novel techniques for
incorporating an assumption of faithfulness to aid in inference. In each case
we seek to establish what is and what is not determined by the data at hand.
"
"  In this work we focus on examination and comparison of whole-brain functional
connectivity patterns measured with fMRI across experimental conditions. Direct
examination and comparison of condition-specific matrices is challenging due to
the large number of elements in a connectivity matrix. We present a framework
that uses network analysis to describe condition-specific functional
connectivity. Treating the brain as a complex system in terms of a network, we
extract the most relevant connectivity information by partitioning each network
into clusters representing functionally connected brain regions. Extracted
clusters are used as features for predicting experimental condition in a new
data set. The approach is illustrated on fMRI data examining functional
connectivity patterns during processing of abstract and concrete concepts.
Topological (brain regions) and functional (level of connectivity and
information flow) systematic differences in the ROI-based functional networks
were identified across participants for concrete and abstract concepts. These
differences were sufficient for classification of previously unseen
connectivity matrices as abstract or concrete based on training data derived
from other people.
"
"  Models of bags of words typically assume topic mixing so that the words in a
single bag come from a limited number of topics. We show here that many sets of
bag of words exhibit a very different pattern of variation than the patterns
that are efficiently captured by topic mixing. In many cases, from one bag of
words to the next, the words disappear and new ones appear as if the theme
slowly and smoothly shifted across documents (providing that the documents are
somehow ordered). Examples of latent structure that describe such ordering are
easily imagined. For example, the advancement of the date of the news stories
is reflected in a smooth change over the theme of the day as certain evolving
news stories fall out of favor and new events create new stories. Overlaps
among the stories of consecutive days can be modeled by using windows over
linearly arranged tight distributions over words. We show here that such
strategy can be extended to multiple dimensions and cases where the ordering of
data is not readily obvious. We demonstrate that this way of modeling
covariation in word occurrences outperforms standard topic models in
classification and prediction tasks in applications in biology, text modeling
and computer vision.
"
"  Cis-regulatory modules (CRMs) composed of multiple transcription factor
binding sites (TFBSs) control gene expression in eukaryotic genomes.
Comparative genomic studies have shown that these regulatory elements are more
conserved across species due to evolutionary constraints. We propose a
statistical method to combine module structure and cross-species orthology in
de novo motif discovery. We use a hidden Markov model (HMM) to capture the
module structure in each species and couple these HMMs through multiple-species
alignment. Evolutionary models are incorporated to consider correlated
structures among aligned sequence positions across different species. Based on
our model, we develop a Markov chain Monte Carlo approach, MultiModule, to
discover CRMs and their component motifs simultaneously in groups of
orthologous sequences from multiple species. Our method is tested on both
simulated and biological data sets in mammals and Drosophila, where significant
improvement over other motif and module discovery methods is observed.
"
"  This paper presents an unsupervised algorithm for nonlinear unmixing of
hyperspectral images. The proposed model assumes that the pixel reflectances
result from a nonlinear function of the abundance vectors associated with the
pure spectral components. We assume that the spectral signatures of the pure
components and the nonlinear function are unknown. The first step of the
proposed method consists of the Bayesian estimation of the abundance vectors
for all the image pixels and the nonlinear function relating the abundance
vectors to the observations. The endmembers are subsequently estimated using
Gaussian process regression. The performance of the unmixing strategy is
evaluated with simulations conducted on synthetic and real data.
"
"  Novel dose-finding designs, using estimation to assign the best estimated
maximum- tolerated-dose (MTD) at each point in the experiment, most commonly
via Bayesian techniques, have recently entered large-scale implementation in
Phase I cancer clinical trials. We examine the small-sample behavior of these
""Bayesian Phase I"" (BP1) designs, and also of non-Bayesian designs sharing the
same main ""long-memory"" traits (hereafter: LMP1s).
  For all LMP1s examined, the number of cohorts treated at the true MTD
(denoted here as n*) was highly variable between numerical runs drawn from the
same toxicity-threshold distribution, especially when compared with
""up-and-down"" (U&D) short-memory designs. Further investigation using the same
set of thresholds in permuted order, produced a nearly-identical magnitude of
variability in n*. Therefore, this LMP1 behavior is driven by a strong
sensitivity to the order in which toxicity thresholds appear in the experiment.
We suggest that the sensitivity is related to LMP1's tendency to ""settle"" early
on a specific dose level - a tendency caused by the repeated likelihood-based
""winner-takes-all"" dose assignment rule, which grants the early cohorts a
disproportionately large influence upon experimental trajectories.
  Presently, U&D designs offer a simpler and more stable alternative, with
roughly equivalent MTD estimation performance. A promising direction for
combining the two approaches is briefly discussed (note: the '3+3' protocol is
not a U&D design).
"
"  This paper studies the sample complexity of searching over multiple
populations. We consider a large number of populations, each corresponding to
either distribution P0 or P1. The goal of the search problem studied here is to
find one population corresponding to distribution P1 with as few samples as
possible. The main contribution is to quantify the number of samples needed to
correctly find one such population. We consider two general approaches:
non-adaptive sampling methods, which sample each population a predetermined
number of times until a population following P1 is found, and adaptive sampling
methods, which employ sequential sampling schemes for each population. We first
derive a lower bound on the number of samples required by any sampling scheme.
We then consider an adaptive procedure consisting of a series of sequential
probability ratio tests, and show it comes within a constant factor of the
lower bound. We give explicit expressions for this constant when samples of the
populations follow Gaussian and Bernoulli distributions. An alternative
adaptive scheme is discussed which does not require full knowledge of P1, and
comes within a constant factor of the optimal scheme. For comparison, a lower
bound on the sampling requirements of any non-adaptive scheme is presented.
"
"  Prediction markets are used in real life to predict outcomes of interest such
as presidential elections. This paper presents a mathematical theory of
artificial prediction markets for supervised learning of conditional
probability estimators. The artificial prediction market is a novel method for
fusing the prediction information of features or trained classifiers, where the
fusion result is the contract price on the possible outcomes. The market can be
trained online by updating the participants' budgets using training examples.
Inspired by the real prediction markets, the equations that govern the market
are derived from simple and reasonable assumptions. Efficient numerical
algorithms are presented for solving these equations. The obtained artificial
prediction market is shown to be a maximum likelihood estimator. It generalizes
linear aggregation, existent in boosting and random forest, as well as logistic
regression and some kernel methods. Furthermore, the market mechanism allows
the aggregation of specialized classifiers that participate only on specific
instances. Experimental comparisons show that the artificial prediction markets
often outperform random forest and implicit online learning on synthetic data
and real UCI datasets. Moreover, an extensive evaluation for pelvic and
abdominal lymph node detection in CT data shows that the prediction market
improves adaboost's detection rate from 79.6% to 81.2% at 3 false
positives/volume.
"
"  Given a graph where vertices represent alternatives and arcs represent
pairwise comparison data, the statistical ranking problem is to find a
potential function, defined on the vertices, such that the gradient of the
potential function agrees with the pairwise comparisons. Our goal in this paper
is to develop a method for collecting data for which the least squares
estimator for the ranking problem has maximal Fisher information. Our approach,
based on experimental design, is to view data collection as a bi-level
optimization problem where the inner problem is the ranking problem and the
outer problem is to identify data which maximizes the informativeness of the
ranking. Under certain assumptions, the data collection problem decouples,
reducing to a problem of finding multigraphs with large algebraic connectivity.
This reduction of the data collection problem to graph-theoretic questions is
one of the primary contributions of this work. As an application, we study the
Yahoo! Movie user rating dataset and demonstrate that the addition of a small
number of well-chosen pairwise comparisons can significantly increase the
Fisher informativeness of the ranking. As another application, we study the
2011-12 NCAA football schedule and propose schedules with the same number of
games which are significantly more informative. Using spectral clustering
methods to identify highly-connected communities within the division, we argue
that the NCAA could improve its notoriously poor rankings by simply scheduling
more out-of-conference games.
"
"  In regression analysis of counts, a lack of simple and efficient algorithms
for posterior computation has made Bayesian approaches appear unattractive and
thus underdeveloped. We propose a lognormal and gamma mixed negative binomial
(NB) regression model for counts, and present efficient closed-form Bayesian
inference; unlike conventional Poisson models, the proposed approach has two
free parameters to include two different kinds of random effects, and allows
the incorporation of prior information, such as sparsity in the regression
coefficients. By placing a gamma distribution prior on the NB dispersion
parameter r, and connecting a lognormal distribution prior with the logit of
the NB probability parameter p, efficient Gibbs sampling and variational Bayes
inference are both developed. The closed-form updates are obtained by
exploiting conditional conjugacy via both a compound Poisson representation and
a Polya-Gamma distribution based data augmentation approach. The proposed
Bayesian inference can be implemented routinely, while being easily
generalizable to more complex settings involving multivariate dependence
structures. The algorithms are illustrated using real examples.
"
"  With reference to the questionnaire adopted within the Italian project
""Ulisse"" to assess health condition of elderly people, we investigate two
important issues: discriminant power and actual number of dimensions measured
by the items composing the questionnaire. The adopted statistical approach is
based on the joint use of the latent class model and a multidimensional item
response theory model based on the 2PL parametrization. The latter allows us to
account for the different discriminant power of these items. The analysis is
based on the data collected on a sample of 1699 elderly people hosted in 37
nursing homes in Italy. This analysis shows that the selected items indeed
measure a different number of dimensions of the health status and that they
considerably differ in terms of discriminant power (effectiveness in measuring
the actual health status). Implications for the assessment of the performance
of nursing homes from a policy-maker prospective are discussed.
"
"  With the possible exception of gambling, meteorology, particularly
precipitation forecasting, may be the area with which the general public is
most familiar with probabilistic assessments of uncertainty. Despite the heavy
use of stochastic models and statistical methods in weather forecasting and
other areas of the atmospheric sciences, papers in these areas have
traditionally been somewhat uncommon in statistics journals. We see signs of
this changing in recent years and we have sought to highlight some present
research directions at the interface of statistics and the atmospheric sciences
in this special section.
"
"  The difference equations $\xi_{k}=af(\xi_{k-1})+\epsilon_{k}$, where
$(\epsilon_k)$ is a square integrable difference martingale, and the
differential equation ${\rm d}\xi=-af(\xi){\rm d}t+{\rm d}\eta$, where $\eta$
is a square integrable martingale, are considered. A family of estimators
depending, besides the sample size $n$ (or the observation period, if time is
continuous) on some random Lipschitz functions is constructed. Asymptotic
optimality of this estimators is investigated.
"
"  We develop mask iterative hard thresholding algorithms (mask IHT and mask
DORE) for sparse image reconstruction of objects with known contour. The
measurements follow a noisy underdetermined linear model common in the
compressive sampling literature. Assuming that the contour of the object that
we wish to reconstruct is known and that the signal outside the contour is
zero, we formulate a constrained residual squared error minimization problem
that incorporates both the geometric information (i.e. the knowledge of the
object's contour) and the signal sparsity constraint. We first introduce a mask
IHT method that aims at solving this minimization problem and guarantees
monotonically non-increasing residual squared error for a given signal sparsity
level. We then propose a double overrelaxation scheme for accelerating the
convergence of the mask IHT algorithm. We also apply convex mask reconstruction
approaches that employ a convex relaxation of the signal sparsity constraint.
In X-ray computed tomography (CT), we propose an automatic scheme for
extracting the convex hull of the inspected object from the measured sinograms;
the obtained convex hull is used to capture the object contour information. We
compare the proposed mask reconstruction schemes with the existing large-scale
sparse signal reconstruction methods via numerical simulations and demonstrate
that, by exploiting both the geometric contour information of the underlying
image and sparsity of its wavelet coefficients, we can reconstruct this image
using a significantly smaller number of measurements than the existing methods.
"
"  The goal of phylodynamics, an area on the intersection of phylogenetics and
population genetics, is to reconstruct population size dynamics from genetic
data. Recently, a series of nonparametric Bayesian methods have been proposed
for such demographic reconstructions. These methods rely on prior
specifications based on Gaussian processes and proceed by approximating the
posterior distribution of population size trajectories via Markov chain Monte
Carlo (MCMC) methods. In this paper, we adapt an integrated nested Laplace
approximation (INLA), a recently proposed approximate Bayesian inference for
latent Gaussian models, to the estimation of population size trajectories. We
show that when a genealogy of sampled individuals can be reliably estimated
from genetic data, INLA enjoys high accuracy and can replace MCMC entirely. We
demonstrate significant computational efficiency over the state-of-the-art MCMC
methods. We illustrate INLA-based population size inference using simulations
and genealogies of hepatitis C and human influenza viruses.
"
"  We propose a new optimization algorithm for Multiple Kernel Learning (MKL)
called SpicyMKL, which is applicable to general convex loss functions and
general types of regularization. The proposed SpicyMKL iteratively solves
smooth minimization problems. Thus, there is no need of solving SVM, LP, or QP
internally. SpicyMKL can be viewed as a proximal minimization method and
converges super-linearly. The cost of inner minimization is roughly
proportional to the number of active kernels. Therefore, when we aim for a
sparse kernel combination, our algorithm scales well against increasing number
of kernels. Moreover, we give a general block-norm formulation of MKL that
includes non-sparse regularizations, such as elastic-net and \ellp -norm
regularizations. Extending SpicyMKL, we propose an efficient optimization
method for the general regularization framework. Experimental results show that
our algorithm is faster than existing methods especially when the number of
kernels is large (> 1000).
"
"  We generalize the orthonormal basis for the Gaussian RKHS described in
\cite{MinhGaussian2010} to an infinite, continuously parametrized, family of
orthonormal bases, along with some implications. The proofs are direct
generalizations of those in \cite{MinhGaussian2010}.
"
"  An energy efficient distributed Change Detection scheme based on Page's CUSUM
algorithm was presented in \cite{icassp}. In this paper we consider a
nonparametric version of this algorithm. In the algorithm in \cite{icassp},
each sensor runs CUSUM and transmits only when the CUSUM is above some
threshold. The transmissions from the sensors are fused at the physical layer.
The channel is modeled as a Multiple Access Channel (MAC) corrupted with noise.
The fusion center performs another CUSUM to detect the change. In this paper,
we generalize the algorithm to also include nonparametric CUSUM and provide a
unified analysis.
"
"  X in R^D has mean zero and finite second moments. We show that there is a
precise sense in which almost all linear projections of X into R^d (for d < D)
look like a scale-mixture of spherical Gaussians -- specifically, a mixture of
distributions N(0, sigma^2 I_d) where the weight of the particular sigma
component is P (| X |^2 = sigma^2 D). The extent of this effect depends upon
the ratio of d to D, and upon a particular coefficient of eccentricity of X's
distribution. We explore this result in a variety of experiments.
"
"  We introduce a new Bayesian model for hierarchical clustering based on a
prior over trees called Kingman's coalescent. We develop novel greedy and
sequential Monte Carlo inferences which operate in a bottom-up agglomerative
fashion. We show experimentally the superiority of our algorithms over others,
and demonstrate our approach in document clustering and phylolinguistics.
"
"  In this article, the problem of semi-parametric inference on the parameters
of a multidimensional L\'{e}vy process $L_t$ with independent components based
on the low-frequency observations of the corresponding time-changed L\'{e}vy
process $L_{\mathcal{T}(t)}$, where $\mathcal{T}$ is a nonnegative,
nondecreasing real-valued process independent of $L_t$, is studied. We show
that this problem is closely related to the problem of composite function
estimation that has recently gotten much attention in statistical literature.
Under suitable identifiability conditions, we propose a consistent estimate for
the L\'{e}vy density of $L_t$ and derive the uniform as well as the pointwise
convergence rates of the estimate proposed. Moreover, we prove that the rates
obtained are optimal in a minimax sense over suitable classes of time-changed
L\'{e}vy models. Finally, we present a simulation study showing the performance
of our estimation algorithm in the case of time-changed Normal Inverse Gaussian
(NIG) L\'{e}vy processes.
"
"  We consider a stochastic evolutionary model for a phenotype developing
amongst n related species with unknown phylogeny. The unknown tree is modelled
by a Yule process conditioned on n contemporary nodes. The trait value is
assumed to evolve along lineages as an Ornstein-Uhlenbeck process. As a result,
the trait values of the n species form a sample with dependent observations. We
establish three limit theorems for the sample mean corresponding to three
domains for the adaptation rate. In the case of fast adaptation, we show that
for large $n$ the normalized sample mean is approximately normally distributed.
Using these limit theorems, we develop novel confidence interval formulae for
the optimal trait value.
"
"  We present an applied study in cancer genomics for integrating data and
inferences from laboratory experiments on cancer cell lines with observational
data obtained from human breast cancer studies. The biological focus is on
improving understanding of transcriptional responses of tumors to changes in
the pH level of the cellular microenvironment. The statistical focus is on
connecting experimentally defined biomarkers of such responses to clinical
outcome in observational studies of breast cancer patients. Our analysis
exemplifies a general strategy for accomplishing this kind of integration
across contexts. The statistical methodologies employed here draw heavily on
Bayesian sparse factor models for identifying, modularizing and correlating
with clinical outcome these signatures of aggregate changes in gene expression.
By projecting patterns of biological response linked to specific experimental
interventions into observational studies where such responses may be evidenced
via variation in gene expression across samples, we are able to define
biomarkers of clinically relevant physiological states and outcomes that are
rooted in the biology of the original experiment. Through this approach we
identify microenvironment-related prognostic factors capable of predicting long
term survival in two independent breast cancer datasets. These results suggest
possible directions for future laboratory studies, as well as indicate the
potential for therapeutic advances though targeted disruption of specific
pathway components.
"
"  Recent research has made significant progress on the problem of bounding log
partition functions for exponential family graphical models. Such bounds have
associated dual parameters that are often used as heuristic estimates of the
marginal probabilities required in inference and learning. However these
variational estimates do not give rigorous bounds on marginal probabilities,
nor do they give estimates for probabilities of more general events than simple
marginals. In this paper we build on this recent work by deriving rigorous
upper and lower bounds on event probabilities for graphical models. Our
approach is based on the use of generalized Chernoff bounds to express bounds
on event probabilities in terms of convex optimization problems; these
optimization problems, in turn, require estimates of generalized log partition
functions. Simulations indicate that this technique can result in useful,
rigorous bounds to complement the heuristic variational estimates, with
comparable computational cost.
"
"  The focus of this paper is an approach to the modeling of longitudinal social
network or relational data. Such data arise from measurements on pairs of
objects or actors made at regular temporal intervals, resulting in a social
network for each point in time. In this article we represent the network and
temporal dependencies with a random effects model, resulting in a stochastic
process defined by a set of stationary covariance matrices. Our approach builds
upon the social relations models of Warner, Kenny and Stoto [Journal of
Personality and Social Psychology 37 (1979) 1742--1757] and Gill and Swartz
[Canad. J. Statist. 29 (2001) 321--331] and allows for an intra- and
inter-temporal representation of network structures. We apply the methodology
to two longitudinal data sets: international trade (continuous response) and
militarized interstate disputes (binary response).
"
"  This paper deals with chain graphs under the classic
Lauritzen-Wermuth-Frydenberg interpretation. We prove that the regular Gaussian
distributions that factorize with respect to a chain graph $G$ with $d$
parameters have positive Lebesgue measure with respect to $\mathbb{R}^d$,
whereas those that factorize with respect to $G$ but are not faithful to it
have zero Lebesgue measure with respect to $\mathbb{R}^d$. This means that, in
the measure-theoretic sense described, almost all the regular Gaussian
distributions that factorize with respect to $G$ are faithful to it.
"
"  We present a weighted-Lasso method to infer the parameters of a first-order
vector auto-regressive model that describes time course expression data
generated by directed gene-to-gene regulation networks. These networks are
assumed to own a prior internal structure of connectivity which drives the
inference method. This prior structure can be either derived from prior
biological knowledge or inferred by the method itself. We illustrate the
performance of this structure-based penalization both on synthetic data and on
two canonical regulatory networks, first yeast cell cycle regulation network by
analyzing Spellman et al's dataset and second E. coli S.O.S. DNA repair network
by analysing U. Alon's lab data.
"
"  Genetic association analyses often involve data from multiple
potentially-heterogeneous subgroups. The expected amount of heterogeneity can
vary from modest (e.g., a typical meta-analysis) to large (e.g., a strong
gene--environment interaction). However, existing statistical tools are limited
in their ability to address such heterogeneity. Indeed, most genetic
association meta-analyses use a ""fixed effects"" analysis, which assumes no
heterogeneity. Here we develop and apply Bayesian association methods to
address this problem. These methods are easy to apply (in the simplest case,
requiring only a point estimate for the genetic effect and its standard error,
from each subgroup) and effectively include standard frequentist meta-analysis
methods, including the usual ""fixed effects"" analysis, as special cases. We
apply these tools to two large genetic association studies: one a meta-analysis
of genome-wide association studies from the Global Lipids consortium, and the
second a cross-population analysis for expression quantitative trait loci
(eQTLs). In the Global Lipids data we find, perhaps surprisingly, that effects
are generally quite homogeneous across studies. In the eQTL study we find that
eQTLs are generally shared among different continental groups, and discuss
consequences of this for study design.
"
"  In this paper, we study statistical properties of semi-supervised learning,
which is considered as an important problem in the community of machine
learning. In the standard supervised learning, only the labeled data is
observed. The classification and regression problems are formalized as the
supervised learning. In semi-supervised learning, unlabeled data is also
obtained in addition to labeled data. Hence, exploiting unlabeled data is
important to improve the prediction accuracy in semi-supervised learning. This
problems is regarded as a semiparametric estimation problem with missing data.
Under the the discriminative probabilistic models, it had been considered that
the unlabeled data is useless to improve the estimation accuracy. Recently, it
was revealed that the weighted estimator using the unlabeled data achieves
better prediction accuracy in comparison to the learning method using only
labeled data, especially when the discriminative probabilistic model is
misspecified. That is, the improvement under the semiparametric model with
missing data is possible, when the semiparametric model is misspecified. In
this paper, we apply the density-ratio estimator to obtain the weight function
in the semi-supervised learning. The benefit of our approach is that the
proposed estimator does not require well-specified probabilistic models for the
probability of the unlabeled data. Based on the statistical asymptotic theory,
we prove that the estimation accuracy of our method outperforms the supervised
learning using only labeled data. Some numerical experiments present the
usefulness of our methods.
"
"  We discuss the connection between information and copula theories by showing
that a copula can be employed to decompose the information content of a
multivariate distribution into marginal and dependence components, with the
latter quantified by the mutual information. We define the information excess
as a measure of deviation from a maximum entropy distribution. The idea of
marginal invariant dependence measures is also discussed and used to show that
empirical linear correlation underestimates the amplitude of the actual
correlation in the case of non-Gaussian marginals. The mutual information is
shown to provide an upper bound for the asymptotic empirical log-likelihood of
a copula. An analytical expression for the information excess of T-copulas is
provided, allowing for simple model identification within this family. We
illustrate the framework in a financial data set.
"
"  A tree-based dictionary learning model is developed for joint analysis of
imagery and associated text. The dictionary learning may be applied directly to
the imagery from patches, or to general feature vectors extracted from patches
or superpixels (using any existing method for image feature extraction). Each
image is associated with a path through the tree (from root to a leaf), and
each of the multiple patches in a given image is associated with one node in
that path. Nodes near the tree root are shared between multiple paths,
representing image characteristics that are common among different types of
images. Moving toward the leaves, nodes become specialized, representing
details in image classes. If available, words (text) are also jointly modeled,
with a path-dependent probability over words. The tree structure is inferred
via a nested Dirichlet process, and a retrospective stick-breaking sampler is
used to infer the tree depth and width.
"
"  In some applications and in order to address real world situations better,
data may be more complex than simple vectors. In some examples, they can be
known through their pairwise dissimilarities only. Several variants of the Self
Organizing Map algorithm were introduced to generalize the original algorithm
to this framework. Whereas median SOM is based on a rough representation of the
prototypes, relational SOM allows representing these prototypes by a virtual
combination of all elements in the data set. However, this latter approach
suffers from two main drawbacks. First, its complexity can be large. Second,
only a batch version of this algorithm has been studied so far and it often
provides results having a bad topographic organization. In this article, an
on-line version of relational SOM is described and justified. The algorithm is
tested on several datasets, including categorical data and graphs, and compared
with the batch version and with other SOM algorithms for non vector data.
"
"  This paper is a follow up to the previous author's paper on convex
optimization. In that paper we began the process of adjusting greedy-type
algorithms from nonlinear approximation for finding sparse solutions of convex
optimization problems. We modified there three the most popular in nonlinear
approximation in Banach spaces greedy algorithms -- Weak Chebyshev Greedy
Algorithm, Weak Greedy Algorithm with Free Relaxation and Weak Relaxed Greedy
Algorithm -- for solving convex optimization problems. We continue to study
sparse approximate solutions to convex optimization problems. It is known that
in many engineering applications researchers are interested in an approximate
solution of an optimization problem as a linear combination of elements from a
given system of elements. There is an increasing interest in building such
sparse approximate solutions using different greedy-type algorithms. In this
paper we concentrate on greedy algorithms that provide expansions, which means
that the approximant at the $m$th iteration is equal to the sum of the
approximant from the previous iteration ($(m-1)$th iteration) and one element
from the dictionary with an appropriate coefficient. The problem of greedy
expansions of elements of a Banach space is well studied in nonlinear
approximation theory. At a first glance the setting of a problem of expansion
of a given element and the setting of the problem of expansion in an
optimization problem are very different. However, it turns out that the same
technique can be used for solving both problems. We show how the technique
developed in nonlinear approximation theory, in particular, the greedy
expansions technique can be adjusted for finding a sparse solution of an
optimization problem given by an expansion with respect to a given dictionary.
"
"  This paper examines the problem of learning with a finite and possibly large
set of p base kernels. It presents a theoretical and empirical analysis of an
approach addressing this problem based on ensembles of kernel predictors. This
includes novel theoretical guarantees based on the Rademacher complexity of the
corresponding hypothesis sets, the introduction and analysis of a learning
algorithm based on these hypothesis sets, and a series of experiments using
ensembles of kernel predictors with several data sets. Both convex combinations
of kernel-based hypotheses and more general Lq-regularized nonnegative
combinations are analyzed. These theoretical, algorithmic, and empirical
results are compared with those achieved by using learning kernel techniques,
which can be viewed as another approach for solving the same problem.
"
"  We introduce a class of learning problems where the agent is presented with a
series of tasks. Intuitively, if there is relation among those tasks, then the
information gained during execution of one task has value for the execution of
another task. Consequently, the agent is intrinsically motivated to explore its
environment beyond the degree necessary to solve the current task it has at
hand. We develop a decision theoretic setting that generalises standard
reinforcement learning tasks and captures this intuition. More precisely, we
consider a multi-stage stochastic game between a learning agent and an
opponent. We posit that the setting is a good model for the problem of
life-long learning in uncertain environments, where while resources must be
spent learning about currently important tasks, there is also the need to
allocate effort towards learning about aspects of the world which are not
relevant at the moment. This is due to the fact that unpredictable future
events may lead to a change of priorities for the decision maker. Thus, in some
sense, the model ""explains"" the necessity of curiosity. Apart from introducing
the general formalism, the paper provides algorithms. These are evaluated
experimentally in some exemplary domains. In addition, performance bounds are
proven for some cases of this problem.
"
"  Crowdsourcing systems, in which numerous tasks are electronically distributed
to numerous ""information piece-workers"", have emerged as an effective paradigm
for human-powered solving of large scale problems in domains such as image
classification, data entry, optical character recognition, recommendation, and
proofreading. Because these low-paid workers can be unreliable, nearly all such
systems must devise schemes to increase confidence in their answers, typically
by assigning each task multiple times and combining the answers in an
appropriate manner, e.g. majority voting.
  In this paper, we consider a general model of such crowdsourcing tasks and
pose the problem of minimizing the total price (i.e., number of task
assignments) that must be paid to achieve a target overall reliability. We give
a new algorithm for deciding which tasks to assign to which workers and for
inferring correct answers from the workers' answers. We show that our
algorithm, inspired by belief propagation and low-rank matrix approximation,
significantly outperforms majority voting and, in fact, is optimal through
comparison to an oracle that knows the reliability of every worker. Further, we
compare our approach with a more general class of algorithms which can
dynamically assign tasks. By adaptively deciding which questions to ask to the
next arriving worker, one might hope to reduce uncertainty more efficiently. We
show that, perhaps surprisingly, the minimum price necessary to achieve a
target reliability scales in the same manner under both adaptive and
non-adaptive scenarios. Hence, our non-adaptive approach is order-optimal under
both scenarios. This strongly relies on the fact that workers are fleeting and
can not be exploited. Therefore, architecturally, our results suggest that
building a reliable worker-reputation system is essential to fully harnessing
the potential of adaptive designs.
"
"  We present a novel algebraic combinatorial view on low-rank matrix completion
based on studying relations between a few entries with tools from algebraic
geometry and matroid theory. The intrinsic locality of the approach allows for
the treatment of single entries in a closed theoretical and practical
framework. More specifically, apart from introducing an algebraic combinatorial
theory of low-rank matrix completion, we present probability-one algorithms to
decide whether a particular entry of the matrix can be completed. We also
describe methods to complete that entry from a few others, and to estimate the
error which is incurred by any method completing that entry. Furthermore, we
show how known results on matrix completion and their sampling assumptions can
be related to our new perspective and interpreted in terms of a completability
phase transition.
"
"  Data sets are often modeled as point clouds in $R^D$, for $D$ large. It is
often assumed that the data has some interesting low-dimensional structure, for
example that of a $d$-dimensional manifold $M$, with $d$ much smaller than $D$.
When $M$ is simply a linear subspace, one may exploit this assumption for
encoding efficiently the data by projecting onto a dictionary of $d$ vectors in
$R^D$ (for example found by SVD), at a cost $(n+D)d$ for $n$ data points. When
$M$ is nonlinear, there are no ""explicit"" constructions of dictionaries that
achieve a similar efficiency: typically one uses either random dictionaries, or
dictionaries obtained by black-box optimization. In this paper we construct
data-dependent multi-scale dictionaries that aim at efficient encoding and
manipulating of the data. Their construction is fast, and so are the algorithms
that map data points to dictionary coefficients and vice versa. In addition,
data points are guaranteed to have a sparse representation in terms of the
dictionary. We think of dictionaries as the analogue of wavelets, but for
approximating point clouds rather than functions.
"
"  Recently there has been much interest in data that, in statistical language,
may be described as having a large crossed and severely unbalanced random
effects structure. Such data sets arise for recommender engines and information
retrieval problems. Many large bipartite weighted graphs have this structure
too. We would like to assess the stability of algorithms fit to such data. Even
for linear statistics, a naive form of bootstrap sampling can be seriously
misleading and McCullagh [Bernoulli 6 (2000) 285--301] has shown that no
bootstrap method is exact. We show that an alternative bootstrap separately
resampling rows and columns of the data matrix satisfies a mean consistency
property even in heteroscedastic crossed unbalanced random effects models. This
alternative does not require the user to fit a crossed random effects model to
the data.
"
"  Finding ""densely connected clusters"" in a graph is in general an important
and well studied problem in the literature \cite{Schaeffer}. It has various
applications in pattern recognition, social networking and data mining
\cite{Duda,Mishra}. Recently, Ames and Vavasis have suggested a novel method
for finding cliques in a graph by using convex optimization over the adjacency
matrix of the graph \cite{Ames, Ames2}. Also, there has been recent advances in
decomposing a given matrix into its ""low rank"" and ""sparse"" components
\cite{Candes, Chandra}. In this paper, inspired by these results, we view
""densely connected clusters"" as imperfect cliques, where imperfections
correspond missing edges, which are relatively sparse. We analyze the problem
in a probabilistic setting and aim to detect disjointly planted clusters. Our
main result basically suggests that, one can find \emph{dense} clusters in a
graph, as long as the clusters are sufficiently large. We conclude by
discussing possible extensions and future research directions.
"
"  This paper has attracted interest around the world from the media (both TV
and newspapers). In addition, we have received letters, emails and telephone
calls. One of our favorites was a voicemail message asking us to return a call
to Australia at which point we would learn who really killed JFK. We welcome
the opportunity to respond to the letter to the editor from Mr. Fiorentino. Mr.
Fiorentino claims that our ``statement relating to the likelihood of a second
assassin based on the premise of three or more separate bullets is demonstrably
false.'' In response we would like to simply quote from page 327 of Gerald
Posner's book Case Closed, one of the most well known works supporting the
single assassin theory: ``If Connally was hit by another bullet, it had to be
fired from a second shooter, since the Warren Commission's own reconstructions
showed that Oswald could not have operated the bolt and refired in 1.4
seconds.'' Mr. Fiorentino also claims that the ``second fatal flaw is the use
of a rather uncomplicated formula based on Bayes Theorem.'' Let $E$ denote the
evidence and $T$ denote the theory that there were just two bullets (and hence
a single shooter). We used Bayes Theorem to hypothetically calculate $P(T|E)$
from $P(E|T)$ and the prior probability $P(T)$. In order to make $P(T|E)$ ten
times more likely than $P(\bar{T}|E)$, the ratio of the prior probabilities
[i.e., $P(T) / P(\bar{T})$] would have to be greater than 15. Thus, we again
conclude that this casts serious doubt on Dr. Guinn's conclusion that the
evidence supported just two bullets. Sadly, this is far from the first time
that probability has been misunderstood and/or misapplied in a case of public
interest. A notable British example is the Clark case. See Nobles and Schiff
(2005) for details. Finally, we welcome and, in fact, encourage members of the
scientific community to provide alternative analyses of the data.
"
"  The aim of this chapter is twofold. In the first part we will provide a brief
overview of the mathematical and statistical foundations of graphical models,
along with their fundamental properties, estimation and basic inference
procedures. In particular we will develop Markov networks (also known as Markov
random fields) and Bayesian networks, which comprise most past and current
literature on graphical models. In the second part we will review some
applications of graphical models in systems biology.
"
"  Discussion of ``Statistical analysis of an archeological find'' by Andrey
Feuerverger [arXiv:0804.0079]
"
"  We study the asymptotic properties of the adaptive Lasso in cointegration
regressions in the case where all covariates are weakly exogenous. We assume
the number of candidate I(1) variables is sub-linear with respect to the sample
size (but possibly larger) and the number of candidate I(0) variables is
polynomial with respect to the sample size. We show that, under classical
conditions used in cointegration analysis, this estimator asymptotically
chooses the correct subset of variables in the model and its asymptotic
distribution is the same as the distribution of the OLS estimate given the
variables in the model were known in beforehand (oracle property). We also
derive an algorithm based on the local quadratic approximation and present a
numerical study to show the adequacy of the method in finite samples.
"
"  This paper aims to enhance our understanding of substantive questions
regarding self-reported happiness and well-being through the specification and
use of multi-level models. To date, there have been numerous quantitative
research studies of the happiness of individuals, based on single-level
regression models, where typically a happiness index is related to a set of
explanatory variables. There are also several single-level studies comparing
aggregate happiness levels between countries. Nevertheless, there have been
very few studies that attempt to simultaneously take into account variations in
happiness and well-being at several different levels, such as individual,
household, and area. Here, multilevel models are used with data from the
British Household Panel Survey to assess the nature and extent of variations in
happiness and well-being to determine the relative importance of the area
(district, region), household and individual characteristics on these outcomes.
Moreover, having taken into account the characteristics at these different
levels in the multilevel models, the paper shows how it is possible to identify
any areas that are associated with especially positive or negative feelings of
happiness and well-being.
"
"  In the framework of supervised classification (discrimination) for functional
data, it is shown that the optimal classification rule can be explicitly
obtained for a class of Gaussian processes with ""triangular"" covariance
functions. This explicit knowledge has two practical consequences. First, the
consistency of the well-known nearest neighbors classifier (which is not
guaranteed in the problems with functional data) is established for the
indicated class of processes. Second, and more important, parametric and
nonparametric plug-in classifiers can be obtained by estimating the unknown
elements in the optimal rule. The performance of these new plug-in classifiers
is checked, with positive results, through a simulation study and a real data
example.
"
"  The problem of topic modeling can be seen as a generalization of the
clustering problem, in that it posits that observations are generated due to
multiple latent factors (e.g., the words in each document are generated as a
mixture of several active topics, as opposed to just one). This increased
representational power comes at the cost of a more challenging unsupervised
learning problem of estimating the topic probability vectors (the distributions
over words for each topic), when only the words are observed and the
corresponding topics are hidden.
  We provide a simple and efficient learning procedure that is guaranteed to
recover the parameters for a wide class of mixture models, including the
popular latent Dirichlet allocation (LDA) model. For LDA, the procedure
correctly recovers both the topic probability vectors and the prior over the
topics, using only trigram statistics (i.e., third order moments, which may be
estimated with documents containing just three words). The method, termed
Excess Correlation Analysis (ECA), is based on a spectral decomposition of low
order moments (third and fourth order) via two singular value decompositions
(SVDs). Moreover, the algorithm is scalable since the SVD operations are
carried out on $k\times k$ matrices, where $k$ is the number of latent factors
(e.g. the number of topics), rather than in the $d$-dimensional observed space
(typically $d \gg k$).
"
"  This paper develops a methodology for approximating the posterior first two
moments of the posterior distribution in Bayesian inference. Partially
specified probability models, which are defined only by specifying means and
variances, are constructed based upon second-order conditional independence, in
order to facilitate posterior updating and prediction of required
distributional quantities. Such models are formulated particularly for
multivariate regression and time series analysis with unknown observational
variance-covariance components. The similarities and differences of these
models with the Bayes linear approach are established. Several subclasses of
important models, including regression and time series models with errors
following multivariate $t$, inverted multivariate $t$ and Wishart
distributions, are discussed in detail. Two numerical examples consisting of
simulated data and of US investment and change in inventory data illustrate the
proposed methodology.
"
"  We show that the herding procedure of Welling (2009) takes exactly the form
of a standard convex optimization algorithm--namely a conditional gradient
algorithm minimizing a quadratic moment discrepancy. This link enables us to
invoke convergence results from convex optimization and to consider faster
alternatives for the task of approximating integrals in a reproducing kernel
Hilbert space. We study the behavior of the different variants through
numerical simulations. The experiments indicate that while we can improve over
herding on the task of approximating integrals, the original herding algorithm
tends to approach more often the maximum entropy distribution, shedding more
light on the learning bias behind herding.
"
"  We consider the problem of calibration and the GREG method as suggested and
studied in Deville and Sarndal (1992). We show that a GREG type estimator is
typically not minimal variance unbiased estimator even asymptotically. We
suggest a similar estimator which is unbiased but is asymptotically with a
minimal variance.
"
"  This work studies the theoretical rules of feature selection in linear
discriminant analysis (LDA), and a new feature selection method is proposed for
sparse linear discriminant analysis. An $l_1$ minimization method is used to
select the important features from which the LDA will be constructed. The
asymptotic results of this proposed two-stage LDA (TLDA) are studied,
demonstrating that TLDA is an optimal classification rule whose convergence
rate is the best compared to existing methods. The experiments on simulated and
real datasets are consistent with the theoretical results and show that TLDA
performs favorably in comparison with current methods. Overall, TLDA uses a
lower minimum number of features or genes than other approaches to achieve a
better result with a reduced misclassification rate.
"
"  Solar flares are large-scale releases of energy in the solar atmosphere,
which are characterised by rapid changes in the hydrodynamic properties of
plasma from the photosphere to the corona. Solar physicists have typically
attempted to understand these complex events using a combination of theoretical
models and observational data. From a statistical perspective, there are many
challenges associated with making accurate and statistically significant
comparisons between theory and observations, due primarily to the large number
of free parameters associated with physical models. This class of ill-posed
statistical problem is ideally suited to Bayesian methods. In this paper, the
solar flare studied by Raftery et al. (2009) is reanalysed using a Bayesian
framework. This enables us to study the evolution of the flare's temperature,
emission measure and energy loss in a statistically self-consistent manner. The
Bayesian-based model selection techniques imply that no decision can be made
regarding which of the conductive or non-thermal beam heating play the most
important role in heating the flare plasma during the impulsive phase of this
event.
"
"  A generalization of Gy's theory for the variance of the fundamental sampling
error is reviewed. Practical situations where the generalized model potentially
leads to more accurate variance estimates are identified as: clustering of
particles, differences in densities or sizes of the particles or repulsive
inter-particle forces. Two general approaches for estimating an input parameter
for the generalized model are discussed. The first approach consists of
modelling based on physical properties of particles such as size, density and
electrostatic forces between particles. The second approach uses image analysis
of actual samples. Further research into both methods is proposed and a
suggestion is made to use line-intercept sampling combined with Markov Chain
modelling in the second approach.
  It is concluded that although, at the moment, it is too early for a routine
application of the generalized theory, the generalization has the potential of
providing more accurate variance estimates than are possible in the theory of
Gy. Therefore, further research into the development and expansion of the
generalized theory is worthwhile.
"
"  The paper is dealing with semi-classical asymptotics of a characteristic
function for a stochastic process. The main technical tool is provided by the
stationary phase method. The extremal range for a stochastic process is defined
by limit values of the complex logarithm of the characteristic function. The
paper also outlines a numerical method for calculating stochastic extrema.
"
"  The fidelity of radio astronomical images is generally assessed by practical
experience, i.e. using rules of thumb, although some aspects and cases have
been treated rigorously. In this paper we present a mathematical framework
capable of describing the fundamental limits of radio astronomical imaging
problems. Although the data model assumes a single snapshot observation, i.e.
variations in time and frequency are not considered, this framework is
sufficiently general to allow extension to synthesis observations. Using tools
from statistical signal processing and linear algebra, we discuss the
tractability of the imaging and deconvolution problem, the redistribution of
noise in the map by the imaging and deconvolution process, the covariance of
the image values due to propagation of calibration errors and thermal noise and
the upper limit on the number of sources tractable by self calibration. The
combination of covariance of the image values and the number of tractable
sources determines the effective noise floor achievable in the imaging process.
The effective noise provides a better figure of merit than dynamic range since
it includes the spatial variations of the noise. Our results provide handles
for improving the imaging performance by design of the array.
"
"  We consider the problem of statistical inference on unknown quantities
structured as a multiway table. We show that such multiway tables are naturally
formed by arranging regression coefficients in complex systems of linear models
for association analysis. In genetics and genomics, the resulting two-way and
three-way tables cover many important applications. Within the Bayesian
hierarchical model framework, we define the structure of a multiway table
through prior specification. Focusing on model comparison and selection, we
derive analytic expressions of Bayes factors and their approximations and
discuss their theoretical and computational properties. Finally, we demonstrate
the strength of our approach using a genomic application of mapping
tissue-specific eQTLs (expression quantitative loci).
"
"  We study the generalization performance of online learning algorithms trained
on samples coming from a dependent source of data. We show that the
generalization error of any stable online algorithm concentrates around its
regret--an easily computable statistic of the online performance of the
algorithm--when the underlying ergodic process is $\beta$- or $\phi$-mixing. We
show high probability error bounds assuming the loss function is convex, and we
also establish sharp convergence rates and deviation bounds for strongly convex
losses and several linear prediction problems such as linear and logistic
regression, least-squares SVM, and boosting on dependent data. In addition, our
results have straightforward applications to stochastic optimization with
dependent data, and our analysis requires only martingale convergence
arguments; we need not rely on more powerful statistical tools such as
empirical process theory.
"
"  We consider the problem of learning a high-dimensional multi-task regression
model, under sparsity constraints induced by presence of grouping structures on
the input covariates and on the output predictors. This problem is primarily
motivated by expression quantitative trait locus (eQTL) mapping, of which the
goal is to discover genetic variations in the genome (inputs) that influence
the expression levels of multiple co-expressed genes (outputs), either
epistatically, or pleiotropically, or both. A structured input-output lasso
(SIOL) model based on an intricate l1/l2-norm penalty over the regression
coefficient matrix is employed to enable discovery of complex sparse
input/output relationships; and a highly efficient new optimization algorithm
called hierarchical group thresholding (HiGT) is developed to solve the
resultant non-differentiable, non-separable, and ultra high-dimensional
optimization problem. We show on both simulation and on a yeast eQTL dataset
that our model leads to significantly better recovery of the structured sparse
relationships between the inputs and the outputs, and our algorithm
significantly outperforms other optimization techniques under the same model.
Additionally, we propose a novel approach for efficiently and effectively
detecting input interactions by exploiting the prior knowledge available from
biological experiments.
"
"  The personalization of treatment via bio-markers and other risk categories
has drawn increasing interest among clinical scientists. Personalized treatment
strategies can be learned using data from clinical trials, but such trials are
very costly to run. This paper explores the use of active learning techniques
to design more efficient trials, addressing issues such as whom to recruit, at
what point in the trial, and which treatment to assign, throughout the duration
of the trial. We propose a minimax bandit model with two different optimization
criteria, and discuss the computational challenges and issues pertaining to
this approach. We evaluate our active learning policies using both simulated
data, and data modeled after a clinical trial for treating depressed
individuals, and contrast our methods with other plausible active learning
policies.
"
"  When a thick cylindrical coin is tossed in the air and lands without bouncing
on an inelastic substrate, it ends up on its face or its side. We account for
the rigid body dynamics of spin and precession and calculate the probability
distribution of heads, tails, and sides for a thick coin as a function of its
dimensions and the distribution of its initial conditions. Our theory yields a
simple expression for the aspect ratio of homogeneous coins with a prescribed
frequency of heads/tails compared to sides, which we validate by tossing
experiments using coins of different aspect ratios.
"
"  Performance evaluation of nursing homes is usually accomplished by the
repeated administration of questionnaires aimed at measuring the health status
of the patients during their period of residence in the nursing home. We
illustrate how a latent Markov model with covariates may effectively be used
for the analysis of data collected in this way. This model relies on a not
directly observable Markov process, whose states represent different levels of
the health status. For the maximum likelihood estimation of the model we apply
an EM algorithm implemented by means of certain recursions taken from the
literature on hidden Markov chains. Of particular interest is the estimation of
the effect of each nursing home on the probability of transition between the
latent states. We show how the estimates of these effects may be used to
construct a set of scores which allows us to rank these facilities in terms of
their efficacy in taking care of the health conditions of their patients. The
method is used within an application based on data concerning a set of nursing
homes located in the Region of Umbria, Italy, which were followed for the
period 2003--2005.
"
"  In 2010, Web users ordered, only in Amazon, 73 items per second and massively
contribute reviews about their consuming experience. As the Web matures and
becomes social and participatory, collaborative filters are the basic
complement in searching online information about people, events and products.
In Web 2.0, what connected consumers create is not simply content (e.g.
reviews) but context. This new contextual framework of consumption emerges
through the aggregation and collaborative filtering of personal preferences
about goods in the Web in massive scale. More importantly, facilitates
connected consumers to search and navigate the complex Web more effectively and
amplifies incentives for quality. The objective of the present article is to
jointly review the basic stylized facts of relevant research in recommendation
systems in computer and marketing studies in order to share some common
insights. After providing a comprehensive definition of goods and Users in the
Web, we describe a classification of recommendation systems based on two
families of criteria: how recommendations are formed and input data
availability. The classification is presented under a common minimal matrix
notation and is used as a bridge to related issues in the business and
marketing literature. We focus our analysis in the fields of one-to-one
marketing, network-based marketing Web merchandising and atmospherics and their
implications in the processes of personalization and adaptation in the Web.
Market basket analysis is investigated in context of recommendation systems.
Discussion on further research refers to the business implications and
technological challenges of recommendation systems.
"
"  Density modeling is notoriously difficult for high dimensional data. One
approach to the problem is to search for a lower dimensional manifold which
captures the main characteristics of the data. Recently, the Gaussian Process
Latent Variable Model (GPLVM) has successfully been used to find low
dimensional manifolds in a variety of complex data. The GPLVM consists of a set
of points in a low dimensional latent space, and a stochastic map to the
observed space. We show how it can be interpreted as a density model in the
observed space. However, the GPLVM is not trained as a density model and
therefore yields bad density estimates. We propose a new training strategy and
obtain improved generalisation performance and better density estimates in
comparative evaluations on several benchmark data sets.
"
"  Evaluation of HIV large scale interventions programme is becoming
increasingly important, but impact estimates frequently hinge on knowledge of
changes in behaviour such as the frequency of condom use (CU) over time, or
other self-reported behaviour changes, for which we generally have limited or
potentially biased data. We employ a Bayesian inference methodology that
incorporates a dynamic HIV transmission dynamics model to estimate CU time
trends from HIV prevalence data. Estimation is implemented via particle Markov
Chain Monte Carlo methods, applied for the first time in this context. The
preliminary choice of the formulation for the time varying parameter reflecting
the proportion of CU is critical in the context studied, due to the very
limited amount of CU and HIV data available We consider various novel
formulations to explore the trajectory of CU in time, based on diffusion-driven
trajectories and smooth sigmoid curves. Extensive series of numerical
simulations indicate that informative results can be obtained regarding the
amplitude of the increase in CU during an intervention, with good levels of
sensitivity and specificity performance in effectively detecting changes. The
application of this method to a real life problem illustrates how it can help
evaluate HIV intervention from few observational studies and suggests that
these methods can potentially be applied in many different contexts.
"
"  We consider the problem of estimating the proportion $\theta$ of true null
hypotheses in a multiple testing context. The setup is classically modeled
through a semiparametric mixture with two components: a uniform distribution on
interval $[0,1]$ with prior probability $\theta$ and a nonparametric density
$f$. We discuss asymptotic efficiency results and establish that two different
cases occur whether $f$ vanishes on a set with non null Lebesgue measure or
not. In the first case, we exhibit estimators converging at parametric rate,
compute the optimal asymptotic variance and conjecture that no estimator is
asymptotically efficient (i.e. attains the optimal asymptotic variance). In the
second case, we prove that the quadratic risk of any estimator does not
converge at parametric rate. We illustrate those results on simulated data.
"
"  We analyze the performance of a class of manifold-learning algorithms that
find their output by minimizing a quadratic form under some normalization
constraints. This class consists of Locally Linear Embedding (LLE), Laplacian
Eigenmap, Local Tangent Space Alignment (LTSA), Hessian Eigenmaps (HLLE), and
Diffusion maps. We present and prove conditions on the manifold that are
necessary for the success of the algorithms. Both the finite sample case and
the limit case are analyzed. We show that there are simple manifolds in which
the necessary conditions are violated, and hence the algorithms cannot recover
the underlying manifolds. Finally, we present numerical results that
demonstrate our claims.
"
"  The task of matching co-referent records is known among other names as rocord
linkage. For large record-linkage problems, often there is little or no labeled
data available, but unlabeled data shows a reasonable clear structure. For such
problems, unsupervised or semi-supervised methods are preferable to supervised
methods. In this paper, we describe a hierarchical graphical model framework
for the linakge-problem in an unsupervised setting. In addition to proposing
new methods, we also cast existing unsupervised probabilistic record-linkage
methods in this framework. Some of the techniques we propose to minimize
overfitting in the above model are of interest in the general graphical model
setting. We describe a method for incorporating monotinicity constraints in a
graphical model. We also outline a bootstrapping approach of using
""single-field"" classifiers to noisily label latent variables in a hierarchical
model. Experimental results show that our proposed unsupervised methods perform
quite competitively even with fully supervised record-linkage methods.
"
"  In this paper we provide a principled approach to solve a transductive
classification problem involving a similar graph (edges tend to connect nodes
with same labels) and a dissimilar graph (edges tend to connect nodes with
opposing labels). Most of the existing methods, e.g., Information
Regularization (IR), Weighted vote Relational Neighbor classifier (WvRN) etc,
assume that the given graph is only a similar graph. We extend the IR and WvRN
methods to deal with mixed graphs. We evaluate the proposed extensions on
several benchmark datasets as well as two real world datasets and demonstrate
the usefulness of our ideas.
"
"  We develop the multilingual topic model for unaligned text (MuTo), a
probabilistic model of text that is designed to analyze corpora composed of
documents in two languages. From these documents, MuTo uses stochastic EM to
simultaneously discover both a matching between the languages and multilingual
latent topics. We demonstrate that MuTo is able to find shared topics on
real-world multilingual corpora, successfully pairing related documents across
languages. MuTo provides a new framework for creating multilingual topic models
without needing carefully curated parallel corpora and allows applications
built using the topic model formalism to be applied to a much wider class of
corpora.
"
"  We propose and analyze a novel framework for learning sparse representations,
based on two statistical techniques: kernel smoothing and marginal regression.
The proposed approach provides a flexible framework for incorporating feature
similarity or temporal information present in data sets, via non-parametric
kernel smoothing. We provide generalization bounds for dictionary learning
using smooth sparse coding and show how the sample complexity depends on the L1
norm of kernel function used. Furthermore, we propose using marginal regression
for obtaining sparse codes, which significantly improves the speed and allows
one to scale to large dictionary sizes easily. We demonstrate the advantages of
the proposed approach, both in terms of accuracy and speed by extensive
experimentation on several real data sets. In addition, we demonstrate how the
proposed approach could be used for improving semi-supervised sparse coding.
"
"  The statistical analysis of covariance matrix data is considered and, in
particular, methodology is discussed which takes into account the non-Euclidean
nature of the space of positive semi-definite symmetric matrices. The main
motivation for the work is the analysis of diffusion tensors in medical image
analysis. The primary focus is on estimation of a mean covariance matrix and,
in particular, on the use of Procrustes size-and-shape space. Comparisons are
made with other estimation techniques, including using the matrix logarithm,
matrix square root and Cholesky decomposition. Applications to diffusion tensor
imaging are considered and, in particular, a new measure of fractional
anisotropy called Procrustes Anisotropy is discussed.
"
"  Suppose that we observe noisy linear measurements of an unknown signal that
can be modeled as the sum of two component signals, each of which arises from a
nonlinear sub-manifold of a high dimensional ambient space. We introduce SPIN,
a first order projected gradient method to recover the signal components.
Despite the nonconvex nature of the recovery problem and the possibility of
underdetermined measurements, SPIN provably recovers the signal components,
provided that the signal manifolds are incoherent and that the measurement
operator satisfies a certain restricted isometry property. SPIN significantly
extends the scope of current recovery models and algorithms for low dimensional
linear inverse problems and matches (or exceeds) the current state of the art
in terms of performance.
"
"  Bayesian model-based reinforcement learning is a formally elegant approach to
learning optimal behaviour under model uncertainty, trading off exploration and
exploitation in an ideal way. Unfortunately, finding the resulting
Bayes-optimal policies is notoriously taxing, since the search space becomes
enormous. In this paper we introduce a tractable, sample-based method for
approximate Bayes-optimal planning which exploits Monte-Carlo tree search. Our
approach outperformed prior Bayesian model-based RL algorithms by a significant
margin on several well-known benchmark problems -- because it avoids expensive
applications of Bayes rule within the search tree by lazily sampling models
from the current beliefs. We illustrate the advantages of our approach by
showing it working in an infinite state space domain which is qualitatively out
of reach of almost all previous work in Bayesian exploration.
"
"  Several convex formulation methods have been proposed previously for
statistical estimation with structured sparsity as the prior. These methods
often require a carefully tuned regularization parameter, often a cumbersome or
heuristic exercise. Furthermore, the estimate that these methods produce might
not belong to the desired sparsity model, albeit accurately approximating the
true parameter. Therefore, greedy-type algorithms could often be more desirable
in estimating structured-sparse parameters. So far, these greedy methods have
mostly focused on linear statistical models. In this paper we study the
projected gradient descent with non-convex structured-sparse parameter model as
the constraint set. Should the cost function have a Stable Model-Restricted
Hessian the algorithm produces an approximation for the desired minimizer. As
an example we elaborate on application of the main results to estimation in
Generalized Linear Model.
"
"  We consider the finite sample properties of the regularized high-dimensional
Cox regression via lasso. Existing literature focuses on linear models or
generalized linear models with Lipschitz loss functions, where the empirical
risk functions are the summations of independent and identically distributed
(iid) losses. The summands in the negative log partial likelihood function for
censored survival data, however, are neither iid nor Lipschitz. We first
approximate the negative log partial likelihood function by a sum of iid
non-Lipschitz terms, then derive the non-asymptotic oracle inequalities for the
lasso penalized Cox regression using pointwise arguments to tackle the
difficulty caused by the lack of iid and Lipschitz property.
"
"  Graphical models are frequently used to represent topological structures of
various complex networks. Current criteria to assess different models of a
network mainly rely on how close a model matches the network in terms of
topological characteristics. Typical topological metrics are clustering
coefficient, distance distribution, the largest eigenvalue of the adjacency
matrix, and the gap between the first and the second largest eigenvalues, which
are widely used to evaluate and compare different models of a network. In this
paper, we show that evaluating complex network models based on the current
topological metrics can be quite misleading. Taking several models of the
AS-level Internet as examples, we show that although a model seems to be good
to describe the Internet in terms of the aforementioned topological
characteristics, it is far from being realistic to represent the real Internet
in performances such as robustness in resisting intentional attacks and traffic
load distributions. We further show that it is not useful to assess network
models by examining some topological characteristics such as clustering
coefficient and distance distribution, if robustness of the Internet against
random node removals is the only concern. Our findings shed new lights on how
to reasonably evaluate different models of a network, not only the Internet but
also other types of complex networks.
"
"  Methods for automated discovery of causal relationships from
non-interventional data have received much attention recently. A widely used
and well understood model family is given by linear acyclic causal models
(recursive structural equation models). For Gaussian data both constraint-based
methods (Spirtes et al., 1993; Pearl, 2000) (which output a single equivalence
class) and Bayesian score-based methods (Geiger and Heckerman, 1994) (which
assign relative scores to the equivalence classes) are available. On the
contrary, all current methods able to utilize non-Gaussianity in the data
(Shimizu et al., 2006; Hoyer et al., 2008) always return only a single graph or
a single equivalence class, and so are fundamentally unable to express the
degree of certainty attached to that output. In this paper we develop a
Bayesian score-based approach able to take advantage of non-Gaussianity when
estimating linear acyclic causal models, and we empirically demonstrate that,
at least on very modest size networks, its accuracy is as good as or better
than existing methods. We provide a complete code package (in R) which
implements all algorithms and performs all of the analysis provided in the
paper, and hope that this will further the application of these methods to
solving causal inference problems.
"
"  --- Objectives --- Prior to foetal karyotyping, the likelihood of Down's
syndrome is often determined combining maternal age, serum free beta-HCG,
PAPP-A levels and embryonic measurements of crown-rump length and nuchal
translucency for gestational ages between 11 and 13 weeks. It appeared
important to get a precise knowledge of these scan parameters' normal values
during the first trimester. This paper focused on crown-rump length. ---
METHODS --- 402 pregnancies from in-vitro fertilization allowing a precise
estimation of foetal ages (FA) were used to determine the best model that
describes crown-rump length (CRL) as a function of FA. Scan measures by a
single operator from 3846 spontaneous pregnancies representative of the general
population from Northern France were used to build a mathematical model linking
FA and CRL in a context as close as possible to normal scan screening used in
Down's syndrome likelihood determination. We modeled both CRL as a function of
FA and FA as a function of CRL. For this, we used a clear methodology and
performed regressions with heteroskedastic corrections and robust regressions.
The results were compared by cross-validation to retain the equations with the
best predictive power. We also studied the errors between observed and
predicted values. --- Results --- Data from 513 spontaneous pregnancies allowed
to model CRL as a function of age of foetal age. The best model was a
polynomial of degree 2. Datation with our equation that models spontaneous
pregnancies from a general population was in quite agreement with objective
datations obtained from 402 IVF pregnancies and thus support the validity of
our model. The most precise measure of CRL was when the SD was minimal
(1.83mm), for a CRL of 23.6 mm where our model predicted a 49.4 days of foetal
age. Our study allowed to model the SD from 30 to 90 days of foetal age and
offers the opportunity of using Zscores in the future to detect growth
abnormalities. --- Conclusion --- With powerful statistical tools we report a
good modeling of the first trimester embryonic growth in the general population
allowing a better knowledge of the date of fertilization useful in the
ultrasound screening of Down's syndrome. The optimal period to measure CRL and
predict foetal age was 49.4 days (9 weeks of gestational age). Our results open
the way to the detection of foetal growth abnormalities using CRL Zscores
throughout the first trimester.
"
"  The performance of stochastic gradient descent (SGD) depends critically on
how learning rates are tuned and decreased over time. We propose a method to
automatically adjust multiple learning rates so as to minimize the expected
error at any one time. The method relies on local gradient variations across
samples. In our approach, learning rates can increase as well as decrease,
making it suitable for non-stationary problems. Using a number of convex and
non-convex learning tasks, we show that the resulting algorithm matches the
performance of SGD or other adaptive approaches with their best settings
obtained through systematic search, and effectively removes the need for
learning rate tuning.
"
"  We outline a model and algorithm to perform inference on the palaeoclimate
and palaeoclimate volatility from pollen proxy data. We use a novel
multivariate non-linear non-Gaussian state space model consisting of an
observation equation linking climate to proxy data and an evolution equation
driving climate change over time. The link from climate to proxy data is
defined by a pre-calibrated forward model, as developed in Salter-Townshend and
Haslett (2012) and Sweeney (2012). Climatic change is represented by a
temporally-uncertain Normal-Inverse Gaussian Levy process, being able to
capture large jumps in multivariate climate whilst remaining temporally
consistent. The pre-calibrated nature of the forward model allows us to cut
feedback between the observation and evolution equations and thus integrate out
the state variable entirely whilst making minimal simplifying assumptions. A
key part of this approach is the creation of mixtures of marginal data
posteriors representing the information obtained about climate from each
individual time point. Our approach allows for an extremely efficient MCMC
algorithm, which we demonstrate with a pollen core from Sluggan Bog, County
Antrim, Northern Ireland.
"
"  We provide self-contained proof of a theorem relating probabilistic coherence
of forecasts to their non-domination by rival forecasts with respect to any
proper scoring rule. The theorem appears to be new but is closely related to
results achieved by other investigators.
"
"  Consider the problem of learning the drift coefficient of a stochastic
differential equation from a sample path. In this paper, we assume that the
drift is parametrized by a high dimensional vector. We address the question of
how long the system needs to be observed in order to learn this vector of
parameters. We prove a general lower bound on this time complexity by using a
characterization of mutual information as time integral of conditional
variance, due to Kadota, Zakai, and Ziv. This general lower bound is applied to
specific classes of linear and non-linear stochastic differential equations. In
the linear case, the problem under consideration is the one of learning a
matrix of interaction coefficients. We evaluate our lower bound for ensembles
of sparse and dense random matrices. The resulting estimates match the
qualitative behavior of upper bounds achieved by computationally efficient
procedures.
"
"  We investigate the use of sparse coding and dictionary learning in the
context of multitask and transfer learning. The central assumption of our
learning method is that the tasks parameters are well approximated by sparse
linear combinations of the atoms of a dictionary on a high or infinite
dimensional space. This assumption, together with the large quantity of
available data in the multitask and transfer learning settings, allows a
principled choice of the dictionary. We provide bounds on the generalization
error of this approach, for both settings. Numerical experiments on one
synthetic and two real datasets show the advantage of our method over single
task learning, a previous method based on orthogonal and dense representation
of the tasks and a related method learning task grouping.
"
"  Neural coding is a field of study that concerns how sensory information is
represented in the brain by networks of neurons. The link between external
stimulus and neural response can be studied from two parallel points of view.
The first, neural encoding refers to the mapping from stimulus to response, and
primarily focuses on understanding how neurons respond to a wide variety of
stimuli, and on constructing models that accurately describe the
stimulus-response relationship. Neural decoding, on the other hand, refers to
the reverse mapping, from response to stimulus, where the challenge is to
reconstruct a stimulus from the spikes it evokes. Since neuronal response is
stochastic, a one-to-one mapping of stimuli into neural responses does not
exist, causing a mismatch between the two viewpoints of neural coding. Here, we
use these two perspectives to investigate the question of what rate coding is,
in the simple setting of a single stationary stimulus parameter and a single
stationary spike train represented by a renewal process. We show that when rate
codes are defined in terms of encoding, i.e., the stimulus parameter is mapped
onto the mean firing rate, the rate decoder given by spike counts or the sample
mean, does not always efficiently decode the rate codes, but can improve
efficiency in reading certain rate codes, when correlations within a spike
train are taken into account.
"
"  We use statistical estimates of the entropy rate of spike train data in order
to make inferences about the underlying structure of the spike train itself. We
first examine a number of different parametric and nonparametric estimators
(some known and some new), including the ``plug-in'' method, several versions
of Lempel-Ziv-based compression algorithms, a maximum likelihood estimator
tailored to renewal processes, and the natural estimator derived from the
Context-Tree Weighting method (CTW). The theoretical properties of these
estimators are examined, several new theoretical results are developed, and all
estimators are systematically applied to various types of synthetic data and
under different conditions.
  Our main focus is on the performance of these entropy estimators on the
(binary) spike trains of 28 neurons recorded simultaneously for a one-hour
period from the primary motor and dorsal premotor cortices of a monkey. We show
how the entropy estimates can be used to test for the existence of long-term
structure in the data, and we construct a hypothesis test for whether the
renewal process model is appropriate for these spike trains. Further, by
applying the CTW algorithm we derive the maximum a posterior (MAP) tree model
of our empirical data, and comment on the underlying structure it reveals.
"
"  How often can we expect a Major League Baseball team to score at least 20
runs in a single game? Considered a rare event in baseball, the outcome of
scoring at least 20 runs in a game has occurred 224 times during regular season
games since 1901 in the American and National Leagues. Each outcome is modeled
as a Poisson process; the time of occurrence of one of these events does not
affect the next future occurrence. Using various distributions, probabilities
of events are generated, goodness-of-fit tests are conducted, and predictions
of future events are offered. The statistical package R is employed for
analysis.
"
"  Under the sociological theory of homophily, people who are similar to one
another are more likely to interact with one another. Marketers often have
access to data on interactions among customers from which, with homophily as a
guiding principle, inferences could be made about the underlying similarities.
However, larger networks face a quadratic explosion in the number of potential
interactions that need to be modeled. This scalability problem renders
probability models of social interactions computationally infeasible for all
but the smallest networks. In this paper we develop a probabilistic framework
for modeling customer interactions that is both grounded in the theory of
homophily, and is flexible enough to account for random variation in who
interacts with whom. In particular, we present a novel Bayesian nonparametric
approach, using Dirichlet processes, to moderate the scalability problems that
marketing researchers encounter when working with networked data. We find that
this framework is a powerful way to draw insights into latent similarities of
customers, and we discuss how marketers can apply these insights to
segmentation and targeting activities.
"
"  In many machine learning problems, labeled training data is limited but
unlabeled data is ample. Some of these problems have instances that can be
factored into multiple views, each of which is nearly sufficent in determining
the correct labels. In this paper we present a new algorithm for probabilistic
multi-view learning which uses the idea of stochastic agreement between views
as regularization. Our algorithm works on structured and unstructured problems
and easily generalizes to partial agreement scenarios. For the full agreement
case, our algorithm minimizes the Bhattacharyya distance between the models of
each view, and performs better than CoBoosting and two-view Perceptron on
several flat and structured classification problems.
"
"  Many real world network problems often concern multivariate nodal attributes
such as image, textual, and multi-view feature vectors on nodes, rather than
simple univariate nodal attributes. The existing graph estimation methods built
on Gaussian graphical models and covariance selection algorithms can not handle
such data, neither can the theories developed around such methods be directly
applied. In this paper, we propose a new principled framework for estimating
graphs from multi-attribute data. Instead of estimating the partial correlation
as in current literature, our method estimates the partial canonical
correlations that naturally accommodate complex nodal features.
Computationally, we provide an efficient algorithm which utilizes the
multi-attribute structure. Theoretically, we provide sufficient conditions
which guarantee consistent graph recovery. Extensive simulation studies
demonstrate performance of our method under various conditions. Furthermore, we
provide illustrative applications to uncovering gene regulatory networks from
gene and protein profiles, and uncovering brain connectivity graph from
functional magnetic resonance imaging data.
"
"  The False Discovery Rate (FDR) is a commonly used type I error rate in
multiple testing problems. It is defined as the expected False Discovery
Proportion (FDP), that is, the expected fraction of false positives among
rejected hypotheses. When the hypotheses are independent, the
Benjamini-Hochberg procedure achieves FDR control at any pre-specified level.
By construction, FDR control offers no guarantee in terms of power, or type II
error. A number of alternative procedures have been developed, including
plug-in procedures that aim at gaining power by incorporating an estimate of
the proportion of true null hypotheses. In this paper, we study the asymptotic
behavior of a class of plug-in procedures based on kernel estimators of the
density of the $p$-values, as the number $m$ of tested hypotheses grows to
infinity. In a setting where the hypotheses tested are independent, we prove
that these procedures are asymptotically more powerful in two respects: (i) a
tighter asymptotic FDR control for any target FDR level and (ii) a broader
range of target levels yielding positive asymptotic power. We also show that
this increased asymptotic power comes at the price of slower, non-parametric
convergence rates for the FDP. These rates are of the form $m^{-k/(2k+1)}$,
where $k$ is determined by the regularity of the density of the $p$-value
distribution, or, equivalently, of the test statistics distribution. These
results are applied to one- and two-sided tests statistics for Gaussian and
Laplace location models, and for the Student model.
"
"  This paper briefly presents several ways to understand the organization of a
large social network (several hundreds of persons). We compare approaches
coming from data mining for clustering the vertices of a graph (spectral
clustering, self-organizing algorithms. . .) and provide methods for
representing the graph from these analysis. All these methods are illustrated
on a medieval social network and the way they can help to understand its
organization is underlined.
"
"  Within the educational context, students' assessment tests are routinely
validated through Item Response Theory (IRT) models which assume
unidimensionality and absence of Differential Item Functioning (DIF). In this
paper, we investigate if such assumptions hold for two national tests
administered in Italy to middle school students in June 2009: the Italian Test
and the Mathematics Test. To this aim, we rely on an extended class of
multidimensional latent class IRT models characterised by: (i) a two-parameter
logistic parameterisation for the conditional probability of a correct
response, (ii) latent traits represented through a random vector with a
discrete distribution, and (iii) the inclusion of (uniform) DIF to account for
students' gender and geographical area. A classification of the items into
unidimensional groups is also proposed and represented by a dendrogram, which
is obtained from a hierarchical clustering algorithm. The results provide
evidence for DIF effects for both Tests. Besides, the assumption of
unidimensionality is strongly rejected for the Italian Test, whereas it is
reasonable for the Mathematics Test.
"
"  Recently Kim and Kvam (2004) and Singh, Sharma, Kumar (2008) proposed
different load-sharing models and developed parametric inference for the these
models. However, their parametric estimates are calculated using iterative
numerical methods. In this note, we provide the general closed-form MLEs for
the two load-sharing models provided by them.
"
"  Given the potential for illicit nuclear material being used for terrorism,
most ports now inspect a large number of goods entering national borders for
radioactive cargo. The U.S. Department of Homeland Security is moving toward
one hundred percent inspection of all containers entering the U.S. at various
ports of entry for nuclear material. We propose a Bayesian classification
approach for the real-time data collected by the inline Polyvinyl Toluene
radiation portal monitors. We study the computational and asymptotic properties
of the proposed method and demonstrate its efficacy in simulations. Given data
available to the authorities, it should be feasible to implement this approach
in practice.
"
"  Many epidemic models approximate social contact behavior by assuming random
mixing within mixing groups (e.g., homes, schools and workplaces). The effect
of more realistic social network structure on estimates of epidemic parameters
is an open area of exploration. We develop a detailed statistical model to
estimate the social contact network within a high school using friendship
network data and a survey of contact behavior. Our contact network model
includes classroom structure, longer durations of contacts to friends than
nonfriends and more frequent contacts with friends, based on reports in the
contact survey. We performed simulation studies to explore which network
structures are relevant to influenza transmission. These studies yield two key
findings. First, we found that the friendship network structure important to
the transmission process can be adequately represented by a dyad-independent
exponential random graph model (ERGM). This means that individual-level sampled
data is sufficient to characterize the entire friendship network. Second, we
found that contact behavior was adequately represented by a static rather than
dynamic contact network.
"
"  Regression models for limited continuous dependent variables having a
non-negligible probability of attaining exactly their limits are presented. The
models differ in the number of parameters and in their flexibility. Fractional
data being a special case of limited dependent data, the models also apply to
variables that are a fraction or a proportion. It is shown how to fit these
models and they are applied to a Loss Given Default dataset from insurance to
which they provide a good fit.
"
"  The sequential parameter optimization (SPOT) package for R is a toolbox for
tuning and understanding simulation and optimization algorithms. Model-based
investigations are common approaches in simulation and optimization. Sequential
parameter optimization has been developed, because there is a strong need for
sound statistical analysis of simulation and optimization algorithms. SPOT
includes methods for tuning based on classical regression and analysis of
variance techniques; tree-based models such as CART and random forest; Gaussian
process models (Kriging), and combinations of different meta-modeling
approaches. This article exemplifies how SPOT can be used for automatic and
interactive tuning.
"
"  This article presents a comparative study of three different types of
estimators used for supervised linear unmixing of two MEx/OMEGA hyperspectral
cubes. The algorithms take into account the constraints of the abundance
fractions, in order to get physically interpretable results. Abundance maps
show that the Bayesian maximum a posteriori probability (MAP) estimator
proposed in Themelis and Rontogiannis (2008) outperforms the other two schemes,
offering a compromise between complexity and estimation performance. Thus, the
MAP estimator is a candidate algorithm to perform ice and minerals detection on
large hyperspectral datasets.
"
"  This article addresses the problem of classification method based on both
labeled and unlabeled data, where we assume that a density function for labeled
data is different from that for unlabeled data. We propose a semi-supervised
logistic regression model for classification problem along with the technique
of covariate shift adaptation. Unknown parameters involved in proposed models
are estimated by regularization with EM algorithm. A crucial issue in the
modeling process is the choices of tuning parameters in our semi-supervised
logistic models. In order to select the parameters, a model selection criterion
is derived from an information-theoretic approach. Some numerical studies show
that our modeling procedure performs well in various cases.
"
"  We consider the problem of learning a set from random samples. We show how
relevant geometric and topological properties of a set can be studied
analytically using concepts from the theory of reproducing kernel Hilbert
spaces. A new kind of reproducing kernel, that we call separating kernel, plays
a crucial role in our study and is analyzed in detail. We prove a new analytic
characterization of the support of a distribution, that naturally leads to a
family of provably consistent regularized learning algorithms and we discuss
the stability of these methods with respect to random sampling. Numerical
experiments show that the approach is competitive, and often better, than other
state of the art techniques.
"
"  We focus on credal nets, which are graphical models that generalise Bayesian
nets to imprecise probability. We replace the notion of strong independence
commonly used in credal nets with the weaker notion of epistemic irrelevance,
which is arguably more suited for a behavioural theory of probability. Focusing
on directed trees, we show how to combine the given local uncertainty models in
the nodes of the graph into a global model, and we use this to construct and
justify an exact message-passing algorithm that computes updated beliefs for a
variable in the tree. The algorithm, which is linear in the number of nodes, is
formulated entirely in terms of coherent lower previsions, and is shown to
satisfy a number of rationality requirements. We supply examples of the
algorithm's operation, and report an application to on-line character
recognition that illustrates the advantages of our approach for prediction. We
comment on the perspectives, opened by the availability, for the first time, of
a truly efficient algorithm based on epistemic irrelevance.
"
"  This paper uses Factored Latent Analysis (FLA) to learn a factorized,
segmental representation for observations of tracked objects over time.
Factored Latent Analysis is latent class analysis in which the observation
space is subdivided and each aspect of the original space is represented by a
separate latent class model. One could simply treat these factors as completely
independent and ignore their interdependencies or one could concatenate them
together and attempt to learn latent class structure for the complete
observation space. Alternatively, FLA allows the interdependencies to be
exploited in estimating an effective model, which is also capable of
representing a factored latent state. In this paper, FLA is used to learn a set
of factored latent classes to represent different modalities of observations of
tracked objects. Different characteristics of the state of tracked objects are
each represented by separate latent class models, including normalized size,
normalized speed, normalized direction, and position. This model also enables
effective temporal segmentation of these sequences. This method is data-driven,
unsupervised using only pairwise observation statistics. This data-driven and
unsupervised activity classi- fication technique exhibits good performance in
multiple challenging environments.
"
"  This paper presents GRASTA (Grassmannian Robust Adaptive Subspace Tracking
Algorithm), an efficient and robust online algorithm for tracking subspaces
from highly incomplete information. The algorithm uses a robust $l^1$-norm cost
function in order to estimate and track non-stationary subspaces when the
streaming data vectors are corrupted with outliers. We apply GRASTA to the
problems of robust matrix completion and real-time separation of background
from foreground in video. In this second application, we show that GRASTA
performs high-quality separation of moving objects from background at
exceptional speeds: In one popular benchmark video example, GRASTA achieves a
rate of 57 frames per second, even when run in MATLAB on a personal laptop.
"
"  We introduce Bayesian Estimation Applied to Multiple Species (BEAMS), an
algorithm designed to deal with parameter estimation when using contaminated
data. We present the algorithm and demonstrate how it works with the help of a
Gaussian simulation. We then apply it to supernova data from the Sloan Digital
Sky Survey (SDSS), showing how the resulting confidence contours of the
cosmological parameters shrink significantly.
"
"  Many machine learning tasks can be expressed as the transformation---or
\emph{transduction}---of input sequences into output sequences: speech
recognition, machine translation, protein secondary structure prediction and
text-to-speech to name but a few. One of the key challenges in sequence
transduction is learning to represent both the input and output sequences in a
way that is invariant to sequential distortions such as shrinking, stretching
and translating. Recurrent neural networks (RNNs) are a powerful sequence
learning architecture that has proven capable of learning such representations.
However RNNs traditionally require a pre-defined alignment between the input
and output sequences to perform transduction. This is a severe limitation since
\emph{finding} the alignment is the most difficult aspect of many sequence
transduction problems. Indeed, even determining the length of the output
sequence is often challenging. This paper introduces an end-to-end,
probabilistic sequence transduction system, based entirely on RNNs, that is in
principle able to transform any input sequence into any finite, discrete output
sequence. Experimental results for phoneme recognition are provided on the
TIMIT speech corpus.
"
"  The goal of this study is to explain and examine the statistical
underpinnings of the Bollinger Band methodology. We start off by elucidating
the rolling regression time series model and deriving its explicit relationship
to Bollinger Bands. Next we illustrate the use of Bollinger Bands in pairs
trading and prove the existence of a specific return duration relationship in
Bollinger Band pairs trading.Then by viewing the Bollinger Band moving average
as an approximation to the random walk plus noise (RWPN) time series model, we
develop a pairs trading variant that we call ""Fixed Forecast Maximum Duration'
Bands"" (FFMDPT). Lastly, we conduct pairs trading simulations using SAP and
Nikkei index data in order to compare the performance of the variant with
Bollinger Bands.
"
"  This paper is concerned with the question of reconstructing a vector in a
finite-dimensional real or complex Hilbert space when only the magnitudes of
the coefficients of the vector under a redundant linear map are known. We
present new invertibility results as well an iterative algorithm that finds the
least-square solution and is robust in the presence of noise. We analyze its
numerical performance by comparing it to two versions of the Cramer-Rao lower
bound.
"
"  In an empirical Bayesian setting, we provide a new multiple testing method,
useful when an additional covariate is available, that influences the
probability of each null hypothesis being true. We measure the posterior
significance of each test conditionally on the covariate and the data, leading
to greater power. Using covariate-based prior information in an unsupervised
fashion, we produce a list of significant hypotheses which differs in length
and order from the list obtained by methods not taking covariate-information
into account. Covariate-modulated posterior probabilities of each null
hypothesis are estimated using a fast approximate algorithm. The new method is
applied to expression quantitative trait loci (eQTL) data.
"
"  This paper considers small-area estimation with lung cancer mortality data,
and discusses the choice of upper-level model for the variation over areas.
Inference about the random effects for the areas may depend strongly on the
choice of this model, but this choice is not a straightforward matter. We give
a general methodology for both evaluating the data evidence for different
models and averaging over plausible models to give robust area effect
distributions. We reanalyze the data of Tsutakawa [Biometrics 41 (1985) 69--79]
on lung cancer mortality rates in Missouri cities, and show the differences in
conclusions about the city rates from this methodology.
"
"  In this paper we consider the problem of model choice for a set of insurance
loss ratios. We use a reversible jump algorithm for our model discrimination
and show how the vanilla reversible jump algorithm can be improved on using
recent methodological advances in reversible jump computation.
"
"  We consider Bayesian analysis of a class of multiple changepoint models.
While there are a variety of efficient ways to analyse these models if the
parameters associated with each segment are independent, there are few general
approaches for models where the parameters are dependent. Under the assumption
that the dependence is Markov, we propose an efficient online algorithm for
sampling from an approximation to the posterior distribution of the number and
position of the changepoints. In a simulation study, we show that the
approximation introduced is negligible. We illustrate the power of our approach
through fitting piecewise polynomial models to data, under a model which allows
for either continuity or discontinuity of the underlying curve at each
changepoint. This method is competitive with, or out-performs, other methods
for inferring curves from noisy data; and uniquely it allows for inference of
the locations of discontinuities in the underlying curve.
"
"  Graphical models are frequently used to explore networks, such as genetic
networks, among a set of variables. This is usually carried out via exploring
the sparsity of the precision matrix of the variables under consideration.
Penalized likelihood methods are often used in such explorations. Yet,
positive-definiteness constraints of precision matrices make the optimization
problem challenging. We introduce nonconcave penalties and the adaptive LASSO
penalty to attenuate the bias problem in the network estimation. Through the
local linear approximation to the nonconcave penalty functions, the problem of
precision matrix estimation is recast as a sequence of penalized likelihood
problems with a weighted $L_1$ penalty and solved using the efficient algorithm
of Friedman et al. [Biostatistics 9 (2008) 432--441]. Our estimation schemes
are applied to two real datasets. Simulation experiments and asymptotic theory
are used to justify our proposed methods.
"
"  This paper investigates two feature-scoring criteria that make use of
estimated class probabilities: one method proposed by \citet{shen} and a
complementary approach proposed below. We develop a theoretical framework to
analyze each criterion and show that both estimate the spread (across all
values of a given feature) of the probability that an example belongs to the
positive class. Based on our analysis, we predict when each scoring technique
will be advantageous over the other and give empirical results validating our
predictions.
"
"  Stability is a general notion that quantifies the sensitivity of a learning
algorithm's output to small change in the training dataset (e.g. deletion or
replacement of a single training sample). Such conditions have recently been
shown to be more powerful to characterize learnability in the general learning
setting under i.i.d. samples where uniform convergence is not necessary for
learnability, but where stability is both sufficient and necessary for
learnability. We here show that similar stability conditions are also
sufficient for online learnability, i.e. whether there exists a learning
algorithm such that under any sequence of examples (potentially chosen
adversarially) produces a sequence of hypotheses that has no regret in the
limit with respect to the best hypothesis in hindsight. We introduce online
stability, a stability condition related to uniform-leave-one-out stability in
the batch setting, that is sufficient for online learnability. In particular we
show that popular classes of online learners, namely algorithms that fall in
the category of Follow-the-(Regularized)-Leader, Mirror Descent, gradient-based
methods and randomized algorithms like Weighted Majority and Hedge, are
guaranteed to have no regret if they have such online stability property. We
provide examples that suggest the existence of an algorithm with such stability
condition might in fact be necessary for online learnability. For the more
restricted binary classification setting, we establish that such stability
condition is in fact both sufficient and necessary. We also show that for a
large class of online learnable problems in the general learning setting,
namely those with a notion of sub-exponential covering, no-regret online
algorithms that have such stability condition exists.
"
"  This article presents a statistical analysis method and introduces the
corresponding software package ""tailstat,"" which is believed to be widely
applicable to today's internet society. The proposed method facilitates
statistical analyses with small sample sets from given populations, which
render the central limit theorem inapplicable. A large-scale case study
demonstrates the effectiveness of the method and provides implications for
applying similar analyses to other cases.
"
"  In this paper, we study classes of graphs with three types of edges that
capture the modified independence structure of a directed acyclic graph (DAG)
after marginalisation over unobserved variables and conditioning on selection
variables using the $m$-separation criterion. These include MC, summary, and
ancestral graphs. As a modification of MC graphs, we define the class of
ribbonless graphs (RGs) that permits the use of the $m$-separation criterion.
RGs contain summary and ancestral graphs as subclasses, and each RG can be
generated by a DAG after marginalisation and conditioning. We derive simple
algorithms to generate RGs, from given DAGs or RGs, and also to generate
summary and ancestral graphs in a simple way by further extension of the
RG-generating algorithm. This enables us to develop a parallel theory on these
three classes and to study the relationships between them as well as the use of
each class.
"
"  Public data repositories have enabled researchers to compare results across
multiple genomic studies in order to replicate findings. A common approach is
to first rank genes according to an hypothesis of interest within each study.
Then, lists of the top-ranked genes within each study are compared across
studies. Genes recaptured as highly ranked (usually above some threshold) in
multiple studies are considered to be significant. However, this comparison
strategy often remains informal, in that type I error and false discovery rate
(FDR) are usually uncontrolled. In this paper, we formalize an inferential
strategy for this kind of list-intersection discovery test. We show how to
compute a $p$-value associated with a ""recaptured"" set of genes, using a
closed-form Poisson approximation to the distribution of the size of the
recaptured set. We investigate operating characteristics of the test as a
function of the total number of studies considered, the rank threshold within
each study, and the number of studies within which a gene must be recaptured to
be declared significant. We investigate the trade off between FDR control and
expected sensitivity (the expected proportion of true-positive genes identified
as significant). We give practical guidance on how to design a bioinformatic
list-intersection study with maximal expected sensitivity and prespecified
control of type I error (at the set level) and false discovery rate (at the
gene level). We show how optimal choice of parameters may depend on particular
alternative hypothesis which might hold. We illustrate our methods using
prostate cancer gene-expression datasets from the curated Oncomine database,
and discuss the effects of dependence between genes on the test.
"
"  This paper focuses on methodological approaches for characterising the
specific topics within a technological field based on scientific literature
data. We introduce a diachronic clustering analysis approach and some
bibliometric indicators. The results are visualised with the software-tool
Stanalyst [1]. We are applying our methods to the field ""Molecular Biology"".
This field has grown a great deal in the last decade.
"
"  Model selection is a crucial issue in machine-learning and a wide variety of
penalisation methods (with possibly data dependent complexity penalties) have
recently been introduced for this purpose. However their empirical performance
is generally not well documented in the literature. It is the goal of this
paper to investigate to which extent such recent techniques can be successfully
used for the tuning of both the regularisation and kernel parameters in support
vector regression (SVR) and the complexity measure in regression trees (CART).
This task is traditionally solved via V-fold cross-validation (VFCV), which
gives efficient results for a reasonable computational cost. A disadvantage
however of VFCV is that the procedure is known to provide an asymptotically
suboptimal risk estimate as the number of examples tends to infinity. Recently,
a penalisation procedure called V-fold penalisation has been proposed to
improve on VFCV, supported by theoretical arguments. Here we report on an
extensive set of experiments comparing V-fold penalisation and VFCV for
SVR/CART calibration on several benchmark datasets. We highlight cases in which
VFCV and V-fold penalisation provide poor estimates of the risk respectively
and introduce a modified penalisation technique to reduce the estimation error.
"
"  Implementations of quantum key distribution as available nowadays suffer from
inefficiencies due to post processing of the raw key that severely cuts down
the final secure key rate. We present a simple model for the error scattering
across the raw key and derive ""closed form"" expressions for the probability of
a parity check failure, or experiencing more than some fixed number of errors.
Our results can serve for improvement for key establishment, as information
reconciliation via interactive error correction and privacy amplification rests
on mostly unproven assumptions. We support those hypotheses on statistical
grounds.
"
"  Truncated Singular Value Decomposition (SVD) calculates the closest rank-$k$
approximation of a given input matrix. Selecting the appropriate rank $k$
defines a critical model order choice in most applications of SVD. To obtain a
principled cut-off criterion for the spectrum, we convert the underlying
optimization problem into a noisy channel coding problem. The optimal
approximation capacity of this channel controls the appropriate strength of
regularization to suppress noise. In simulation experiments, this information
theoretic method to determine the optimal rank competes with state-of-the art
model selection techniques.
"
"  A quarter-century of statistical research has shown that census coverage
surveys, valuable as they are in offering a report card on each decennial
census, do not provide usable estimates of geographical differences in
coverage. The determining reason is the large number of ``doubly missing''
people missing both from the census enumeration and from coverage survey
estimates. Future coverage surveys should be designed to meet achievable goals,
foregoing efforts at spatial specificity. One implication is a sample size no
more than about $30,000$, setting free resources for controlling processing
errors and investing in coverage improvement. Possible integration of coverage
measurement with the American Community Survey would have many benefits and
should be given careful consideration.
"
"  Graph construction is a crucial step in spectral clustering (SC) and
graph-based semi-supervised learning (SSL). Spectral methods applied on
standard graphs such as full-RBF, $\epsilon$-graphs and $k$-NN graphs can lead
to poor performance in the presence of proximal and unbalanced data. This is
because spectral methods based on minimizing RatioCut or normalized cut on
these graphs tend to put more importance on balancing cluster sizes over
reducing cut values. We propose a novel graph construction technique and show
that the RatioCut solution on this new graph is able to handle proximal and
unbalanced data. Our method is based on adaptively modulating the neighborhood
degrees in a $k$-NN graph, which tends to sparsify neighborhoods in low density
regions. Our method adapts to data with varying levels of unbalancedness and
can be naturally used for small cluster detection. We justify our ideas through
limit cut analysis. Unsupervised and semi-supervised experiments on synthetic
and real data sets demonstrate the superiority of our method.
"
"  The sum-product or belief propagation (BP) algorithm is a widely used
message-passing technique for computing approximate marginals in graphical
models. We introduce a new technique, called stochastic orthogonal series
message-passing (SOSMP), for computing the BP fixed point in models with
continuous random variables. It is based on a deterministic approximation of
the messages via orthogonal series expansion, and a stochastic approximation
via Monte Carlo estimates of the integral updates of the basis coefficients. We
prove that the SOSMP iterates converge to a \delta-neighborhood of the unique
BP fixed point for any tree-structured graph, and for any graphs with cycles in
which the BP updates satisfy a contractivity condition. In addition, we
demonstrate how to choose the number of basis coefficients as a function of the
desired approximation accuracy \delta and smoothness of the compatibility
functions. We illustrate our theory with both simulated examples and in
application to optical flow estimation.
"
"  I introduce Forecastable Component Analysis (ForeCA), a novel dimension
reduction technique for temporally dependent signals. Based on a new
forecastability measure, ForeCA finds an optimal transformation to separate a
multivariate time series into a forecastable and an orthogonal white noise
space. I present a converging algorithm with a fast eigenvector solution.
Applications to financial and macro-economic time series show that ForeCA can
successfully discover informative structure, which can be used for forecasting
as well as classification. The R package ForeCA
(http://cran.r-project.org/web/packages/ForeCA/index.html) accompanies this
work and is publicly available on CRAN.
"
"  Multi-task learning is a learning paradigm which seeks to improve the
generalization performance of a learning task with the help of some other
related tasks. In this paper, we propose a regularization formulation for
learning the relationships between tasks in multi-task learning. This
formulation can be viewed as a novel generalization of the regularization
framework for single-task learning. Besides modeling positive task correlation,
our method, called multi-task relationship learning (MTRL), can also describe
negative task correlation and identify outlier tasks based on the same
underlying principle. Under this regularization framework, the objective
function of MTRL is convex. For efficiency, we use an alternating method to
learn the optimal model parameters for each task as well as the relationships
between tasks. We study MTRL in the symmetric multi-task learning setting and
then generalize it to the asymmetric setting as well. We also study the
relationships between MTRL and some existing multi-task learning methods.
Experiments conducted on a toy problem as well as several benchmark data sets
demonstrate the effectiveness of MTRL.
"
"  A statistical framework is introduced for a broad class of problems involving
synchronization or registration of data across a sensor network in the presence
of noise. This framework enables an estimation-theoretic approach to the design
and characterization of synchronization algorithms. The Fisher information is
expressed in terms of the distribution of the measurement noise and standard
mathematical descriptors of the network's graph structure for several important
cases. This leads to maximum likelihood and approximate maximum-likelihood
registration algorithms and also to distributed iterative algorithms that, when
they converge, attain statistically optimal solutions. The relationship between
optimal estimation in this setting and Kirchhoff's laws is also elucidated.
"
"  Separation of the sources and analysis of their connectivity have been an
important topic in EEG/MEG analysis. To solve this problem in an automatic
manner, we propose a two-layer model, in which the sources are conditionally
uncorrelated from each other, but not independent; the dependence is caused by
the causality in their time-varying variances (envelopes). The model is
identified in two steps. We first propose a new source separation technique
which takes into account the autocorrelations (which may be time-varying) and
time-varying variances of the sources. The causality in the envelopes is then
discovered by exploiting a special kind of multivariate GARCH (generalized
autoregressive conditional heteroscedasticity) model. The resulting causal
diagram gives the effective connectivity between the separated sources; in our
experimental results on MEG data, sources with similar functions are grouped
together, with negative influences between groups, and the groups are connected
via some interesting sources.
"
"  The central problem of concern to Serrano, Boguna and Vespignani (""Extracting
the multiscale backbone of complex weighted networks"", Proc Natl Acad Sci
106:6483-6488 [2009]) can be effectively and elegantly addressed using a
well-established two-stage algorithm that has been applied to internal
migration flows for numerous nations and several other forms of ""transaction
flow data"".
"
"  Dynamic networks models describe a growing number of important scientific
processes, from cell biology and epidemiology to sociology and finance. There
are many aspects of dynamical networks that require statistical considerations.
In this paper we focus on determining network structure. Estimating dynamic
networks is a difficult task since the number of components involved in the
system is very large. As a result, the number of parameters to be estimated is
bigger than the number of observations. However, a characteristic of many
networks is that they are sparse. For example, the molecular structure of genes
make interactions with other components a highly-structured and therefore
sparse process.
  Penalized Gaussian graphical models have been used to estimate sparse
networks. However, the literature has focussed on static networks, which lack
specific temporal constraints. We propose a structured Gaussian dynamical
graphical model, where structures can consist of specific time dynamics, known
presence or absence of links and block equality constraints on the parameters.
Thus, the number of parameters to be estimated is reduced and accuracy of the
estimates, including the identification of the network, can be tuned up. Here,
we show that the constrained optimization problem can be solved by taking
advantage of an efficient solver, logdetPPA, developed in convex optimization.
Moreover, model selection methods for checking the sensitivity of the inferred
networks are described. Finally, synthetic and real data illustrate the
proposed methodologies.
"
"  We present a distributed proximal-gradient method for optimizing the average
of convex functions, each of which is the private local objective of an agent
in a network with time-varying topology. The local objectives have distinct
differentiable components, but they share a common nondifferentiable component,
which has a favorable structure suitable for effective computation of the
proximal operator. In our method, each agent iteratively updates its estimate
of the global minimum by optimizing its local objective function, and
exchanging estimates with others via communication in the network. Using
Nesterov-type acceleration techniques and multiple communication steps per
iteration, we show that this method converges at the rate 1/k (where k is the
number of communication rounds between the agents), which is faster than the
convergence rate of the existing distributed methods for solving this problem.
The superior convergence rate of our method is also verified by numerical
experiments.
"
"  We study a norm for structured sparsity which leads to sparse linear
predictors whose supports are unions of prede ned overlapping groups of
variables. We call the obtained formulation latent group Lasso, since it is
based on applying the usual group Lasso penalty on a set of latent variables. A
detailed analysis of the norm and its properties is presented and we
characterize conditions under which the set of groups associated with latent
variables are correctly identi ed. We motivate and discuss the delicate choice
of weights associated to each group, and illustrate this approach on simulated
data and on the problem of breast cancer prognosis from gene expression data.
"
"  A collaborative convex framework for factoring a data matrix $X$ into a
non-negative product $AS$, with a sparse coefficient matrix $S$, is proposed.
We restrict the columns of the dictionary matrix $A$ to coincide with certain
columns of the data matrix $X$, thereby guaranteeing a physically meaningful
dictionary and dimensionality reduction. We use $l_{1,\infty}$ regularization
to select the dictionary from the data and show this leads to an exact convex
relaxation of $l_0$ in the case of distinct noise free data. We also show how
to relax the restriction-to-$X$ constraint by initializing an alternating
minimization approach with the solution of the convex model, obtaining a
dictionary close to but not necessarily in $X$. We focus on applications of the
proposed framework to hyperspectral endmember and abundances identification and
also show an application to blind source separation of NMR data.
"
"  Given a matrix M of low-rank, we consider the problem of reconstructing it
from noisy observations of a small, random subset of its entries. The problem
arises in a variety of applications, from collaborative filtering (the `Netflix
problem') to structure-from-motion and positioning. We study a low complexity
algorithm introduced by Keshavan et al.(2009), based on a combination of
spectral techniques and manifold optimization, that we call here OptSpace. We
prove performance guarantees that are order-optimal in a number of
circumstances.
"
"  We examine the complete dataset of baby name popularity collected by U.S.
Social Security Administration for the last 131 years (1880-2010). The ranked
baby name popularity can be fitted empirically by a piecewise function
consisting of Beta function for the high-ranking names and power-law function
for low-ranking names, but not power-law (Zipf's law) or Beta function by
itself.
"
"  Imputation of missing data in large regions of satellite imagery is necessary
when the acquired image has been damaged by shadows due to clouds, or
information gaps produced by sensor failure.
  The general approach for imputation of missing data, that could not be
considered missed at random, suggests the use of other available data. Previous
work, like local linear histogram matching, take advantage of a co-registered
older image obtained by the same sensor, yielding good results in filling
homogeneous regions, but poor results if the scenes being combined have radical
differences in target radiance due, for example, to the presence of sun glint
or snow.
  This study proposes three different alternatives for filling the data gaps.
The first two involves merging radiometric information from a lower resolution
image acquired at the same time, in the Fourier domain (Method A), and using
linear regression (Method B). The third method consider segmentation as the
main target of processing, and propose a method to fill the gaps in the map of
classes, avoiding direct imputation (Method C).
  All the methods were compared by means of a large simulation study,
evaluating performance with a multivariate response vector with four measures:
Q, RMSE, Kappa and Overall Accuracy coefficients. Difference in performance
were tested with a MANOVA mixed model design with two main effects, imputation
method and type of lower resolution extra data, and a blocking third factor
with a nested sub-factor, introduced by the real Landsat image and the
sub-images that were used. Method B proved to be the best for all criteria.
"
"  Learning the parameters of a (potentially partially observable) random field
model is intractable in general. Instead of focussing on a single optimal
parameter value we propose to treat parameters as dynamical quantities. We
introduce an algorithm to generate complex dynamics for parameters and (both
visible and hidden) state vectors. We show that under certain conditions
averages computed over trajectories of the proposed dynamical system converge
to averages computed over the data. Our ""herding dynamics"" does not require
expensive operations such as exponentiation and is fully deterministic.
"
"  In this paper we introduce a novel method to conduct inference with models
defined through a continuous-time Markov process, and we apply these results to
a classical stochastic SIR model as a case study. Using the inverse-size
expansion of van Kampen we obtain approximations for first and second moments
for the state variables. These approximate moments are in turn matched to the
moments of an inputed generic discrete distribution aimed at generating an
approximate likelihood that is valid both for low count or high count data. We
conduct a full Bayesian inference to estimate epidemic parameters using
informative priors. Excellent estimations and predictions are obtained both in
a synthetic data scenario and in two Dengue fever case studies.
"
"  The structure of a Bayesian network includes a great deal of information
about the probability distribution of the data, which is uniquely identified
given some general distributional assumptions. Therefore it's important to
study its variability, which can be used to compare the performance of
different learning algorithms and to measure the strength of any arbitrary
subset of arcs.
  In this paper we will introduce some descriptive statistics and the
corresponding parametric and Monte Carlo tests on the undirected graph
underlying the structure of a Bayesian network, modeled as a multivariate
Bernoulli random variable. A simple numeric example and the comparison of the
performance of some structure learning algorithm on small samples will then
illustrate their use.
"
"  The motivation for this paper is to apply Bayesian structure learning using
Model Averaging in large-scale networks. Currently, Bayesian model averaging
algorithm is applicable to networks with only tens of variables, restrained by
its super-exponential complexity. We present a novel framework, called
LSBN(Large-Scale Bayesian Network), making it possible to handle networks with
infinite size by following the principle of divide-and-conquer. The method of
LSBN comprises three steps. In general, LSBN first performs the partition by
using a second-order partition strategy, which achieves more robust results.
LSBN conducts sampling and structure learning within each overlapping community
after the community is isolated from other variables by Markov Blanket. Finally
LSBN employs an efficient algorithm, to merge structures of overlapping
communities into a whole. In comparison with other four state-of-art
large-scale network structure learning algorithms such as ARACNE, PC, Greedy
Search and MMHC, LSBN shows comparable results in five common benchmark
datasets, evaluated by precision, recall and f-score. What's more, LSBN makes
it possible to learn large-scale Bayesian structure by Model Averaging which
used to be intractable. In summary, LSBN provides an scalable and parallel
framework for the reconstruction of network structures. Besides, the complete
information of overlapping communities serves as the byproduct, which could be
used to mine meaningful clusters in biological networks, such as
protein-protein-interaction network or gene regulatory network, as well as in
social network.
"
"  The effect of vigorous physical activity on mortality in the elderly is
difficult to estimate using conventional approaches to causal inference that
define this effect by comparing the mortality risks corresponding to
hypothetical scenarios in which all subjects in the target population engage in
a given level of vigorous physical activity. A causal effect defined on the
basis of such a static treatment intervention can only be identified from
observed data if all subjects in the target population have a positive
probability of selecting each of the candidate treatment options, an assumption
that is highly unrealistic in this case since subjects with serious health
problems will not be able to engage in higher levels of vigorous physical
activity. This problem can be addressed by focusing instead on causal effects
that are defined on the basis of realistic individualized treatment rules and
intention-to-treat rules that explicitly take into account the set of treatment
options that are available to each subject. We present a data analysis to
illustrate that estimators of static causal effects in fact tend to
overestimate the beneficial impact of high levels of vigorous physical activity
while corresponding estimators based on realistic individualized treatment
rules and intention-to-treat rules can yield unbiased estimates. We emphasize
that the problems encountered in estimating static causal effects are not
restricted to the IPTW estimator, but are also observed with the
$G$-computation estimator, the DR-IPTW estimator, and the targeted MLE. Our
analyses based on realistic individualized treatment rules and
intention-to-treat rules suggest that high levels of vigorous physical activity
may confer reductions in mortality risk on the order of 15-30%, although in
most cases the evidence for such an effect does not quite reach the 0.05 level
of significance.
"
"  Successful attempts to predict judges' votes shed light into how legal
decisions are made and, ultimately, into the behavior and evolution of the
judiciary. Here, we investigate to what extent it is possible to make
predictions of a justice's vote based on the other justices' votes in the same
case. For our predictions, we use models and methods that have been developed
to uncover hidden associations between actors in complex social networks. We
show that these methods are more accurate at predicting justice's votes than
forecasts made by legal experts and by algorithms that take into consideration
the content of the cases. We argue that, within our framework, high
predictability is a quantitative proxy for stable justice (and case) blocks,
which probably reflect stable a priori attitudes toward the law. We find that
U. S. Supreme Court justice votes are more predictable than one would expect
from an ideal court composed of perfectly independent justices. Deviations from
ideal behavior are most apparent in divided 5-4 decisions, where justice blocks
seem to be most stable. Moreover, we find evidence that justice predictability
decreased during the 50-year period spanning from the Warren Court to the
Rehnquist Court, and that aggregate court predictability has been significantly
lower during Democratic presidencies. More broadly, our results show that it is
possible to use methods developed for the analysis of complex social networks
to quantitatively investigate historical questions related to political
decision-making.
"
"  The paper focuses on the adaptation of local polynomial filters at the end of
the sample period. We show that for real time estimation of signals (i.e.,
exactly at the boundary of the time support) we cannot rely on the automatic
adaptation of the local polynomial smoothers, since the direct real time filter
turns out to be strongly localized, and thereby yields extremely volatile
estimates. As an alternative, we evaluate a general family of asymmetric
filters that minimizes the mean square revision error subject to polynomial
reproduction constraints; in the case of the Henderson filter it nests the
well-known Musgrave's surrogate filters. The class of filters depends on
unknown features of the series such as the slope and the curvature of the
underlying signal, which can be estimated from the data. Several empirical
examples illustrate the effectiveness of our proposal.
"
"  Chordal graphs can be used to encode dependency models that are representable
by both directed acyclic and undirected graphs. This paper discusses a very
simple and efficient algorithm to learn the chordal structure of a
probabilistic model from data. The algorithm is a greedy hill-climbing search
algorithm that uses the inclusion boundary neighborhood over chordal graphs. In
the limit of a large sample size and under appropriate hypotheses on the
scoring criterion, we prove that the algorithm will find a structure that is
inclusion-optimal when the dependency model of the data-generating distribution
can be represented exactly by an undirected graph. The algorithm is evaluated
on simulated datasets.
"
"  This paper considers the recovery of a low-rank matrix from an observed
version that simultaneously contains both (a) erasures: most entries are not
observed, and (b) errors: values at a constant fraction of (unknown) locations
are arbitrarily corrupted. We provide a new unified performance guarantee on
when the natural convex relaxation of minimizing rank plus support succeeds in
exact recovery. Our result allows for the simultaneous presence of random and
deterministic components in both the error and erasure patterns. On the one
hand, corollaries obtained by specializing this one single result in different
ways recover (up to poly-log factors) all the existing works in matrix
completion, and sparse and low-rank matrix recovery. On the other hand, our
results also provide the first guarantees for (a) recovery when we observe a
vanishing fraction of entries of a corrupted matrix, and (b) deterministic
matrix completion.
"
"  In 1980, a burial tomb was unearthed in Jerusalem containing ossuaries
(limestone coffins) bearing such inscriptions as Yeshua son of Yehosef, Marya,
Yoseh--names which match those of New Testament (NT) figures, but were
otherwise in common use. This paper discusses certain statistical aspects of
authenticating or repudiating links between this find and the NT family. The
available data are laid out, and we examine the distribution of names
(onomasticon) of the era. An approach is proposed for measuring the
``surprisingness'' of the observed outcome relative to a ``hypothesis'' that
the tombsite belonged to the NT family. On the basis of a particular--but far
from uncontested--set of assumptions, our measure of ``surprisingness'' is
significantly high.
"
"  As increasing amounts of sensitive personal information is aggregated into
data repositories, it has become important to develop mechanisms for processing
the data without revealing information about individual data instances. The
differential privacy model provides a framework for the development and
theoretical analysis of such mechanisms. In this paper, we propose an algorithm
for learning a discriminatively trained multi-class Gaussian classifier that
satisfies differential privacy using a large margin loss function with a
perturbed regularization term. We present a theoretical upper bound on the
excess risk of the classifier introduced by the perturbation.
"
"  We consider the problem of estimating the inverse covariance matrix by
maximizing the likelihood function with a penalty added to encourage the
sparsity of the resulting matrix. We propose a new approach based on the split
Bregman method to solve the regularized maximum likelihood estimation problem.
We show that our method is significantly faster than the widely used graphical
lasso method, which is based on blockwise coordinate descent, on both
artificial and real-world data. More importantly, different from the graphical
lasso, the split Bregman based method is much more general, and can be applied
to a class of regularization terms other than the $\ell_1$ norm
"
"  This paper introduces an iterative tomogravity algorithm for the estimation
of a network traffic matrix based on one snapshot observation of the link loads
in the network. The proposed method does not require complete observation of
the total load on individual edge links or proper tuning of a penalty parameter
as existing methods do. Numerical results are presented to demonstrate that the
iterative tomogravity method controls the estimation error well when the link
data is fully observed and produces robust results with moderate amount of
missing link data.
"
"  Conventional, parametric multinomial logit models are in general not
sufficient for detecting the complex patterns voter profiles nowadays typically
exhibit. In this manuscript, we use a semiparametric multinomial logit model to
give a detailed analysis of the composition of a subsample of the German
electorate in 2006. Germany is a particularly strong case for more flexible
nonparametric approaches in this context, since due to the reunification and
the preceding different political histories the composition of the electorate
is very complex and nuanced. Our analysis reveals strong interactions of the
covariates age and income, and highly nonlinear shapes of the factor impacts
for each party's likelihood to be voted. Notably, we develop and provide a
smoothed likelihood estimator for semiparametric multinomial logit models,
which can be applied also in other application fields, such as, e.g.,
marketing.
"
"  Portfolio managers are typically constrained by turnover limits, minimum and
maximum stock positions, cardinality, a target market capitalization and
sometimes the need to hew to a style (such as growth or value). In addition,
portfolio managers often use multifactor stock models to choose stocks based
upon their respective fundamental data.
  We use multiobjective evolutionary algorithms (MOEAs) to satisfy the above
real-world constraints. The portfolios generated consistently outperform
typical performance benchmarks and have statistically significant asset
selection.
"
"  Familial Searching is the process of searching in a DNA database for
relatives of a certain individual. It is well known that in order to evaluate
the genetic evidence in favour of a certain given form of relatedness between
two individuals, one needs to calculate the appropriate likelihood ratio, which
is in this context called a Kinship Index. Suppose that the database contains,
for a given type of relative, at most one related individual. Given prior
probabilities for being the relative for all persons in the database, we derive
the likelihood ratio for each database member in favour of being that relative.
This likelihood ratio takes all the Kinship Indices between the target
individual and the members of the database into account. We also compute the
corresponding posterior probabilities. We then discuss two methods to select a
subset from the database that contains the relative with a known probability,
or at least a useful lower bound thereof. One method needs prior probabilities
and yields posterior probabilities, the other does not. We discuss the relation
between the approaches, and illustrate the methods with familial searching
carried out in the Dutch National DNA Database.
"
"  A recent way to model and analyze downlink cellular networks is by using
random spatial models. Assuming user equipment (UE) distribution to be uniform,
the analysis is performed at a typical UE located at the origin. While this
method of sampling UEs provides statistics averaged over the UE locations, it
is not possible to sample cell interior and cell edge UEs separately. This
complicates the problem of analyzing deployment scenarios involving non-uniform
distribution of UEs, especially when the locations of the UEs and the base
stations (BSs) are dependent. To facilitate this separation, we propose a new
tractable method of sampling UEs by conditionally thinning the BS point process
and show that the resulting framework can be used as a tractable generative
model to study cellular networks with non-uniform UE distribution.
"
"  Nonlinear stochastic differential equation models with unobservable variables
are now widely used in the analysis of PK/PD data. The unobservable variables
are often estimated with extended Kalman filter (EKF), and the unknown
pharmacokinetic parameters are usually estimated by maximum likelihood
estimator. However, EKF is inadequate for nonlinear PK/PD models, and MLE is
known to be biased downwards. A density-based Monte Carlo filter (DMF) is
proposed to estimate the unobservable variables, and a simulation-based
procedure is proposed to estimate the unknown parameters in this paper, where a
genetic algorithm is designed to search the optimal values of pharmacokinetic
parameters. The performances of EKF and DMF are compared through simulations,
and it is found that the results based on DMF are more accurate than those
given by EKF with respect to mean absolute error.
"
"  The object of this paper is a one-dimensional generalized porous media
equation (PDE) with possibly discontinuous coefficient $\beta$, which is
well-posed as an evolution problem in $L^1(\mathbb{R})$. In some recent papers
of Blanchard et alia and Barbu et alia, the solution was represented by the
solution of a non-linear stochastic differential equation in law if the initial
condition is a bounded integrable function. We first extend this result, at
least when $\beta$ is continuous and the initial condition is only integrable
with some supplementary technical assumption. The main purpose of the article
consists in introducing and implementing a stochastic particle algorithm to
approach the solution to (PDE) which also fits in the case when $\beta$ is
possibly irregular, to predict some long-time behavior of the solution and in
comparing with some recent numerical deterministic techniques.
"
"  We formulate and prove an axiomatic characterization of conditional
information geometry, for both the normalized and the nonnormalized cases. This
characterization extends the axiomatic derivation of the Fisher geometry by
Cencov and Campbell to the cone of positive conditional models, and as a
special case to the manifold of conditional distributions. Due to the close
connection between the conditional I-divergence and the product Fisher
information metric the characterization provides a new axiomatic interpretation
of the primal problems underlying logistic regression and AdaBoost.
"
"  Nonnegative matrix factorization (NMF) is now a common tool for audio source
separation. When learning NMF on large audio databases, one major drawback is
that the complexity in time is O(FKN) when updating the dictionary (where (F;N)
is the dimension of the input power spectrograms, and K the number of basis
spectra), thus forbidding its application on signals longer than an hour. We
provide an online algorithm with a complexity of O(FK) in time and memory for
updates in the dictionary. We show on audio simulations that the online
approach is faster for short audio signals and allows to analyze audio signals
of several hours.
"
"  In chemical analysis made by laboratories one has the problem of determining
the concentration of a chemical element in a sample. In order to tackle this
problem the guide EURACHEM/CITAC recommends the application of the linear
calibration model, so implicitly assume that there is no measurement error in
the independent variable $X$. In this work, it is proposed a new calibration
model assuming that the independent variable is controlled. This assumption is
appropriate in chemical analysis where the process tempting to attain the fixed
known value $X$ generates an error and the resulting value is $x$, which is not
an observable. However, observations on its surrogate $X$ are available. A
simulation study is carried out in order to verify some properties of the
estimators derived for the new model and it is also considered the usual
calibration model to compare it with the new approach. Three applications are
considered to verify the performance of the new approach.
"
"  EDML is a recently proposed algorithm for learning MAP parameters in Bayesian
networks. In this paper, we present a number of new advances and insights on
the EDML algorithm. First, we provide the multivalued extension of EDML,
originally proposed for Bayesian networks over binary variables. Next, we
identify a simplified characterization of EDML that further implies a simple
fixed-point algorithm for the convex optimization problem that underlies it.
This characterization further reveals a connection between EDML and EM: a fixed
point of EDML is a fixed point of EM, and vice versa. We thus identify also a
new characterization of EM fixed points, but in the semantics of EDML. Finally,
we propose a hybrid EDML/EM algorithm that takes advantage of the improved
empirical convergence behavior of EDML, while maintaining the monotonic
improvement property of EM.
"
"  Group therapy is a central treatment modality for behavioral health disorders
such as alcohol and other drug use (AOD) and depression. Group therapy is often
delivered under a rolling (or open) admissions policy, where new clients are
continuously enrolled into a group as space permits. Rolling admissions
policies result in a complex correlation structure among client outcomes.
Despite the ubiquity of rolling admissions in practice, little guidance on the
analysis of such data is available. We discuss the limitations of previously
proposed approaches in the context of a study that delivered group cognitive
behavioral therapy for depression to clients in residential substance abuse
treatment. We improve upon previous rolling group analytic approaches by fully
modeling the interrelatedness of client depressive symptom scores using a
hierarchical Bayesian model that assumes a conditionally autoregressive prior
for session-level random effects. We demonstrate improved performance using our
method for estimating the variance of model parameters and the enhanced ability
to learn about the complex correlation structure among participants in rolling
therapy groups. Our approach broadly applies to any group therapy setting where
groups have changing client composition. It will lead to more efficient
analyses of client-level data and improve the group therapy research
community's ability to understand how the dynamics of rolling groups lead to
client outcomes.
"
"  In linear regression problems with related predictors, it is desirable to do
variable selection and estimation by maintaining the hierarchical or structural
relationships among predictors. In this paper we propose non-negative garrote
methods that can naturally incorporate such relationships defined through
effect heredity principles or marginality principles. We show that the methods
are very easy to compute and enjoy nice theoretical properties. We also show
that the methods can be easily extended to deal with more general regression
problems such as generalized linear models. Simulations and real examples are
used to illustrate the merits of the proposed methods.
"
"  In this paper we study convex stochastic search problems where a noisy
objective function value is observed after a decision is made. There are many
stochastic search problems whose behavior depends on an exogenous state
variable which affects the shape of the objective function. Currently, there is
no general purpose algorithm to solve this class of problems. We use
nonparametric density estimation to take observations from the joint
state-outcome distribution and use them to infer the optimal decision for a
given query state. We propose two solution methods that depend on the problem
characteristics: function-based and gradient-based optimization. We examine two
weighting schemes, kernel-based weights and Dirichlet process-based weights,
for use with the solution methods. The weights and solution methods are tested
on a synthetic multi-product newsvendor problem and the hour-ahead wind
commitment problem. Our results show that in some cases Dirichlet process
weights offer substantial benefits over kernel based weights and more generally
that nonparametric estimation methods provide good solutions to otherwise
intractable problems.
"
"  Network models have been popular for modeling and representing complex
relationships and dependencies between observed variables. When data comes from
a dynamic stochastic process, a single static network model cannot adequately
capture transient dependencies, such as, gene regulatory dependencies
throughout a developmental cycle of an organism. Kolar et al (2010b) proposed a
method based on kernel-smoothing l1-penalized logistic regression for
estimating time-varying networks from nodal observations collected from a
time-series of observational data. In this paper, we establish conditions under
which the proposed method consistently recovers the structure of a time-varying
network. This work complements previous empirical findings by providing sound
theoretical guarantees for the proposed estimation procedure. For completeness,
we include numerical simulations in the paper.
"
"  An important field of research in functional neuroimaging is the discovery of
integrated, distributed brain systems and networks, whose different regions
need to work in unison for normal functioning.
  The EEG is a non-invasive technique that can provide information for massive
connectivity analyses. Cortical signals of time varying electric neuronal
activity can be estimated from the EEG. Although such techniques have very high
time resolution, two cortical signals even at distant locations will appear to
be highly similar due to the low spatial resolution nature of the EEG.
  In this study a method for eliminating the effect of common sources due to
low spatial resolution is presented. It is based on an efficient estimation of
the whole-cortex partial coherence matrix. Using as a starting point any linear
EEG tomography that satisfies the EEG forward equation, it is shown that the
generalized partial coherences for the cortical grey matter current density
time series are invariant to the selected tomography. It is empirically shown
with simulation experiments that the generalized partial coherences have higher
spatial resolution than the classical coherences. The results demonstrate that
with as little as 19 electrodes, lag-connected brain regions can often be
missed and misplaced even with lagged coherence measures, while the new method
detects and localizes correctly the connected regions using the lagged partial
coherences.
"
"  In this note, we focus on a selection model problem: a mono-exponential model
versus a bi-exponential one. This is done in the biological context of living
cells, where small data are available. Classical statistics are revisited to
improve existing results. Some unavoidable limits are also pointed out.
"
"  This paper has been withdrawn by the authors. We present a framework for
sequential decision making in problems described by graphical models. The
setting is given by dependent discrete random variables with associated costs
or revenues. In our examples, the dependent variables are the potential
outcomes (oil, gas or dry) when drilling a petroleum well. The goal is to
develop an optimal selection strategy that incorporates a chosen utility
function within an approximated dynamic programming scheme. We propose and
compare different approximations, from simple heuristics to more complex
iterative schemes, and we discuss their computational properties. We apply our
strategies to oil exploration over multiple prospects modeled by a directed
acyclic graph, and to a reservoir drilling decision problem modeled by a Markov
random field. The results show that the suggested strategies clearly improve
the simpler intuitive constructions, and this is useful when selecting
exploration policies.
"
"  We derive expressions for the predicitive information rate (PIR) for the
class of autoregressive Gaussian processes AR(N), both in terms of the
prediction coefficients and in terms of the power spectral density. The latter
result suggests a duality between the PIR and the multi-information rate for
processes with mutually inverse power spectra (i.e. with poles and zeros of the
transfer function exchanged). We investigate the behaviour of the PIR in
relation to the multi-information rate for some simple examples, which suggest,
somewhat counter-intuitively, that the PIR is maximised for very `smooth' AR
processes whose power spectra have multiple poles at zero frequency. We also
obtain results for moving average Gaussian processes which are consistent with
the duality conjectured earlier. One consequence of this is that the PIR is
unbounded for MA(N) processes.
"
"  Within the framework of statistical learning theory we analyze in detail the
so-called elastic-net regularization scheme proposed by Zou and Hastie for the
selection of groups of correlated variables. To investigate on the statistical
properties of this scheme and in particular on its consistency properties, we
set up a suitable mathematical framework. Our setting is random-design
regression where we allow the response variable to be vector-valued and we
consider prediction functions which are linear combination of elements ({\em
features}) in an infinite-dimensional dictionary. Under the assumption that the
regression function admits a sparse representation on the dictionary, we prove
that there exists a particular ``{\em elastic-net representation}'' of the
regression function such that, if the number of data increases, the elastic-net
estimator is consistent not only for prediction but also for variable/feature
selection. Our results include finite-sample bounds and an adaptive scheme to
select the regularization parameter. Moreover, using convex analysis tools, we
derive an iterative thresholding algorithm for computing the elastic-net
solution which is different from the optimization procedure originally proposed
by Zou and Hastie
"
"  We consider the problem of training probabilistic conditional random fields
(CRFs) in the context of a task where performance is measured using a specific
loss function. While maximum likelihood is the most common approach to training
CRFs, it ignores the inherent structure of the task's loss function. We
describe alternatives to maximum likelihood which take that loss into account.
These include a novel adaptation of a loss upper bound from the structured SVMs
literature to the CRF context, as well as a new loss-inspired KL divergence
objective which relies on the probabilistic nature of CRFs. These
loss-sensitive objectives are compared to maximum likelihood using ranking as a
benchmark task. This comparison confirms the importance of incorporating loss
information in the probabilistic training of CRFs, with the loss-inspired KL
outperforming all other objectives.
"
"  Nonparametric estimation of the gap time distribution in a simple renewal
process may be considered a problem in survival analysis under particular
sampling frames corresponding to how the renewal process is observed. This note
describes several such situations where simple product limit estimators, though
inefficient, may still be useful.
"
"  Probabilistic inference in graphical models is the task of computing marginal
and conditional densities of interest from a factorized representation of a
joint probability distribution. Inference algorithms such as variable
elimination and belief propagation take advantage of constraints embedded in
this factorization to compute such densities efficiently. In this paper, we
propose an algorithm which computes interventional distributions in latent
variable causal models represented by acyclic directed mixed graphs(ADMGs). To
compute these distributions efficiently, we take advantage of a recursive
factorization which generalizes the usual Markov factorization for DAGs and the
more recent factorization for ADMGs. Our algorithm can be viewed as a
generalization of variable elimination to the mixed graph case. We show our
algorithm is exponential in the mixed graph generalization of treewidth.
"
"  For many decades, ultrahigh energy charged particles of unknown origin that
can be observed from the ground have been a puzzle for particle physicists and
astrophysicists. As an attempt to discriminate among several possible
production scenarios, astrophysicists try to test the statistical isotropy of
the directions of arrival of these cosmic rays. At the highest energies, they
are supposed to point toward their sources with good accuracy. However, the
observations are so rare that testing the distribution of such samples of
directional data on the sphere is nontrivial. In this paper, we choose a
nonparametric framework that makes weak hypotheses on the alternative
distributions and allows in turn to detect various and possibly unexpected
forms of anisotropy. We explore two particular procedures. Both are derived
from fitting the empirical distribution with wavelet expansions of densities.
We use the wavelet frame introduced by [SIAM J. Math. Anal. 38 (2006b) 574-594
(electronic)], the so-called needlets. The expansions are truncated at scale
indices no larger than some ${J^{\star}}$, and the $L^p$ distances between
those estimates and the null density are computed. One family of tests (called
Multiple) is based on the idea of testing the distance from the null for each
choice of $J=1,\ldots,{J^{\star}}$, whereas the so-called PlugIn approach is
based on the single full ${J^{\star}}$ expansion, but with thresholded wavelet
coefficients. We describe the practical implementation of these two procedures
and compare them to other methods in the literature. As alternatives to
isotropy, we consider both very simple toy models and more realistic
nonisotropic models based on Physics-inspired simulations. The Monte Carlo
study shows good performance of the Multiple test, even at moderate sample
size, for a wide sample of alternative hypotheses and for different choices of
the parameter ${J^{\star}}$. On the 69 most energetic events published by the
Pierre Auger Collaboration, the needlet-based procedures suggest statistical
evidence for anisotropy. Using several values for the parameters of the
methods, our procedures yield $p$-values below 1%, but with uncontrolled
multiplicity issues. The flexibility of this method and the possibility to
modify it to take into account a large variety of extensions of the problem
make it an interesting option for future investigation of the origin of
ultrahigh energy cosmic rays.
"
"  We present and analyse three online algorithms for learning in discrete
Hidden Markov Models (HMMs) and compare them with the Baldi-Chauvin Algorithm.
Using the Kullback-Leibler divergence as a measure of generalisation error we
draw learning curves in simplified situations. The performance for learning
drifting concepts of one of the presented algorithms is analysed and compared
with the Baldi-Chauvin algorithm in the same situations. A brief discussion
about learning and symmetry breaking based on our results is also presented.
"
"  Dyadic data are common in the social and behavioral sciences, in which
members of dyads are correlated due to the interdependence structure within
dyads. The analysis of longitudinal dyadic data becomes complex when
nonignorable dropouts occur. We propose a fully Bayesian selection-model-based
approach to analyze longitudinal dyadic data with nonignorable dropouts. We
model repeated measures on subjects by a transition model and account for
within-dyad correlations by random effects. In the model, we allow subject's
outcome to depend on his/her own characteristics and measure history, as well
as those of the other member in the dyad. We further account for the
nonignorable missing data mechanism using a selection model in which the
probability of dropout depends on the missing outcome. We propose a Gibbs
sampler algorithm to fit the model. Simulation studies show that the proposed
method effectively addresses the problem of nonignorable dropouts. We
illustrate our methodology using a longitudinal breast cancer study.
"
"  Successful implementation of California's Renewable Portfolio Standard (RPS)
mandating 33 percent renewable energy generation by 2020 requires inclusion of
a robust strategy to mitigate increased risk of energy deficits (blackouts) due
to short time-scale (sub 1 hour) intermittencies in renewable energy sources.
Of these RPS sources, wind energy has the fastest growth rate--over 25%
year-over-year. If these growth trends continue, wind energy could make up 15
percent of California's energy portfolio by 2016 (wRPS15). However, the
hour-to-hour variations in wind energy (speed) will create large hourly energy
deficits that require installation of other, more predictable, compensation
generation capacity and infrastructure. Compensating for the energy deficits of
wRPS15 could potentially cost tens of billions in additional dollar-expenditure
for fossil and / or nuclear generation capacity. There is a real possibility
that carbon dioxide and other greenhouse gas (GHG) emission reductions will
miss the California Assembly Bill 32 (CA AB 32) target by a wide margin once
the wRPS15 compensation system is in place. This work presents a set of
analytics tools that show the impact of short-term intermittencies to help
policy makers understand and plan for wRPS15 integration. What are the right
policy choices for RPS that include wind energy?
"
"  We examine the problem of optimal design in the context of filtering multiple
random walks. Specifically, we define the steady state E-optimal design
criterion and show that the underlying optimization problem leads to a second
order cone program. The developed methodology is applied to tracking network
flow volumes using sampled data, where the design variable corresponds to
controlling the sampling rate. The optimal design is numerically compared to a
myopic and a naive strategy. Finally, we relate our work to the general problem
of steady state optimal design for state space models.
"
"  Experience is an important asset in almost any professional activity. In
basketball, there is believed to be a positive association between coaching
experience and effective use of team timeouts. Here, we analyze both the extent
to which a team's change in scoring margin per possession after timeouts
deviate from the team's average scoring margin per possession---what we called
timeout factor, and the extent to which this performance measure is associated
with coaching experience across all teams in the National Basketball
Association over the 2009-2012 seasons. We find that timeout factor plays a
minor role in the scoring dynamics of basketball. Surprisingly, we find that
timeout factor is negatively associated with coaching experience. Our findings
support empirical studies showing that, under certain conditions, mentors early
in their careers can have a stronger positive impact on their teams than later
in their careers.
"
"  Originating from a system theory and an input/output point of view, I
introduce a new class of generalized distributions. A parametric nonlinear
transformation converts a random variable $X$ into a so-called Lambert $W$
random variable $Y$, which allows a very flexible approach to model skewed
data. Its shape depends on the shape of $X$ and a skewness parameter $\gamma$.
In particular, for symmetric $X$ and nonzero $\gamma$ the output $Y$ is skewed.
Its distribution and density function are particular variants of their input
counterparts. Maximum likelihood and method of moments estimators are
presented, and simulations show that in the symmetric case additional
estimation of $\gamma$ does not affect the quality of other parameter
estimates. Applications in finance and biomedicine show the relevance of this
class of distributions, which is particularly useful for slightly skewed data.
A practical by-result of the Lambert $W$ framework: data can be ""unskewed."" The
$R$ package http://cran.r-project.org/web/packages/LambertWLambertW developed
by the author is publicly available (http://cran.r-project.orgCRAN).
"
"  Learning multiple tasks across heterogeneous domains is a challenging problem
since the feature space may not be the same for different tasks. We assume the
data in multiple tasks are generated from a latent common domain via sparse
domain transforms and propose a latent probit model (LPM) to jointly learn the
domain transforms, and the shared probit classifier in the common domain. To
learn meaningful task relatedness and avoid over-fitting in classification, we
introduce sparsity in the domain transforms matrices, as well as in the common
classifier. We derive theoretical bounds for the estimation error of the
classifier in terms of the sparsity of domain transforms. An
expectation-maximization algorithm is derived for learning the LPM. The
effectiveness of the approach is demonstrated on several real datasets.
"
"  Logistic regression has been widely applied in the field of biomedical
research for a long time. In some applications, covariates of interest have a
natural structure, such as being a matrix, at the time of collection. The rows
and columns of the covariate matrix then have certain physical meanings, and
they must contain useful information regarding the response. If we simply stack
the covariate matrix as a vector and fit the conventional logistic regression
model, relevant information can be lost, and the problem of inefficiency will
arise. Motivated from these reasons, we propose in this paper the matrix
variate logistic (MV-logistic) regression model. Advantages of MV-logistic
regression model include the preservation of the inherent matrix structure of
covariates and the parsimony of parameters needed. In the EEG Database Data
Set, we successfully extract the structural effects of covariate matrix, and a
high classification accuracy is achieved.
"
"  In this paper we discuss testing for an interaction in the two-way ANOVA with
just one observation per cell. The known results are reviewed and a simulation
study is performed to evaluate type I and type II risks of the tests. It is
shown that the Tukey and Mandel additivity tests have very low power in case of
more general interaction scheme. A modification of Tukey's test is developed to
resolve this issue. All tests mentioned in the paper have been implemented in R
package AdditivityTests.
"
"  We identify robust statistical patterns in the frequency and severity of
violent attacks by terrorist organizations as they grow and age. Using
group-level static and dynamic analyses of terrorist events worldwide from
1968-2008 and a simulation model of organizational dynamics, we show that the
production of violent events tends to accelerate with increasing size and
experience. This coupling of frequency, experience and size arises from a
fundamental positive feedback loop in which attacks lead to growth which leads
to increased production of new attacks. In contrast, event severity is
independent of both size and experience. Thus larger, more experienced
organizations are more deadly because they attack more frequently, not because
their attacks are more deadly, and large events are equally likely to come from
large and small organizations. These results hold across political ideologies
and time, suggesting that the frequency and severity of terrorism may be
constrained by fundamental processes.
"
"  Technological advances in genotyping have given rise to hypothesis-based
association studies of increasing scope. As a result, the scientific hypotheses
addressed by these studies have become more complex and more difficult to
address using existing analytic methodologies. Obstacles to analysis include
inference in the face of multiple comparisons, complications arising from
correlations among the SNPs (single nucleotide polymorphisms), choice of their
genetic parametrization and missing data. In this paper we present an efficient
Bayesian model search strategy that searches over the space of genetic markers
and their genetic parametrization. The resulting method for Multilevel
Inference of SNP Associations, MISA, allows computation of multilevel posterior
probabilities and Bayes factors at the global, gene and SNP level, with the
prior distribution on SNP inclusion in the model providing an intrinsic
multiplicity correction. We use simulated data sets to characterize MISA's
statistical power, and show that MISA has higher power to detect association
than standard procedures. Using data from the North Carolina Ovarian Cancer
Study (NCOCS), MISA identifies variants that were not identified by standard
methods and have been externally ``validated'' in independent studies. We
examine sensitivity of the NCOCS results to prior choice and method for
imputing missing data. MISA is available in an R package on CRAN.
"
"  We would like to congratulate Lee, Nadler and Wasserman on their contribution
to clustering and data reduction methods for high $p$ and low $n$ situations. A
composite of clustering and traditional principal components analysis, treelets
is an innovative method for multi-resolution analysis of unordered data. It is
an improvement over traditional PCA and an important contribution to clustering
methodology. Their paper [arXiv:0707.0481] presents theory and supporting
applications addressing the two main goals of the treelet method: (1) Uncover
the underlying structure of the data and (2) Data reduction prior to
statistical learning methods. We will organize our discussion into two main
parts to address their methodology in terms of each of these two goals. We will
present and discuss treelets in terms of a clustering algorithm and an
improvement over traditional PCA. We will also discuss the applicability of
treelets to more general data, in particular, the application of treelets to
microarray data.
"
"  Semisupervised methods are techniques for using labeled data
$(X_1,Y_1),\ldots,(X_n,Y_n)$ together with unlabeled data $X_{n+1},\ldots,X_N$
to make predictions. These methods invoke some assumptions that link the
marginal distribution $P_X$ of X to the regression function f(x). For example,
it is common to assume that f is very smooth over high density regions of
$P_X$. Many of the methods are ad-hoc and have been shown to work in specific
examples but are lacking a theoretical foundation. We provide a minimax
framework for analyzing semisupervised methods. In particular, we study methods
based on metrics that are sensitive to the distribution $P_X$. Our model
includes a parameter $\alpha$ that controls the strength of the semisupervised
assumption. We then use the data to adapt to $\alpha$.
"
"  We propose a graphical model for representing networks of stochastic
processes, the minimal generative model graph. It is based on reduced
factorizations of the joint distribution over time. We show that under
appropriate conditions, it is unique and consistent with another type of
graphical model, the directed information graph, which is based on a
generalization of Granger causality. We demonstrate how directed information
quantifies Granger causality in a particular sequential prediction setting. We
also develop efficient methods to estimate the topological structure from data
that obviate estimating the joint statistics. One algorithm assumes
upper-bounds on the degrees and uses the minimal dimension statistics
necessary. In the event that the upper-bounds are not valid, the resulting
graph is nonetheless an optimal approximation. Another algorithm uses
near-minimal dimension statistics when no bounds are known but the distribution
satisfies a certain criterion. Analogous to how structure learning algorithms
for undirected graphical models use mutual information estimates, these
algorithms use directed information estimates. We characterize the
sample-complexity of two plug-in directed information estimators and obtain
confidence intervals. For the setting when point estimates are unreliable, we
propose an algorithm that uses confidence intervals to identify the best
approximation that is robust to estimation error. Lastly, we demonstrate the
effectiveness of the proposed algorithms through analysis of both synthetic
data and real data from the Twitter network. In the latter case, we identify
which news sources influence users in the network by merely analyzing tweet
times.
"
"  We consider the problems of detection and localization of a contiguous block
of weak activation in a large matrix, from a small number of noisy, possibly
adaptive, compressive (linear) measurements. This is closely related to the
problem of compressed sensing, where the task is to estimate a sparse vector
using a small number of linear measurements. Contrary to results in compressed
sensing, where it has been shown that neither adaptivity nor contiguous
structure help much, we show that for reliable localization the magnitude of
the weakest signals is strongly influenced by both structure and the ability to
choose measurements adaptively while for detection neither adaptivity nor
structure reduce the requirement on the magnitude of the signal. We
characterize the precise tradeoffs between the various problem parameters, the
signal strength and the number of measurements required to reliably detect and
localize the block of activation. The sufficient conditions are complemented
with information theoretic lower bounds.
"
"  Kepler provides light curves of 156,000 stars with unprecedented precision.
However, the raw data as they come from the spacecraft contain significant
systematic and stochastic errors. These errors, which include discontinuities,
systematic trends, and outliers, obscure the astrophysical signals in the light
curves. To correct these errors is the task of the Presearch Data Conditioning
(PDC) module of the Kepler data analysis pipeline. The original version of PDC
in Kepler did not meet the extremely high performance requirements for the
detection of miniscule planet transits or highly accurate analysis of stellar
activity and rotation. One particular deficiency was that astrophysical
features were often removed as a side-effect to removal of errors. In this
paper we introduce the completely new and significantly improved version of PDC
which was implemented in Kepler SOC 8.0. This new PDC version, which utilizes a
Bayesian approach for removal of systematics, reliably corrects errors in the
light curves while at the same time preserving planet transits and other
astrophysically interesting signals. We describe the architecture and the
algorithms of this new PDC module, show typical errors encountered in Kepler
data, and illustrate the corrections using real light curve examples.
"
"  We propose a new sparsity-smoothness penalty for high-dimensional generalized
additive models. The combination of sparsity and smoothness is crucial for
mathematical theory as well as performance for finite-sample data. We present a
computationally efficient algorithm, with provable numerical convergence
properties, for optimizing the penalized likelihood. Furthermore, we provide
oracle results which yield asymptotic optimality of our estimator for high
dimensional but sparse additive models. Finally, an adaptive version of our
sparsity-smoothness penalized approach yields large additional performance
gains.
"
"  The problem of optimal data collection to efficiently learn the model
parameters of a graphite nitridation experiment is studied in the context of
Bayesian analysis using both synthetic and real experimental data. The paper
emphasizes that the optimal design can be obtained as a result of an
information theoretic sensitivity analysis. Thus, the preferred design is where
the statistical dependence between the model parameters and observables is the
highest possible. In this paper, the statistical dependence between random
variables is quantified by mutual information and estimated using a k-nearest
neighbor based approximation. It is shown, that by monitoring the inference
process via measures such as entropy or Kullback-Leibler divergence, one can
determine when to stop the data collection process. The methodology is applied
to select the most informative designs on both a simulated data set and on an
experimental data set, previously published in the literature. It is also shown
that the sequential Bayesian analysis used in the experimental design can also
be useful in detecting conflicting information between measurements and model
predictions.
"
"  The sunspot area fluctuations for the northern and the southern hemispheres
of the Sun over the epoch of 12 cycles (12-23) are investigated. Because of the
asymmetry of their probability distributions, the positive and the negative
fluctuations are considered separately. The auto-correlation analysis of them
shows three quasi-periodicities at 10, 17 and 23 solar rotations. The wavelets
gives the 10-rotation quasi-periodicity. For the original and the negative
fluctuations the correlation coefficient between the wavelet and the
auto-correlation results is about 0.9 for 90% of the auto-correlation peaks.
For the positive fluctuations it is also 0.9 for 70% of the peaks. For 90% of
cycles in both hemispheres the auto-correlation analysis of negative
fluctuations shows that two longer periods can be represented as the multiple
of the shortest period. For positive fluctuations such dependences are found
for more than 50% of cases.
"
"  The Cosmological Microwave Background (CMB) is of premier importance for the
cosmologists to study the birth of our universe. Unfortunately, most CMB
experiments such as COBE, WMAP or Planck do not provide a direct measure of the
cosmological signal; CMB is mixed up with galactic foregrounds and point
sources. For the sake of scientific exploitation, measuring the CMB requires
extracting several different astrophysical components (CMB, Sunyaev-Zel'dovich
clusters, galactic dust) form multi-wavelength observations. Mathematically
speaking, the problem of disentangling the CMB map from the galactic
foregrounds amounts to a component or source separation problem. In the field
of CMB studies, a very large range of source separation methods have been
applied which all differ from each other in the way they model the data and the
criteria they rely on to separate components. Two main difficulties are i) the
instrument's beam varies across frequencies and ii) the emission laws of most
astrophysical components vary across pixels. This paper aims at introducing a
very accurate modeling of CMB data, based on sparsity, accounting for beams
variability across frequencies as well as spatial variations of the components'
spectral characteristics. Based on this new sparse modeling of the data, a
sparsity-based component separation method coined Local-Generalized
Morphological Component Analysis (L-GMCA) is described. Extensive numerical
experiments have been carried out with simulated Planck data. These experiments
show the high efficiency of the proposed component separation methods to
estimate a clean CMB map with a very low foreground contamination, which makes
L-GMCA of prime interest for CMB studies.
"
"  Exponential random graph models are a class of widely used exponential family
models for social networks. The topological structure of an observed network is
modelled by the relative prevalence of a set of local sub-graph configurations
termed network statistics. One of the key tasks in the application of these
models is which network statistics to include in the model. This can be thought
of as statistical model selection problem. This is a very challenging
problem---the posterior distribution for each model is often termed ""doubly
intractable"" since computation of the likelihood is rarely available, but also,
the evidence of the posterior is, as usual, intractable. The contribution of
this paper is the development of a fully Bayesian model selection method based
on a reversible jump Markov chain Monte Carlo algorithm extension of Caimo and
Friel (2011) which estimates the posterior probability for each competing
model.
"
"  In this article, the logic rule ensembles approach to supervised learning is
applied to the unsupervised or semi-supervised clustering. Logic rules which
were obtained by combining simple conjunctive rules are used to partition the
input space and an ensemble of these rules is used to define a similarity
matrix. Similarity partitioning is used to partition the data in an
hierarchical manner. We have used internal and external measures of cluster
validity to evaluate the quality of clusterings or to identify the number of
clusters.
"
"  This work considers the problem of learning the structure of multivariate
linear tree models, which include a variety of directed tree graphical models
with continuous, discrete, and mixed latent variables such as linear-Gaussian
models, hidden Markov models, Gaussian mixture models, and Markov evolutionary
trees. The setting is one where we only have samples from certain observed
variables in the tree, and our goal is to estimate the tree structure (i.e.,
the graph of how the underlying hidden variables are connected to each other
and to the observed variables). We propose the Spectral Recursive Grouping
algorithm, an efficient and simple bottom-up procedure for recovering the tree
structure from independent samples of the observed variables. Our finite sample
size bounds for exact recovery of the tree structure reveal certain natural
dependencies on underlying statistical and structural properties of the
underlying joint distribution. Furthermore, our sample complexity guarantees
have no explicit dependence on the dimensionality of the observed variables,
making the algorithm applicable to many high-dimensional settings. At the heart
of our algorithm is a spectral quartet test for determining the relative
topology of a quartet of variables from second-order statistics.
"
"  We consider the problem of identifying whether findings replicate from one
study of high dimension to another, when the primary study guides the selection
of hypotheses to be examined in the follow-up study as well as when there is no
division of roles into the primary and the follow-up study. We show that
existing meta-analysis methods are not appropriate for this problem, and
suggest novel methods instead. We prove that our multiple testing procedures
control for appropriate error-rates. The suggested FWER controlling procedure
is valid for arbitrary dependence among the test statistics within each study.
A more powerful procedure is suggested for FDR control. We prove that this
procedure controls the FDR if the test statistics are independent within the
primary study, and independent or have dependence of type PRDS in the follow-up
study. For arbitrary dependence within the primary study, and either arbitrary
dependence or dependence of type PRDS in the follow-up study, simple
conservative modifications of the procedure control the FDR. We demonstrate the
usefulness of these procedures via simulations and real data examples.
"
"  The ultimate goal of optimization is to find the minimizer of a target
function.However, typical criteria for active optimization often ignore the
uncertainty about the minimizer. We propose a novel criterion for global
optimization and an associated sequential active learning strategy using
Gaussian processes.Our criterion is the reduction of uncertainty in the
posterior distribution of the function minimizer. It can also flexibly
incorporate multiple global minimizers. We implement a tractable approximation
of the criterion and demonstrate that it obtains the global minimizer
accurately compared to conventional Bayesian optimization criteria.
"
"  The general linear model (GLM) is a well established tool for analyzing
functional magnetic resonance imaging (fMRI) data. Most fMRI analyses via GLM
proceed in a massively univariate fashion where the same design matrix is used
for analyzing data from each voxel. A major limitation of this approach is the
locally varying nature of signals of interest as well as associated confounds.
This local variability results in a potentially large bias and uncontrolled
increase in variance for the contrast of interest. The main contributions of
this paper are two fold (1) We develop a statistical framework called SMART
that enables estimation of an optimal design matrix while explicitly
controlling the bias variance decomposition over a set of potential design
matrices and (2) We develop and validate a numerical algorithm for computing
optimal design matrices for general fMRI data sets. The implications of this
framework include the ability to match optimally the magnitude of underlying
signals to their true magnitudes while also matching the ""null"" signals to zero
size thereby optimizing both the sensitivity and specificity of signal
detection. By enabling the capture of multiple profiles of interest using a
single contrast (as opposed to an F-test) in a way that optimizes for both bias
and variance enables the passing of first level parameter estimates and their
variances to the higher level for group analysis which is not possible using
F-tests. We demonstrate the application of this approach to in vivo
pharmacological fMRI data capturing the acute response to a drug infusion, to
task-evoked, block design fMRI and to the estimation of a haemodynamic response
function (HRF) response in event-related fMRI. Our framework is quite general
and has potentially wide applicability to a variety of disciplines.
"
"  We carefully study how well minimizing convex surrogate loss functions,
corresponds to minimizing the misclassification error rate for the problem of
binary classification with linear predictors. In particular, we show that
amongst all convex surrogate losses, the hinge loss gives essentially the best
possible bound, of all convex loss functions, for the misclassification error
rate of the resulting linear predictor in terms of the best possible margin
error rate. We also provide lower bounds for specific convex surrogates that
show how different commonly used losses qualitatively differ from each other.
"
"  In a recent paper entitled ""Inconsistencies of Recently Proposed Citation
Impact Indicators and how to Avoid Them,"" Schreiber (2012, at arXiv:1202.3861)
proposed (i) a method to assess tied ranks consistently and (ii) fractional
attribution to percentile ranks in the case of relatively small samples (e.g.,
for n < 100). Schreiber's solution to the problem of how to handle tied ranks
is convincing, in my opinion (cf. Pudovkin & Garfield, 2009). The fractional
attribution, however, is computationally intensive and cannot be done manually
for even moderately large batches of documents. Schreiber attributed scores
fractionally to the six percentile rank classes used in the Science and
Engineering Indicators of the U.S. National Science Board, and thus missed, in
my opinion, the point that fractional attribution at the level of hundred
percentiles-or equivalently quantiles as the continuous random variable-is only
a linear, and therefore much less complex problem. Given the quantile-values,
the non-linear attribution to the six classes or any other evaluation scheme is
then a question of aggregation. A new routine based on these principles
(including Schreiber's solution for tied ranks) is made available as software
for the assessment of documents retrieved from the Web of Science (at
http://www.leydesdorff.net/software/i3).
"
"  We propose in this paper an exploratory analysis algorithm for functional
data. The method partitions a set of functions into $K$ clusters and represents
each cluster by a simple prototype (e.g., piecewise constant). The total number
of segments in the prototypes, $P$, is chosen by the user and optimally
distributed among the clusters via two dynamic programming algorithms. The
practical relevance of the method is shown on two real world datasets.
"
"  Fitting high-dimensional data involves a delicate tradeoff between faithful
representation and the use of sparse models. Too often, sparsity assumptions on
the fitted model are too restrictive to provide a faithful representation of
the observed data. In this paper, we present a novel framework incorporating
sparsity in different domains.We decompose the observed covariance matrix into
a sparse Gaussian Markov model (with a sparse precision matrix) and a sparse
independence model (with a sparse covariance matrix). Our framework
incorporates sparse covariance and sparse precision estimation as special cases
and thus introduces a richer class of high-dimensional models. We characterize
sufficient conditions for identifiability of the two models, \viz Markov and
independence models. We propose an efficient decomposition method based on a
modification of the popular $\ell_1$-penalized maximum-likelihood estimator
($\ell_1$-MLE). We establish that our estimator is consistent in both the
domains, i.e., it successfully recovers the supports of both Markov and
independence models, when the number of samples $n$ scales as $n = \Omega(d^2
\log p)$, where $p$ is the number of variables and $d$ is the maximum node
degree in the Markov model. Our experiments validate these results and also
demonstrate that our models have better inference accuracy under simple
algorithms such as loopy belief propagation.
"
"  In this paper we address the problem of understanding the success of
algorithms that organize patches according to graph-based metrics. Algorithms
that analyze patches extracted from images or time series have led to
state-of-the art techniques for classification, denoising, and the study of
nonlinear dynamics. The main contribution of this work is to provide a
theoretical explanation for the above experimental observations. Our approach
relies on a detailed analysis of the commute time metric on prototypical graph
models that epitomize the geometry observed in general patch graphs. We prove
that a parametrization of the graph based on commute times shrinks the mutual
distances between patches that correspond to rapid local changes in the signal,
while the distances between patches that correspond to slow local changes
expand. In effect, our results explain why the parametrization of the set of
patches based on the eigenfunctions of the Laplacian can concentrate patches
that correspond to rapid local changes, which would otherwise be shattered in
the space of patches. While our results are based on a large sample analysis,
numerical experimentations on synthetic and real data indicate that the results
hold for datasets that are very small in practice.
"
"  This paper proposes a simple method to evaluate batsmen and bowlers in
cricket. The idea in this paper refines ""book cricket"" and evaluates a batsman
by answering the question: How many runs a team consisting of same player
replicated eleven times will score?
"
"  In the United States the preferred method of obtaining dietary intake data is
the 24-hour dietary recall, yet the measure of most interest is usual or
long-term average daily intake, which is impossible to measure. Thus, usual
dietary intake is assessed with considerable measurement error. Also, diet
represents numerous foods, nutrients and other components, each of which have
distinctive attributes. Sometimes, it is useful to examine intake of these
components separately, but increasingly nutritionists are interested in
exploring them collectively to capture overall dietary patterns. Consumption of
these components varies widely: some are consumed daily by almost everyone on
every day, while others are episodically consumed so that 24-hour recall data
are zero-inflated. In addition, they are often correlated with each other.
Finally, it is often preferable to analyze the amount of a dietary component
relative to the amount of energy (calories) in a diet because dietary
recommendations often vary with energy level. The quest to understand overall
dietary patterns of usual intake has to this point reached a standstill. There
are no statistical methods or models available to model such complex
multivariate data with its measurement error and zero inflation. This paper
proposes the first such model, and it proposes the first workable solution to
fit such a model. After describing the model, we use survey-weighted MCMC
computations to fit the model, with uncertainty estimation coming from balanced
repeated replication.
"
"  Discussion of ``Statistical analysis of an archeological find'' by Andrey
Feuerverger [arXiv:0804.0079]
"
"  Abc-boost is a new line of boosting algorithms for multi-class
classification, by utilizing the commonly used sum-to-zero constraint. To
implement abc-boost, a base class must be identified at each boosting step.
Prior studies used a very expensive procedure based on exhaustive search for
determining the base class at each boosting step. Good testing performances of
abc-boost (implemented as abc-mart and abc-logitboost) on a variety of datasets
were reported.
  For large datasets, however, the exhaustive search strategy adopted in prior
abc-boost algorithms can be too prohibitive. To overcome this serious
limitation, this paper suggests a heuristic by introducing Gaps when computing
the base class during training. That is, we update the choice of the base class
only for every $G$ boosting steps (i.e., G=1 in prior studies). We test this
idea on large datasets (Covertype and Poker) as well as datasets of moderate
sizes. Our preliminary results are very encouraging. On the large datasets,
even with G=100 (or larger), there is essentially no loss of test accuracy. On
the moderate datasets, no obvious loss of test accuracy is observed when G<=
20~50. Therefore, aided by this heuristic, it is promising that abc-boost will
be a practical tool for accurate multi-class classification.
"
"  We consider the problem of estimating the topology of spatial interactions in
a discrete state, discrete time spatio-temporal graphical model where the
interactions affect the temporal evolution of each agent in a network. Among
other models, the susceptible, infected, recovered ($SIR$) model for
interaction events fall into this framework. We pose the problem as a structure
learning problem and solve it using an $\ell_1$-penalized likelihood convex
program. We evaluate the solution on a simulated spread of infectious over a
complex network. Our topology estimates outperform those of a standard spatial
Markov random field graphical model selection using $\ell_1$-regularized
logistic regression.
"
"  Modern data acquisition routinely produces massive amounts of network data.
Though many methods and models have been proposed to analyze such data, the
research of network data is largely disconnected with the classical theory of
statistical learning and signal processing. In this paper, we present a new
framework for modeling network data, which connects two seemingly different
areas: network data analysis and compressed sensing. From a nonparametric
perspective, we model an observed network using a large dictionary. In
particular, we consider the network clique detection problem and show
connections between our formulation with a new algebraic tool, namely Randon
basis pursuit in homogeneous spaces. Such a connection allows us to identify
rigorous recovery conditions for clique detection problems. Though this paper
is mainly conceptual, we also develop practical approximation algorithms for
solving empirical problems and demonstrate their usefulness on real-world
datasets.
"
"  The sparse pseudo-input Gaussian process (SPGP) is a new approximation method
for speeding up GP regression in the case of a large number of data points N.
The approximation is controlled by the gradient optimization of a small set of
M `pseudo-inputs', thereby reducing complexity from N^3 to NM^2. One limitation
of the SPGP is that this optimization space becomes impractically big for high
dimensional data sets. This paper addresses this limitation by performing
automatic dimensionality reduction. A projection of the input space to a low
dimensional space is learned in a supervised manner, alongside the
pseudo-inputs, which now live in this reduced space. The paper also
investigates the suitability of the SPGP for modeling data with input-dependent
noise. A further extension of the model is made to make it even more powerful
in this regard - we learn an uncertainty parameter for each pseudo-input. The
combination of sparsity, reduced dimension, and input-dependent noise makes it
possible to apply GPs to much larger and more complex data sets than was
previously practical. We demonstrate the benefits of these methods on several
synthetic and real world problems.
"
"  Signal averaging is the process that consists in computing a mean shape from
a set of noisy signals. In the presence of geometric variability in time in the
data, the usual Euclidean mean of the raw data yields a mean pattern that does
not reflect the typical shape of the observed signals. In this setting, it is
necessary to use alignment techniques for a precise synchronization of the
signals, and then to average the aligned data to obtain a consistent mean
shape. In this paper, we study the numerical performances of Fr\'echet means of
curves which are extensions of the usual Euclidean mean to spaces endowed with
non-Euclidean metrics. This yields a new algorithm for signal averaging without
a reference template. We apply this approach to the estimation of a mean heart
cycle from ECG records.
"
"  This article establishes the performance of stochastic blockmodels in
addressing the co-clustering problem of partitioning a binary array into
subsets, assuming only that the data are generated by a nonparametric process
satisfying the condition of separate exchangeability. We provide oracle
inequalities with rate of convergence $\mathcal{O}_P(n^{-1/4})$ corresponding
to profile likelihood maximization and mean-square error minimization, and show
that the blockmodel can be interpreted in this setting as an optimal
piecewise-constant approximation to the generative nonparametric model. We also
show for large sample sizes that the detection of co-clusters in such data
indicates with high probability the existence of co-clusters of equal size and
asymptotically equivalent connectivity in the underlying generative process.
"
"  We investigate the effect that the choice of measurement scale has upon
inference and extrapolation in extreme value analysis. Separate analyses of
variables from a single process on scales which are linked by a nonlinear
transformation may lead to discrepant conclusions concerning the tail behavior
of the process. We propose the use of a Box--Cox power transformation
incorporated as part of the inference procedure to account parametrically for
the uncertainty surrounding the scale of extrapolation. This has the additional
feature of increasing the rate of convergence of the distribution tails to an
extreme value form in certain cases and thus reducing bias in the model
estimation. Inference without reparameterization is practicably infeasible, so
we explore a reparameterization which exploits the asymptotic theory of
normalizing constants required for nondegenerate limit distributions. Inference
is carried out in a Bayesian setting, an advantage of this being the
availability of posterior predictive return levels. The methodology is
illustrated on both simulated data and significant wave height data from the
North Sea.
"
"  Discussion on ""Brownian distance covariance"" by G\'{a}bor J. Sz\'{e}kely,
Maria L. Rizzo [arXiv:1010.0297]
"
"  Gaussian Process (GP) models are often used as mathematical approximations of
computationally expensive experiments. Provided that its kernel is suitably
chosen and that enough data is available to obtain a reasonable fit of the
simulator, a GP model can beneficially be used for tasks such as prediction,
optimization, or Monte-Carlo-based quantification of uncertainty. However, the
former conditions become unrealistic when using classical GPs as the dimension
of input increases. One popular alternative is then to turn to Generalized
Additive Models (GAMs), relying on the assumption that the simulator's response
can approximately be decomposed as a sum of univariate functions. If such an
approach has been successfully applied in approximation, it is nevertheless not
completely compatible with the GP framework and its versatile applications. The
ambition of the present work is to give an insight into the use of GPs for
additive models by integrating additivity within the kernel, and proposing a
parsimonious numerical method for data-driven parameter estimation. The first
part of this article deals with the kernels naturally associated to additive
processes and the properties of the GP models based on such kernels. The second
part is dedicated to a numerical procedure based on relaxation for additive
kernel parameter estimation. Finally, the efficiency of the proposed method is
illustrated and compared to other approaches on Sobol's g-function.
"
"  We consider the problem of learning a binary classifier from a training set
of positive and unlabeled examples, both in the inductive and in the
transductive setting. This problem, often referred to as \emph{PU learning},
differs from the standard supervised classification problem by the lack of
negative examples in the training set. It corresponds to an ubiquitous
situation in many applications such as information retrieval or gene ranking,
when we have identified a set of data of interest sharing a particular
property, and we wish to automatically retrieve additional data sharing the
same property among a large and easily available pool of unlabeled data. We
propose a conceptually simple method, akin to bagging, to approach both
inductive and transductive PU learning problems, by converting them into series
of supervised binary classification problems discriminating the known positive
examples from random subsamples of the unlabeled set. We empirically
demonstrate the relevance of the method on simulated and real data, where it
performs at least as well as existing methods while being faster.
"
"  Researchers often calculate ratios of measured quantities. Specifying
confidence limits for ratios is difficult and the appropriate methods are often
unknown. Appropriate methods are described (Fieller, Taylor, special bootstrap
methods). For the Fieller method a simple geometrical interpretation is given.
Monte Carlo simulations show when these methods are appropriate and that the
most frequently used methods (index method and zero-variance method) can lead
to large liberal deviations from the desired confidence level. It is discussed
when we can use standard regression or measurement error models and when we
have to resort to specific models for heteroscedastic data. Finally, an old
warning is repeated that we should be aware of the problems of spurious
correlations if we use ratios.
"
"  A lot of attention has been devoted to multimedia indexing over the past few
years. In the literature, we often consider two kinds of fusion schemes: The
early fusion and the late fusion. In this paper we focus on late classifier
fusion, where one combines the scores of each modality at the decision level.
To tackle this problem, we investigate a recent and elegant well-founded
quadratic program named MinCq coming from the Machine Learning PAC-Bayes
theory. MinCq looks for the weighted combination, over a set of real-valued
functions seen as voters, leading to the lowest misclassification rate, while
making use of the voters' diversity. We provide evidence that this method is
naturally adapted to late fusion procedure. We propose an extension of MinCq by
adding an order- preserving pairwise loss for ranking, helping to improve Mean
Averaged Precision measure. We confirm the good behavior of the MinCq-based
fusion approaches with experiments on a real image benchmark.
"
"  In many areas of machine learning, it becomes necessary to find the
eigenvector decompositions of large matrices. We discuss two methods for
reducing the computational burden of spectral decompositions: the more
venerable Nystom extension and a newly introduced algorithm based on random
projections. Previous work has centered on the ability to reconstruct the
original matrix. We argue that a more interesting and relevant comparison is
their relative performance in clustering and classification tasks using the
approximate eigenvectors as features. We demonstrate that performance is task
specific and depends on the rank of the approximation.
"
"  A general stochastic model is developed for the total interference in
wideband systems, denoted as the PNSC(alpha) Interference Model. It allows one
to obtain, analytic representations in situations where (a) interferers are
distributed according to either a homogeneous or an inhomogeneous in time or
space Cox point process and (b) when the frequency bands occupied by each of
the unknown number of interferers is also a random variable in the allowable
bandwidth. The analytic representations obtained are generalizations of Cox
processes to the family of sub-exponential models characterized by
distributions from the alpha-stable family. We develop general parametric
density representations for the interference models via doubly stochastic
Poisson mixture representations of Scaled Mixture of Normal's via the
Normal-Stable variance mixture. To illustrate members of this class of
interference model we also develop two special cases for a moderately impulsive
interference (alpha=3/2) and a highly impulsive interference (alpha=2/3) where
closed form representations can be obtained either by the SMiN representation
or via function expansions based on the Holtsmark distribution or Whittaker
functions. To illustrate the paper we propose expressions for the Capacity of a
BPSK system under a PNSC(alpha) interference, via analytic expressions for the
Likelihood Ratio Test statistic.
"
"  Computational methods for discovering patterns of local correlations in
sequences are important in computational biology. Here we show how to determine
the optimal partitioning of aligned sequences into non-overlapping segments
such that positions in the same segment are strongly correlated while positions
in different segments are not. Our approach involves discovering the hidden
variables of a Bayesian network that interact with observed sequences so as to
form a set of independent mixture models. We introduce a dynamic program to
efficiently discover the optimal segmentation, or equivalently the optimal set
of hidden variables. We evaluate our approach on two computational biology
tasks. One task is related to the design of vaccines against polymorphic
pathogens and the other task involves analysis of single nucleotide
polymorphisms (SNPs) in human DNA. We show how common tasks in these problems
naturally correspond to inference procedures in the learned models. Error rates
of our learned models for the prediction of missing SNPs are up to 1/3 less
than the error rates of a state-of-the-art SNP prediction method. Source code
is available at www.uwm.edu/~joebock/segmentation.
"
"  We study optimal solutions to an abstract optimization problem for measures,
which is a generalization of classical variational problems in information
theory and statistical physics. In the classical problems, information and
relative entropy are defined using the Kullback-Leibler divergence, and for
this reason optimal measures belong to a one-parameter exponential family.
Measures within such a family have the property of mutual absolute continuity.
Here we show that this property characterizes other families of optimal
positive measures if a functional representing information has a strictly
convex dual. Mutual absolute continuity of optimal probability measures allows
us to strictly separate deterministic and non-deterministic Markov transition
kernels, which play an important role in theories of decisions, estimation,
control, communication and computation. We show that deterministic transitions
are strictly sub-optimal, unless information resource with a strictly convex
dual is unconstrained. For illustration, we construct an example where, unlike
non-deterministic, any deterministic kernel either has negatively infinite
expected utility (unbounded expected error) or communicates infinite
information.
"
"  A large set of signals can sometimes be described sparsely using a
dictionary, that is, every element can be represented as a linear combination
of few elements from the dictionary. Algorithms for various signal processing
applications, including classification, denoising and signal separation, learn
a dictionary from a set of signals to be represented. Can we expect that the
representation found by such a dictionary for a previously unseen example from
the same source will have L_2 error of the same magnitude as those for the
given examples? We assume signals are generated from a fixed distribution, and
study this questions from a statistical learning theory perspective.
  We develop generalization bounds on the quality of the learned dictionary for
two types of constraints on the coefficient selection, as measured by the
expected L_2 error in representation when the dictionary is used. For the case
of l_1 regularized coefficient selection we provide a generalization bound of
the order of O(sqrt(np log(m lambda)/m)), where n is the dimension, p is the
number of elements in the dictionary, lambda is a bound on the l_1 norm of the
coefficient vector and m is the number of samples, which complements existing
results. For the case of representing a new signal as a combination of at most
k dictionary elements, we provide a bound of the order O(sqrt(np log(m k)/m))
under an assumption on the level of orthogonality of the dictionary (low Babel
function). We further show that this assumption holds for most dictionaries in
high dimensions in a strong probabilistic sense. Our results further yield fast
rates of order 1/m as opposed to 1/sqrt(m) using localized Rademacher
complexity. We provide similar results in a general setting using kernels with
weak smoothness requirements.
"
"  Many nonparametric regressors were recently shown to converge at rates that
depend only on the intrinsic dimension of data. These regressors thus escape
the curse of dimension when high-dimensional data has low intrinsic dimension
(e.g. a manifold). We show that k-NN regression is also adaptive to intrinsic
dimension. In particular our rates are local to a query x and depend only on
the way masses of balls centered at x vary with radius.
  Furthermore, we show a simple way to choose k = k(x) locally at any x so as
to nearly achieve the minimax rate at x in terms of the unknown intrinsic
dimension in the vicinity of x. We also establish that the minimax rate does
not depend on a particular choice of metric space or distribution, but rather
that this minimax rate holds for any metric space and doubling measure.
"
"  Many complex dynamical phenomena can be effectively modeled by a system that
switches among a set of conditionally linear dynamical modes. We consider two
such models: the switching linear dynamical system (SLDS) and the switching
vector autoregressive (VAR) process. Our Bayesian nonparametric approach
utilizes a hierarchical Dirichlet process prior to learn an unknown number of
persistent, smooth dynamical modes. We additionally employ automatic relevance
determination to infer a sparse set of dynamic dependencies allowing us to
learn SLDS with varying state dimension or switching VAR processes with varying
autoregressive order. We develop a sampling algorithm that combines a truncated
approximation to the Dirichlet process with efficient joint sampling of the
mode and state sequences. The utility and flexibility of our model are
demonstrated on synthetic data, sequences of dancing honey bees, the IBOVESPA
stock index, and a maneuvering target tracking application.
"
"  In generalized linear regression problems with an abundant number of
features, lasso-type regularization which imposes an $\ell^1$-constraint on the
regression coefficients has become a widely established technique. Deficiencies
of the lasso in certain scenarios, notably strongly correlated design, were
unmasked when Zou and Hastie [J. Roy. Statist. Soc. Ser. B 67 (2005) 301--320]
introduced the elastic net. In this paper we propose to extend the elastic net
by admitting general nonnegative quadratic constraints as a second form of
regularization. The generalized ridge-type constraint will typically make use
of the known association structure of features, for example, by using temporal-
or spatial closeness. We study properties of the resulting ""structured elastic
net"" regression estimation procedure, including basic asymptotics and the issue
of model selection consistency. In this vein, we provide an analog to the
so-called ""irrepresentable condition"" which holds for the lasso. Moreover, we
outline algorithmic solutions for the structured elastic net within the
generalized linear model family. The rationale and the performance of our
approach is illustrated by means of simulated and real world data, with a focus
on signal regression.
"
"  In genetics it is often of interest to discover single nucleotide
polymorphisms (SNPs) that are directly related to a disease, rather than just
being associated with it. Few methods exist, however, addressing this so-called
`true sparsity recovery' issue. In a thorough simulation study, we show that
for moderate or low correlation between predictors, lasso-based methods perform
well at true sparsity recovery, despite not being specifically designed for
this purpose. For large correlations, however, more specialised methods are
needed. Stability selection and direct effect testing perform well in all
situations, including when the correlation is large.
"
"  Joinpoint regression is used to determine the number of segments needed to
adequately explain the relationship between two variables. This methodology can
be widely applied to real problems, but we focus on epidemiological data, the
main goal being to uncover changes in the mortality time trend of a specific
disease under study. Traditionally, Joinpoint regression problems have paid
little or no attention to the quantification of uncertainty in the estimation
of the number of change-points. In this context, we found a satisfactory way to
handle the problem in the Bayesian methodology. Nevertheless, this novel
approach involves significant difficulties (both theoretical and practical)
since it implicitly entails a model selection (or testing) problem. In this
study we face these challenges through (i) a novel reparameterization of the
model, (ii) a conscientious definition of the prior distributions used and
(iii) an encompassing approach which allows the use of MCMC simulation-based
techniques to derive the results. The resulting methodology is flexible enough
to make it possible to consider mortality counts (for epidemiological
applications) as Poisson variables. The methodology is applied to the study of
annual breast cancer mortality during the period 1980--2007 in Castell\'{o}n, a
province in Spain.
"
"  In order to conduct analyses of networked systems where connections between
individuals take on a range of values - counts, continuous strengths or ordinal
rankings - a common technique is to dichotomize the data according to their
positions with respect to a threshold value. However, there are two issues to
consider: how the results of the analysis depend on the choice of threshold,
and what role the presence of noise has on a system with respect to a fixed
threshold value. We show that while there are principled criteria of keeping
information from the valued graph in the dichotomized version, they produce
such a wide range of binary graphs that only a fraction of the relevant
information will be kept. Additionally, while dichotomization of predictors in
linear models has a known asymptotic efficiency loss, the same process applied
to network edges in a time series model will lead to an efficiency loss that
grows larger as the network increases in size.
"
"  We develop the necessary theory in computational algebraic geometry to place
Bayesian networks into the realm of algebraic statistics. We present an
algebra{statistics dictionary focused on statistical modeling. In particular,
we link the notion of effiective dimension of a Bayesian network with the
notion of algebraic dimension of a variety. We also obtain the independence and
non{independence constraints on the distributions over the observable variables
implied by a Bayesian network with hidden variables, via a generating set of an
ideal of polynomials associated to the network. These results extend previous
work on the subject. Finally, the relevance of these results for model
selection is discussed.
"
"  Discussion of ""A statistical analysis of multiple temperature proxies: Are
reconstructions of surface temperatures over the last 1000 years reliable?"" by
B.B. McShane and A.J. Wyner [arXiv:1104.4002]
"
"  Scaled sparse linear regression jointly estimates the regression coefficients
and noise level in a linear model. It chooses an equilibrium with a sparse
regression method by iteratively estimating the noise level via the mean
residual square and scaling the penalty in proportion to the estimated noise
level. The iterative algorithm costs little beyond the computation of a path or
grid of the sparse regression estimator for penalty levels above a proper
threshold. For the scaled lasso, the algorithm is a gradient descent in a
convex minimization of a penalized joint loss function for the regression
coefficients and noise level. Under mild regularity conditions, we prove that
the scaled lasso simultaneously yields an estimator for the noise level and an
estimated coefficient vector satisfying certain oracle inequalities for
prediction, the estimation of the noise level and the regression coefficients.
These inequalities provide sufficient conditions for the consistency and
asymptotic normality of the noise level estimator, including certain cases
where the number of variables is of greater order than the sample size.
Parallel results are provided for the least squares estimation after model
selection by the scaled lasso. Numerical results demonstrate the superior
performance of the proposed methods over an earlier proposal of joint convex
minimization.
"
"  Conditional independence testing is an important problem, especially in
Bayesian network learning and causal discovery. Due to the curse of
dimensionality, testing for conditional independence of continuous variables is
particularly challenging. We propose a Kernel-based Conditional Independence
test (KCI-test), by constructing an appropriate test statistic and deriving its
asymptotic distribution under the null hypothesis of conditional independence.
The proposed method is computationally efficient and easy to implement.
Experimental results show that it outperforms other methods, especially when
the conditioning set is large or the sample size is not very large, in which
case other methods encounter difficulties.
"
"  We give improved constants for data dependent and variance sensitive
confidence bounds, called empirical Bernstein bounds, and extend these
inequalities to hold uniformly over classes of functionswhose growth function
is polynomial in the sample size n. The bounds lead us to consider sample
variance penalization, a novel learning method which takes into account the
empirical variance of the loss function. We give conditions under which sample
variance penalization is effective. In particular, we present a bound on the
excess risk incurred by the method. Using this, we argue that there are
situations in which the excess risk of our method is of order 1/n, while the
excess risk of empirical risk minimization is of order 1/sqrt/{n}. We show some
experimental results, which confirm the theory. Finally, we discuss the
potential application of our results to sample compression schemes.
"
"  We investigate the implications of free probability for random matrices. From
rules for calculating all possible joint moments of two free random matrices,
we develop a notion of partial freeness which is quantified by the breakdown of
these rules. We provide a combinatorial interpretation for partial freeness as
the presence of closed paths in Hilbert space defined by particular joint
moments. We also discuss how asymptotic moment expansions provide an error term
on the density of states. We present MATLAB code for the calculation of moments
and free cumulants of arbitrary random matrices.
"
"  Covariance graphical lasso applies a lasso penalty on the elements of the
covariance matrix. This method is useful because it not only produces sparse
estimation of covariance matrix but also discovers marginal independence
structures by generating zeros in the covariance matrix. We propose and explore
two new algorithms for solving the covariance graphical lasso problem. Our new
algorithms are based on coordinate descent and ECM. We show that these two
algorithms are more attractive than the only existing competing algorithm of
Bien and Tibshirani (2011) in terms of simplicity, speed and stability. We also
discuss convergence properties of our algorithms.
"
"  In this paper, we consider the problem of partitioning a small data sample
drawn from a mixture of $k$ product distributions. We are interested in the
case that individual features are of low average quality $\gamma$, and we want
to use as few of them as possible to correctly partition the sample. We analyze
a spectral technique that is able to approximately optimize the total data
size--the product of number of data points $n$ and the number of features
$K$--needed to correctly perform this partitioning as a function of $1/\gamma$
for $K>n$. Our goal is motivated by an application in clustering individuals
according to their population of origin using markers, when the divergence
between any two of the populations is small.
"
"  Dynamic gene-regulatory networks are complex since the number of potential
components involved in the system is very large. Estimating dynamic networks is
an important task because they compromise valuable information about
interactions among genes. Graphical models are a powerful class of models to
estimate conditional independence among random variables, e.g. interactions in
dynamic systems. Indeed, these interactions tend to vary over time. However,
the literature has been focused on static networks, which can only reveal
overall structures. Time-course experiments are performed in order to tease out
significant changes in networks. It is typically reasonable to assume that
changes in genomic networks are few because systems in biology tend to be
stable. We introduce a new model for estimating slowly changes in dynamic
gene-regulatory networks which is suitable for a high-dimensional dataset, e.g.
time-course genomic data. Our method is based on i) the penalized likelihood
with $\ell_1$-norm, ii) the penalized differences between conditional
independence elements across time points and iii) the heuristic search strategy
to find optimal smoothing parameters. We implement a set of linear constraints
necessary to estimate sparse graphs and penalized changing in dynamic networks.
These constraints are not in the linear form. For this reason, we introduce
slack variables to re-write our problem into a standard convex optimization
problem subject to equality linear constraints. We show that GL$_\Delta$
performs well in a simulation study. Finally, we apply the proposed model to a
time-course genetic dataset T-cell.
"
"  This paper examines historical patterns of ROA (return on assets) for a
cohort of 53,038 publicly traded firms across 93 countries, measured over the
past 45 years. Our goal is to screen for firms whose ROA trajectories suggest
that they have systematically outperformed their peer groups over time. Such a
project faces at least three statistical difficulties: adjustment for relevant
covariates, massive multiplicity, and longitudinal dependence. We conclude
that, once these difficulties are taken into account, demonstrably superior
performance appears to be quite rare. We compare our findings with other recent
management studies on the same subject, and with the popular literature on
corporate success. Our methodological contribution is to propose a new class of
priors for use in large-scale simultaneous testing. These priors are based on
the hypergeometric inverted-beta family, and have two main attractive features:
heavy tails and computational tractability. The family is a four-parameter
generalization of the normal/inverted-beta prior, and is the natural conjugate
prior for shrinkage coefficients in a hierarchical normal model. Our results
emphasize the usefulness of these heavy-tailed priors in large multiple-testing
problems, as they have a mild rate of tail decay in the marginal likelihood
$m(y)$---a property long recognized to be important in testing.
"
"  In the target tracking and its engineering applications, recursive state
estimation of the target is of fundamental importance. This paper presents a
recursive performance bound for dynamic estimation and filtering problem, in
the framework of the finite set statistics for the first time. The number of
tracking algorithms with set-valued observations and state of targets is
increased sharply recently. Nevertheless, the bound for these algorithms has
not been fully discussed. Treating the measurement as set, this bound can be
applied when the probability of detection is less than unity. Moreover, the
state is treated as set, which is singleton or empty with certain probability
and accounts for the appearance and the disappearance of the targets. When the
existence of the target state is certain, our bound is as same as the most
accurate results of the bound with probability of detection is less than unity
in the framework of random vector statistics. When the uncertainty is taken
into account, both linear and non-linear applications are presented to confirm
the theory and reveal this bound is more general than previous bounds in the
framework of random vector statistics.In fact, the collection of such
measurements could be treated as a random finite set (RFS).
"
"  We consider a dynamical system with small noise for which the drift is
parametrized by a finite dimensional parameter. For this model we consider
minimum distance estimation from continuous time observations under
$l^p$-penalty imposed on the parameters in the spirit of the Lasso approach
with the aim of simultaneous estimation and model selection. We study the
consistency and the asymptotic distribution of these Lasso-type estimators for
different values of $p$. For $p=1$ we also consider the adaptive version of the
Lasso estimator and establish its oracle properties.
"
"  The spatial modeling of extreme snow is important for adequate risk
management in Alpine and high altitude countries. A natural approach to such
modeling is through the theory of max-stable processes, an infinite-dimensional
extension of multivariate extreme value theory. In this paper we describe the
application of such processes in modeling the spatial dependence of extreme
snow depth in Switzerland, based on data for the winters 1966--2008 at 101
stations. The models we propose rely on a climate transformation that allows us
to account for the presence of climate regions and for directional effects,
resulting from synoptic weather patterns. Estimation is performed through
pairwise likelihood inference and the models are compared using penalized
likelihood criteria. The max-stable models provide a much better fit to the
joint behavior of the extremes than do independence or full dependence models.
"
"  Statistical modeling of nuclear data provides a novel approach to nuclear
systematics complementary to established theoretical and phenomenological
approaches based on quantum theory. Continuing previous studies in which global
statistical modeling is pursued within the general framework of machine
learning theory, we implement advances in training algorithms designed to
improved generalization, in application to the problem of reproducing and
predicting the halflives of nuclear ground states that decay 100% by the beta^-
mode. More specifically, fully-connected, multilayer feedforward artificial
neural network models are developed using the Levenberg-Marquardt optimization
algorithm together with Bayesian regularization and cross-validation. The
predictive performance of models emerging from extensive computer experiments
is compared with that of traditional microscopic and phenomenological models as
well as with the performance of other learning systems, including earlier
neural network models as well as the support vector machines recently applied
to the same problem. In discussing the results, emphasis is placed on
predictions for nuclei that are far from the stability line, and especially
those involved in the r-process nucleosynthesis. It is found that the new
statistical models can match or even surpass the predictive performance of
conventional models for beta-decay systematics and accordingly should provide a
valuable additional tool for exploring the expanding nuclear landscape.
"
"  Despite the recent progress towards efficient multiple kernel learning (MKL),
the structured output case remains an open research front. Current approaches
involve repeatedly solving a batch learning problem, which makes them
inadequate for large scale scenarios. We propose a new family of online
proximal algorithms for MKL (as well as for group-lasso and variants thereof),
which overcomes that drawback. We show regret, convergence, and generalization
bounds for the proposed method. Experiments on handwriting recognition and
dependency parsing testify for the successfulness of the approach.
"
"  Prediction of the remaining life of high-voltage power transformers is an
important issue for energy companies because of the need for planning
maintenance and capital expenditures. Lifetime data for such transformers are
complicated because transformer lifetimes can extend over many decades and
transformer designs and manufacturing practices have evolved. We were asked to
develop statistically-based predictions for the lifetimes of an energy
company's fleet of high-voltage transmission and distribution transformers. The
company's data records begin in 1980, providing information on installation and
failure dates of transformers. Although the dataset contains many units that
were installed before 1980, there is no information about units that were
installed and failed before 1980. Thus, the data are left truncated and right
censored. We use a parametric lifetime model to describe the lifetime
distribution of individual transformers. We develop a statistical procedure,
based on age-adjusted life distributions, for computing a prediction interval
for remaining life for individual transformers now in service. We then extend
these ideas to provide predictions and prediction intervals for the cumulative
number of failures, over a range of time, for the overall fleet of
transformers.
"
"  We consider the problem of adaptive stratified sampling for Monte Carlo
integration of a differentiable function given a finite number of evaluations
to the function. We construct a sampling scheme that samples more often in
regions where the function oscillates more, while allocating the samples such
that they are well spread on the domain (this notion shares similitude with low
discrepancy). We prove that the estimate returned by the algorithm is almost
similarly accurate as the estimate that an optimal oracle strategy (that would
know the variations of the function everywhere) would return, and provide a
finite-sample analysis.
"
"  We extend the well-known BFGS quasi-Newton method and its memory-limited
variant LBFGS to the optimization of nonsmooth convex objectives. This is done
in a rigorous fashion by generalizing three components of BFGS to
subdifferentials: the local quadratic model, the identification of a descent
direction, and the Wolfe line search conditions. We prove that under some
technical conditions, the resulting subBFGS algorithm is globally convergent in
objective function value. We apply its memory-limited variant (subLBFGS) to
L_2-regularized risk minimization with the binary hinge loss. To extend our
algorithm to the multiclass and multilabel settings, we develop a new,
efficient, exact line search algorithm. We prove its worst-case time complexity
bounds, and show that our line search can also be used to extend a recently
developed bundle method to the multiclass and multilabel settings. We also
apply the direction-finding component of our algorithm to L_1-regularized risk
minimization with logistic loss. In all these contexts our methods perform
comparable to or better than specialized state-of-the-art solvers on a number
of publicly available datasets. An open source implementation of our algorithms
is freely available.
"
"  A challenging problem in estimating high-dimensional graphical models is to
choose the regularization parameter in a data-dependent way. The standard
techniques include $K$-fold cross-validation ($K$-CV), Akaike information
criterion (AIC), and Bayesian information criterion (BIC). Though these methods
work well for low-dimensional problems, they are not suitable in high
dimensional settings. In this paper, we present StARS: a new stability-based
method for choosing the regularization parameter in high dimensional inference
for undirected graphs. The method has a clear interpretation: we use the least
amount of regularization that simultaneously makes a graph sparse and
replicable under random sampling. This interpretation requires essentially no
conditions. Under mild conditions, we show that StARS is partially sparsistent
in terms of graph estimation: i.e. with high probability, all the true edges
will be included in the selected model even when the graph size diverges with
the sample size. Empirically, the performance of StARS is compared with the
state-of-the-art model selection procedures, including $K$-CV, AIC, and BIC, on
both synthetic data and a real microarray dataset. StARS outperforms all these
competing procedures.
"
"  Structure learning in random fields has attracted considerable attention due
to its difficulty and importance in areas such as remote sensing, computational
biology, natural language processing, protein networks, and social network
analysis. We consider the problem of estimating the probabilistic graph
structure associated with a Gaussian Markov Random Field (GMRF), the Ising
model and the Potts model, by extending previous work on $l_1$ regularized
neighborhood estimation to include the elastic net $l_1+l_2$ penalty.
Additionally, we show numerical evidence that the edge density plays a role in
the graph recovery process. Finally, we introduce a novel method for augmenting
neighborhood estimation by leveraging pair-wise neighborhood union estimates.
"
"  Understanding the seizure initiation process and its propagation pattern(s)
is a critical task in epilepsy research. Characteristics of the pre-seizure
electroencephalograms (EEGs) such as oscillating powers and high-frequency
activities are believed to be indicative of the seizure onset and spread
patterns. In this article, we analyze epileptic EEG time series using
nonparametric spectral estimation methods to extract information on
seizure-specific power and characteristic frequency [or frequency band(s)].
Because the EEGs may become nonstationary before seizure events, we develop
methods for both stationary and local stationary processes. Based on penalized
Whittle likelihood, we propose a direct generalized maximum likelihood (GML)
and generalized approximate cross-validation (GACV) methods to estimate
smoothing parameters in both smoothing spline spectrum estimation of a
stationary process and smoothing spline ANOVA time-varying spectrum estimation
of a locally stationary process. We also propose permutation methods to test if
a locally stationary process is stationary. Extensive simulations indicate that
the proposed direct methods, especially the direct GML, are stable and perform
better than other existing methods. We apply the proposed methods to the
intracranial electroencephalograms (IEEGs) of an epileptic patient to gain
insights into the seizure generation process.
"
"  OBJECTIVE. A computer program tells me that a mean value is 12.3456789012,
but how many of these digits are significant (the rest being random junk)?
Should I report: 12.3?, 12.3456?, or even 10 (if only the first digit is
significant)? There are several rules-of-thumb but, surprisingly (given that
the problem is so common in science), none seem to be evidence-based. RESULTS.
Here I show how the significance of a digit in a particular decade of a mean
depends on the standard error of the mean (SEM). I define an index, DM that can
be plotted in graphs. From these a simple evidence-based rule for the number of
significant digits (""sigdigs"") is distilled: the last sigdig in the mean is in
the same decade as the first or second non-zero digit in the SEM. As example,
for mean 34.63 (SEM 25.62), with n = 17, the reported value should be 35 (SEM
26). Digits beyond these contain little or no useful information, and should
not be reported lest they damage your credibility.
"
"  We present a large catalog of optically selected galaxy clusters from the
application of a new Gaussian Mixture Brightest Cluster Galaxy (GMBCG)
algorithm to SDSS Data Release 7 data. The algorithm detects clusters by
identifying the red sequence plus Brightest Cluster Galaxy (BCG) feature, which
is unique for galaxy clusters and does not exist among field galaxies. Red
sequence clustering in color space is detected using an Error Corrected
Gaussian Mixture Model. We run GMBCG on 8240 square degrees of photometric data
from SDSS DR7 to assemble the largest ever optical galaxy cluster catalog,
consisting of over 55,000 rich clusters across the redshift range from 0.1 < z
< 0.55. We present Monte Carlo tests of completeness and purity and perform
cross-matching with X-ray clusters and with the maxBCG sample at low redshift.
These tests indicate high completeness and purity across the full redshift
range for clusters with 15 or more members.
"
"  The speed of convergence of the Expectation Maximization (EM) algorithm for
Gaussian mixture model fitting is known to be dependent on the amount of
overlap among the mixture components. In this paper, we study the impact of
mixing coefficients on the convergence of EM. We show that when the mixture
components exhibit some overlap, the convergence of EM becomes slower as the
dynamic range among the mixing coefficients increases. We propose a
deterministic anti-annealing algorithm, that significantly improves the speed
of convergence of EM for such mixtures with unbalanced mixing coefficients. The
proposed algorithm is compared against other standard optimization techniques
like BFGS, Conjugate Gradient, and the traditional EM algorithm. Finally, we
propose a similar deterministic anti-annealing based algorithm for the
Dirichlet process mixture model and demonstrate its advantages over the
conventional variational Bayesian approach.
"
"  We propose a version of least-mean-square (LMS) algorithm for sparse system
identification. Our algorithm called online linearized Bregman iteration (OLBI)
is derived from minimizing the cumulative prediction error squared along with
an l1-l2 norm regularizer. By systematically treating the non-differentiable
regularizer we arrive at a simple two-step iteration. We demonstrate that OLBI
is bias free and compare its operation with existing sparse LMS algorithms by
rederiving them in the online convex optimization framework. We perform
convergence analysis of OLBI for white input signals and derive theoretical
expressions for both the steady state and instantaneous mean square deviations
(MSD). We demonstrate numerically that OLBI improves the performance of LMS
type algorithms for signals generated from sparse tap weights.
"
"  Many problems in machine learning and statistics can be formulated as
(generalized) eigenproblems. In terms of the associated optimization problem,
computing linear eigenvectors amounts to finding critical points of a quadratic
function subject to quadratic constraints. In this paper we show that a certain
class of constrained optimization problems with nonquadratic objective and
constraints can be understood as nonlinear eigenproblems. We derive a
generalization of the inverse power method which is guaranteed to converge to a
nonlinear eigenvector. We apply the inverse power method to 1-spectral
clustering and sparse PCA which can naturally be formulated as nonlinear
eigenproblems. In both applications we achieve state-of-the-art results in
terms of solution quality and runtime. Moving beyond the standard eigenproblem
should be useful also in many other applications and our inverse power method
can be easily adapted to new problems.
"
"  The Gaussian process (GP) is a popular way to specify dependencies between
random variables in a probabilistic model. In the Bayesian framework the
covariance structure can be specified using unknown hyperparameters.
Integrating over these hyperparameters considers different possible
explanations for the data when making predictions. This integration is often
performed using Markov chain Monte Carlo (MCMC) sampling. However, with
non-Gaussian observations standard hyperparameter sampling approaches require
careful tuning and may converge slowly. In this paper we present a slice
sampling approach that requires little tuning while mixing well in both strong-
and weak-data regimes.
"
"  We consider the P(M,lambda,tau) maintenance policy of a dam using the total
discounted and long-run average costs, when the input process is inverse
Gaussian.
"
"  We present a new similarity measure tailored to posts in an online forum. Our
measure takes into account all the available information about user interest
and interaction --- the content of posts, the threads in the forum, and the
author of the posts. We use this post similarity to build a similarity between
users, based on principal coordinate analysis. This allows easy visualization
of the user activity as well. Similarity between users has numerous
applications, such as clustering or classification. We show that including the
author of a post in the post similarity has a smoothing effect on principal
coordinate projections. We demonstrate our method on real data drawn from an
internal corporate forum, and compare our results to those given by a standard
document classification method. We conclude our method gives a more detailed
picture of both the local and global network structure.
"
"  Recommender systems are promising ways to filter the overabundant information
in modern society. Their algorithms help individuals to explore decent items,
but it is unclear how they allocate popularity among items. In this paper, we
simulate successive recommendations and measure their influence on the
dispersion of item popularity by Gini coefficient. Our result indicates that
local diffusion and collaborative filtering reinforce the popularity of hot
items, widening the popularity dispersion. On the other hand, the heat
conduction algorithm increases the popularity of the niche items and generates
smaller dispersion of item popularity. Simulations are compared to mean-field
predictions. Our results suggest that recommender systems have reinforcing
influence on global diversification.
"
"  The Group-Lasso is a well-known tool for joint regularization in machine
learning methods. While the l_{1,2} and the l_{1,\infty} version have been
studied in detail and efficient algorithms exist, there are still open
questions regarding other l_{1,p} variants. We characterize conditions for
solutions of the l_{1,p} Group-Lasso for all p-norms with 1 <= p <= \infty, and
we present a unified active set algorithm. For all p-norms, a highly efficient
projected gradient algorithm is presented. This new algorithm enables us to
compare the prediction performance of many variants of the Group-Lasso in a
multi-task learning setting, where the aim is to solve many learning problems
in parallel which are coupled via the Group-Lasso constraint. We conduct
large-scale experiments on synthetic data and on two real-world data sets. In
accordance with theoretical characterizations of the different norms we observe
that the weak-coupling norms with p between 1.5 and 2 consistently outperform
the strong-coupling norms with p >> 2.
"
"  We present sparse topical coding (STC), a non-probabilistic formulation of
topic models for discovering latent representations of large collections of
data. Unlike probabilistic topic models, STC relaxes the normalization
constraint of admixture proportions and the constraint of defining a normalized
likelihood function. Such relaxations make STC amenable to: 1) directly control
the sparsity of inferred representations by using sparsity-inducing
regularizers; 2) be seamlessly integrated with a convex error function (e.g.,
SVM hinge loss) for supervised learning; and 3) be efficiently learned with a
simply structured coordinate descent algorithm. Our results demonstrate the
advantages of STC and supervised MedSTC on identifying topical meanings of
words and improving classification accuracy and time efficiency.
"
"  Concept Relation Discovery and Innovation Enabling Technology (CORDIET), is a
toolbox for gaining new knowledge from unstructured text data. At the core of
CORDIET is the C-K theory which captures the essential elements of innovation.
The tool uses Formal Concept Analysis (FCA), Emergent Self Organizing Maps
(ESOM) and Hidden Markov Models (HMM) as main artifacts in the analysis
process. The user can define temporal, text mining and compound attributes. The
text mining attributes are used to analyze the unstructured text in documents,
the temporal attributes use these document's timestamps for analysis. The
compound attributes are XML rules based on text mining and temporal attributes.
The user can cluster objects with object-cluster rules and can chop the data in
pieces with segmentation rules. The artifacts are optimized for efficient data
analysis; object labels in the FCA lattice and ESOM map contain an URL on which
the user can click to open the selected document.
"
"  In this paper, we propose to (seamlessly) integrate b-bit minwise hashing
with linear SVM to substantially improve the training (and testing) efficiency
using much smaller memory, with essentially no loss of accuracy. Theoretically,
we prove that the resemblance matrix, the minwise hashing matrix, and the b-bit
minwise hashing matrix are all positive definite matrices (kernels).
Interestingly, our proof for the positive definiteness of the b-bit minwise
hashing kernel naturally suggests a simple strategy to integrate b-bit hashing
with linear SVM. Our technique is particularly useful when the data can not fit
in memory, which is an increasingly critical issue in large-scale machine
learning. Our preliminary experimental results on a publicly available webspam
dataset (350K samples and 16 million dimensions) verified the effectiveness of
our algorithm. For example, the training time was reduced to merely a few
seconds. In addition, our technique can be easily extended to many other linear
and nonlinear machine learning applications such as logistic regression.
"
"  We derive exponential tail inequalities for sums of random matrices with no
dependence on the explicit matrix dimensions. These are similar to the matrix
versions of the Chernoff bound and Bernstein inequality except with the
explicit matrix dimensions replaced by a trace quantity that can be small even
when the dimension is large or infinite. Some applications to principal
component analysis and approximate matrix multiplication are given to
illustrate the utility of the new bounds.
"
"  This paper investigates the bias and the weak Bahadur representation of a
local polynomial estimator of the conditional quantile function and its
derivatives. The bias and Bahadur remainder term are studied uniformly with
respect to the quantile level, the covariates and the smoothing parameter. The
order of the local polynomial estimator can be higher than the
differentiability order of the conditional quantile function. Applications of
the results deal with global optimal consistency rates of the local polynomial
quantile estimator, performance of random bandwidths and estimation of the
conditional quantile density function. The latter allows to obtain a simple
estimator of the conditional quantile function of the private values in a first
price sealed bids auctions under the independent private values paradigm and
risk neutrality.
"
"  An important feature of linear mixed models and generalized linear mixed
models is that the conditional mean of the response given the random effects,
after transformed by a link function, is linearly related to the fixed
covariate effects and random effects. Therefore, it is of practical importance
to test the adequacy of this assumption, particularly the assumption of linear
covariate effects. In this paper, we review procedures that can be used for
testing polynomial covariate effects in these popular models. Specifically,
four types of hypothesis testing approaches are reviewed, i.e. R tests,
likelihood ratio tests, score tests and residual-based tests. Derivation and
performance of each testing procedure will be discussed, including a small
simulation study for comparing the likelihood ratio tests with the score tests.
"
"  While a user's preference is directly reflected in the interactive choice
process between her and the recommender, this wealth of information was not
fully exploited for learning recommender models. In particular, existing
collaborative filtering (CF) approaches take into account only the binary
events of user actions but totally disregard the contexts in which users'
decisions are made. In this paper, we propose Collaborative Competitive
Filtering (CCF), a framework for learning user preferences by modeling the
choice process in recommender systems. CCF employs a multiplicative latent
factor model to characterize the dyadic utility function. But unlike CF, CCF
models the user behavior of choices by encoding a local competition effect. In
this way, CCF allows us to leverage dyadic data that was previously lumped
together with missing data in existing CF models. We present two formulations
and an efficient large scale optimization algorithm. Experiments on three
real-world recommendation data sets demonstrate that CCF significantly
outperforms standard CF approaches in both offline and online evaluations.
"
"  Air pollution is a great concern because of its impact on human health and on
the environment. Statistical models play an important role in improving
knowledge of this complex spatio-temporal phenomenon and in supporting public
agencies and policy makers. We focus on the class of hierarchical models that
provides a flexible framework for incorporating spatio-temporal interactions at
different hierarchical levels. The challenge is to choose a model that is
satisfactory in terms of goodness of fit, interpretability, parsimoniousness,
prediction capability and computational costs. In order to support this choice,
we propose a comparison approach based on a set of criteria summarized in a
table that can be easily communicated to non-statisticians. Our proposal -
simple in principle but articulated in practice - holds true for many
environmental phenomena where a hierarchical structure is suitable, a
large-scale trend is included and a spatio-temporal covariance function has to
be chosen. We illustrate the details of our proposal through a case study
concerning particulate matter concentrations in Piemonte region (Italy) during
the cold season October 2005-March 2006. From the evaluation of the proposed
criteria for our case study we draw some conclusions. First, a model with a
complex hierarchical structure is globally preferable to one with a complex
spatio-temporal covariance function. Moreover, in the absence of suitable
computational resources, a model simple in structure and with a simple
covariance function can be chosen, since it shows good prediction performance
at reasonable computational costs.
"
"  We investigate the detectability of modules in large networks when the number
of modules is not known in advance. We employ the minimum description length
(MDL) principle which seeks to minimize the total amount of information
required to describe the network, and avoid overfitting. According to this
criterion, we obtain general bounds on the detectability of any prescribed
block structure, given the number of nodes and edges in the sampled network. We
also obtain that the maximum number of detectable blocks scales as $\sqrt{N}$,
where $N$ is the number of nodes in the network, for a fixed average degree
$<k>$. We also show that the simplicity of the MDL approach yields an efficient
multilevel Monte Carlo inference algorithm with a complexity of $O(\tau N\log
N)$, if the number of blocks is unknown, and $O(\tau N)$ if it is known, where
$\tau$ is the mixing time of the Markov chain. We illustrate the application of
the method on a large network of actors and films with over $10^6$ edges, and a
dissortative, bipartite block structure.
"
"  We present a method to stop the evaluation of a decision making process when
the result of the full evaluation is obvious. This trait is highly desirable
for online margin-based machine learning algorithms where a classifier
traditionally evaluates all the features for every example. We observe that
some examples are easier to classify than others, a phenomenon which is
characterized by the event when most of the features agree on the class of an
example. By stopping the feature evaluation when encountering an easy to
classify example, the learning algorithm can achieve substantial gains in
computation. Our method provides a natural attention mechanism for learning
algorithms. By modifying Pegasos, a margin-based online learning algorithm, to
include our attentive method we lower the number of attributes computed from
$n$ to an average of $O(\sqrt{n})$ features without loss in prediction
accuracy. We demonstrate the effectiveness of Attentive Pegasos on MNIST data.
"
"  We propose a novel and efficient method, that we shall call TopRank in the
following paper, for detecting change-points in high-dimensional data. This
issue is of growing concern to the network security community since network
anomalies such as Denial of Service (DoS) attacks lead to changes in Internet
traffic. Our method consists of a data reduction stage based on record
filtering, followed by a nonparametric change-point detection test based on
$U$-statistics. Using this approach, we can address massive data streams and
perform anomaly detection and localization on the fly. We show how it applies
to some real Internet traffic provided by France-T\'el\'ecom (a French Internet
service provider) in the framework of the ANR-RNRT OSCAR project. This approach
is very attractive since it benefits from a low computational load and is able
to detect and localize several types of network anomalies. We also assess the
performance of the TopRank algorithm using synthetic data and compare it with
alternative approaches based on random aggregation.
"
"  The performance (accuracy and robustness) of several clustering algorithms is
studied for linearly dependent random variables in the presence of noise. It
turns out that the error percentage quickly increases when the number of
observations is less than the number of variables. This situation is common
situation in experiments with DNA microarrays. Moreover, an {\it a posteriori}
criterion to choose between two discordant clustering algorithm is presented.
"
"  Future photometric supernova surveys will produce vastly more candidates than
can be followed up spectroscopically, highlighting the need for effective
classification methods based on lightcurves alone. Here we introduce boosting
and kernel density estimation techniques which have minimal astrophysical
input, and compare their performance on 20,000 simulated Dark Energy Survey
lightcurves. We demonstrate that these methods are comparable to the best
template fitting methods currently used, and in particular do not require the
redshift of the host galaxy or candidate. However both methods require a
training sample that is representative of the full population, so typical
spectroscopic supernova subsamples will lead to poor performance. To enable the
full potential of such blind methods, we recommend that representative training
samples should be used and so specific attention should be given to their
creation in the design phase of future photometric surveys.
"
"  Probability Density Estimation (PDE) is a multivariate discrimination
technique based on sampling signal and background densities defined by event
samples from data or Monte-Carlo (MC) simulations in a multi-dimensional phase
space. In this paper, we present a modification of the PDE method that uses a
self-adapting binning method to divide the multi-dimensional phase space in a
finite number of hyper-rectangles (cells). The binning algorithm adjusts the
size and position of a predefined number of cells inside the multi-dimensional
phase space, minimising the variance of the signal and background densities
inside the cells. The implementation of the binning algorithm PDE-Foam is based
on the MC event-generation package Foam. We present performance results for
representative examples (toy models) and discuss the dependence of the obtained
results on the choice of parameters. The new PDE-Foam shows improved
classification capability for small training samples and reduced classification
time compared to the original PDE method based on range searching.
"
"  A Bayesian Survival Analysis method is motivated and developed for analysing
sequences of scores made by a batsman in test or first class cricket. In
particular, we expect the presence of an effect whereby the distribution of
scores has more probability near zero than a geometric distribution, due to the
fact that batting is more difficult when the batsman is new at the crease. A
Metropolis-Hastings algorithm is found to be efficient at estimating the
proposed parameters, allowing us to quantify exactly how large this
early-innings effect is, and how long a batsman needs to be at the crease in
order to ``get their eye in''. Applying this model to several modern players
shows that a batsman is typically only playing at about half of their potential
ability when they first arrive at the crease, and gets their eye in
surprisingly quickly. Additionally, some players are more ``robust'' (have a
smaller early-innings effect) than others, which may have implications for
selection policy.
"
"  In this paper we discuss the variable selection method from \ell0-norm
constrained regression, which is equivalent to the problem of finding the best
subset of a fixed size. Our study focuses on two aspects, consistency and
computation. We prove that the sparse estimator from such a method can retain
all of the important variables asymptotically for even exponentially growing
dimensionality under regularity conditions. This indicates that the best subset
regression method can efficiently shrink the full model down to a submodel of a
size less than the sample size, which can be analyzed by well-developed
regression techniques for such cases in a follow-up study. We provide an
iterative algorithm, called orthogonalizing subset selection (OSS), to address
computational issues in best subset regression. OSS is an EM algorithm, and
thus possesses the monotonicity property. For any sparse estimator, OSS can
improve its fit of the model by putting it as an initial point. After this
improvement, the sparsity of the estimator is kept. Another appealing feature
of OSS is that, similarly to an effective algorithm for a continuous
optimization problem, OSS can converge to the global solution to the \ell0-norm
constrained regression problem if the initial point lies in a neighborhood of
the global solution. An accelerating algorithm of OSS and its combination with
forward stepwise selection are also investigated. Simulations and a real
example are presented to evaluate the performances of the proposed methods.
"
"  Respondent-Driven Sampling (RDS) employs a variant of a link-tracing network
sampling strategy to collect data from hard-to-reach populations. By tracing
the links in the underlying social network, the process exploits the social
structure to expand the sample and reduce its dependence on the initial
(convenience) sample.
  The primary goal of RDS is typically to estimate population averages in the
hard-to-reach population. The current estimates make strong assumptions in
order to treat the data as a probability sample. In particular, we evaluate
three critical sensitivities of the estimators: to bias induced by the initial
sample, to uncontrollable features of respondent behavior, and to the
without-replacement structure of sampling.
  This paper sounds a cautionary note for the users of RDS. While current RDS
methodology is powerful and clever, the favorable statistical properties
claimed for the current estimates are shown to be heavily dependent on often
unrealistic assumptions.
"
"  We consider a Gaussian process formulation of the multiple kernel learning
problem. The goal is to select the convex combination of kernel matrices that
best explains the data and by doing so improve the generalisation on unseen
data. Sparsity in the kernel weights is obtained by adopting a hierarchical
Bayesian approach: Gaussian process priors are imposed over the latent
functions and generalised inverse Gaussians on their associated weights. This
construction is equivalent to imposing a product of heavy-tailed process priors
over function space. A variational inference algorithm is derived for
regression and binary classification.
"
"  We propose a novel method of introducing structure into existing machine
learning techniques by developing structure-based similarity and distance
measures. To learn structural information, low-dimensional structure of the
data is captured by solving a non-linear, low-rank representation problem. We
show that this low-rank representation can be kernelized, has a closed-form
solution, allows for separation of independent manifolds, and is robust to
noise. From this representation, similarity between observations based on
non-linear structure is computed and can be incorporated into existing feature
transformations, dimensionality reduction techniques, and machine learning
methods. Experimental results on both synthetic and real data sets show
performance improvements for clustering, and anomaly detection through the use
of structural similarity.
"
"  There have been more hitting streaks in Major League Baseball than we would
expect. All batting lines of MLB hitters from 1957-2006 were randomly permuted
10,000 times and the number of hitting streaks of each length from 2 to 100 was
measured. The average count of each length streak was then compared to the
corresponding total from real-life, when the games were in chronological order.
The number of streaks in real-life was significantly higher than over the
random permutations. Non-starts (such as pinch-hitting appearances) were
removed since these may be unduly reducing the number of streaks in the
permutations; the number of streaks in the permutations increased but was still
significantly lower than real-life totals. Possible explanations are given for
why more streaks have appeared in real-life than we would expect, including
possibly the hot hand idea. Contact at trentm@email.unc.edu
"
"  Transportation distances have been used for more than a decade now in machine
learning to compare histograms of features. They have one parameter: the ground
metric, which can be any metric between the features themselves. As is the case
for all parameterized distances, transportation distances can only prove useful
in practice when this parameter is carefully chosen. To date, the only option
available to practitioners to set the ground metric parameter was to rely on a
priori knowledge of the features, which limited considerably the scope of
application of transportation distances. We propose to lift this limitation and
consider instead algorithms that can learn the ground metric using only a
training set of labeled histograms. We call this approach ground metric
learning. We formulate the problem of learning the ground metric as the
minimization of the difference of two polyhedral convex functions over a convex
set of distance matrices. We follow the presentation of our algorithms with
promising experimental results on binary classification tasks using GIST
descriptors of images taken in the Caltech-256 set.
"
"  In a wide class of paired comparisons, especially in the sports games, in
which all subjects are divided into several groups, the intragroup comparisons
are dense and the intergroup comparisons are sparse. Typical examples include
the NFL regular season. Motivated by these situations, we propose group
sparsity for paired comparisons and show the consistency and asymptotical
normality of the maximum likelihood estimate in the Bradley-Terry model when
the number of parameters goes to infinity in this paper. Simulations are
carried out to illustrate the group sparsity and asymptotical results.
"
"  Low-dimensional embedding, manifold learning, clustering, classification, and
anomaly detection are among the most important problems in machine learning.
The existing methods usually consider the case when each instance has a fixed,
finite-dimensional feature representation. Here we consider a different
setting. We assume that each instance corresponds to a continuous probability
distribution. These distributions are unknown, but we are given some i.i.d.
samples from each distribution. Our goal is to estimate the distances between
these distributions and use these distances to perform low-dimensional
embedding, clustering/classification, or anomaly detection for the
distributions. We present estimation algorithms, describe how to apply them for
machine learning tasks on distributions, and show empirical results on
synthetic data, real word images, and astronomical data sets.
"
"  This paper describes a recursive estimation procedure for multivariate binary
densities (probability distributions of vectors of Bernoulli random variables)
using orthogonal expansions. For $d$ covariates, there are $2^d$ basis
coefficients to estimate, which renders conventional approaches computationally
prohibitive when $d$ is large. However, for a wide class of densities that
satisfy a certain sparsity condition, our estimator runs in probabilistic
polynomial time and adapts to the unknown sparsity of the underlying density in
two key ways: (1) it attains near-minimax mean-squared error for moderate
sample sizes, and (2) the computational complexity is lower for sparser
densities. Our method also allows for flexible control of the trade-off between
mean-squared error and computational complexity.
"
"  We develop a Bayesian ""sum-of-trees"" model where each tree is constrained by
a regularization prior to be a weak learner, and fitting and inference are
accomplished via an iterative Bayesian backfitting MCMC algorithm that
generates samples from a posterior. Effectively, BART is a nonparametric
Bayesian regression approach which uses dimensionally adaptive random basis
elements. Motivated by ensemble methods in general, and boosting algorithms in
particular, BART is defined by a statistical model: a prior and a likelihood.
This approach enables full posterior inference including point and interval
estimates of the unknown regression function as well as the marginal effects of
potential predictors. By keeping track of predictor inclusion frequencies, BART
can also be used for model-free variable selection. BART's many features are
illustrated with a bake-off against competing methods on 42 different data
sets, with a simulation experiment and on a drug discovery classification
problem.
"
"  Using recent advances in the econometrics literature, we disentangle from
high frequency observations on the transaction prices of a large sample of NYSE
stocks a fundamental component and a microstructure noise component. We then
relate these statistical measurements of market microstructure noise to
observable characteristics of the underlying stocks and, in particular, to
different financial measures of their liquidity. We find that more liquid
stocks based on financial characteristics have lower noise and noise-to-signal
ratio measured from their high frequency returns. We then examine whether there
exists a common, market-wide, factor in high frequency stock-level measurements
of noise, and whether that factor is priced in asset returns.
"
"  In this paper we address the problem of uncertainty management for robust
design, and verification of large dynamic networks whose performance is
affected by an equally large number of uncertain parameters. Many such networks
(e.g. power, thermal and communication networks) are often composed of weakly
interacting subnetworks. We propose intrusive and non-intrusive iterative
schemes that exploit such weak interconnections to overcome dimensionality
curse associated with traditional uncertainty quantification methods (e.g.
generalized Polynomial Chaos, Probabilistic Collocation) and accelerate
uncertainty propagation in systems with large number of uncertain parameters.
This approach relies on integrating graph theoretic methods and waveform
relaxation with generalized Polynomial Chaos, and Probabilistic Collocation,
rendering these techniques scalable. We analyze convergence properties of this
scheme and illustrate it on several examples.
"
"  The data in many disciplines such as social networks, web analysis, etc. is
link-based, and the link structure can be exploited for many different data
mining tasks. In this paper, we consider the problem of temporal link
prediction: Given link data for times 1 through T, can we predict the links at
time T+1? If our data has underlying periodic structure, can we predict out
even further in time, i.e., links at time T+2, T+3, etc.? In this paper, we
consider bipartite graphs that evolve over time and consider matrix- and
tensor-based methods for predicting future links. We present a weight-based
method for collapsing multi-year data into a single matrix. We show how the
well-known Katz method for link prediction can be extended to bipartite graphs
and, moreover, approximated in a scalable way using a truncated singular value
decomposition. Using a CANDECOMP/PARAFAC tensor decomposition of the data, we
illustrate the usefulness of exploiting the natural three-dimensional structure
of temporal link data. Through several numerical experiments, we demonstrate
that both matrix- and tensor-based techniques are effective for temporal link
prediction despite the inherent difficulty of the problem. Additionally, we
show that tensor-based techniques are particularly effective for temporal data
with varying periodic patterns.
"
"  New fast estimation methods stemming from control theory lead to a fresh look
at time series, which bears some resemblance to ""technical analysis"". The
results are applied to a typical object of financial engineering, namely the
forecast of foreign exchange rates, via a ""model-free"" setting, i.e., via
repeated identifications of low order linear difference equations on sliding
short time windows. Several convincing computer simulations, including the
prediction of the position and of the volatility with respect to the forecasted
trendline, are provided. $\mathcal{Z}$-transform and differential algebra are
the main mathematical tools.
"
"  We introduce in this paper a new algorithm for Multi-Armed Bandit (MAB)
problems. A machine learning paradigm popular within Cognitive Network related
topics (e.g., Spectrum Sensing and Allocation). We focus on the case where the
rewards are exponentially distributed, which is common when dealing with
Rayleigh fading channels. This strategy, named Multiplicative Upper Confidence
Bound (MUCB), associates a utility index to every available arm, and then
selects the arm with the highest index. For every arm, the associated index is
equal to the product of a multiplicative factor by the sample mean of the
rewards collected by this arm. We show that the MUCB policy has a low
complexity and is order optimal.
"
"  In this paper we examine rigorously the evidence for dependence among data
size, transfer rate and duration in Internet flows. We emphasize two
statistical approaches for studying dependence, including Pearson's correlation
coefficient and the extremal dependence analysis method. We apply these methods
to large data sets of packet traces from three networks. Our major results show
that Pearson's correlation coefficients between size and duration are much
smaller than one might expect. We also find that correlation coefficients
between size and rate are generally small and can be strongly affected by
applying thresholds to size or duration. Based on Transmission Control Protocol
connection startup mechanisms, we argue that thresholds on size should be more
useful than thresholds on duration in the analysis of correlations. Using
extremal dependence analysis, we draw a similar conclusion, finding remarkable
independence for extremal values of size and rate.
"
"  We develop a method to perform model averaging in two-stage linear regression
systems subject to endogeneity. Our method extends an existing Gibbs sampler
for instrumental variables to incorporate a component of model uncertainty.
Direct evaluation of model probabilities is intractable in this setting. We
show that by nesting model moves inside the Gibbs sampler, model comparison can
be performed via conditional Bayes factors, leading to straightforward
calculations. This new Gibbs sampler is only slightly more involved than the
original algorithm and exhibits no evidence of mixing difficulties. We conclude
with a study of two different modeling challenges: incorporating uncertainty
into the determinants of macroeconomic growth, and estimating a demand function
by instrumenting wholesale on retail prices.
"
"  In unsupervised classification, Hidden Markov Models (HMM) are used to
account for a neighborhood structure between observations. The emission
distributions are often supposed to belong to some parametric family. In this
paper, a semiparametric modeling where the emission distributions are a mixture
of parametric distributions is proposed to get a higher flexibility. We show
that the classical EM algorithm can be adapted to infer the model parameters.
For the initialisation step, starting from a large number of components, a
hierarchical method to combine them into the hidden states is proposed. Three
likelihood-based criteria to select the components to be combined are
discussed. To estimate the number of hidden states, BIC-like criteria are
derived. A simulation study is carried out both to determine the best
combination between the merging criteria and the model selection criteria and
to evaluate the accuracy of classification. The proposed method is also
illustrated using a biological dataset from the model plant Arabidopsis
thaliana. A R package HMMmix is freely available on the CRAN.
"
"  Multitype branching processes with immigration in one type are used to model
the dynamics of stage-structured plant populations. Parametric inference is
first carried out when count data of all types are observed. Statistical
identifiability is proved together with derivation of consistent and
asymptotically Gaussian estimators for all the parameters ruling the population
dynamics model. However, for many ecological data, some stages (i.e. types)
cannot be observed in practice. We study which mechanisms can still be
estimated given the model and the data available in this context. Parametric
inference is investigated in the case of Poisson distributions. We prove that
identifiability holds for only a subset of the parameter set depend- ing on the
number of generations observed, together with consistent and asymptotic
properties of estimators. Finally, simulations are performed to study the
behaviour of the estimators when the model is no longer Poisson. Quite good
results are obtained for a large class of models with distributions having mean
and variance within the same order of magnitude, leading to some stability
results with respect to the Poisson assumption.
"
"  We provide rigorous guarantees on learning with the weighted trace-norm under
arbitrary sampling distributions. We show that the standard weighted trace-norm
might fail when the sampling distribution is not a product distribution (i.e.
when row and column indexes are not selected independently), present a
corrected variant for which we establish strong learning guarantees, and
demonstrate that it works better in practice. We provide guarantees when
weighting by either the true or empirical sampling distribution, and suggest
that even if the true distribution is known (or is uniform), weighting by the
empirical distribution may be beneficial.
"
"  Coping with outliers contaminating dynamical processes is of major importance
in various applications because mismatches from nominal models are not uncommon
in practice. In this context, the present paper develops novel fixed-lag and
fixed-interval smoothing algorithms that are robust to outliers simultaneously
present in the measurements {\it and} in the state dynamics. Outliers are
handled through auxiliary unknown variables that are jointly estimated along
with the state based on the least-squares criterion that is regularized with
the $\ell_1$-norm of the outliers in order to effect sparsity control. The
resultant iterative estimators rely on coordinate descent and the alternating
direction method of multipliers, are expressed in closed form per iteration,
and are provably convergent. Additional attractive features of the novel doubly
robust smoother include: i) ability to handle both types of outliers; ii)
universality to unknown nominal noise and outlier distributions; iii)
flexibility to encompass maximum a posteriori optimal estimators with reliable
performance under nominal conditions; and iv) improved performance relative to
competing alternatives at comparable complexity, as corroborated via simulated
tests.
"
"  We develop Graph-Coupled Hidden Markov Models (GCHMMs) for modeling the
spread of infectious disease locally within a social network. Unlike most
previous research in epidemiology, which typically models the spread of
infection at the level of entire populations, we successfully leverage mobile
phone data collected from 84 people over an extended period of time to model
the spread of infection on an individual level. Our model, the GCHMM, is an
extension of widely-used Coupled Hidden Markov Models (CHMMs), which allow
dependencies between state transitions across multiple Hidden Markov Models
(HMMs), to situations in which those dependencies are captured through the
structure of a graph, or to social networks that may change over time. The
benefit of making infection predictions on an individual level is enormous, as
it allows people to receive more personalized and relevant health advice.
"
"  This paper considers fixed effects estimation and inference in linear and
nonlinear panel data models with random coefficients and endogenous regressors.
The quantities of interest -- means, variances, and other moments of the random
coefficients -- are estimated by cross sectional sample moments of GMM
estimators applied separately to the time series of each individual. To deal
with the incidental parameter problem introduced by the noise of the
within-individual estimators in short panels, we develop bias corrections.
These corrections are based on higher-order asymptotic expansions of the GMM
estimators and produce improved point and interval estimates in moderately long
panels. Under asymptotic sequences where the cross sectional and time series
dimensions of the panel pass to infinity at the same rate, the uncorrected
estimator has an asymptotic bias of the same order as the asymptotic variance.
The bias corrections remove the bias without increasing variance. An empirical
example on cigarette demand based on Becker, Grossman and Murphy (1994) shows
significant heterogeneity in the price effect across U.S. states.
"
"  We use convex relaxation techniques to provide a sequence of solutions to the
matrix completion problem. Using the nuclear norm as a regularizer, we provide
simple and very efficient algorithms for minimizing the reconstruction error
subject to a bound on the nuclear norm. Our algorithm iteratively replaces the
missing elements with those obtained from a thresholded SVD. With warm starts
this allows us to efficiently compute an entire regularization path of
solutions.
"
"  Kernel density estimation (KDE) is a popular statistical technique for
estimating the underlying density distribution with minimal assumptions.
Although they can be shown to achieve asymptotic estimation optimality for any
input distribution, cross-validating for an optimal parameter requires
significant computation dominated by kernel summations. In this paper we
present an improvement to the dual-tree algorithm, the first practical kernel
summation algorithm for general dimension. Our extension is based on the
series-expansion for the Gaussian kernel used by fast Gauss transform. First,
we derive two additional analytical machinery for extending the original
algorithm to utilize a hierarchical data structure, demonstrating the first
truly hierarchical fast Gauss transform. Second, we show how to integrate the
series-expansion approximation within the dual-tree approach to compute kernel
summations with a user-controllable relative error bound. We evaluate our
algorithm on real-world datasets in the context of optimal bandwidth selection
in kernel density estimation. Our results demonstrate that our new algorithm is
the only one that guarantees a hard relative error bound and offers fast
performance across a wide range of bandwidths evaluated in cross validation
procedures.
"
"  We study the scenario of graph-based clustering algorithms such as spectral
clustering. Given a set of data points, one first has to construct a graph on
the data points and then apply a graph clustering algorithm to find a suitable
partition of the graph. Our main question is if and how the construction of the
graph (choice of the graph, choice of parameters, choice of weights) influences
the outcome of the final clustering result. To this end we study the
convergence of cluster quality measures such as the normalized cut or the
Cheeger cut on various kinds of random geometric graphs as the sample size
tends to infinity. It turns out that the limit values of the same objective
function are systematically different on different types of graphs. This
implies that clustering results systematically depend on the graph and can be
very different for different types of graph. We provide examples to illustrate
the implications on spectral clustering.
"
"  We express the classic ARMA time-series model as a directed graphical model.
In doing so, we find that the deterministic relationships in the model make it
effectively impossible to use the EM algorithm for learning model parameters.
To remedy this problem, we replace the deterministic relationships with
Gaussian distributions having a small variance, yielding the stochastic ARMA
(ARMA) model. This modification allows us to use the EM algorithm to learn
parmeters and to forecast,even in situations where some data is missing. This
modification, in conjunction with the graphicalmodel approach, also allows us
to include cross predictors in situations where there are multiple times series
and/or additional nontemporal covariates. More surprising,experiments suggest
that the move to stochastic ARMA yields improved accuracy through better
smoothing. We demonstrate improvements afforded by cross prediction and better
smoothing on real data.
"
"  The explore{exploit dilemma is one of the central challenges in Reinforcement
Learning (RL). Bayesian RL solves the dilemma by providing the agent with
information in the form of a prior distribution over environments; however,
full Bayesian planning is intractable. Planning with the mean MDP is a common
myopic approximation of Bayesian planning. We derive a novel reward bonus that
is a function of the posterior distribution over environments, which, when
added to the reward in planning with the mean MDP, results in an agent which
explores efficiently and effectively. Although our method is similar to
existing methods when given an uninformative or unstructured prior, unlike
existing methods, our method can exploit structured priors. We prove that our
method results in a polynomial sample complexity and empirically demonstrate
its advantages in a structured exploration task.
"
"  We exhibit compelling evidence regarding how well does the MaxEnt principle
describe the rank-distribution of city-populations via an exhaustive study of
the 50 Spanish provinces (more than 8000 cities) in a time-window of 15 years
(1996-2010). We show that the dynamics that governs the population-growth is
the deciding factor that originates the observed distributions. The connection
between dynamics and distributions is unravelled via MaxEnt.
"
"  This thesis is dedicated to the statistical analysis of multi-sub ject fMRI
data, with the purpose of identifying bain structures involved in certain
cognitive or sensori-motor tasks, in a reproducible way across sub jects. To
overcome certain limitations of standard voxel-based testing methods, as
implemented in the Statistical Parametric Mapping (SPM) software, we introduce
a Bayesian model selection approach to this problem, meaning that the most
probable model of cerebral activity given the data is selected from a
pre-defined collection of possible models. Based on a parcellation of the brain
volume into functionally homogeneous regions, each model corresponds to a
partition of the regions into those involved in the task under study and those
inactive. This allows to incorporate prior information, and avoids the
dependence of the SPM-like approach on an arbitrary threshold, called the
cluster- forming threshold, to define active regions. By controlling a Bayesian
risk, our approach balances false positive and false negative risk control.
Furthermore, it is based on a generative model that accounts for the spatial
uncertainty on the localization of individual effects, due to spatial
normalization errors. On both simulated and real fMRI datasets, we show that
this new paradigm corrects several biases of the SPM-like approach, which
either swells or misses the different active regions, depending on the choice
of a cluster-forming threshold.
"
"  We propose a number of techniques for obtaining a global ranking from data
that may be incomplete and imbalanced -- characteristics almost universal to
modern datasets coming from e-commerce and internet applications. We are
primarily interested in score or rating-based cardinal data. From raw ranking
data, we construct pairwise rankings, represented as edge flows on an
appropriate graph. Our statistical ranking method uses the graph Helmholtzian,
the graph theoretic analogue of the Helmholtz operator or vector Laplacian, in
much the same way the graph Laplacian is an analogue of the Laplace operator or
scalar Laplacian. We study the graph Helmholtzian using combinatorial Hodge
theory: we show that every edge flow representing pairwise ranking can be
resolved into two orthogonal components, a gradient flow that represents the
L2-optimal global ranking and a divergence-free flow (cyclic) that measures the
validity of the global ranking obtained -- if this is large, then the data does
not have a meaningful global ranking. This divergence-free flow can be further
decomposed orthogonally into a curl flow (locally cyclic) and a harmonic flow
(locally acyclic but globally cyclic); these provides information on whether
inconsistency arises locally or globally. An obvious advantage over the NP-hard
Kemeny optimization is that discrete Hodge decomposition may be computed via a
linear least squares regression. We also investigated the L1-projection of edge
flows, showing that this is dual to correlation maximization over bounded
divergence-free flows, and the L1-approximate sparse cyclic ranking, showing
that this is dual to correlation maximization over bounded curl-free flows. We
discuss relations with Kemeny optimization, Borda count, and Kendall-Smith
consistency index from social choice theory and statistics.
"
"  Support vector machines (SVMs) are invaluable tools for many practical
applications in artificial intelligence, e.g., classification and event
recognition. However, popular SVM solvers are not sufficiently efficient for
applications with a great deal of samples as well as a large number of
features. In this paper, thus, we present NESVM, a fast gradient SVM solver
that can optimize various SVM models, e.g., classical SVM, linear programming
SVM and least square SVM. Compared against SVM-Perf
\cite{SVM_Perf}\cite{PerfML} (its convergence rate in solving the dual SVM is
upper bounded by $\mathcal O(1/\sqrt{k})$, wherein $k$ is the number of
iterations.) and Pegasos \cite{Pegasos} (online SVM that converges at rate
$\mathcal O(1/k)$ for the primal SVM), NESVM achieves the optimal convergence
rate at $\mathcal O(1/k^{2})$ and a linear time complexity. In particular,
NESVM smoothes the non-differentiable hinge loss and $\ell_1$-norm in the
primal SVM. Then the optimal gradient method without any line search is adopted
to solve the optimization. In each iteration round, the current gradient and
historical gradients are combined to determine the descent direction, while the
Lipschitz constant determines the step size. Only two matrix-vector
multiplications are required in each iteration round. Therefore, NESVM is more
efficient than existing SVM solvers. In addition, NESVM is available for both
linear and nonlinear kernels. We also propose ""homotopy NESVM"" to accelerate
NESVM by dynamically decreasing the smooth parameter and using the continuation
method. Our experiments on census income categorization, indoor/outdoor scene
classification, event recognition and scene recognition suggest the efficiency
and the effectiveness of NESVM. The MATLAB code of NESVM will be available on
our website for further assessment.
"
"  We address the problem of learning the parameters in graphical models when
inference is intractable. A common strategy in this case is to replace the
partition function with its Bethe approximation. We show that there exists a
regime of empirical marginals where such Bethe learning will fail. By failure
we mean that the empirical marginals cannot be recovered from the approximated
maximum likelihood parameters (i.e., moment matching is not achieved). We
provide several conditions on empirical marginals that yield outer and inner
bounds on the set of Bethe learnable marginals. An interesting implication of
our results is that there exists a large class of marginals that cannot be
obtained as stable fixed points of belief propagation. Taken together our
results provide a novel approach to analyzing learning with Bethe
approximations and highlight when it can be expected to work or fail.
"
"  Multitask learning algorithms are typically designed assuming some fixed, a
priori known latent structure shared by all the tasks. However, it is usually
unclear what type of latent task structure is the most appropriate for a given
multitask learning problem. Ideally, the ""right"" latent task structure should
be learned in a data-driven manner. We present a flexible, nonparametric
Bayesian model that posits a mixture of factor analyzers structure on the
tasks. The nonparametric aspect makes the model expressive enough to subsume
many existing models of latent task structures (e.g, mean-regularized tasks,
clustered tasks, low-rank or linear/non-linear subspace assumption on tasks,
etc.). Moreover, it can also learn more general task structures, addressing the
shortcomings of such models. We present a variational inference algorithm for
our model. Experimental results on synthetic and real-world datasets, on both
regression and classification problems, demonstrate the effectiveness of the
proposed method.
"
"  In this paper changes in wind speed and wind direction from a measured wind
field are being analyzed at high frequencies. This is used to estimate changes
in the angle of attack (AOA) on a blade segment over short time periods for
different estimated turbine concepts. Here a statistical approach is chosen to
grasp the characteristics of the probability distributions to give an over all
view of the magnitude and rate of the changes. The main interest is the
generation of basic distributions for the calculation of dynamic stall effects
and stall flutter due to wind fluctuations.
"
"  Many classification problems involve data instances that are interlinked with
each other, such as webpages connected by hyperlinks. Techniques for
""collective classification"" (CC) often increase accuracy for such data graphs,
but usually require a fully-labeled training graph. In contrast, we examine how
to improve the semi-supervised learning of CC models when given only a
sparsely-labeled graph, a common situation. We first describe how to use novel
combinations of classifiers to exploit the different characteristics of the
relational features vs. the non-relational features. We also extend the ideas
of ""label regularization"" to such hybrid classifiers, enabling them to leverage
the unlabeled data to bias the learning process. We find that these techniques,
which are efficient and easy to implement, significantly increase accuracy on
three real datasets. In addition, our results explain conflicting findings from
prior related studies.
"
"  We present a method to stop the evaluation of a prediction process when the
result of the full evaluation is obvious. This trait is highly desirable in
prediction tasks where a predictor evaluates all its features for every example
in large datasets. We observe that some examples are easier to classify than
others, a phenomenon which is characterized by the event when most of the
features agree on the class of an example. By stopping the feature evaluation
when encountering an easy- to-classify example, the predictor can achieve
substantial gains in computation. Our method provides a natural attention
mechanism for linear predictors where the predictor concentrates most of its
computation on hard-to-classify examples and quickly discards easy-to-classify
ones. By modifying a linear prediction algorithm such as an SVM or AdaBoost to
include our attentive method we prove that the average number of features
computed is O(sqrt(n log 1/sqrt(delta))) where n is the original number of
features, and delta is the error rate incurred due to early stopping. We
demonstrate the effectiveness of Attentive Prediction on MNIST, Real-sim,
Gisette, and synthetic datasets.
"
"  We propose a novel Bayesian approach to solve stochastic optimization
problems that involve finding extrema of noisy, nonlinear functions. Previous
work has focused on representing possible functions explicitly, which leads to
a two-step procedure of first, doing inference over the function space and
second, finding the extrema of these functions. Here we skip the representation
step and directly model the distribution over extrema. To this end, we devise a
non-parametric conjugate prior based on a kernel regressor. The resulting
posterior distribution directly captures the uncertainty over the maximum of
the unknown function. We illustrate the effectiveness of our model by
optimizing a noisy, high-dimensional, non-convex objective function.
"
"  We present a Bayesian surrogate model for the analysis of periodic or
quasi-periodic time series data. We describe a computationally efficient
implementation that enables Bayesian model comparison. We apply this model to
simulated and real exoplanet observations. We discuss the results and
demonstrate some of the challenges for applying our surrogate model to
realistic exoplanet data sets. In particular, we find that analyses of real
world data should pay careful attention to the effects of uneven spacing of
observations and the choice of prior for the ""jitter"" parameter.
"
"  Models in which the number of goals scored by a team in a soccer match follow
a Poisson distribution, or a closely related one, have been widely discussed.
We here consider a soccer match as an experiment to assess which of two teams
is superior and examine the probability that the outcome of the experiment
(match) truly represents the relative abilities of the two teams. Given a final
score, it is possible by using a Bayesian approach to quantify the probability
that it was or was not the case that 'the best team won'. For typical scores,
the probability of a misleading result is significant. Modifying the rules of
the game to increase the typical number of goals scored would improve the
situation, but a level of confidence that would normally be regarded as
satisfactory could not be obtained unless the character of the game was
radically changed.
"
"  Originally devised for baseball, the Pythagorean Won-Loss formula estimates
the percentage of games a team should have won at a particular point in a
season. For decades, this formula had no mathematical justification. In 2006,
Steven Miller provided a statistical derivation by making some heuristic
assumptions about the distributions of runs scored and allowed by baseball
teams. We make a similar set of assumptions about hockey teams and show that
the formula is just as applicable to hockey as it is to baseball. We hope that
this work spurs research in the use of the Pythagorean Won-Loss formula as an
evaluative tool for sports outside baseball.
"
"  Variable selection is recognized as one of the most critical steps in
statistical modeling. The problems encountered in engineering and social
sciences are commonly characterized by over-abundance of explanatory variables,
non-linearities and unknown interdependencies between the regressors. An added
difficulty is that the analysts may have little or no prior knowledge on the
relative importance of the variables. To provide a robust method for model
selection, this paper introduces the Multi-objective Genetic Algorithm for
Variable Selection (MOGA-VS) that provides the user with an optimal set of
regression models for a given data-set. The algorithm considers the regression
problem as a two objective task, and explores the Pareto-optimal (best subset)
models by preferring those models over the other which have less number of
regression coefficients and better goodness of fit. The model exploration can
be performed based on in-sample or generalization error minimization. The model
selection is proposed to be performed in two steps. First, we generate the
frontier of Pareto-optimal regression models by eliminating the dominated
models without any user intervention. Second, a decision making process is
executed which allows the user to choose the most preferred model using
visualisations and simple metrics. The method has been evaluated on a recently
published real dataset on Communities and Crime within United States.
"
"  Cognitive Radio generates a big interest as a key cost-effective solution for
the underutilization of frequency spectrum in legacy communication networks.
The objective of this work lies in conducting a performance evaluation of the
end-to-end message delivery under both Markovian and Poissonian primary
traffics in lossy Cognitive Radio networks. We aim at inferring the most
appropriate conditions for an efficient secondary service provision according
to the Cognitive Radio network characteristics. Meanwhile, we have performed a
general analysis for many still open issues in Cognitive Radio, but at the end
only two critical aspects have been considered, namely, the unforeseen primary
reclaims in addition to the collided cognitive transmissions due to the
Opportunistic Spectrum Sharing. Some graphs, in view of the average Spectral
Efficiency, have been computed and plotted to report some comparative results
for a given video transmission under the Markovian and the Poissonian primary
interruptions.
"
"  In recent years, the crucial importance of metrics in machine learning
algorithms has led to an increasing interest for optimizing distance and
similarity functions. Most of the state of the art focus on learning
Mahalanobis distances (requiring to fulfill a constraint of positive
semi-definiteness) for use in a local k-NN algorithm. However, no theoretical
link is established between the learned metrics and their performance in
classification. In this paper, we make use of the formal framework of good
similarities introduced by Balcan et al. to design an algorithm for learning a
non PSD linear similarity optimized in a nonlinear feature space, which is then
used to build a global linear classifier. We show that our approach has uniform
stability and derive a generalization bound on the classification error.
Experiments performed on various datasets confirm the effectiveness of our
approach compared to state-of-the-art methods and provide evidence that (i) it
is fast, (ii) robust to overfitting and (iii) produces very sparse classifiers.
"
"  This paper addresses the issue of model selection for hidden Markov models
(HMMs). We generalize factorized asymptotic Bayesian inference (FAB), which has
been recently developed for model selection on independent hidden variables
(i.e., mixture models), for time-dependent hidden variables. As with FAB in
mixture models, FAB for HMMs is derived as an iterative lower bound
maximization algorithm of a factorized information criterion (FIC). It
inherits, from FAB for mixture models, several desirable properties for
learning HMMs, such as asymptotic consistency of FIC with marginal
log-likelihood, a shrinkage effect for hidden state selection, monotonic
increase of the lower FIC bound through the iterative optimization. Further, it
does not have a tunable hyper-parameter, and thus its model selection process
can be fully automated. Experimental results shows that FAB outperforms
states-of-the-art variational Bayesian HMM and non-parametric Bayesian HMM in
terms of model selection accuracy and computational efficiency.
"
"  The AdaBoost algorithm was designed to combine many ""weak"" hypotheses that
perform slightly better than random guessing into a ""strong"" hypothesis that
has very low error. We study the rate at which AdaBoost iteratively converges
to the minimum of the ""exponential loss."" Unlike previous work, our proofs do
not require a weak-learning assumption, nor do they require that minimizers of
the exponential loss are finite. Our first result shows that at iteration $t$,
the exponential loss of AdaBoost's computed parameter vector will be at most
$\epsilon$ more than that of any parameter vector of $\ell_1$-norm bounded by
$B$ in a number of rounds that is at most a polynomial in $B$ and $1/\epsilon$.
We also provide lower bounds showing that a polynomial dependence on these
parameters is necessary. Our second result is that within $C/\epsilon$
iterations, AdaBoost achieves a value of the exponential loss that is at most
$\epsilon$ more than the best possible value, where $C$ depends on the dataset.
We show that this dependence of the rate on $\epsilon$ is optimal up to
constant factors, i.e., at least $\Omega(1/\epsilon)$ rounds are necessary to
achieve within $\epsilon$ of the optimal exponential loss.
"
"  We consider the problem of search through comparisons, where a user is
presented with two candidate objects and reveals which is closer to her
intended target. We study adaptive strategies for finding the target, that
require knowledge of rank relationships but not actual distances between
objects. We propose a new strategy based on rank nets, and show that for target
distributions with a bounded doubling constant, it finds the target in a number
of comparisons close to the entropy of the target distribution and, hence, of
the optimum. We extend these results to the case of noisy oracles, and compare
this strategy to prior art over multiple datasets.
"
"  Sparse coding--that is, modelling data vectors as sparse linear combinations
of basis elements--is widely used in machine learning, neuroscience, signal
processing, and statistics. This paper focuses on the large-scale matrix
factorization problem that consists of learning the basis set, adapting it to
specific data. Variations of this problem include dictionary learning in signal
processing, non-negative matrix factorization and sparse principal component
analysis. In this paper, we propose to address these tasks with a new online
optimization algorithm, based on stochastic approximations, which scales up
gracefully to large datasets with millions of training samples, and extends
naturally to various matrix factorization formulations, making it suitable for
a wide range of learning problems. A proof of convergence is presented, along
with experiments with natural images and genomic data demonstrating that it
leads to state-of-the-art performance in terms of speed and optimization for
both small and large datasets.
"
"  Often in sequential trials additional data become available after a stopping
boundary has been reached. A method of incorporating such information from
overrunning is developed, based on the ``adding weighted Zs'' method of
combining $p$-values. This yields a combined $p$-value for the primary test and
a median-unbiased estimate and confidence bounds for the parameter under test.
When the amount of overrunning information is proportional to the amount
available upon terminating the sequential test, exact inference methods are
provided; otherwise, approximate methods are given and evaluated. The context
is that of observing a Brownian motion with drift, with either linear stopping
boundaries in continuous time or discrete-time group-sequential boundaries. The
method is compared with other available methods and is exemplified with data
from two sequential clinical trials.
"
"  We mathematically prove that an existing linear predictor of baseball teams'
winning percentages (Jones and Tappin 2005) is simply just a first-order
approximation to Bill James' Pythagorean Won-Loss formula and can thus be
written in terms of the formula's well-known exponent. We estimate the linear
model on twenty seasons of Major League Baseball data and are able to verify
that the resulting coefficient estimate, with 95% confidence, is virtually
identical to the empirically accepted value of 1.82. Our work thus helps
explain why this simple and elegant model is such a strong linear predictor.
"
"  We address the problem of computing approximate marginals in Gaussian
probabilistic models by using mean field and fractional Bethe approximations.
As an extension of Welling and Teh (2001), we define the Gaussian fractional
Bethe free energy in terms of the moment parameters of the approximate
marginals and derive an upper and lower bound for it. We give necessary
conditions for the Gaussian fractional Bethe free energies to be bounded from
below. It turns out that the bounding condition is the same as the pairwise
normalizability condition derived by Malioutov et al. (2006) as a sufficient
condition for the convergence of the message passing algorithm. By giving a
counterexample, we disprove the conjecture in Welling and Teh (2001): even when
the Bethe free energy is not bounded from below, it can possess a local minimum
to which the minimization algorithms can converge.
"
"  We introduce a novel implementation in ANSI C of the MINE family of
algorithms for computing maximal information-based measures of dependence
between two variables in large datasets, with the aim of a low memory footprint
and ease of integration within bioinformatics pipelines. We provide the
libraries minerva (with the R interface) and minepy for Python, MATLAB, Octave
and C++. The C solution reduces the large memory requirement of the original
Java implementation, has good upscaling properties, and offers a native
parallelization for the R interface. Low memory requirements are demonstrated
on the MINE benchmarks as well as on large (n=1340) microarray and Illumina
GAII RNA-seq transcriptomics datasets.
  Availability and Implementation: Source code and binaries are freely
available for download under GPL3 licence at http://minepy.sourceforge.net for
minepy and through the CRAN repository http://cran.r-project.org for the R
package minerva. All software is multiplatform (MS Windows, Linux and OSX).
"
"  Observations consisting of measurements on relationships for pairs of objects
arise in many settings, such as protein interaction and gene regulatory
networks, collections of author-recipient email, and social networks. Analyzing
such data with probabilisic models can be delicate because the simple
exchangeability assumptions underlying many boilerplate models no longer hold.
In this paper, we describe a latent variable model of such data called the
mixed membership stochastic blockmodel. This model extends blockmodels for
relational data to ones which capture mixed membership latent relational
structure, thus providing an object-specific low-dimensional representation. We
develop a general variational inference algorithm for fast approximate
posterior inference. We explore applications to social and protein interaction
networks.
"
"  Removing noise from piecewise constant (PWC) signals, is a challenging signal
processing problem arising in many practical contexts. For example, in
exploration geosciences, noisy drill hole records need separating into
stratigraphic zones, and in biophysics, jumps between molecular dwell states
need extracting from noisy fluorescence microscopy signals. Many PWC denoising
methods exist, including total variation regularization, mean shift clustering,
stepwise jump placement, running medians, convex clustering shrinkage and
bilateral filtering; conventional linear signal processing methods are
fundamentally unsuited however. This paper shows that most of these methods are
associated with a special case of a generalized functional, minimized to
achieve PWC denoising. The minimizer can be obtained by diverse solver
algorithms, including stepwise jump placement, convex programming, finite
differences, iterated running medians, least angle regression, regularization
path following, and coordinate descent. We introduce novel PWC denoising
methods, which, for example, combine global mean shift clustering with local
total variation smoothing. Head-to-head comparisons between these methods are
performed on synthetic data, revealing that our new methods have a useful role
to play. Finally, overlaps between the methods of this paper and others such as
wavelet shrinkage, hidden Markov models, and piecewise smooth filtering are
touched on.
"
"  The present paper aims at locating the breakings of the integration process
of an international system observed during about 50 years in the 19th century.
A historical study could link them to special events, which operated as
exogenous shocks on this process. The indicator of integration used is the
spread between the highest and the lowest among the London, Hamburg and Paris
gold-silver prices. Three algorithms are combined to study this integration: a
periodization obtained with the SOM algorithm is confronted to the estimation
of a two-regime Markov switching model, in order to give an interpretation of
the changes of regime; in the same time change-points are identified over the
whole period providing a more precise interpretation of the various types of
regulation.
"
"  The detection of molecular signatures of selection is one of the major
concerns of modern population genetics. A widely used strategy in this context
is to compare samples from several populations, and to look for genomic regions
with outstanding genetic differentiation between these populations. Genetic
differentiation is generally based on allele frequency differences between
populations, which are measured by Fst or related statistics. Here we introduce
a new statistic, denoted hapFLK, which focuses instead on the differences of
haplotype frequencies between populations. In contrast to most existing
statistics, hapFLK accounts for the hierarchical structure of the sampled
populations. Using computer simulations, we show that each of these two
features - the use of haplotype information and of the hierarchical structure
of populations - significantly improves the detection power of selected loci,
and that combining them in the hapFLK statistic provides even greater power. We
also show that hapFLK is robust with respect to bottlenecks and migration and
improves over existing approaches in many situations. Finally, we apply hapFLK
to a set of six sheep breeds from Northern Europe, and identify seven regions
under selection, which include already reported regions but also several new
ones. We propose a method to help identifying the population(s) under selection
in a detected region, which reveals that in many of these regions selection
most likely occurred in more than one population. Furthermore, several of the
detected regions correspond to incomplete sweeps, where the favourable
haplotype is only at intermediate frequency in the population(s) under
selection.
"
"  We introduce a new framework for the analysis of the dynamics of networks,
based on randomly reinforced urn (RRU) processes, in which the weight of the
edges is determined by a reinforcement mechanism. We rigorously explain the
empirical evidence that in many real networks there is a subset of ""dominant
edges"" that control a major share of the total weight of the network.
Furthermore, we introduce a new statistical procedure to study the evolution of
networks over time, assessing if a given instance of the nework is taken at its
steady state or not. Our results are quite general, since they are not based on
a particular probability distribution or functional form of the weights. We
test our model in the context of the International Trade Network, showing the
existence of a core of dominant links and determining its size.
"
"  It is known that fixed points of loopy belief propagation (BP) correspond to
stationary points of the Bethe variational problem, where we minimize the Bethe
free energy subject to normalization and marginalization constraints.
Unfortunately, this does not entirely explain BP because BP is a dual rather
than primal algorithm to solve the Bethe variational problem -- beliefs are
infeasible before convergence. Thus, we have no better understanding of BP than
as an algorithm to seek for a common zero of a system of non-linear functions,
not explicitly related to each other. In this theoretical paper, we show that
these functions are in fact explicitly related -- they are the partial
derivatives of a single function of reparameterizations. That means, BP seeks
for a stationary point of a single function, without any constraints. This
function has a very natural form: it is a linear combination of local
log-partition functions, exactly as the Bethe entropy is the same linear
combination of local entropies.
"
"  We consider the problem of decentralized estimation using wireless sensor
networks. Specifically, we propose a novel framework based on level-triggered
sampling, a non-uniform sampling strategy, and sequential estimation. The
proposed estimator can be used as an asymptotically optimal fixed-sample-size
decentralized estimator under non-fading listening channels (through which
sensors collect their observations), as an alternative to the one-shot
estimators commonly found in the literature. It can also be used as an
asymptotically optimal sequential decentralized estimator under fading
listening channels. We show that the optimal centralized estimator under
Gaussian noise is characterized by two processes, namely the observed Fisher
information U_t, and the observed correlation V_t. It is noted that under
non-fading listening channels only V_t is random, whereas under fading
listening channels both U_t and V_t are random. In the proposed scheme, each
sensor computes its local random process(es), and sends a single bit to the
fusion center (FC) whenever the local random process(es) pass(es) certain
predefined levels. The FC, upon receiving a bit from a sensor, updates its
approximation to the corresponding global random process, and accordingly its
estimate. The sequential estimation process terminates when the observed Fisher
information (or the approximation to it) reaches a target value. We provide an
asymptotic analysis for the proposed estimator and also the one based on
conventional uniform-in-time sampling under both non-fading and fading
channels; and determine the conditions under which they are asymptotically
optimal, consistent, and asymptotically unbiased. Analytical results, together
with simulation results, demonstrate the superiority of the proposed estimator
based on level-triggered sampling over the traditional decentralized estimator
based on uniform sampling.
"
"  The result of 2-dimensional Gaussian lattice fit to a speckle intensity
pattern based on a linear model that includes nearest-neighbor interactions is
presented. We also include a Monte Carlo simulation of the same spatial speckle
pattern that takes the nearest-neighbor interactions into account. These
nearest-neighbor interactions lead to a spatial variance structure on the
lattice. The resulting spatial pattern fluctuates in value from point to point
in a manner characteristic of a stationary stochastic process. The value at a
lattice point in the simulation is interpreted as an inten-sity level and the
difference in values in neighboring cells produces a fluctuating intensity
pattern on the lattice. Changing the size of the mesh changes the relative size
of the speckles. Increasing the mesh size tends to average out the intensity in
the direction of the mean of the stationary process.
"
"  Graphs and networks are common ways of depicting biological information. In
biology, many different biological processes are represented by graphs, such as
regulatory networks, metabolic pathways and protein--protein interaction
networks. This kind of a priori use of graphs is a useful supplement to the
standard numerical data such as microarray gene expression data. In this paper
we consider the problem of regression analysis and variable selection when the
covariates are linked on a graph. We study a graph-constrained regularization
procedure and its theoretical properties for regression analysis to take into
account the neighborhood information of the variables measured on a graph. This
procedure involves a smoothness penalty on the coefficients that is defined as
a quadratic form of the Laplacian matrix associated with the graph. We
establish estimation and model selection consistency results and provide
estimation bounds for both fixed and diverging numbers of parameters in
regression models. We demonstrate by simulations and a real data set that the
proposed procedure can lead to better variable selection and prediction than
existing methods that ignore the graph information associated with the
covariates.
"
"  The frequency-dependent spectrum based seismic intensity, also called
instrumental intensity, is calculated basically from the integration of the
square values of spectral acceleration ordinates. The values of the
instrumental intensity are calibrated to match the values of the EMS-98
intensity scale, providing a promising analytical indicator for estimating the
destructive potential of earthquakes. Previous studies have shown that the
proposed index could be used as a basis for the development of a new improved
seismic intensity scale. The paper presents a set of maps describing the
spatial distribution of instrumental intensity ordinates for three seismic
events recorded in 1986 and 1990. These events, generated by the Vrancea
source, are the strongest earthquakes in Romania for which accelerographic data
was recorded at multiple stations. Intensity maps were generated for separate
significant frequency bands, in order to reveal the destructiveness of the
considered earthquakes for different building categories. Results were compared
and correlated with previous studies on Vrancea earthquakes and with
information provided by building damage reports from the considered
earthquakes.
"
"  Counterfactual distributions are important ingredients for policy analysis
and decomposition analysis in empirical economics. In this article we develop
modeling and inference tools for counterfactual distributions based on
regression methods. The counterfactual scenarios that we consider consist of
ceteris paribus changes in either the distribution of covariates related to the
outcome of interest or the conditional distribution of the outcome given
covariates. For either of these scenarios we derive joint functional central
limit theorems and bootstrap validity results for regression-based estimators
of the status quo and counterfactual outcome distributions. These results allow
us to construct simultaneous confidence sets for function-valued effects of the
counterfactual changes, including the effects on the entire distribution and
quantile functions of the outcome as well as on related functionals. These
confidence sets can be used to test functional hypotheses such as no-effect,
positive effect, or stochastic dominance. Our theory applies to general
counterfactual changes and covers the main regression methods including
classical, quantile, duration, and distribution regressions. We illustrate the
results with an empirical application to wage decompositions using data for the
United States.
  As a part of developing the main results, we introduce distribution
regression as a comprehensive and flexible tool for modeling and estimating the
\textit{entire} conditional distribution. We show that distribution regression
encompasses the Cox duration regression and represents a useful alternative to
quantile regression. We establish functional central limit theorems and
bootstrap validity results for the empirical distribution regression process
and various related functionals.
"
"  Discussion of ""A statistical analysis of multiple temperature proxies: Are
reconstructions of surface temperatures over the last 1000 years reliable?"" by
B.B. McShane and A.J. Wyner [arXiv:1104.4002]
"
"  Array comparative genomic hybridization(CGH) is a high resolution technique
to assess DNA copy number variation. Identifying breakpoints where copy number
changes will enhance the understanding of the pathogenesis of human diseases,
such as cancers. However, the biological variation and experimental errors
contained in array CGH data may lead to false positive identification of
breakpoints. We propose a robust state space model for array CGH data analysis.
The model consists of two equations: an observation equation and a state
equation, in which both the measurement error and evolution error are specified
to follow t-distributions with small degrees of freedom. The completely
unspecified CGH profiles are estimated by a Markov Chain Monte Carlo(MCMC)
algorithm. Breakpoints and outliers are identified by a novel backward
selection procedure based on posterior draws of the CGH profiles. Compared to
three other popular methods, our method demonstrates several desired features,
including false positive rate control, robustness against outliers, and
superior power of breakpoint detection. All these properties are illustrated
using simulated and real datasets.
"
"  We consider a statistical model for pairs of traded assets, based on a
Cointegrated Vector Auto Regression (CVAR) Model. We extend standard CVAR
models to incorporate estimation of model parameters in the presence of price
series level shifts which are not accurately modeled in the standard Gaussian
error correction model (ECM) framework. This involves developing a novel matrix
variate Bayesian CVAR mixture model comprised of Gaussian errors intra-day and
Alpha-stable errors inter-day in the ECM framework. To achieve this we derive a
novel conjugate posterior model for the Scaled Mixtures of Normals (SMiN CVAR)
representation of Alpha-stable inter-day innovations. These results are
generalized to asymmetric models for the innovation noise at inter-day
boundaries allowing for skewed Alpha-stable models.
  Our proposed model and sampling methodology is general, incorporating the
current literature on Gaussian models as a special subclass and also allowing
for price series level shifts either at random estimated time points or known a
priori time points. We focus analysis on regularly observed non-Gaussian level
shifts that can have significant effect on estimation performance in
statistical models failing to account for such level shifts, such as at the
close and open of markets. We compare the estimation accuracy of our model and
estimation approach to standard frequentist and Bayesian procedures for CVAR
models when non-Gaussian price series level shifts are present in the
individual series, such as inter-day boundaries. We fit a bi-variate
Alpha-stable model to the inter-day jumps and model the effect of such jumps on
estimation of matrix-variate CVAR model parameters using the likelihood based
Johansen procedure and a Bayesian estimation. We illustrate our model and the
corresponding estimation procedures we develop on both synthetic and actual
data.
"
"  A central push in operations models over the last decade has been the
incorporation of models of customer choice. Real world implementations of many
of these models face the formidable stumbling block of simply identifying the
`right' model of choice to use. Thus motivated, we visit the following problem:
For a `generic' model of consumer choice (namely, distributions over preference
lists) and a limited amount of data on how consumers actually make decisions
(such as marginal information about these distributions), how may one predict
revenues from offering a particular assortment of choices? We present a
framework to answer such questions and design a number of tractable algorithms
from a data and computational standpoint for the same. This paper thus takes a
significant step towards `automating' the crucial task of choice model
selection in the context of operational decision problems.
"
"  I propose a frequency domain adaptation of the Expectation Maximization (EM)
algorithm to group a family of time series in classes of similar dynamic
structure. It does this by viewing the magnitude of the discrete Fourier
transform (DFT) of each signal (or power spectrum) as a probability
density/mass function (pdf/pmf) on the unit circle: signals with similar
dynamics have similar pdfs; distinct patterns have distinct pdfs. An advantage
of this approach is that it does not rely on any parametric form of the dynamic
structure, but can be used for non-parametric, robust and model-free
classification. This new method works for non-stationary signals of similar
shape as well as stationary signals with similar auto-correlation structure.
Applications to neural spike sorting (non-stationary) and pattern-recognition
in socio-economic time series (stationary) demonstrate the usefulness and wide
applicability of the proposed method.
"
"  We propose a versatile and computationally efficient estimating equation
method for a class of hierarchical multiplicative generalized linear mixed
models with additive dispersion components, based on explicit modelling of the
covariance structure. The class combines longitudinal and random effects models
and retains a marginal as well as a conditional interpretation. The estimation
procedure combines that of generalized estimating equations for the regression
with residual maximum likelihood estimation for the association parameters.
This avoids the multidimensional integral of the conventional generalized
linear mixed models likelihood and allows an extension of the robust empirical
sandwich estimator for use with both association and regression parameters. The
method is applied to a set of otolith data, used for age determination of fish.
"
"  Deep Boltzmann machines are in principle powerful models for extracting the
hierarchical structure of data. Unfortunately, attempts to train layers jointly
(without greedy layer-wise pretraining) have been largely unsuccessful. We
propose a modification of the learning algorithm that initially recenters the
output of the activation functions to zero. This modification leads to a better
conditioned Hessian and thus makes learning easier. We test the algorithm on
real data and demonstrate that our suggestion, the centered deep Boltzmann
machine, learns a hierarchy of increasingly abstract representations and a
better generative model of data.
"
"  Ready access to emerging databases of gene annotation and functional pathways
has shifted assessments of differential expression in DNA microarray studies
from single genes to groups of genes with shared biological function. This
paper takes a critical look at existing methods for assessing the differential
expression of a group of genes (functional category), and provides some
suggestions for improved performance. We begin by presenting a general
framework, in which the set of genes in a functional category is compared to
the complementary set of genes on the array. The framework includes tests for
overrepresentation of a category within a list of significant genes, and
methods that consider continuous measures of differential expression. Existing
tests are divided into two classes. Class 1 tests assume gene-specific measures
of differential expression are independent, despite overwhelming evidence of
positive correlation. Analytic and simulated results are presented that
demonstrate Class 1 tests are strongly anti-conservative in practice. Class 2
tests account for gene correlation, typically through array permutation that by
construction has proper Type I error control for the induced null. However,
both Class 1 and Class 2 tests use a null hypothesis that all genes have the
same degree of differential expression. We introduce a more sensible and
general (Class 3) null under which the profile of differential expression is
the same within the category and complement. Under this broader null, Class 2
tests are shown to be conservative. We propose standard bootstrap methods for
testing against the Class 3 null and demonstrate they provide valid Type I
error control and more power than array permutation in simulated datasets and
real microarray experiments.
"
"  With the coming data deluge from synoptic surveys, there is a growing need
for frameworks that can quickly and automatically produce calibrated
classification probabilities for newly-observed variables based on a small
number of time-series measurements. In this paper, we introduce a methodology
for variable-star classification, drawing from modern machine-learning
techniques. We describe how to homogenize the information gleaned from light
curves by selection and computation of real-numbered metrics (""feature""),
detail methods to robustly estimate periodic light-curve features, introduce
tree-ensemble methods for accurate variable star classification, and show how
to rigorously evaluate the classification results using cross validation. On a
25-class data set of 1542 well-studied variable stars, we achieve a 22.8%
overall classification error using the random forest classifier; this
represents a 24% improvement over the best previous classifier on these data.
This methodology is effective for identifying samples of specific science
classes: for pulsational variables used in Milky Way tomography we obtain a
discovery efficiency of 98.2% and for eclipsing systems we find an efficiency
of 99.1%, both at 95% purity. We show that the random forest (RF) classifier is
superior to other machine-learned methods in terms of accuracy, speed, and
relative immunity to features with no useful class information; the RF
classifier can also be used to estimate the importance of each feature in
classification. Additionally, we present the first astronomical use of
hierarchical classification methods to incorporate a known class taxonomy in
the classifier, which further reduces the catastrophic error rate to 7.8%.
Excluding low-amplitude sources, our overall error rate improves to 14%, with a
catastrophic error rate of 3.5%.
"
"  Variable selection is a difficult problem that is particularly challenging in
the analysis of high-dimensional genomic data. Here, we introduce the CAR
score, a novel and highly effective criterion for variable ranking in linear
regression based on Mahalanobis-decorrelation of the explanatory variables. The
CAR score provides a canonical ordering that encourages grouping of correlated
predictors and down-weights antagonistic variables. It decomposes the
proportion of variance explained and it is an intermediate between marginal
correlation and the standardized regression coefficient. As a population
quantity, any preferred inference scheme can be applied for its estimation.
Using simulations we demonstrate that variable selection by CAR scores is very
effective and yields prediction errors and true and false positive rates that
compare favorably with modern regression techniques such as elastic net and
boosting. We illustrate our approach by analyzing data concerned with diabetes
progression and with the effect of aging on gene expression in the human brain.
The R package ""care"" implementing CAR score regression is available from CRAN.
"
"  Flood frequency analysis is usually based on the fitting of an extreme value
distribution to the local streamflow series. However, when the local data
series is short, frequency analysis results become unreliable. Regional
frequency analysis is a convenient way to reduce the estimation uncertainty. In
this work, we propose a regional Bayesian model for short record length sites.
This model is less restrictive than the index flood model while preserving the
formalism of ""homogeneous regions"". The performance of the proposed model is
assessed on a set of gauging stations in France. The accuracy of quantile
estimates as a function of the degree of homogeneity of the pooling group is
also analysed. The results indicate that the regional Bayesian model
outperforms the index flood model and local estimators. Furthermore, it seems
that working with relatively large and homogeneous regions may lead to more
accurate results than working with smaller and highly homogeneous regions.
"
"  Multiple kernel learning algorithms are proposed to combine kernels in order
to obtain a better similarity measure or to integrate feature representations
coming from different data sources. Most of the previous research on such
methods is focused on the computational efficiency issue. However, it is still
not feasible to combine many kernels using existing Bayesian approaches due to
their high time complexity. We propose a fully conjugate Bayesian formulation
and derive a deterministic variational approximation, which allows us to
combine hundreds or thousands of kernels very efficiently. We briefly explain
how the proposed method can be extended for multiclass learning and
semi-supervised learning. Experiments with large numbers of kernels on
benchmark data sets show that our inference method is quite fast, requiring
less than a minute. On one bioinformatics and three image recognition data
sets, our method outperforms previously reported results with better
generalization performance.
"
"  An efficient way to learn deep density models that have many layers of latent
variables is to learn one layer at a time using a model that has only one layer
of latent variables. After learning each layer, samples from the posterior
distributions for that layer are used as training data for learning the next
layer. This approach is commonly used with Restricted Boltzmann Machines, which
are undirected graphical models with a single hidden layer, but it can also be
used with Mixtures of Factor Analysers (MFAs) which are directed graphical
models. In this paper, we present a greedy layer-wise learning algorithm for
Deep Mixtures of Factor Analysers (DMFAs). Even though a DMFA can be converted
to an equivalent shallow MFA by multiplying together the factor loading
matrices at different levels, learning and inference are much more efficient in
a DMFA and the sharing of each lower-level factor loading matrix by many
different higher level MFAs prevents overfitting. We demonstrate empirically
that DMFAs learn better density models than both MFAs and two types of
Restricted Boltzmann Machine on a wide variety of datasets.
"
"  Research in several fields now requires the analysis of data sets in which
multiple high-dimensional types of data are available for a common set of
objects. In particular, The Cancer Genome Atlas (TCGA) includes data from
several diverse genomic technologies on the same cancerous tumor samples. In
this paper we introduce Joint and Individual Variation Explained (JIVE), a
general decomposition of variation for the integrated analysis of such data
sets. The decomposition consists of three terms: a low-rank approximation
capturing joint variation across data types, low-rank approximations for
structured variation individual to each data type, and residual noise. JIVE
quantifies the amount of joint variation between data types, reduces the
dimensionality of the data and provides new directions for the visual
exploration of joint and individual structures. The proposed method represents
an extension of Principal Component Analysis and has clear advantages over
popular two-block methods such as Canonical Correlation Analysis and Partial
Least Squares. A JIVE analysis of gene expression and miRNA data on
Glioblastoma Multiforme tumor samples reveals gene-miRNA associations and
provides better characterization of tumor types. Data and software are
available at https://genome.unc.edu/jive/
"
"  In recent years, some spectrum sensing algorithms using multiple antennas,
such as the eigenvalue based detection (EBD), have attracted a lot of
attention. In this paper, we are interested in deriving the asymptotic
distributions of the test statistics of the EBD algorithms. Two EBD algorithms
using sample covariance matrices are considered: maximum eigenvalue detection
(MED) and condition number detection (CND). The earlier studies usually assume
that the number of antennas (K) and the number of samples (N) are both large,
thus random matrix theory (RMT) can be used to derive the asymptotic
distributions of the maximum and minimum eigenvalues of the sample covariance
matrices. While assuming the number of antennas being large simplifies the
derivations, in practice, the number of antennas equipped at a single secondary
user is usually small, say 2 or 3, and once designed, this antenna number is
fixed. Thus in this paper, our objective is to derive the asymptotic
distributions of the eigenvalues and condition numbers of the sample covariance
matrices for any fixed K but large N, from which the probability of detection
and probability of false alarm can be obtained. The proposed methodology can
also be used to analyze the performance of other EBD algorithms. Finally,
computer simulations are presented to validate the accuracy of the derived
results.
"
"  It is of increasing importance to develop learning methods for ranking. In
contrast to many learning objectives, however, the ranking problem presents
difficulties due to the fact that the space of permutations is not smooth. In
this paper, we examine the class of rank-linear objective functions, which
includes popular metrics such as precision and discounted cumulative gain. In
particular, we observe that expectations of these gains are completely
characterized by the marginals of the corresponding distribution over
permutation matrices. Thus, the expectations of rank-linear objectives can
always be described through locations in the Birkhoff polytope, i.e.,
doubly-stochastic matrices (DSMs). We propose a technique for learning
DSM-based ranking functions using an iterative projection operator known as
Sinkhorn normalization. Gradients of this operator can be computed via
backpropagation, resulting in an algorithm we call Sinkhorn propagation, or
SinkProp. This approach can be combined with a wide range of gradient-based
approaches to rank learning. We demonstrate the utility of SinkProp on several
information retrieval data sets.
"
"  This paper uses Bayesian tree models for statistical benchmarking in data
sets with awkward marginals and complicated dependence structures. The method
is applied to a very large database on corporate performance over the last four
decades. The results of this study provide a formal basis for making
cross-peer-group comparisons among companies in very different industries and
operating environments. This is done by using models for Bayesian multiple
hypothesis testing to determine which firms, if any, have systematically
outperformed their peer groups over time. We conclude that systematic
outperformance, while it seems to exist, is quite rare worldwide.
"
"  What do auto-encoders learn about the underlying data generating
distribution? Recent work suggests that some auto-encoder variants do a good
job of capturing the local manifold structure of data. This paper clarifies
some of these previous observations by showing that minimizing a particular
form of regularized reconstruction error yields a reconstruction function that
locally characterizes the shape of the data generating density. We show that
the auto-encoder captures the score (derivative of the log-density with respect
to the input). It contradicts previous interpretations of reconstruction error
as an energy function. Unlike previous results, the theorems provided here are
completely generic and do not depend on the parametrization of the
auto-encoder: they show what the auto-encoder would tend to if given enough
capacity and examples. These results are for a contractive training criterion
we show to be similar to the denoising auto-encoder training criterion with
small corruption noise, but with contraction applied on the whole
reconstruction function rather than just encoder. Similarly to score matching,
one can consider the proposed training criterion as a convenient alternative to
maximum likelihood because it does not involve a partition function. Finally,
we show how an approximate Metropolis-Hastings MCMC can be setup to recover
samples from the estimated distribution, and this is confirmed in sampling
experiments.
"
"  In the last few years, due to the growing ubiquity of unlabeled data, much
effort has been spent by the machine learning community to develop better
understanding and improve the quality of classifiers exploiting unlabeled data.
Following the manifold regularization approach, Laplacian Support Vector
Machines (LapSVMs) have shown the state of the art performance in
semi--supervised classification. In this paper we present two strategies to
solve the primal LapSVM problem, in order to overcome some issues of the
original dual formulation. Whereas training a LapSVM in the dual requires two
steps, using the primal form allows us to collapse training to a single step.
Moreover, the computational complexity of the training algorithm is reduced
from O(n^3) to O(n^2) using preconditioned conjugate gradient, where n is the
combined number of labeled and unlabeled examples. We speed up training by
using an early stopping strategy based on the prediction on unlabeled data or,
if available, on labeled validation examples. This allows the algorithm to
quickly compute approximate solutions with roughly the same classification
accuracy as the optimal ones, considerably reducing the training time. Due to
its simplicity, training LapSVM in the primal can be the starting point for
additional enhancements of the original LapSVM formulation, such as those for
dealing with large datasets. We present an extensive experimental evaluation on
real world data showing the benefits of the proposed approach.
"
"  A method for estimating the axis of reflectional symmetry of an image
$f(x,y)$ on the unit disc $D=\{(x,y):x^2+y^2\leq1\}$ is proposed, given that
noisy data of $f(x,y)$ are observed on a discrete grid of edge width $\Delta$.
Our estimation procedure is based on minimizing over $\beta\in[0,\pi)$ the
$L_2$ distance between empirical versions of $f$ and $\tau_{\beta}f$, the image
of $f$ after reflection at the axis along $(\cos\beta,\sin\beta)$. Here, $f$
and $\tau_{\beta}f$ are estimated using truncated radial series of the Zernike
type. The inherent symmetry properties of the Zernike functions result in a
particularly simple estimation procedure for $\beta$. It is shown that the
estimate $\hat{\beta}$ converges at the parametric rate $\Delta^{-1}$ for
images $f$ of bounded variation. Further, we establish asymptotic normality of
$\hat{\beta}$ if $f$ is Lipschitz continuous. The method is applied to
calibrating the point spread function (PSF) for the deconvolution of images
from confocal microscopy. For various reasons the PSF characterizing the
problem may not be rotationally invariant but rather only reflection symmetric
with respect to two orthogonal axes. For an image of a bead acquired by a
confocal laser scanning microscope (Leica TCS), these axes are estimated and
corresponding confidence intervals are constructed. They turn out to be close
to the coordinate axes of the imaging device. As cause for deviation from
rotational invariance, this indicates some slight misalignment of the optical
system or anisotropy of the immersion medium rather than some irregular shape
of the bead. In an extensive simulation study, we show that using a symmetrized
version of the observed PSF significantly improves the subsequent
reconstruction process of the target image.
"
"  Hepatitis C virus (HCV) coinfection has become one of the most challenging
clinical situations to manage in HIV-infected patients. Recently the effect of
HCV coinfection on HIV dynamics following initiation of highly active
antiretroviral therapy (HAART) has drawn considerable attention. Post-HAART HIV
dynamics are commonly studied in short-term clinical trials with frequent data
collection design. For example, the elimination process of plasma virus during
treatment is closely monitored with daily assessments in viral dynamics studies
of AIDS clinical trials. In this article instead we use infrequent cohort data
from long-term natural history studies and develop a model for characterizing
post-HAART HIV dynamics and their associations with HCV coinfection.
Specifically, we propose a joint model for doubly interval-censored data for
the time between HAART initiation and viral suppression, and the longitudinal
CD4 count measurements relative to the viral suppression. Inference is
accomplished using a fully Bayesian approach. Doubly interval-censored data are
modeled semiparametrically by Dirichlet process priors and Bayesian penalized
splines are used for modeling population-level and individual-level mean CD4
count profiles. We use the proposed methods and data from the HIV Epidemiology
Research Study (HERS) to investigate the effect of HCV coinfection on the
response to HAART.
"
"  A predictive Bayesian model selection approach is presented to discriminate
coupled models used to predict an unobserved quantity of interest (QoI). The
need for accurate predictions arises in a variety of critical applications such
as climate, aerospace and defense. A model problem is introduced to study the
prediction yielded by the coupling of two physics/sub-components. For each
single physics domain, a set of model classes and a set of sensor observations
are available. A goal-oriented algorithm using a predictive approach to
Bayesian model selection is then used to select the combination of single
physics models that best predict the QoI. It is shown that the best coupled
model for prediction is the one that provides the most robust predictive
distribution for the QoI.
"
"  Microarray cancer gene expression data comprise of very high dimensions.
Reducing the dimensions helps in improving the overall analysis and
classification performance. We propose two hybrid techniques, Biogeography -
based Optimization - Random Forests (BBO - RF) and BBO - SVM (Support Vector
Machines) with gene ranking as a heuristic, for microarray gene expression
analysis. This heuristic is obtained from information gain filter ranking
procedure. The BBO algorithm generates a population of candidate subset of
genes, as part of an ecosystem of habitats, and employs the migration and
mutation processes across multiple generations of the population to improve the
classification accuracy. The fitness of each gene subset is assessed by the
classifiers - SVM and Random Forests. The performances of these hybrid
techniques are evaluated on three cancer gene expression datasets retrieved
from the Kent Ridge Biomedical datasets collection and the libSVM data
repository. Our results demonstrate that genes selected by the proposed
techniques yield classification accuracies comparable to previously reported
algorithms.
"
"  Suppose, contrary to fact, in 1950, we had put the cohort of 18 year old
non-smoking American men on a stringent mandatory diet that guaranteed that no
one would ever weigh more than their baseline weight established at age 18. How
would the counter-factual mortality of these 18 year olds have compared to
their actual observed mortality through 2007? We describe in detail how this
counterfactual contrast could be estimated from longitudinal epidemiologic data
similiar to that stored in the electronic medical records of a large HMO by
applying g-estimation to a novel structural nested model. Our analytic approach
differs from any alternative approach in that in that, in the abscence of model
misspecification, it can successfully adjust for (i) measured time-varying
confounders such as exercise, hypertension and diabetes that are simultaneously
intermediate variables on the causal pathway from weight gain to death and
determinants of future weight gain, (ii) unmeasured confounding by undiagnosed
preclinical disease (i.e reverse causation) that can cause both poor weight
gain and premature mortality [provided an upper bound can be specified for the
maximum length of time a subject may suffer from a subclinical illness severe
enough to affect his weight without the illness becomes clinically manifest],
and (iii) the prescence of particular identifiable subgroups, such as those
suffering from serious renal, liver, pulmonary, and/or cardiac disease, in whom
confounding by unmeasured prognostic factors so severe as to render useless any
attempt at direct analytic adjustment.
"
"  Inspired by recent work on convex formulations of clustering (Lashkari &
Golland, 2008; Nowozin & Bakir, 2008) we investigate a new formulation of the
Sparse Coding Problem (Olshausen & Field, 1997). In sparse coding we attempt to
simultaneously represent a sequence of data-vectors sparsely (i.e. sparse
approximation (Tropp et al., 2006)) in terms of a 'code' defined by a set of
basis elements, while also finding a code that enables such an approximation.
As existing alternating optimization procedures for sparse coding are
theoretically prone to severe local minima problems, we propose a convex
relaxation of the sparse coding problem and derive a boosting-style algorithm,
that (Nowozin & Bakir, 2008) serves as a convex 'master problem' which calls a
(potentially non-convex) sub-problem to identify the next code element to add.
Finally, we demonstrate the properties of our boosted coding algorithm on an
image denoising task.
"
"  Protein interaction networks are a promising type of data for studying
complex biological systems. However, despite the rich information embedded in
these networks, they face important data quality challenges of noise and
incompleteness that adversely affect the results obtained from their analysis.
Here, we explore the use of the concept of common neighborhood similarity
(CNS), which is a form of local structure in networks, to address these issues.
Although several CNS measures have been proposed in the literature, an
understanding of their relative efficacies for the analysis of interaction
networks has been lacking. We follow the framework of graph transformation to
convert the given interaction network into a transformed network corresponding
to a variety of CNS measures evaluated. The effectiveness of each measure is
then estimated by comparing the quality of protein function predictions
obtained from its corresponding transformed network with those from the
original network. Using a large set of S. cerevisiae interactions, and a set of
136 GO terms, we find that several of the transformed networks produce more
accurate predictions than those obtained from the original network. In
particular, the $HC.cont$ measure proposed here performs particularly well for
this task. Further investigation reveals that the two major factors
contributing to this improvement are the abilities of CNS measures, especially
$HC.cont$, to prune out noisy edges and introduce new links between
functionally related proteins.
"
"  Sparse coding is an unsupervised learning algorithm that learns a succinct
high-level representation of the inputs given only unlabeled data; it
represents each input as a sparse linear combination of a set of basis
functions. Originally applied to modeling the human visual cortex, sparse
coding has also been shown to be useful for self-taught learning, in which the
goal is to solve a supervised classification task given access to additional
unlabeled data drawn from different classes than that in the supervised
learning problem. Shift-invariant sparse coding (SISC) is an extension of
sparse coding which reconstructs a (usually time-series) input using all of the
basis functions in all possible shifts. In this paper, we present an efficient
algorithm for learning SISC bases. Our method is based on iteratively solving
two large convex optimization problems: The first, which computes the linear
coefficients, is an L1-regularized linear least squares problem with
potentially hundreds of thousands of variables. Existing methods typically use
a heuristic to select a small subset of the variables to optimize, but we
present a way to efficiently compute the exact solution. The second, which
solves for bases, is a constrained linear least squares problem. By optimizing
over complex-valued variables in the Fourier domain, we reduce the coupling
between the different variables, allowing the problem to be solved efficiently.
We show that SISC's learned high-level representations of speech and music
provide useful features for classification tasks within those domains. When
applied to classification, under certain conditions the learned features
outperform state of the art spectral and cepstral features.
"
"  In 2000 Nicholas J. Mackintosh (2000) published an article in ""Nature""
referring to the concept of general intelligence (""g"") claiming that there is
clear empirical evidence for the existence of the g factor and psychologists
are ""united in their support of g"". Surprisingly, his view remained yet
unchallenged although this issue is by no means as clear-cut as Mackintosh
argues. Let us therefore attempt to clarify some common but unfortunately major
misconceptions about g, which Mackintosh, following Jensen's (1998) precedent,
recounted in his ""Nature"" article. The bottom line is that Spearman's g does
not exist, that this has been known and acknowledged by leading scholars
(Guttman, 1992; Thurstone, 1947) of factor analysis for decades so that the
task of objectively defining human intelligence remains unfinished.
"
"  We compare the risk of ridge regression to a simple variant of ordinary least
squares, in which one simply projects the data onto a finite dimensional
subspace (as specified by a Principal Component Analysis) and then performs an
ordinary (un-regularized) least squares regression in this subspace. This note
shows that the risk of this ordinary least squares method is within a constant
factor (namely 4) of the risk of ridge regression.
"
"  A sensitivity analysis in an observational study determines the magnitude of
bias from nonrandom treatment assignment that would need to be present to alter
the qualitative conclusions of a na\""{\i}ve analysis that presumes all biases
were removed by matching or by other analytic adjustments. The power of a
sensitivity analysis and the design sensitivity anticipate the outcome of a
sensitivity analysis under an assumed model for the generation of the data. It
is known that the power of a sensitivity analysis is affected by the choice of
test statistic, and, in particular, that a statistic with good Pitman
efficiency in a randomized experiment, such as Wilcoxon's signed rank
statistic, may have low power in a sensitivity analysis and low design
sensitivity when compared to other statistics. For instance, for an additive
treatment effect and errors that are Normal or logistic or $t$-distributed with
3 degrees of freedom, Brown's combined quantile average test has Pitman
efficiency close to that of Wilcoxon's test but has higher power in a
sensitivity analysis, while a version of Noether's test has poor Pitman
efficiency in a randomized experiment but much higher design sensitivity so it
is vastly more powerful than Wilcoxon's statistic in a sensitivity analysis if
the sample size is sufficiently large.
"
"  This paper develops strategic foundations for an important statistical model
of random networks with heterogeneous expected degrees. Based on this, we show
how social networking services that subtly alter the costs and indirect
benefits of relationships can cause large changes in behavior and welfare. In
the model, agents who value friends and friends of friends choose how much to
socialize, which increases the probabilities of links but is costly. There is a
sharp transition from fragmented, sparse equilibrium networks to connected,
dense ones when the value of friends of friends crosses a cost-dependent
threshold. This transition mitigates an extreme inefficiency.
"
"  Many probabilistic models introduce strong dependencies between variables
using a latent multivariate Gaussian distribution or a Gaussian process. We
present a new Markov chain Monte Carlo algorithm for performing inference in
models with multivariate Gaussian priors. Its key properties are: 1) it has
simple, generic code applicable to many models, 2) it has no free parameters,
3) it works well for a variety of Gaussian process based models. These
properties make our method ideal for use while model building, removing the
need to spend time deriving and tuning updates for more complex algorithms.
"
"  We introduce a new graphical model for tracking radio-tagged animals and
learning their movement patterns. The model provides a principled way to
combine radio telemetry data with an arbitrary set of userdefined, spatial
features. We describe an efficient stochastic gradient algorithm for fitting
model parameters to data and demonstrate its effectiveness via asymptotic
analysis and synthetic experiments. We also apply our model to real datasets,
and show that it outperforms the most popular radio telemetry software package
used in ecology. We conclude that integration of different data sources under a
single statistical framework, coupled with appropriate parameter and state
estimation procedures, produces both accurate location estimates and an
interpretable statistical model of animal movement.
"
"  The data of F1000 provide us with the unique opportunity to investigate the
relationship between peers' ratings and bibliometric metrics on a broad and
comprehensive data set with high-quality ratings. F1000 is a post-publication
peer review system of the biomedical literature. The comparison of metrics with
peer evaluation has been widely acknowledged as a way of validating metrics.
Based on the seven indicators offered by InCites, we analyzed the validity of
raw citation counts (Times Cited, 2nd Generation Citations, and 2nd Generation
Citations per Citing Document), normalized indicators (Journal Actual/Expected
Citations, Category Actual/Expected Citations, and Percentile in Subject Area),
and a journal based indicator (Journal Impact Factor). The data set consists of
125 papers published in 2008 and belonging to the subject category cell biology
or immunology. As the results show, Percentile in Subject Area achieves the
highest correlation with F1000 ratings; we can assert that for further three
other indicators (Times Cited, 2nd Generation Citations, and Category
Actual/Expected Citations) the 'true' correlation with the ratings reaches at
least a medium effect size.
"
"  A typical approach in estimating the learning rate of a regularized learning
scheme is to bound the approximation error by the sum of the sampling error,
the hypothesis error and the regularization error. Using a reproducing kernel
space that satisfies the linear representer theorem brings the advantage of
discarding the hypothesis error from the sum automatically. Following this
direction, we illustrate how reproducing kernel Banach spaces with the l1 norm
can be applied to improve the learning rate estimate of l1-regularization in
machine learning.
"
"  This paper proposes and evaluates the k-greedy equivalence search algorithm
(KES) for learning Bayesian networks (BNs) from complete data. The main
characteristic of KES is that it allows a trade-off between greediness and
randomness, thus exploring different good local optima. When greediness is set
at maximum, KES corresponds to the greedy equivalence search algorithm (GES).
When greediness is kept at minimum, we prove that under mild assumptions KES
asymptotically returns any inclusion optimal BN with nonzero probability.
Experimental results for both synthetic and real data are reported showing that
KES often finds a better local optima than GES. Moreover, we use KES to
experimentally confirm that the number of different local optima is often huge.
"
"  We introduce a novel geometric framework for separating the phase and the
amplitude variability in functional data of the type frequently studied in
growth curve analysis. This framework uses the Fisher-Rao Riemannian metric to
derive a proper distance on the quotient space of functions modulo the
time-warping group. A convenient square-root velocity function (SRVF)
representation transforms the Fisher-Rao metric into the standard $\ltwo$
metric, simplifying the computations. This distance is then used to define a
Karcher mean template and warp the individual functions to align them with the
Karcher mean template. The strength of this framework is demonstrated by
deriving a consistent estimator of a signal observed under random warping,
scaling, and vertical translation. These ideas are demonstrated using both
simulated and real data from different application domains: the Berkeley growth
study, handwritten signature curves, neuroscience spike trains, and gene
expression signals. The proposed method is empirically shown to be be superior
in performance to several recently published methods for functional alignment.
"
"  Despite the great promise of machine-learning algorithms to classify and
predict astrophysical parameters for the vast numbers of astrophysical sources
and transients observed in large-scale surveys, the peculiarities of the
training data often manifest as strongly biased predictions on the data of
interest. Typically, training sets are derived from historical surveys of
brighter, more nearby objects than those from more extensive, deeper surveys
(testing data). This sample selection bias can cause catastrophic errors in
predictions on the testing data because a) standard assumptions for
machine-learned model selection procedures break down and b) dense regions of
testing space might be completely devoid of training data. We explore possible
remedies to sample selection bias, including importance weighting (IW),
co-training (CT), and active learning (AL). We argue that AL---where the data
whose inclusion in the training set would most improve predictions on the
testing set are queried for manual follow-up---is an effective approach and is
appropriate for many astronomical applications. For a variable star
classification problem on a well-studied set of stars from Hipparcos and OGLE,
AL is the optimal method in terms of error rate on the testing data, beating
the off-the-shelf classifier by 3.4% and the other proposed methods by at least
3.0%. To aid with manual labeling of variable stars, we developed a web
interface which allows for easy light curve visualization and querying of
external databases. Finally, we apply active learning to classify variable
stars in the ASAS survey, finding dramatic improvement in our agreement with
the ACVS catalog, from 65.5% to 79.5%, and a significant increase in the
classifier's average confidence for the testing set, from 14.6% to 42.9%, after
a few AL iterations.
"
"  A variation on Janowski's cubeful equity model is proposed for cube handling
in backgammon money games. Instead of approximating the cubeful take point as
an interpolation between the dead and live cube limits, a new model is
developed where the cubeless probability of win evolves through a series of
random jumps instead of continuous diffusion. Each jump is drawn from a
distribution with zero mean and an expected absolute jump size called the ""jump
volatility"" that can be a function of game state but is assumed to be small
compared to the market window. Closed form approximations for cubeful equities
and cube decision points are developed as a function of local and remote jump
volatility. The local jump volatility can be calculated for specific game
states, leading to crisper doubling decisions.
"
"  Modeling data with linear combinations of a few elements from a learned
dictionary has been the focus of much recent research in machine learning,
neuroscience and signal processing. For signals such as natural images that
admit such sparse representations, it is now well established that these models
are well suited to restoration tasks. In this context, learning the dictionary
amounts to solving a large-scale matrix factorization problem, which can be
done efficiently with classical optimization tools. The same approach has also
been used for learning features from data for other purposes, e.g., image
classification, but tuning the dictionary in a supervised way for these tasks
has proven to be more difficult. In this paper, we present a general
formulation for supervised dictionary learning adapted to a wide variety of
tasks, and present an efficient algorithm for solving the corresponding
optimization problem. Experiments on handwritten digit classification, digital
art identification, nonlinear inverse image problems, and compressed sensing
demonstrate that our approach is effective in large-scale settings, and is well
suited to supervised and semi-supervised classification, as well as regression
tasks for data that admit sparse representations.
"
"  Here we present a theoretical study on the main properties of Fractionally
Integrated Exponential Generalized Autoregressive Conditional Heteroskedastic
(FIEGARCH) processes. We analyze the conditions for the existence, the
invertibility, the stationarity and the ergodicity of these processes. We prove
that, if $\{X_t\}_{t \in \mathds{Z}}$ is a FIEGARCH$(p,d,q)$ process then,
under mild conditions, $\{\ln(X_t^2)\}_{t\in\mathds{Z}}$ is an ARFIMA$(q,d,0)$,
that is, an autoregressive fractionally integrated moving average process. The
convergence order for the polynomial coefficients that describes the volatility
is presented and results related to the spectral representation and to the
covariance structure of both processes $\{\ln(X_t^2)\}_{t\in\mathds{Z}}$ and $\
{\ln(\sigma_t^2)\}_{t\in\mathds{Z}}$ are also discussed. Expressions for the
kurtosis and the asymmetry measures for any stationary FIEGARCH$(p,d,q)$
process are also derived. The $h$-step ahead forecast for the processes
$\{X_t\}_{t \in \mathds{Z}}$, $\{\ln(\sigma_t^2)\}_{t\in\mathds{Z}}$ and
$\{\ln(X_t^2)\}_{t\in\mathds{Z}}$ are given with their respective mean square
error forecast. The work also presents a Monte Carlo simulation study showing
how to generate, estimate and forecast based on six different FIEGARCH models.
The forecasting performance of six models belonging to the class of
autoregressive conditional heteroskedastic models (namely, ARCH-type models)
and radial basis models is compared through an empirical application to
Brazilian stock market exchange index.
"
"  We discuss here the mean-field theory for a cellular automata model of
meta-learning. The meta-learning is the process of combining outcomes of
individual learning procedures in order to determine the final decision with
higher accuracy than any single learning method. Our method is constructed from
an ensemble of interacting, learning agents, that acquire and process incoming
information using various types, or different versions of machine learning
algorithms. The abstract learning space, where all agents are located, is
constructed here using a fully connected model that couples all agents with
random strength values. The cellular automata network simulates the higher
level integration of information acquired from the independent learning trials.
The final classification of incoming input data is therefore defined as the
stationary state of the meta-learning system using simple majority rule, yet
the minority clusters that share opposite classification outcome can be
observed in the system. Therefore, the probability of selecting proper class
for a given input data, can be estimated even without the prior knowledge of
its affiliation. The fuzzy logic can be easily introduced into the system, even
if learning agents are build from simple binary classification machine learning
algorithms by calculating the percentage of agreeing agents.
"
"  While Gaussian probability densities are omnipresent in applied mathematics,
Gaussian cumulative probabilities are hard to calculate in any but the
univariate case. We study the utility of Expectation Propagation (EP) as an
approximate integration method for this problem. For rectangular integration
regions, the approximation is highly accurate. We also extend the derivations
to the more general case of polyhedral integration regions. However, we find
that in this polyhedral case, EP's answer, though often accurate, can be almost
arbitrarily wrong. We consider these unexpected results empirically and
theoretically, both for the problem of Gaussian probabilities and for EP more
generally. These results elucidate an interesting and non-obvious feature of EP
not yet studied in detail.
"
"  We study the problem of learning Bayesian network structures from data.
Koivisto and Sood (2004) and Koivisto (2006) presented algorithms that can
compute the exact marginal posterior probability of a subnetwork, e.g., a
single edge, in O(n2n) time and the posterior probabilities for all n(n-1)
potential edges in O(n2n) total time, assuming that the number of parents per
node or the indegree is bounded by a constant. One main drawback of their
algorithms is the requirement of a special structure prior that is non uniform
and does not respect Markov equivalence. In this paper, we develop an algorithm
that can compute the exact posterior probability of a subnetwork in O(3n) time
and the posterior probabilities for all n(n-1) potential edges in O(n3n) total
time. Our algorithm also assumes a bounded indegree but allows general
structure priors. We demonstrate the applicability of the algorithm on several
data sets with up to 20 variables.
"
"  Concerns are expressed for the Monotonic Imbalance Bounding (MIB) property
(Iacus et al. 2011) and for MIB matching because i) the definition of the MIB
property leads to inconsistencies and the nature of the imbalance measure is
not clearly defined, ii) MIB property does not generalize Equal Percent Bias
Reducing (EPBR) property, iii) MIB matching does not provide statistical
information available with EPBR matching.
"
"  The goal of this paper is to study the similarity between sequences using a
distance between the \emph{context} trees associated to the sequences. These
trees are defined in the framework of \emph{Sparse Probabilistic Suffix Trees}
(SPST), and can be estimated using the SPST algorithm. We implement the
Phyl-SPST package to compute the distance between the sparse context trees
estimated with the SPST algorithm. The distance takes into account the
structure of the trees, and indirectly the transition probabilities. We apply
this approach to reconstruct a phylogenetic tree of protein sequences in the
globin family of vertebrates. We compare this tree with the one obtained using
the well-known PAM distance.
"
"  We consider the setting of sequential prediction of arbitrary sequences based
on specialized experts. We first provide a review of the relevant literature
and present two theoretical contributions: a general analysis of the specialist
aggregation rule of Freund et al. (1997) and an adaptation of fixed-share rules
of Herbster and Warmuth (1998) in this setting. We then apply these rules to
the sequential short-term (one-day-ahead) forecasting of electricity
consumption; to do so, we consider two data sets, a Slovakian one and a French
one, respectively concerned with hourly and half-hourly predictions. We follow
a general methodology to perform the stated empirical studies and detail in
particular tuning issues of the learning parameters. The introduced aggregation
rules demonstrate an improved accuracy on the data sets at hand; the
improvements lie in a reduced mean squared error but also in a more robust
behavior with respect to large occasional errors.
"
"  This paper proposes an organized generalization of Newman and Girvan's
modularity measure for graph clustering. Optimized via a deterministic
annealing scheme, this measure produces topologically ordered graph clusterings
that lead to faithful and readable graph representations based on clustering
induced graphs. Topographic graph clustering provides an alternative to more
classical solutions in which a standard graph clustering method is applied to
build a simpler graph that is then represented with a graph layout algorithm. A
comparative study on four real world graphs ranging from 34 to 1 133 vertices
shows the interest of the proposed approach with respect to classical solutions
and to self-organizing maps for graphs.
"
"  Current state-of-the-art discrete optimization methods struggle behind when
it comes to challenging contrast-enhancing discrete energies (i.e., favoring
different labels for neighboring variables). This work suggests a multiscale
approach for these challenging problems. Deriving an algebraic representation
allows us to coarsen any pair-wise energy using any interpolation in a
principled algebraic manner. Furthermore, we propose an energy-aware
interpolation operator that efficiently exposes the multiscale landscape of the
energy yielding an effective coarse-to-fine optimization scheme. Results on
challenging contrast-enhancing energies show significant improvement over
state-of-the-art methods.
"
"  Could John Kerry have gained votes in the 2004 Presidential election by more
clearly distinguishing himself from George Bush on economic policy? At first
thought, the logic of political preferences would suggest not: the Republicans
are to the right of most Americans on economic policy, and so in a
one-dimensional space with party positions measured with no error, the optimal
strategy for the Democrats would be to stand infinitesimally to the left of the
Republicans. The median voter theorem suggests that each party should keep its
policy positions just barely distinguishable from the opposition. In a
multidimensional setting, however, or when voters vary in their perceptions of
the parties' positions, a party can benefit from putting some daylight between
itself and the other party on an issue where it has a public-opinion advantage
(such as economic policy for the Democrats). We set up a plausible theoretical
model in which the Democrats could achieve a net gain in votes by moving to the
left on economic policy, given the parties' positions on a range of issue
dimensions. We then evaluate this model based on survey data on voters'
perceptions of their own positions and those of the candidates in 2004. Under
our model, it turns out to be optimal for the Democrats to move slightly to the
right but staying clearly to the left of the Republicans' current position on
economic issues.
"
"  Misperceptions about extreme dependencies between different financial assets
have been an im- portant element of the recent financial crisis. This paper
studies inhomogeneity in dependence structures using Markov switching regular
vine copulas. These account for asymmetric depen- dencies and tail dependencies
in high dimensional data. We develop methods for fast maximum likelihood as
well as Bayesian inference. Our algorithms are validated in simulations and
applied to financial data. We find that regime switches are present in the
dependence structure of various data sets and show that regime switching models
could provide tools for the accurate description of inhomogeneity during times
of crisis.
"
"  Storm surge, the onshore rush of sea water caused by the high winds and low
pressure associated with a hurricane, can compound the effects of inland
flooding caused by rainfall, leading to loss of property and loss of life for
residents of coastal areas. Numerical ocean models are essential for creating
storm surge forecasts for coastal areas. These models are driven primarily by
the surface wind forcings. Currently, the gridded wind fields used by ocean
models are specified by deterministic formulas that are based on the central
pressure and location of the storm center. While these equations incorporate
important physical knowledge about the structure of hurricane surface wind
fields, they cannot always capture the asymmetric and dynamic nature of a
hurricane. A new Bayesian multivariate spatial statistical modeling framework
is introduced combining data with physical knowledge about the wind fields to
improve the estimation of the wind vectors. Many spatial models assume the data
follow a Gaussian distribution. However, this may be overly-restrictive for
wind fields data which often display erratic behavior, such as sudden changes
in time or space. In this paper we develop a semiparametric multivariate
spatial model for these data. Our model builds on the stick-breaking prior,
which is frequently used in Bayesian modeling to capture uncertainty in the
parametric form of an outcome. The stick-breaking prior is extended to the
spatial setting by assigning each location a different, unknown distribution,
and smoothing the distributions in space with a series of kernel functions.
This semiparametric spatial model is shown to improve prediction compared to
usual Bayesian Kriging methods for the wind field of Hurricane Ivan.
"
"  Many latent (factorized) models have been proposed for recommendation tasks
like collaborative filtering and for ranking tasks like document or image
retrieval and annotation. Common to all those methods is that during inference
the items are scored independently by their similarity to the query in the
latent embedding space. The structure of the ranked list (i.e. considering the
set of items returned as a whole) is not taken into account. This can be a
problem because the set of top predictions can be either too diverse (contain
results that contradict each other) or are not diverse enough. In this paper we
introduce a method for learning latent structured rankings that improves over
existing methods by providing the right blend of predictions at the top of the
ranked list. Particular emphasis is put on making this method scalable.
Empirical results on large scale image annotation and music recommendation
tasks show improvements over existing approaches.
"
"  In this paper is proposed a new heuristic approach belonging to the field of
evolutionary Estimation of Distribution Algorithms (EDAs). EDAs builds a
probability model and a set of solutions is sampled from the model which
characterizes the distribution of such solutions. The main framework of the
proposed method is an estimation of distribution algorithm, in which an
adaptive Gibbs sampling is used to generate new promising solutions and, in
combination with a local search strategy, it improves the individual solutions
produced in each iteration. The Estimation of Distribution Algorithm with
Adaptive Gibbs Sampling we are proposing in this paper is called AGEDA. We
experimentally evaluate and compare this algorithm against two deterministic
procedures and several stochastic methods in three well known test problems for
unconstrained global optimization. It is empirically shown that our heuristic
is robust in problems that involve three central aspects that mainly determine
the difficulty of global optimization problems, namely high-dimensionality,
multi-modality and non-smoothness.
"
"  This paper presents the current state of a work in progress, whose objective
is to better understand the effects of factors that significantly influence the
performance of Latent Semantic Analysis (LSA). A difficult task, which consists
in answering (French) biology Multiple Choice Questions, is used to test the
semantic properties of the truncated singular space and to study the relative
influence of main parameters. A dedicated software has been designed to fine
tune the LSA semantic space for the Multiple Choice Questions task. With
optimal parameters, the performances of our simple model are quite surprisingly
equal or superior to those of 7th and 8th grades students. This indicates that
semantic spaces were quite good despite their low dimensions and the small
sizes of training data sets. Besides, we present an original entropy global
weighting of answers' terms of each question of the Multiple Choice Questions
which was necessary to achieve the model's success.
"
"  Risk bounds for Classification and Regression Trees (CART, Breiman et. al.
1984) classifiers are obtained under a margin condition in the binary
supervised classification framework. These risk bounds are obtained
conditionally on the construction of the maximal deep binary tree and permit to
prove that the linear penalty used in the CART pruning algorithm is valid under
a margin condition. It is also shown that, conditionally on the construction of
the maximal tree, the final selection by test sample does not alter
dramatically the estimation accuracy of the Bayes classifier. In the two-class
classification framework, the risk bounds that are proved, obtained by using
penalized model selection, validate the CART algorithm which is used in many
data mining applications such as Biology, Medicine or Image Coding.
"
"  Computer generated academic papers have been used to expose a lack of
thorough human review at several computer science conferences. We assess the
problem of classifying such documents. After identifying and evaluating several
quantifiable features of academic papers, we apply methods from machine
learning to build a binary classifier. In tests with two hundred papers, the
resulting classifier correctly labeled papers either as human written or as
computer generated with no false classifications of computer generated papers
as human and a 2% false classification rate for human papers as computer
generated. We believe generalizations of these features are applicable to
similar classification problems. While most current text-based spam detection
techniques focus on the keyword-based classification of email messages, a new
generation of unsolicited computer-generated advertisements masquerade as
legitimate postings in online groups, message boards and social news sites. Our
results show that taking the formatting and contextual clues offered by these
environments into account may be of central importance when selecting features
with which to identify such unwanted postings.
"
"  The problem of replicating the flexibility of human common-sense reasoning
has captured the imagination of computer scientists since the early days of
Alan Turing's foundational work on computation and the philosophy of artificial
intelligence. In the intervening years, the idea of cognition as computation
has emerged as a fundamental tenet of Artificial Intelligence (AI) and
cognitive science. But what kind of computation is cognition?
  We describe a computational formalism centered around a probabilistic Turing
machine called QUERY, which captures the operation of probabilistic
conditioning via conditional simulation. Through several examples and analyses,
we demonstrate how the QUERY abstraction can be used to cast common-sense
reasoning as probabilistic inference in a statistical model of our observations
and the uncertain structure of the world that generated that experience. This
formulation is a recent synthesis of several research programs in AI and
cognitive science, but it also represents a surprising convergence of several
of Turing's pioneering insights in AI, the foundations of computation, and
statistics.
"
"  Compression-based similarity measures are effectively employed in
applications on diverse data types with a basically parameter-free approach.
Nevertheless, there are problems in applying these techniques to
medium-to-large datasets which have been seldom addressed. This paper proposes
a similarity measure based on compression with dictionaries, the Fast
Compression Distance (FCD), which reduces the complexity of these methods,
without degradations in performance. On its basis a content-based color image
retrieval system is defined, which can be compared to state-of-the-art methods
based on invariant color features. Through the FCD a better understanding of
compression-based techniques is achieved, by performing experiments on datasets
which are larger than the ones analyzed so far in literature.
"
"  The concepts of probability, statistics and stochastic theory are being
successfully used in structural engineering. Markov Chain modelling is a simple
stochastic process model that has found its application in both describing
stochastic evolution of system and in system reliability estimation. The recent
developments in Markov Chain Monte Carlo and the possible integration of
Bayesian theory within Markov Chain theory have enhanced its application
possibilities. However, the application possibility can be furthered to range
over wider scales of application (perhaps from nano- to macro-) by considering
the developments in Physics (in particular Quantum Physics). This paper tries
to present the results of quantum physics that would help in interpretation of
transition probability matrix. However, care has to be taken in the choice of
densities in computing the transition probability matrix. The paper is based on
available literature, and the aim is only to make an attempt to show how Markov
Chain can be used to model systems at various scales.
"
"  Currently, the high-precision estimation of nonlinear parameters such as Gini
indices, low-income proportions or other measures of inequality is particularly
crucial. In the present paper, we propose a general class of estimators for
such parameters that take into account univariate auxiliary information assumed
to be known for every unit in the population. Through a nonparametric
model-assisted approach, we construct a unique system of survey weights that
can be used to estimate any nonlinear parameter associated with any study
variable of the survey, using a plug-in principle. Based on a rigorous
functional approach and a linearization principle, the asymptotic variance of
the proposed estimators is derived, and variance estimators are shown to be
consistent under mild assumptions. The theory is fully detailed for penalized
B-spline estimators together with suggestions for practical implementation and
guidelines for choosing the smoothing parameters. The validity of the method is
demonstrated on data extracted from the French Labor Force Survey. Point and
confidence intervals estimation for the Gini index and the low-income
proportion are derived. Theoretical and empirical results highlight our
interest in using a nonparametric approach versus a parametric one when
estimating nonlinear parameters in the presence of auxiliary information.
"
"  Machine learning is used to approximate density functionals. For the model
problem of the kinetic energy of non-interacting fermions in 1d, mean absolute
errors below 1 kcal/mol on test densities similar to the training set are
reached with fewer than 100 training densities. A predictor identifies if a
test density is within the interpolation region. Via principal component
analysis, a projected functional derivative finds highly accurate
self-consistent densities. Challenges for application of our method to real
electronic structure problems are discussed.
"
"  We develop an improved bound for the approximation error of the Nystr\""{o}m
method under the assumption that there is a large eigengap in the spectrum of
kernel matrix. This is based on the empirical observation that the eigengap has
a significant impact on the approximation error of the Nystr\""{o}m method. Our
approach is based on the concentration inequality of integral operator and the
theory of matrix perturbation. Our analysis shows that when there is a large
eigengap, we can improve the approximation error of the Nystr\""{o}m method from
$O(N/m^{1/4})$ to $O(N/m^{1/2})$ when measured in Frobenius norm, where $N$ is
the size of the kernel matrix, and $m$ is the number of sampled columns.
"
"  Classical multidimensional scaling (MDS) is a method for visualizing
high-dimensional point clouds by mapping to low-dimensional Euclidean space.
This mapping is defined in terms of eigenfunctions of a matrix of interpoint
dissimilarities. In this paper we analyze in detail multidimensional scaling
applied to a specific dataset: the 2005 United States House of Representatives
roll call votes. Certain MDS and kernel projections output ``horseshoes'' that
are characteristic of dimensionality reduction techniques. We show that, in
general, a latent ordering of the data gives rise to these patterns when one
only has local information. That is, when only the interpoint distances for
nearby points are known accurately. Our results provide a rigorous set of
results and insight into manifold learning in the special case where the
manifold is a curve.
"
"  The large amount of data on galaxies, up to higher and higher redshifts, asks
for sophisticated statistical approaches to build adequate classifications.
Multivariate cluster analyses, that compare objects for their global
similarities, are still confidential in astrophysics, probably because their
results are somewhat difficult to interpret. We believe that the missing key is
the unavoidable characteristics in our Universe: evolution. Our approach, known
as Astrocladistics, is based on the evolutionary nature of both galaxies and
their properties. It gathers objects according to their ""histories"" and
establishes an evolutionary scenario among groups of objects. In this
presentation, I show two recent results on globular clusters and earlytype
galaxies to illustrate how the evolutionary concepts of Astrocladistics can
also be useful for multivariate analyses such as K-means Cluster Analysis.
"
"  The influence of climate on biodiversity is an important ecological question.
Various theories try to link climate change to allelic richness and therefore
to predict the impact of global warming on genetic diversity. We model the
relationship between genetic diversity in the European beech forests and curves
of temperature and precipitation reconstructed from pollen databases. Our model
links the genetic measure to the climate curves through a linear functional
regression. The interaction in climate variables is assumed to be bilinear.
Since the data are georeferenced, our methodology accounts for the spatial
dependence among the observations. The practical issues of these extensions are
discussed.
"
"  In this paper we prove the probabilistic continuous complexity conjecture. In
continuous complexity theory, this states that the complexity of solving a
continuous problem with probability approaching 1 converges (in this limit) to
the complexity of solving the same problem in its worst case. We prove the
conjecture holds if and only if space of problem elements is uniformly convex.
The non-uniformly convex case has a striking counterexample in the problem of
identifying a Brownian path in Wiener space, where it is shown that
probabilistic complexity converges to only half of the worst case complexity in
this limit.
"
"  We describe the shrinking neighborhood approach of Robust Statistics, which
applies to general smoothly parametrized models, especially, exponential
families. Equal generality is achieved by object oriented implementation of the
optimally robust estimators. We evaluate the estimates on real datasets from
literature by means of our R packages ROptEst and RobLox.
"
"  We present a quantitative analysis of throwing ability for major league
outfielders and catchers. We use detailed game event data to tabulate success
and failure events in outfielder and catcher throwing opportunities. We
attribute a run contribution to each success or failure which are tabulated for
each player in each season. We use four seasons of data to estimate the overall
throwing ability of each player using a Bayesian hierarchical model. This model
allows us to shrink individual player estimates towards an overall population
mean depending on the number of opportunities for each player. We use the
posterior distribution of player abilities from this model to identify players
with significant positive and negative throwing contributions.
"
"  In this paper, we consider the problem of compressed sensing where the goal
is to recover almost all the sparse vectors using a small number of fixed
linear measurements. For this problem, we propose a novel partial
hard-thresholding operator that leads to a general family of iterative
algorithms. While one extreme of the family yields well known hard thresholding
algorithms like ITI (Iterative Thresholding with Inversion) and HTP (Hard
Thresholding Pursuit), the other end of the spectrum leads to a novel algorithm
that we call Orthogonal Matching Pursuit with Replacement (OMPR). OMPR, like
the classic greedy algorithm OMP, adds exactly one coordinate to the support at
each iteration, based on the correlation with the current residual. However,
unlike OMP, OMPR also removes one coordinate from the support. This simple
change allows us to prove that OMPR has the best known guarantees for sparse
recovery in terms of the Restricted Isometry Property (a condition on the
measurement matrix). In contrast, OMP is known to have very weak performance
guarantees under RIP. Given its simple structure, we are able to extend OMPR
using locality sensitive hashing to get OMPR-Hash, the first provably
sub-linear (in dimensionality) algorithm for sparse recovery. Our proof
techniques are novel and flexible enough to also permit the tightest known
analysis of popular iterative algorithms such as CoSaMP and Subspace Pursuit.
We provide experimental results on large problems providing recovery for
vectors of size up to million dimensions. We demonstrate that for large-scale
problems our proposed methods are more robust and faster than existing methods.
"
"  In the last decades the estimation of the intrinsic dimensionality of a
dataset has gained considerable importance. Despite the great deal of research
work devoted to this task, most of the proposed solutions prove to be
unreliable when the intrinsic dimensionality of the input dataset is high and
the manifold where the points lie is nonlinearly embedded in a higher
dimensional space. In this paper we propose a novel robust intrinsic
dimensionality estimator that exploits the twofold complementary information
conveyed both by the normalized nearest neighbor distances and by the angles
computed on couples of neighboring points, providing also closed-forms for the
Kullback-Leibler divergences of the respective distributions. Experiments
performed on both synthetic and real datasets highlight the robustness and the
effectiveness of the proposed algorithm when compared to state of the art
methodologies.
"
"  Using a large deviations approach we calculate the probability distribution
of the mutual information of MIMO channels in the limit of large antenna
numbers. In contrast to previous methods that only focused at the distribution
close to its mean (thus obtaining an asymptotically Gaussian distribution), we
calculate the full distribution, including its tails which strongly deviate
from the Gaussian behavior near the mean. The resulting distribution
interpolates seamlessly between the Gaussian approximation for rates $R$ close
to the ergodic value of the mutual information and the approach of Zheng and
Tse for large signal to noise ratios $\rho$. This calculation provides us with
a tool to obtain outage probabilities analytically at any point in the $(R,
\rho, N)$ parameter space, as long as the number of antennas $N$ is not too
small. In addition, this method also yields the probability distribution of
eigenvalues constrained in the subspace where the mutual information per
antenna is fixed to $R$ for a given $\rho$. Quite remarkably, this eigenvalue
density is of the form of the Marcenko-Pastur distribution with square-root
singularities, and it depends on the values of $R$ and $\rho$.
"
"  An extension of the latent Markov Rasch model is described for the analysis
of binary longitudinal data with covariates when subjects are collected in
clusters, e.g. students clustered in classes. For each subject, the latent
process is used to represent the characteristic of interest (e.g. ability)
conditional on the effect of the cluster to which he/she belongs. The latter
effect is modeled by a discrete latent variable associated with each cluster.
For the maximum likelihood estimation of the model parameters we outline an EM
algorithm. We show how the proposed model may be used for assessing the
development of cognitive Math achievement. This approach is applied to the
analysis of a dataset collected in the Lombardy Region (Italy) and based on
test scores over three years of middle-school students attending public and
private schools.
"
"  We introduce the nonparametric metadata dependent relational (NMDR) model, a
Bayesian nonparametric stochastic block model for network data. The NMDR allows
the entities associated with each node to have mixed membership in an unbounded
collection of latent communities. Learned regression models allow these
memberships to depend on, and be predicted from, arbitrary node metadata. We
develop efficient MCMC algorithms for learning NMDR models from partially
observed node relationships. Retrospective MCMC methods allow our sampler to
work directly with the infinite stick-breaking representation of the NMDR,
avoiding the need for finite truncations. Our results demonstrate recovery of
useful latent communities from real-world social and ecological networks, and
the usefulness of metadata in link prediction tasks.
"
"  Partial-monitoring games constitute a mathematical framework for sequential
decision making problems with imperfect feedback: The learner repeatedly
chooses an action, opponent responds with an outcome, and then the learner
suffers a loss and receives a feedback signal, both of which are fixed
functions of the action and the outcome. The goal of the learner is to minimize
his total cumulative loss. We make progress towards the classification of these
games based on their minimax expected regret. Namely, we classify almost all
games with two outcomes and finite number of actions: We show that their
minimax expected regret is either zero, $\widetilde{\Theta}(\sqrt{T})$,
$\Theta(T^{2/3})$, or $\Theta(T)$ and we give a simple and efficiently
computable classification of these four classes of games. Our hope is that the
result can serve as a stepping stone toward classifying all finite
partial-monitoring games.
"
"  We present the extention and application of a new unsupervised statistical
learning technique--the Partition Decoupling Method--to gene expression data.
Because it has the ability to reveal non-linear and non-convex geometries
present in the data, the PDM is an improvement over typical gene expression
analysis algorithms, permitting a multi-gene analysis that can reveal
phenotypic differences even when the individual genes do not exhibit
differential expression. Here, we apply the PDM to publicly-available gene
expression data sets, and demonstrate that we are able to identify cell types
and treatments with higher accuracy than is obtained through other approaches.
By applying it in a pathway-by-pathway fashion, we demonstrate how the PDM may
be used to find sets of mechanistically-related genes that discriminate
phenotypes.
"
"  Consider a problem of predicting a response variable using a set of
covariates in a linear regression model. If it is \emph{a priori} known or
suspected that a subset of the covariates do not significantly contribute to
the overall fit of the model, a restricted model that excludes these
covariates, may be sufficient. If, on the other hand, the subset provides
useful information, shrinkage method combines restricted and unrestricted
estimators to obtain the parameter estimates. Such an estimator outperforms the
classical maximum likelihood estimators. Any \emph{prior} information may be
validated through preliminary test (or pretest), and depending on the validity,
may be incorporated in the model as a parametric restriction. Thus, pretest
estimator chooses between the restricted and unrestricted estimators depending
on the outcome of the preliminary test. Examples using three real life data
sets are provided to illustrate the application of shrinkage and pretest
estimation. Performance of positive-shrinkage and pretest estimators are
compared with unrestricted estimator under varying degree of uncertainty of the
prior information. Monte Carlo study reconfirms the asymptotic properties of
the estimators available in the literature.
"
"  Most scientific institutions acknowledge the importance of opening the
so-called 'ivory tower' of academic research through popularization, industrial
collaboration or teaching. However, little is known about the actual openness
of scientific institutions and how their proclaimed priorities translate into
concrete measures. This paper gives an idea of some actual practices by
studying three key points: the proportion of researchers who are active in
wider dissemination, the academic productivity of these scientists, and the
institutional recognition of their wider dissemination activities in terms of
their careers. We analyze extensive data about the academic production, career
recognition and teaching or public/industrial outreach of several thousand of
scientists, from many disciplines, from France's Centre National de la
Recherche Scientifique. We find that, contrary to what is often suggested,
scientists active in wider dissemination are also more active academically.
However, their dissemination activities have almost no impact (positive or
negative) on their careers.
"
"  We investigate crossing path probabilities for two agents that move randomly
in a bounded region of the plane or on a sphere (denoted $R$). At each discrete
time-step the agents move, independently, fixed distances $d_1$ and $d_2$ at
angles that are uniformly distributed in $(0,2\pi)$. If $R$ is large enough and
the initial positions of the agents are uniformly distributed in $R$, then the
probability of paths crossing at the first time-step is close to $ 2d_1d_2/(\pi
A[R])$, where $A[R]$ is the area of $R$. Simulations suggest that the long-run
rate at which paths cross is also close to $2d_1d_2/(\pi A[R])$ (despite marked
departures from uniformity and independence conditions needed for such a
conclusion).
"
"  We apply multiple testing procedures to the validation of estimated default
probabilities in credit rating systems. The goal is to identify rating classes
for which the probability of default is estimated inaccurately, while still
maintaining a predefined level of committing type I errors as measured by the
familywise error rate (FWER) and the false discovery rate (FDR). For FWER, we
also consider procedures that take possible discreteness of the data resp. test
statistics into account. The performance of these methods is illustrated in a
simulation setting and for empirical default data.
"
"  The rupture of an abdominal aortic aneurysm (AAA) is associated with a high
mortality. When an AAA ruptures, 50% of the patients die before reaching the
hospital. Of the patients that are able to reach the operating room, only 50%
have it successfully repaired (Fillinger et al, 2003). Therefore, it is
important to find good predictors for immediate risk of rupture. Clinically,
the size of the aneurysm is the variable vascular surgeons usually use to
evaluate this risk. Patients with large aneurysms are often sent to surgery.
However, many studies have shown that even small aneurysms can rupture and
deserve attention as well. It is important to find good predictors of rupture
that also avoid unnecessary surgery as all surgeries are associated with
possible complications. Here, we use data obtained from 144 computed
tomographies of patients from the Western Pennsylvania Allegheny Health System
to predict the high risk of rupture of an aneurysm and also to examine which
features are important for this goal.
"
"  A nonparametric Bayesian extension of Factor Analysis (FA) is proposed where
observed data $\mathbf{Y}$ is modeled as a linear superposition, $\mathbf{G}$,
of a potentially infinite number of hidden factors, $\mathbf{X}$. The Indian
Buffet Process (IBP) is used as a prior on $\mathbf{G}$ to incorporate sparsity
and to allow the number of latent features to be inferred. The model's utility
for modeling gene expression data is investigated using randomly generated data
sets based on a known sparse connectivity matrix for E. Coli, and on three
biological data sets of increasing complexity.
"
"  This paper describes a new median algorithm and a median approximation
algorithm. The former has O(n) average running time and the latter has O(n)
worst-case running time. These algorithms are highly competitive with the
standard algorithm when computing the median of a single data set, but are
significantly faster in updating the median when more data is added.
"
"  We investigate the problem of counting co-authorhip in order to quantify the
impact and relevance of scientific research output through normalized
\textit{h-index} and \textit{g-index}. We use the papers whose authors belong
to a subset of full professors of the Italian Settore Scientifico Disciplinare
(SSD) FIS01 - Experimental Physics. In this SSD two populations, characterized
by the number of co-authors of each paper, are roughly present. The total
number of citations for each individuals, as well as their h-index and g-index,
strongly depends on the average number of co-authors. We show that, in order to
remove the dependence of the various indices on the two populations, the best
way to define a fractional counting of autorship is to divide the number of
citations received by each paper by the square root of the number of
co-authors. This allows us to obtain some information which can be used for a
better understanding of the scientific knowledge made through the process of
writing and publishing papers.
"
"  Most of the reliability literature on modeling the effect of repairs on
systems assumes the failure rate functions are monotonically increasing. For
systems with non-monotonic failure rate functions, most models deal with
minimal repairs (which do not affect the working condition of the system) or
replacements (which return the working condition to that of a new and identical
system). We explore a new approach to model repairs of a system with a
non-monotonic failure rate function; in particular, we consider systems with a
bathtub-shaped failure rate function. We propose a repair model specified in
terms of modifications to the virtual age function of the system, while
preserving the usual definitions of the types of repair (minimal, imperfect and
perfect repairs) and distinguishing between perfect repair and replacement. In
addition, we provide a numerical illustration of the proposed repair model.
"
"  This paper gives a method for computing distributions associated with
patterns in the state sequence of a hidden Markov model, conditional on
observing all or part of the observation sequence. Probabilities are computed
for very general classes of patterns (competing patterns and generalized later
patterns), and thus, the theory includes as special cases results for a large
class of problems that have wide application. The unobserved state sequence is
assumed to be Markovian with a general order of dependence. An auxiliary Markov
chain is associated with the state sequence and is used to simplify the
computations. Two examples are given to illustrate the use of the methodology.
Whereas the first application is more to illustrate the basic steps in applying
the theory, the second is a more detailed application to DNA sequences, and
shows that the methods can be adapted to include restrictions related to
biological knowledge.
"
"  We study the problem of unsupervised domain adaptation, which aims to adapt
classifiers trained on a labeled source domain to an unlabeled target domain.
Many existing approaches first learn domain-invariant features and then
construct classifiers with them. We propose a novel approach that jointly learn
the both. Specifically, while the method identifies a feature space where data
in the source and the target domains are similarly distributed, it also learns
the feature space discriminatively, optimizing an information-theoretic metric
as an proxy to the expected misclassification error on the target domain. We
show how this optimization can be effectively carried out with simple
gradient-based methods and how hyperparameters can be cross-validated without
demanding any labeled data from the target domain. Empirical studies on
benchmark tasks of object recognition and sentiment analysis validated our
modeling assumptions and demonstrated significant improvement of our method
over competing ones in classification accuracies.
"
"  There is much interest in the Hierarchical Dirichlet Process Hidden Markov
Model (HDP-HMM) as a natural Bayesian nonparametric extension of the
traditional HMM. However, in many settings the HDP-HMM's strict Markovian
constraints are undesirable, particularly if we wish to learn or encode
non-geometric state durations. We can extend the HDP-HMM to capture such
structure by drawing upon explicit-duration semi-Markovianity, which has been
developed in the parametric setting to allow construction of highly
interpretable models that admit natural prior information on state durations.
In this paper we introduce the explicitduration HDP-HSMM and develop posterior
sampling algorithms for efficient inference in both the direct-assignment and
weak-limit approximation settings. We demonstrate the utility of the model and
our inference methods on synthetic data as well as experiments on a speaker
diarization problem and an example of learning the patterns in Morse code.
"
"  In spectral clustering and spectral image segmentation, the data is partioned
starting from a given matrix of pairwise similarities S. the matrix S is
constructed by hand, or learned on a separate training set. In this paper we
show how to achieve spectral clustering in unsupervised mode. Our algorithm
starts with a set of observed pairwise features, which are possible components
of an unknown, parametric similarity function. This function is learned
iteratively, at the same time as the clustering of the data. The algorithm
shows promosing results on synthetic and real data.
"
"  The discovery of non-linear causal relationship under additive non-Gaussian
noise models has attracted considerable attention recently because of their
high flexibility. In this paper, we propose a novel causal inference algorithm
called least-squares independence regression (LSIR). LSIR learns the additive
noise model through the minimization of an estimator of the squared-loss mutual
information between inputs and residuals. A notable advantage of LSIR over
existing approaches is that tuning parameters such as the kernel width and the
regularization parameter can be naturally optimized by cross-validation,
allowing us to avoid overfitting in a data-dependent fashion. Through
experiments with real-world datasets, we show that LSIR compares favorably with
a state-of-the-art causal inference method.
"
"  In various situations in the insurance industry, in finance, in epidemiology,
etc., one needs to represent the joint evolution of the number of occurrences
of an event. In this paper, we present a multivariate integer-valued
autoregressive (MINAR) model, derive its properties and apply the model to
earthquake occurrences across various pairs of tectonic plates. The model is an
extension of Pedelis & Karlis (2011) where cross autocorrelation (spatial
contagion in a seismic context) is considered. We fit various bivariate count
models and find that for many contiguous tectonic plates, spatial contagion is
significant in both directions. Furthermore, ignoring cross autocorrelation can
underestimate the potential for high numbers of occurrences over the
short-term. Our overall findings seem to further confirm Parsons & Velasco
(2001).
"
"  SDYNA is a general framework designed to address large stochastic
reinforcement learning problems. Unlike previous model based methods in FMDPs,
it incrementally learns the structure and the parameters of a RL problem using
supervised learning techniques. Then, it integrates decision-theoric planning
algorithms based on FMDPs to compute its policy. SPITI is an instanciation of
SDYNA that exploits ITI, an incremental decision tree algorithm, to learn the
reward function and the Dynamic Bayesian Networks with local structures
representing the transition function of the problem. These representations are
used by an incremental version of the Structured Value Iteration algorithm. In
order to learn the structure, SPITI uses Chi-Square tests to detect the
independence between two probability distributions. Thus, we study the relation
between the threshold used in the Chi-Square test, the size of the model built
and the relative error of the value function of the induced policy with respect
to the optimal value. We show that, on stochastic problems, one can tune the
threshold so as to generate both a compact model and an efficient policy. Then,
we show that SPITI, while keeping its model compact, uses the generalization
property of its learning method to perform better than a stochastic classical
tabular algorithm in large RL problem with an unknown structure. We also
introduce a new measure based on Chi-Square to qualify the accuracy of the
model learned by SPITI. We qualitatively show that the generalization property
in SPITI within the FMDP framework may prevent an exponential growth of the
time required to learn the structure of large stochastic RL problems.
"
"  Using 55 years of daily average temperatures from a local weather station, I
made a least-absolute-deviations (LAD) regression model that accounts for three
effects: seasonal variations, the 11-year solar cycle, and a linear trend. The
model was formulated as a linear programming problem and solved using widely
available optimization software. The solution indicates that temperatures have
gone up by about 2 degrees Fahrenheit over the 55 years covered by the data. It
also correctly identifies the known phase of the solar cycle; i.e., the date of
the last solar minimum. It turns out that the maximum slope of the solar cycle
sinusoid in the regression model is about the same size as the slope produced
by the linear trend. The fact that the solar cycle was correctly extracted by
the model is a strong indicator that effects of this size, in particular the
slope of the linear trend, can be accurately determined from the 55 years of
data analyzed.
  The main purpose for doing this analysis is to demonstrate that it is easy to
find and analyze archived temperature data for oneself. In particular, this
problem makes a good class project for upper-level undergraduate courses in
optimization or in statistics.
  It is worth noting that a similar least-squares model failed to characterize
the solar cycle correctly and hence even though it too indicates that
temperatures have been rising locally, one can be less confident in this
result.
  The paper ends with a section presenting similar results from a few thousand
sites distributed world-wide, some results from a modification of the model
that includes both temperature and humidity, as well as a number of suggestions
for future work and/or ideas for enhancements that could be used as classroom
projects.
"
"  Fading is the time-dependent variation in transmitted signal strength through
a complex medium, due to interference or temporally evolving multipath
scattering. In this paper we use random matrix theory (RMT) to establish a
first-principles model for fading, including both universal and non-universal
effects. This model provides a more general understanding of the most common
statistical models (Rayleigh fading and Rice fading) and provides a detailed
physical basis for their parameters. We also report experimental tests on two
ray-chaotic microwave cavities. The results show that our RMT model agrees with
the Rayleigh/Rice models in the high loss regime, but there are strong
deviations in low-loss systems where the RMT approach describes the data well.
"
"  We consider multivariate two-sample tests of means, where the location shift
between the two populations is expected to be related to a known graph
structure. An important application of such tests is the detection of
differentially expressed genes between two patient populations, as shifts in
expression levels are expected to be coherent with the structure of graphs
reflecting gene properties such as biological process, molecular function,
regulation or metabolism. For a fixed graph of interest, we demonstrate that
accounting for graph structure can yield more powerful tests under the
assumption of smooth distribution shift on the graph. We also investigate the
identification of nonhomogeneous subgraphs of a given large graph, which poses
both computational and multiple hypothesis testing problems. The relevance and
benefits of the proposed approach are illustrated on synthetic data and on
breast and bladder cancer gene expression data analyzed in the context of KEGG
and NCI pathways.
"
"  We propose a nonparametric Bayesian factor regression model that accounts for
uncertainty in the number of factors, and the relationship between factors. To
accomplish this, we propose a sparse variant of the Indian Buffet Process and
couple this with a hierarchical model over factors, based on Kingman's
coalescent. We apply this model to two problems (factor analysis and factor
regression) in gene-expression data analysis.
"
"  Traditional multi-view learning approaches suffer in the presence of view
disagreement,i.e., when samples in each view do not belong to the same class
due to view corruption, occlusion or other noise processes. In this paper we
present a multi-view learning approach that uses a conditional entropy
criterion to detect view disagreement. Once detected, samples with view
disagreement are filtered and standard multi-view learning methods can be
successfully applied to the remaining samples. Experimental evaluation on
synthetic and audio-visual databases demonstrates that the detection and
filtering of view disagreement considerably increases the performance of
traditional multi-view learning approaches.
"
"  We construct an infinitely exchangeable process on the set $\cate$ of subsets
of the power set of the natural numbers $\mathbb{N}$ via a Poisson point
process with mean measure $\Lambda$ on the power set of $\mathbb{N}$. Each
$E\in\cate$ has a least monotone cover in $\catf$, the collection of monotone
subsets of $\cate$, and every monotone subset maps to an undirected graph
$G\in\catg$, the space of undirected graphs with vertex set $\mathbb{N}$. We
show a natural mapping $\cate\rightarrow\catf\rightarrow\catg$ which induces an
infinitely exchangeable measure on the projective system $\catg^{\rest}$ of
graphs $\catg$ under permutation and restriction mappings given an infinitely
exchangeable family of measures on the projective system $\cate^{\rest}$ of
subsets with permutation and restriction maps. We show potential connections of
this process to applications in cluster analysis, machine learning,
classification and Bayesian inference.
"
"  We consider the problem of comparing two diagnostic tests based on a sample
of paired test results without true state determinations, in cases where the
second test can reasonably be assumed to be at least as specific as the first.
For such cases, we provide two informative confidence bounds: A lower one for
the prevalence times the sensitivity gain of the second test with respect to
the first, and an upper one for the sensitivity of the first test. Neither
conditional independence of the two tests nor perfectness of any of them needs
to be assumd.
  An application of the proposed confidence bounds to a sample of 256 pairs of
laboratory test results for toxigenic Clostridium difficile provides evidence
for a dramatic sensitivity gain through first appropriately culturing
Clostridium difficile from stool samples before applying an
enzyme-immuno-assay.
"
"  To obtain the optimal number of communities is an important problem in
detecting community structure. In this paper, we extend the measurement of
community detecting algorithms to find the optimal community number. Based on
the normalized mutual information index, which has been used as a measure for
similarity of communities, a statistic $\Omega(c)$ is proposed to detect the
optimal number of communities. In general, when $\Omega(c)$ reaches its local
maximum, especially the first one, the corresponding number of communities
\emph{c} is likely to be optimal in community detection. Moreover, the
statistic $\Omega(c)$ can also measure the significance of community structures
in complex networks, which has been paid more attention recently. Numerical and
empirical results show that the index $\Omega(c)$ is effective in both
artificial and real world networks.
"
"  We present empirical data on misprints in citations to twelve high-profile
papers. The great majority of misprints are identical to misprints in articles
that earlier cited the same paper. The distribution of the numbers of misprint
repetitions follows a power law. We develop a stochastic model of the citation
process, which explains these findings and shows that about 70-90% of
scientific citations are copied from the lists of references used in other
papers. Citation copying can explain not only why some misprints become
popular, but also why some papers become highly cited. We show that a model
where a scientist picks few random papers, cites them, and copies a fraction of
their references accounts quantitatively for empirically observed distribution
of citations.
"
"  Many investigations have used panel methods to study the relationships
between fluctuations in economic activity and mortality. A broad consensus has
emerged on the overall procyclical nature of mortality: perhaps
counter-intuitively, mortality typically rises above its trend during
expansions. This consensus has been tarnished by inconsistent reports on the
specific age groups and mortality causes involved. We show that these
inconsistencies result, in part, from the trend specifications used in previous
panel models. Standard econometric panel analysis involves fitting regression
models using ordinary least squares, employing standard errors which are robust
to temporal autocorrelation. The model specifications include a fixed effect,
and possibly a linear trend, for each time series in the panel. We propose
alternative methodology based on nonlinear detrending. Applying our methodology
on data for the 50 US states from 1980 to 2006, we obtain more precise and
consistent results than previous studies. We find procyclical mortality in all
age groups. We find clear procyclical mortality due to respiratory disease and
traffic injuries. Predominantly procyclical cardiovascular disease mortality
and countercyclical suicide are subject to substantial state-to-state
variation. Neither cancer nor homicide have significant macroeconomic
association.
"
"  This paper studies business cycle patterns in UK sectoral output. It analyzes
the distinction between white noise processes and their non-white noise
counterparts in the frequency domain and further examines the associated
features and patterns for the process where white noise conditions are
violated. The characteristics of these sectors, arising from their
institutional features that may influence business cycles behavior and
patterns, are discussed. The study then investigates the output of UK GDP
sectors empirically, revealing their similarities and differences in their
business cycle patterns.
"
"  We introduce a general framework to handle structured models (sparse and
block-sparse with possibly overlapping blocks). We discuss new methods for
their recovery from incomplete observation, corrupted with deterministic and
stochastic noise, using block-$\ell_1$ regularization. While the current theory
provides promising bounds for the recovery errors under a number of different,
yet mostly hard to verify conditions, our emphasis is on verifiable conditions
on the problem parameters (sensing matrix and the block structure) which
guarantee accurate recovery. Verifiability of our conditions not only leads to
efficiently computable bounds for the recovery error but also allows us to
optimize these error bounds with respect to the method parameters, and
therefore construct estimators with improved statistical properties. To justify
our approach, we also provide an oracle inequality, which links the properties
of the proposed recovery algorithms and the best estimation performance.
Furthermore, utilizing these verifiable conditions, we develop a
computationally cheap alternative to block-$\ell_1$ minimization, the
non-Euclidean Block Matching Pursuit algorithm. We close by presenting a
numerical study to investigate the effect of different block regularizations
and demonstrate the performance of the proposed recoveries.
"
"  The technological applications of hidden Markov models have been extremely
diverse and successful, including natural language processing, gesture
recognition, gene sequencing, and Kalman filtering of physical measurements.
HMMs are highly non-linear statistical models, and just as linear models are
amenable to linear algebraic techniques, non-linear models are amenable to
commutative algebra and algebraic geometry.
  This paper closely examines HMMs in which all the hidden random variables are
binary. Its main contributions are (1) a birational parametrization for every
such HMM, with an explicit inverse for recovering the hidden parameters in
terms of observables, (2) a semialgebraic model membership test for every such
HMM, and (3) minimal defining equations for the 4-node fully binary model,
comprising 21 quadrics and 29 cubics, which were computed using Grobner bases
in the cumulant coordinates of Sturmfels and Zwiernik. The new model parameters
in (1) are rationally identifiable in the sense of Sullivant, Garcia-Puente,
and Spielvogel, and each model's Zariski closure is therefore a rational
projective variety of dimension 5. Grobner basis computations for the model and
its graph are found to be considerably faster using these parameters. In the
case of two hidden states, item (2) supersedes a previous algorithm of
Schonhuth which is only generically defined, and the defining equations (3)
yield new invariants for HMMs of all lengths $\geq 4$. Such invariants have
been used successfully in model selection problems in phylogenetics, and one
can hope for similar applications in the case of HMMs.
"
"  A kernel based procedure for correcting experimental data for distortions due
to the finite resolution and limited detector acceptance is presented. The
unfolding problem is known to be an ill-posed problem that can not be solved
without some a priori information about solution such as, for example,
smoothness or positivity. In the approach presented here the true distribution
is estimated by a weighted sum of kernels, with the width of the kernels acting
as a regularization parameter responsible for the smoothness of the result.
Cross-validation is used to determine an optimal value for this parameter. A
numerical example with a simulation study of systematical and statistical
errors is presented to illustrate the procedure.
"
"  We consider the sparse inverse covariance regularization problem or graphical
lasso with regularization parameter $\rho$. Suppose the co- variance graph
formed by thresholding the entries of the sample covariance matrix at $\rho$ is
decomposed into connected components. We show that the vertex-partition induced
by the thresholded covariance graph is exactly equal to that induced by the
estimated concentration graph. This simple rule, when used as a wrapper around
existing algorithms, leads to enormous performance gains. For large values of
$\rho$, our proposal splits a large graphical lasso problem into smaller
tractable problems, making it possible to solve an otherwise infeasible large
scale graphical lasso problem.
"
"  A field known as Compressive Sensing (CS) has recently emerged to help
address the growing challenges of capturing and processing high-dimensional
signals and data sets. CS exploits the surprising fact that the information
contained in a sparse signal can be preserved in a small number of compressive
(or random) linear measurements of that signal. Strong theoretical guarantees
have been established on the accuracy to which sparse or near-sparse signals
can be recovered from noisy compressive measurements. In this paper, we address
similar questions in the context of a different modeling framework. Instead of
sparse models, we focus on the broad class of manifold models, which can arise
in both parametric and non-parametric signal families. Building upon recent
results concerning the stable embeddings of manifolds within the measurement
space, we establish both deterministic and probabilistic instance-optimal
bounds in $\ell_2$ for manifold-based signal recovery and parameter estimation
from noisy compressive measurements. In line with analogous results for
sparsity-based CS, we conclude that much stronger bounds are possible in the
probabilistic setting. Our work supports the growing empirical evidence that
manifold-based models can be used with high accuracy in compressive signal
processing.
"
"  The machine learning community has recently devoted much attention to the
problem of inferring causal relationships from statistical data. Most of this
work has focused on uncovering connections among scalar random variables. We
generalize existing methods to apply to collections of multi-dimensional random
vectors, focusing on techniques applicable to linear models. The performance of
the resulting algorithms is evaluated and compared in simulations, which show
that our methods can, in many cases, provide useful information on causal
relationships even for relatively small sample sizes.
"
"  Today's data-heavy research environment requires the integration of different
sources of information into structured data sets that can not be analyzed as
simple matrices. We introduce an old technique, known in the European data
analyses circles as the Duality Diagram Approach, put to new uses through the
use of a variety of metrics and ways of combining different diagrams together.
This issue of the Annals of Applied Statistics contains contemporary examples
of how this approach provides solutions to hard problems in data integration.
We present here the genesis of the technique and how it can be seen as a
precursor of the modern kernel based approaches.
"
"  A single, stationary topic model such as latent Dirichlet allocation is
inappropriate for modeling corpora that span long time periods, as the
popularity of topics is likely to change over time. A number of models that
incorporate time have been proposed, but in general they either exhibit limited
forms of temporal variation, or require computationally expensive inference
methods. In this paper we propose non-parametric Topics over Time (npTOT), a
model for time-varying topics that allows an unbounded number of topics and
exible distribution over the temporal variations in those topics' popularity.
We develop a collapsed Gibbs sampler for the proposed model and compare against
existing models on synthetic and real document sets.
"
"  Feature selection and regularization are becoming increasingly prominent
tools in the efforts of the reinforcement learning (RL) community to expand the
reach and applicability of RL. One approach to the problem of feature selection
is to impose a sparsity-inducing form of regularization on the learning method.
Recent work on $L_1$ regularization has adapted techniques from the supervised
learning literature for use with RL. Another approach that has received renewed
attention in the supervised learning community is that of using a simple
algorithm that greedily adds new features. Such algorithms have many of the
good properties of the $L_1$ regularization methods, while also being extremely
efficient and, in some cases, allowing theoretical guarantees on recovery of
the true form of a sparse target function from sampled data. This paper
considers variants of orthogonal matching pursuit (OMP) applied to
reinforcement learning. The resulting algorithms are analyzed and compared
experimentally with existing $L_1$ regularized approaches. We demonstrate that
perhaps the most natural scenario in which one might hope to achieve sparse
recovery fails; however, one variant, OMP-BRM, provides promising theoretical
guarantees under certain assumptions on the feature dictionary. Another
variant, OMP-TD, empirically outperforms prior methods both in approximation
accuracy and efficiency on several benchmark problems.
"
"  In many fields, researchers are interested in large and complex biological
processes. Two important examples are gene expression and DNA methylation in
genetics. One key problem is to identify aberrant patterns of these processes
and discover biologically distinct groups. In this article we develop a
model-based method for clustering such data. The basis of our method involves
the construction of a likelihood for any given partition of the subjects. We
introduce cluster specific latent indicators that, along with some standard
assumptions, impose a specific mixture distribution on each cluster. Estimation
is carried out using the EM algorithm. The methods extend naturally to multiple
data types of a similar nature, which leads to an integrated analysis over
multiple data platforms, resulting in higher discriminating power.
"
"  Graphical models are popular statistical tools which are used to represent
dependent or causal complex systems. Statistically equivalent causal or
directed graphical models are said to belong to a Markov equivalent class. It
is of great interest to describe and understand the space of such classes.
However, with currently known algorithms, sampling over such classes is only
feasible for graphs with fewer than approximately 20 vertices. In this paper,
we design reversible irreducible Markov chains on the space of Markov
equivalent classes by proposing a perfect set of operators that determine the
transitions of the Markov chain. The stationary distribution of a proposed
Markov chain has a closed form and can be computed easily. Specifically, we
construct a concrete perfect set of operators on sparse Markov equivalence
classes by introducing appropriate conditions on each possible operator.
Algorithms and their accelerated versions are provided to efficiently generate
Markov chains and to explore properties of Markov equivalence classes of sparse
directed acyclic graphs (DAGs) with thousands of vertices. We find
experimentally that in most Markov equivalence classes of sparse DAGs, (1) most
edges are directed, (2) most undirected subgraphs are small and (3) the number
of these undirected subgraphs grows approximately linearly with the number of
vertices. The article contains supplement arXiv:1303.0632,
http://dx.doi.org/10.1214/13-AOS1125SUPP
"
"  Multi-dimensional classification (MDC) is the supervised learning problem
where an instance is associated with multiple classes, rather than with a
single class, as in traditional classification problems. Since these classes
are often strongly correlated, modeling the dependencies between them allows
MDC methods to improve their performance - at the expense of an increased
computational cost. In this paper we focus on the classifier chains (CC)
approach for modeling dependencies, one of the most popular and highest-
performing methods for multi-label classification (MLC), a particular case of
MDC which involves only binary classes (i.e., labels). The original CC
algorithm makes a greedy approximation, and is fast but tends to propagate
errors along the chain. Here we present novel Monte Carlo schemes, both for
finding a good chain sequence and performing efficient inference. Our
algorithms remain tractable for high-dimensional data sets and obtain the best
predictive performance across several real data sets.
"
"  We present the group fused Lasso for detection of multiple change-points
shared by a set of co-occurring one-dimensional signals. Change-points are
detected by approximating the original signals with a constraint on the
multidimensional total variation, leading to piecewise-constant approximations.
Fast algorithms are proposed to solve the resulting optimization problems,
either exactly or approximately. Conditions are given for consistency of both
algorithms as the number of signals increases, and empirical evidence is
provided to support the results on simulated and array comparative genomic
hybridization data.
"
"  We present a novel Bayesian method for the joint reconstruction of
cosmological matter density fields, peculiar velocities and power-spectra in
the quasi-nonlinear regime. We study its applicability to the Ly-alpha forest
based on multiple quasar absorption spectra. Our approach to this problem
includes a multiscale, nonlinear, two-step scheme since the statistics
describing the matter distribution depends on scale, being strongly
non-Gaussian on small scales (< 0.1 h^{-1} Mpc) and closely lognormal on scales
>~10 h^{-1} Mpc. The first step consists on performing 1D highly resolved
matter density reconstructions along the line-of-sight towards z~2-3 quasars
based on an arbitrary non-Gaussian univariate model for matter statistics. The
second step consists on Gibbs-sampling based on conditional PDFs. The matter
density field is sampled in real space with Hamiltonian-sampling using the
Poisson/Gamma-lognormal model, while redshift distortions are corrected with
linear Lagrangian perturbation theory. The power-spectrum of the lognormal
transformed variable which is Gaussian distributed (and thus close to the
linear regime) can consistently be sampled with the inverse Gamma distribution
function. We test our method through numerical N-body simulations with a
computational volume large enough (> 1 h^{-3} Gpc^3) to show that the linear
power-spectra are nicely recovered over scales larger than >~20 h^{-1} Mpc,
i.e. the relevant range where features imprinted by the baryon-acoustics
oscillations (BAOs) appear.
"
"  Overrides of credit ratings are important correctives of ratings that are
determined by statistical rating models. Financial institutions and banking
regulators agree on this because on the one hand errors with ratings of
corporates or banks can have fatal consequences for the lending institutions
and on the other hand errors by statistical methods can be minimised but not
completely avoided. Nonetheless, rating overrides can be misused in order to
conceal the real riskiness of borrowers or even entire portfolios. That is why
rating overrides usually are strictly governed and carefully recorded. It is
not clear, however, which frequency of overrides is appropriate for a given
rating model within a predefined time period. This paper argues that there is a
natural error rate associated with a statistical rating model that may be used
to inform assessment of whether or not an observed override rate is adequate.
The natural error rate is closely related to the rating model's discriminatory
power and can readily be calculated.
"
"  Ultra-high energy cosmic rays (UHECRs) are atomic nuclei with energies over
ten million times energies accessible to human-made particle accelerators.
Evidence suggests that they originate from relatively nearby extragalactic
sources, but the nature of the sources is unknown. We develop a multilevel
Bayesian framework for assessing association of UHECRs and candidate source
populations, and Markov chain Monte Carlo algorithms for estimating model
parameters and comparing models by computing, via Chib's method, marginal
likelihoods and Bayes factors. We demonstrate the framework by analyzing
measurements of 69 UHECRs observed by the Pierre Auger Observatory (PAO) from
2004-2009, using a volume-complete catalog of 17 local active galactic nuclei
(AGN) out to 15 megaparsecs as candidate sources. An early portion of the data
(""period 1,"" with 14 events) was used by PAO to set an energy cut maximizing
the anisotropy in period 1; the 69 measurements include this ""tuned"" subset,
and subsequent ""untuned"" events with energies above the same cutoff. Also,
measurement errors are approximately summarized. These factors are problematic
for independent analyses of PAO data. Within the context of ""standard candle""
source models (i.e., with a common isotropic emission rate), and considering
only the 55 untuned events, there is no significant evidence favoring
association of UHECRs with local AGN vs. an isotropic background. The
highest-probability associations are with the two nearest, adjacent AGN,
Centaurus A and NGC 4945. If the association model is adopted, the fraction of
UHECRs that may be associated is likely nonzero but is well below 50%. Our
framework enables estimation of the angular scale for deflection of cosmic rays
by cosmic magnetic fields; relatively modest scales of $\approx\!3^{\circ}$ to
$30^{\circ}$ are favored. Models that assign a large fraction of UHECRs to a
single nearby source (e.g., Centaurus A) are ruled out unless very large
deflection scales are specified a priori, and even then they are disfavored.
However, including the period 1 data alters the conclusions significantly, and
a simulation study supports the idea that the period 1 data are anomalous,
presumably due to the tuning. Accurate and optimal analysis of future data will
likely require more complete disclosure of the data.
"
"  A beta-negative binomial (BNB) process is proposed, leading to a
beta-gamma-Poisson process, which may be viewed as a ""multi-scoop""
generalization of the beta-Bernoulli process. The BNB process is augmented into
a beta-gamma-gamma-Poisson hierarchical structure, and applied as a
nonparametric Bayesian prior for an infinite Poisson factor analysis model. A
finite approximation for the beta process Levy random measure is constructed
for convenient implementation. Efficient MCMC computations are performed with
data augmentation and marginalization techniques. Encouraging results are shown
on document count matrix factorization.
"
"  We apply information-based complexity analysis to support vector machine
(SVM) algorithms, with the goal of a comprehensive continuous algorithmic
analysis of such algorithms. This involves complexity measures in which some
higher order operations (e.g., certain optimizations) are considered primitive
for the purposes of measuring complexity. We consider classes of information
operators and algorithms made up of scaled families, and investigate the
utility of scaling the complexities to minimize error. We look at the division
of statistical learning into information and algorithmic components, at the
complexities of each, and at applications to support vector machine (SVM) and
more general machine learning algorithms. We give applications to SVM
algorithms graded into linear and higher order components, and give an example
in biomedical informatics.
"
"  In this paper, we have analyzed item response times measured at a large scale
unspeeded low stakes test for primary-school students. We have demonstrated the
existence of significant difference in the response time for boys and girls as
well as difference in response time of correct and incorrect answers on this
test. We have also demonstrated existence of the warm up effect for this test.
The results show that responses given by girls exhibit much greater warm up
effect and that difference appears to be the most important cause of the
difference on the test level.
"
"  We use a minimum requirement approach to derive the number of jobs in
proximity services per inhabitant in French rural municipalities. We first
classify the municipalities according to their time distance to the
municipality where the inhabitants go the most frequently to get services
(called MFM). For each set corresponding to a range of time distance to MFM, we
perform a quantile regression estimating the minimum number of service jobs per
inhabitant, that we interpret as an estimation of the number of proximity jobs
per inhabitant. We observe that the minimum number of service jobs per
inhabitant is smaller in small municipalities. Moreover, for municipalities of
similar sizes, when the distance to the MFM increases, we find that the number
of jobs of proximity services per inhabitant increases.
"
"  A parametrization of hypergraphs based on the geometry of points in
$\mathbf{R}^d$ is developed. Informative prior distributions on hypergraphs are
induced through this parametrization by priors on point configurations via
spatial processes. This prior specification is used to infer conditional
independence models or Markov structure of multivariate distributions.
Specifically, we can recover both the junction tree factorization as well as
the hyper Markov law. This approach offers greater control on the distribution
of graph features than Erd\""os-R\'enyi random graphs, supports inference of
factorizations that cannot be retrieved by a graph alone, and leads to new
Metropolis\slash Hastings Markov chain Monte Carlo algorithms with both local
and global moves in graph space. We illustrate the utility of this
parametrization and prior specification using simulations.
"
"  Recent reports have described that the equivalent sample size (ESS) in a
Dirichlet prior plays an important role in learning Bayesian networks. This
paper provides an asymptotic analysis of the marginal likelihood score for a
Bayesian network. Results show that the ratio of the ESS and sample size
determine the penalty of adding arcs in learning Bayesian networks. The number
of arcs increases monotonically as the ESS increases; the number of arcs
monotonically decreases as the ESS decreases. Furthermore, the marginal
likelihood score provides a unified expression of various score metrics by
changing prior knowledge.
"
"  The Lady Maisry ballads afford us a framework within which to segment a
storyline into its major components. Segments and as a consequence nodal points
are discussed for nine different variants of the Lady Maisry story of a (young)
woman being burnt to death by her family, on account of her becoming pregnant
by a foreign personage. We motivate the importance of nodal points in textual
and literary analysis. We show too how the openings of the nine variants can be
analyzed comparatively, and also the conclusions of the ballads.
"
"  The generation of multi-step density forecasts for non-Gaussian data mostly
relies on Monte Carlo simulations which are computationally intensive. Using
aggregated wind power in Ireland, we study two approaches of multi-step density
forecasts which can be obtained from simple iterations so that intensive
computations are avoided. In the first approach, we apply a logistic
transformation to normalize the data approximately and describe the transformed
data using ARIMA--GARCH models so that multi-step forecasts can be iterated
easily. In the second approach, we describe the forecast densities by truncated
normal distributions which are governed by two parameters, namely, the
conditional mean and conditional variance. We apply exponential smoothing
methods to forecast the two parameters simultaneously. Since the underlying
model of exponential smoothing is Gaussian, we are able to obtain multi-step
forecasts of the parameters by simple iterations and thus generate forecast
densities as truncated normal distributions. We generate forecasts for wind
power from 15 minutes to 24 hours ahead. Results show that the first approach
generates superior forecasts and slightly outperforms the second approach under
various proper scores. Nevertheless, the second approach is computationally
more efficient and gives more robust results under different lengths of
training data. It also provides an attractive alternative approach since one is
allowed to choose a particular parametric density for the forecasts, and is
valuable when there are no obvious transformations to normalize the data.
"
"  We propose a general information-theoretic approach called Seraph
(SEmi-supervised metRic leArning Paradigm with Hyper-sparsity) for metric
learning that does not rely upon the manifold assumption. Given the probability
parameterized by a Mahalanobis distance, we maximize the entropy of that
probability on labeled data and minimize it on unlabeled data following entropy
regularization, which allows the supervised and unsupervised parts to be
integrated in a natural and meaningful way. Furthermore, Seraph is regularized
by encouraging a low-rank projection induced from the metric. The optimization
of Seraph is solved efficiently and stably by an EM-like scheme with the
analytical E-Step and convex M-Step. Experiments demonstrate that Seraph
compares favorably with many well-known global and local metric learning
methods.
"
"  In this article, we derive concentration inequalities for the
cross-validation estimate of the generalization error for stable predictors in
the context of risk assessment. The notion of stability has been first
introduced by \cite{DEWA79} and extended by \cite{KEA95}, \cite{BE01} and
\cite{KUNIY02} to characterize class of predictors with infinite VC dimension.
In particular, this covers $k$-nearest neighbors rules, bayesian algorithm
(\cite{KEA95}), boosting,... General loss functions and class of predictors are
considered. We use the formalism introduced by \cite{DUD03} to cover a large
variety of cross-validation procedures including leave-one-out
cross-validation, $k$-fold cross-validation, hold-out cross-validation (or
split sample), and the leave-$\upsilon$-out cross-validation.
  In particular, we give a simple rule on how to choose the cross-validation,
depending on the stability of the class of predictors. In the special case of
uniform stability, an interesting consequence is that the number of elements in
the test set is not required to grow to infinity for the consistency of the
cross-validation procedure. In this special case, the particular interest of
leave-one-out cross-validation is emphasized.
"
"  Mapping expression Quantitative Trait Loci (eQTLs) represents a powerful and
widely-adopted approach to identifying putative regulatory variants and linking
them to specific genes. Up to now eQTL studies have been conducted in a
relatively narrow range of tissues or cell types. However, understanding the
biology of organismal phenotypes will involve understanding regulation in
multiple tissues, and ongoing studies are collecting eQTL data in dozens of
cell types. Here we present a statistical framework for powerfully detecting
eQTLs in multiple tissues or cell types (or, more generally, multiple
subgroups). The framework explicitly models the potential for each eQTL to be
active in some tissues and inactive in others. By modeling the sharing of
active eQTLs among tissues this framework increases power to detect eQTLs that
are present in more than one tissue compared with ""tissue-by-tissue"" analyses
that examine each tissue separately. Conversely, by modeling the inactivity of
eQTLs in some tissues, the framework allows the proportion of eQTLs shared
across different tissues to be formally estimated as parameters of a model,
addressing the difficulties of accounting for incomplete power when comparing
overlaps of eQTLs identified by tissue-by-tissue analyses. Applying our
framework to re-analyze data from transformed B cells, T cells and fibroblasts
we find that it substantially increases power compared with tissue-by-tissue
analysis, identifying 63% more genes with eQTLs (at FDR=0.05). Further the
results suggest that, in contrast to previous analyses of the same data, the
majority of eQTLs detectable in these data are shared among all three tissues.
"
"  In data-mining applications, we are frequently faced with a large fraction of
missing entries in the data matrix, which is problematic for most discriminant
machine learning algorithms. A solution that we explore in this paper is the
use of a generative model (a mixture of Gaussians) to compute the conditional
expectation of the missing variables given the observed variables. Since
training a Gaussian mixture with many different patterns of missing values can
be computationally very expensive, we introduce a spanning-tree based algorithm
that significantly speeds up training in these conditions. We also observe that
good results can be obtained by using the generative model to fill-in the
missing values for a separate discriminant learning algorithm.
"
"  We study the regret of optimal strategies for online convex optimization
games. Using von Neumann's minimax theorem, we show that the optimal regret in
this adversarial setting is closely related to the behavior of the empirical
minimization algorithm in a stochastic process setting: it is equal to the
maximum, over joint distributions of the adversary's action sequence, of the
difference between a sum of minimal expected losses and the minimal empirical
loss. We show that the optimal regret has a natural geometric interpretation,
since it can be viewed as the gap in Jensen's inequality for a concave
functional--the minimizer over the player's actions of expected loss--defined
on a set of probability distributions. We use this expression to obtain upper
and lower bounds on the regret of an optimal strategy for a variety of online
learning problems. Our method provides upper bounds without the need to
construct a learning algorithm; the lower bounds provide explicit optimal
strategies for the adversary.
"
"  Determinantal point processes (DPPs) are elegant probabilistic models of
repulsion that arise in quantum physics and random matrix theory. In contrast
to traditional structured models like Markov random fields, which become
intractable and hard to approximate in the presence of negative correlations,
DPPs offer efficient and exact algorithms for sampling, marginalization,
conditioning, and other inference tasks. We provide a gentle introduction to
DPPs, focusing on the intuitions, algorithms, and extensions that are most
relevant to the machine learning community, and show how DPPs can be applied to
real-world applications like finding diverse sets of high-quality search
results, building informative summaries by selecting diverse sentences from
documents, modeling non-overlapping human poses in images or video, and
automatically building timelines of important news stories.
"
"  A class of robust estimators which are obtained from dual representation of
$\phi$-divergences, are studied empirically for the normal location model.
Members of this class of estimators are compared, and it is found that they are
efficient at the true model and offer an attractive alternative to the maximum
likelihood, in term of robustness .
"
"  Incorporating domain knowledge into the modeling process is an effective way
to improve learning accuracy. However, as it is provided by humans, domain
knowledge can only be specified with some degree of uncertainty. We propose to
explicitly model such uncertainty through probabilistic constraints over the
parameter space. In contrast to hard parameter constraints, our approach is
effective also when the domain knowledge is inaccurate and generally results in
superior modeling accuracy. We focus on generative and conditional modeling
where the parameters are assigned a Dirichlet or Gaussian prior and demonstrate
the framework with experiments on both synthetic and real-world data.
"
"  Climate models have become an important tool in the study of climate and
climate change, and ensemble experiments consisting of multiple climate-model
runs are used in studying and quantifying the uncertainty in climate-model
output. However, there are often only a limited number of model runs available
for a particular experiment, and one of the statistical challenges is to
characterize the distribution of the model output. To that end, we have
developed a multivariate hierarchical approach, at the heart of which is a new
representation of a multivariate Markov random field. This approach allows for
flexible modeling of the multivariate spatial dependencies, including the
cross-dependencies between variables. We demonstrate this statistical model on
an ensemble arising from a regional-climate-model experiment over the western
United States, and we focus on the projected change in seasonal temperature and
precipitation over the next 50 years.
"
"  We present a number of variously rearranged matrix plots of the $3, 107
\times 3, 107$ 1995-2000 (asymmetric) intercounty migration table for the
United States, principally in its bistochasticized form (all 3,107 row and
column sums iteratively proportionally fitted to equal 1). In one set of plots,
the counties are seriated on the bases of the subdominant (left and right)
eigenvectors of the bistochastic matrix. In another set, we use the ordering of
counties in the dendrogram generated by the associated strong component
hierarchical clustering. Interesting, diverse features of U. S. intercounty
migration emerge--such as a contrast in centralized, hub-like
(cosmopolitan/provincial) properties between cosmopolitan ""Sunbelt"" and
provincial ""Black Belt"" counties. The methodologies employed should also be
insightful for the many other diverse forms of interesting transaction
flow-type data--interjournal citations being an obvious, much-studied example,
where one might expect that the journals Science, Nature and PNAS would display
""cosmopolitan"" characteristics.
"
"  In this paper we introduce deep Gaussian process (GP) models. Deep GPs are a
deep belief network based on Gaussian process mappings. The data is modeled as
the output of a multivariate GP. The inputs to that Gaussian process are then
governed by another GP. A single layer model is equivalent to a standard GP or
the GP latent variable model (GP-LVM). We perform inference in the model by
approximate variational marginalization. This results in a strict lower bound
on the marginal likelihood of the model which we use for model selection
(number of layers and nodes per layer). Deep belief networks are typically
applied to relatively large data sets using stochastic gradient descent for
optimization. Our fully Bayesian treatment allows for the application of deep
models even when data is scarce. Model selection by our variational bound shows
that a five layer hierarchy is justified even when modelling a digit data set
containing only 150 examples.
"
"  This paper continues description of applications of signed chord length
distribution started in part I (arXiv:0711.4734). It is shown simple relation
between equation for some transfer integrals with source and target bodies and
different geometrical distributions for union of this bodies. The union of
disjoint bodies is always nonconvex object and for such a case derivatives of
correlation function (used for definition of signed radii and chord lengths
distributions) always produce (quasi)densities with negative values. Many
equations used in this part are direct consequences of analogue formulas in
part I.
"
"  We consider the problem of function estimation in the case where an
underlying causal model can be inferred. This has implications for popular
scenarios such as covariate shift, concept drift, transfer learning and
semi-supervised learning. We argue that causal knowledge may facilitate some
approaches for a given problem, and rule out others. In particular, we
formulate a hypothesis for when semi-supervised learning can help, and
corroborate it with empirical results.
"
"  Gene expression microarray technologies provide the simultaneous measurements
of a large number of genes. Typical analyses of such data focus on the
individual genes, but recent work has demonstrated that evaluating changes in
expression across predefined sets of genes often increases statistical power
and produces more robust results. We introduce a new methodology for
identifying gene sets that are differentially expressed under varying
experimental conditions. Our approach uses a hierarchical Bayesian framework
where a hyperparameter measures the significance of each gene set. Using
simulated data, we compare our proposed method to alternative approaches, such
as Gene Set Enrichment Analysis (GSEA) and Gene Set Analysis (GSA). Our
approach provides the best overall performance. We also discuss the application
of our method to experimental data based on p53 mutation status.
"
"  For the vast majority of genome wide association studies (GWAS) published so
far, statistical analysis was performed by testing markers individually. In
this article we present some elementary statistical considerations which
clearly show that in case of complex traits the approach based on multiple
regression or generalized linear models is preferable to multiple testing. We
introduce a model selection approach to GWAS based on modifications of Bayesian
Information Criterion (BIC) and develop some simple search strategies to deal
with the huge number of potential models. Comprehensive simulations based on
real SNP data confirm that model selection has larger power than multiple
testing to detect causal SNPs in complex models. On the other hand multiple
testing has substantial problems with proper ranking of causal SNPs and tends
to detect a certain number of false positive SNPs, which are not linked to any
of the causal mutations. We show that this behavior is typical in GWAS for
complex traits and can be explained by an aggregated influence of many small
random sample correlations between genotypes of a SNP under investigation and
other causal SNPs. We believe that our findings at least partially explain
problems with low power and nonreplicability of results in many real data GWAS.
Finally, we discuss the advantages of our model selection approach in the
context of real data analysis, where we consider publicly available gene
expression data as traits for individuals from the HapMap project.
"
"  In vaccine studies for infectious diseases such as human immunodeficiency
virus (HIV), the frequency and type of contacts between study participants and
infectious sources are among the most informative risk factors, but are often
not adequately adjusted for in standard analyses. Such adjustment can improve
the assessment of vaccine efficacy as well as the assessment of risk factors.
It can be attained by modeling transmission per contact with infectious
sources. However, information about contacts that rely on self-reporting by
study participants are subject to nontrivial measurement error in many studies.
We develop a Bayesian hierarchical model fitted using Markov chain Monte Carlo
(MCMC) sampling to estimate the vaccine efficacy controlled for exposure to
infection, while adjusting for measurement error in contact-related factors.
Our method is used to re-analyze two recent HIV vaccine studies, and the
results are compared with the published primary analyses that used standard
methods. The proposed method could also be used for other vaccines where
contact information is collected, such as human papilloma virus vaccines.
"
"  This paper proposes a simple but effective graph-based agglomerative
algorithm, for clustering high-dimensional data. We explore the different roles
of two fundamental concepts in graph theory, indegree and outdegree, in the
context of clustering. The average indegree reflects the density near a sample,
and the average outdegree characterizes the local geometry around a sample.
Based on such insights, we define the affinity measure of clusters via the
product of average indegree and average outdegree. The product-based affinity
makes our algorithm robust to noise. The algorithm has three main advantages:
good performance, easy implementation, and high computational efficiency. We
test the algorithm on two fundamental computer vision problems: image
clustering and object matching. Extensive experiments demonstrate that it
outperforms the state-of-the-arts in both applications.
"
"  We present a new application and covering number bound for the framework of
""Machine Learning with Operational Costs (MLOC),"" which is an exploratory form
of decision theory. The MLOC framework incorporates knowledge about how a
predictive model will be used for a subsequent task, thus combining machine
learning with the decision that is made afterwards. In this work, we use the
MLOC framework to study a problem that has implications for power grid
reliability and maintenance, called the Machine Learning and Traveling
Repairman Problem ML&TRP. The goal of the ML&TRP is to determine a route for a
""repair crew,"" which repairs nodes on a graph. The repair crew aims to minimize
the cost of failures at the nodes, but as in many real situations, the failure
probabilities are not known and must be estimated. The MLOC framework allows us
to understand how this uncertainty influences the repair route. We also present
new covering number generalization bounds for the MLOC framework.
"
"  Having observed an $m\times n$ matrix $X$ whose rows are possibly correlated,
we wish to test the hypothesis that the columns are independent of each other.
Our motivation comes from microarray studies, where the rows of $X$ record
expression levels for $m$ different genes, often highly correlated, while the
columns represent $n$ individual microarrays, presumably obtained
independently. The presumption of independence underlies all the familiar
permutation, cross-validation and bootstrap methods for microarray analysis, so
it is important to know when independence fails. We develop nonparametric and
normal-theory testing methods. The row and column correlations of $X$ interact
with each other in a way that complicates test procedures, essentially by
reducing the accuracy of the relevant estimators.
"
"  Density estimates based on point processes are often restrained to regions
with irregular boundaries or holes. We propose a density estimator, the
lattice-based density estimator, which produces reasonable density estimates
under these circumstances. The estimation process starts with overlaying the
region with nodes, linking these together in a lattice and then computing the
density of random walks of length k on the lattice. We use an approximation to
the unbiased crossvalidation criterion to find the optimal walk length k. The
technique is illustrated using walleye (Sander vitreus) radiotelemetry
relocations in Lake Monroe, Indiana. We also use simulation to compare the
technique to the traditional kernel density estimate in the situation where
there are no significant boundary effects.
"
"  Recovery of low-rank matrices has recently seen significant activity in many
areas of science and engineering, motivated by recent theoretical results for
exact reconstruction guarantees and interesting practical applications. A
number of methods have been developed for this recovery problem. However, a
principled method for choosing the unknown target rank is generally not
provided. In this paper, we present novel recovery algorithms for estimating
low-rank matrices in matrix completion and robust principal component analysis
based on sparse Bayesian learning (SBL) principles. Starting from a matrix
factorization formulation and enforcing the low-rank constraint in the
estimates as a sparsity constraint, we develop an approach that is very
effective in determining the correct rank while providing high recovery
performance. We provide connections with existing methods in other similar
problems and empirical results and comparisons with current state-of-the-art
methods that illustrate the effectiveness of this approach.
"
"  Assessment of circulating CD4 count change over time in HIV-infected subjects
on antiretroviral therapy (ART) is a central component of disease monitoring.
The increasing number of HIV-infected subjects starting therapy and the limited
capacity to support CD4 count testing within resource-limited settings have
fueled interest in identifying correlates of CD4 count change such as total
lymphocyte count, among others. The application of modeling techniques will be
essential to this endeavor due to the typically nonlinear CD4 trajectory over
time and the multiple input variables necessary for capturing CD4 variability.
We propose a prediction-based classification approach that involves first stage
modeling and subsequent classification based on clinically meaningful
thresholds. This approach draws on existing analytical methods described in the
receiver operating characteristic curve literature while presenting an
extension for handling a continuous outcome. Application of this method to an
independent test sample results in greater than 98% positive predictive value
for CD4 count change. The prediction algorithm is derived based on a cohort of
$n=270$ HIV-1 infected individuals from the Royal Free Hospital, London who
were followed for up to three years from initiation of ART. A test sample
comprised of $n=72$ individuals from Philadelphia and followed for a similar
length of time is used for validation. Results suggest that this approach may
be a useful tool for prioritizing limited laboratory resources for CD4 testing
after subjects start antiretroviral therapy.
"
"  Various metrics for comparing diffusion tensors have been recently proposed
in the literature. We consider a broad family of metrics which is indexed by a
single power parameter. A likelihood-based procedure is developed for choosing
the most appropriate metric from the family for a given dataset at hand. The
approach is analogous to using the Box-Cox transformation that is frequently
investigated in regression analysis. The methodology is illustrated with a
simulation study and an application to a real dataset of diffusion tensor
images of canine hearts.
"
"  A new test is proposed for the weak white noise null hypothesis. The test is
based on a new automatic choice of the order for a Box-Pierce or Hong test
statistic. The test uses Lobato (2001) or Kuan and Lee (2006) HAC critical
values. The data-driven order choice is tailored to detect a new class of
alternatives with autocorrelation coefficients which can be $o(n^{-1/2})$
provided there are enough of them. A simulation experiment illustrates the good
behavior of the test both under the weak white noise null and the alternative.
"
"  Many real life problems can be reduced to the solution of a complex
exponentials approximation problem which is usually ill posed. Recently a new
transform for solving this problem, formulated as a specific moments problem in
the plane, has been proposed in a theoretical framework. In this work some
computational issues are addressed to make this new tool useful in practice. An
algorithm is developed and used to solve a Nuclear Magnetic Resonance
spectrometry problem, two time series interpolation and extrapolation problems
and a shape from moments problem.
"
"  Many common diseases are highly polygenic, modulated by a large number
genetic factors with small effects on susceptibility to disease. These small
effects are difficult to map reliably in genetic association studies. To
address this problem, researchers have developed methods that aggregate
information over sets of related genes, such as biological pathways, to
identify gene sets that are enriched for genetic variants associated with
disease. However, these methods fail to answer a key question: which genes and
genetic variants are associated with disease risk? We develop a method based on
sparse multiple regression that simultaneously identifies enriched pathways,
and prioritizes the variants within these pathways, to locate additional
variants associated with disease susceptibility. A central feature of our
approach is an estimate of the strength of enrichment, which yields a coherent
way to prioritize variants in enriched pathways. We illustrate the benefits of
our approach in a genome-wide association study of Crohn's disease with
~440,000 genetic variants genotyped for ~4700 study subjects. We obtain strong
support for enrichment of IL-12, IL-23 and other cytokine signaling pathways.
Furthermore, prioritizing variants in these enriched pathways yields support
for additional disease-association variants, all of which have been
independently reported in other case-control studies for Crohn's disease.
"
"  For a power system operating in the vicinity of the power transfer limit of
its transmission system, effect of stochastic fluctuations of power loads can
become critical as a sufficiently strong such fluctuation may activate voltage
instability and lead to a large scale collapse of the system. Considering the
effect of these stochastic fluctuations near a codimension 1 saddle-node
bifurcation, we explicitly calculate the autocorrelation function of the state
vector and show how its behavior explains the phenomenon of critical
slowing-down often observed for power systems on the threshold of blackout. We
also estimate the collapse probability/mean clearing time for the power system
and construct a new indicator function signaling the proximity to a large scale
collapse. The new indicator function is easy to estimate in real time using PMU
data feeds as well as SCADA information about fluctuations of power load on the
nodes of the power grid. We discuss control strategies leading to the
minimization of the collapse probability.
"
"  We propose a scale-free network model with a tunable power-law exponent. The
Poisson growth model, as we call it, is an offshoot of the celebrated model of
Barab\'{a}si and Albert where a network is generated iteratively from a small
seed network; at each step a node is added together with a number of incident
edges preferentially attached to nodes already in the network. A key feature of
our model is that the number of edges added at each step is a random variable
with Poisson distribution, and, unlike the Barab\'{a}si-Albert model where this
quantity is fixed, it can generate any network. Our model is motivated by an
application in Bayesian inference implemented as Markov chain Monte Carlo to
estimate a network; for this purpose, we also give a formula for the
probability of a network under our model.
"
"  We address the problem of static OD matrix estimation from a formal
statistical viewpoint. We adopt a novel Bayesian framework to develop a class
of models that explicitly cast trip configurations in the study region as
random variables. As a consequence, classical solutions from growth factor,
gravity, and maximum entropy models are identified to specific estimators under
the proposed models. We show that each of these solutions usually account for
only a small fraction of the posterior probability mass in the ensemble and we
then contend that the uncertainty in the inference should be propagated to
later analyses or next-stage models. We also propose alternative, more robust
estimators and devise Markov chain Monte Carlo sampling schemes to obtain them
and perform other types of inference. We present several examples showcasing
the proposed models and approach, and highlight how other sources of data can
be incorporated in the model and inference in a principled, non-heuristic way.
"
"  In a Gaussian graphical model, the conditional independence between two
variables are characterized by the corresponding zero entries in the inverse
covariance matrix. Maximum likelihood method using the smoothly clipped
absolute deviation (SCAD) penalty (Fan and Li, 2001) and the adaptive LASSO
penalty (Zou, 2006) have been proposed in literature. In this article, we
establish the result that using Bayesian information criterion (BIC) to select
the tuning parameter in penalized likelihood estimation with both types of
penalties can lead to consistent graphical model selection. We compare the
empirical performance of BIC with cross validation method and demonstrate the
advantageous performance of BIC criterion for tuning parameter selection
through simulation studies.
"
"  This article addresses the modeling of reverberant recording environments in
the context of under-determined convolutive blind source separation. We model
the contribution of each source to all mixture channels in the time-frequency
domain as a zero-mean Gaussian random variable whose covariance encodes the
spatial characteristics of the source. We then consider four specific
covariance models, including a full-rank unconstrained model. We derive a
family of iterative expectationmaximization (EM) algorithms to estimate the
parameters of each model and propose suitable procedures to initialize the
parameters and to align the order of the estimated sources across all frequency
bins based on their estimated directions of arrival (DOA). Experimental results
over reverberant synthetic mixtures and live recordings of speech data show the
effectiveness of the proposed approach.
"
"  Assume that we observe a large number of curves, all of them with identical,
although unknown, shape, but with a different random shift. The objective is to
estimate the individual time shifts and their distribution. Such an objective
appears in several biological applications like neuroscience or ECG signal
processing, in which the estimation of the distribution of the elapsed time
between repetitive pulses with a possibly low signal-noise ratio, and without a
knowledge of the pulse shape is of interest. We suggest an M-estimator leading
to a three-stage algorithm: we split our data set in blocks, on which the
estimation of the shifts is done by minimizing a cost criterion based on a
functional of the periodogram; the estimated shifts are then plugged into a
standard density estimator. We show that under mild regularity assumptions the
density estimate converges weakly to the true shift distribution. The theory is
applied both to simulations and to alignment of real ECG signals. The estimator
of the shift distribution performs well, even in the case of low
signal-to-noise ratio, and is shown to outperform the standard methods for
curve alignment.
"
"  In this paper, we consider the sparse eigenvalue problem wherein the goal is
to obtain a sparse solution to the generalized eigenvalue problem. We achieve
this by constraining the cardinality of the solution to the generalized
eigenvalue problem and obtain sparse principal component analysis (PCA), sparse
canonical correlation analysis (CCA) and sparse Fisher discriminant analysis
(FDA) as special cases. Unlike the $\ell_1$-norm approximation to the
cardinality constraint, which previous methods have used in the context of
sparse PCA, we propose a tighter approximation that is related to the negative
log-likelihood of a Student's t-distribution. The problem is then framed as a
d.c. (difference of convex functions) program and is solved as a sequence of
convex programs by invoking the majorization-minimization method. The resulting
algorithm is proved to exhibit \emph{global convergence} behavior, i.e., for
any random initialization, the sequence (subsequence) of iterates generated by
the algorithm converges to a stationary point of the d.c. program. The
performance of the algorithm is empirically demonstrated on both sparse PCA
(finding few relevant genes that explain as much variance as possible in a
high-dimensional gene dataset) and sparse CCA (cross-language document
retrieval and vocabulary selection for music retrieval) applications.
"
"  In semi-supervised learning on graphs, response variables observed at one
node are used to estimate missing values at other nodes. The methods exploit
correlations between nearby nodes in the graph. In this paper we prove that
many such proposals are equivalent to kriging predictors based on a fixed
covariance matrix driven by the link structure of the graph. We then propose a
data-driven estimator of the correlation structure that exploits patterns among
the observed response values. By incorporating even a small fraction of
observed covariation into the predictions, we are able to obtain much improved
prediction on two graph data sets.
"
"  This paper considers the problem of clustering a collection of unlabeled data
points assumed to lie near a union of lower-dimensional planes. As is common in
computer vision or unsupervised learning applications, we do not know in
advance how many subspaces there are nor do we have any information about their
dimensions. We develop a novel geometric analysis of an algorithm named sparse
subspace clustering (SSC) [In IEEE Conference on Computer Vision and Pattern
Recognition, 2009. CVPR 2009 (2009) 2790-2797. IEEE], which significantly
broadens the range of problems where it is provably effective. For instance, we
show that SSC can recover multiple subspaces, each of dimension comparable to
the ambient dimension. We also prove that SSC can correctly cluster data points
even when the subspaces of interest intersect. Further, we develop an extension
of SSC that succeeds when the data set is corrupted with possibly
overwhelmingly many outliers. Underlying our analysis are clear geometric
insights, which may bear on other sparse recovery problems. A numerical study
complements our theoretical analysis and demonstrates the effectiveness of
these methods.
"
"  Tiling arrays make possible a large scale exploration of the genome thanks to
probes which cover the whole genome with very high density until 2 000 000
probes. Biological questions usually addressed are either the expression
difference between two conditions or the detection of transcribed regions. In
this work we propose to consider simultaneously both questions as an
unsupervised classification problem by modeling the joint distribution of the
two conditions. In contrast to previous methods, we account for all available
information on the probes as well as biological knowledge like annotation and
spatial dependence between probes. Since probes are not biologically relevant
units we propose a classification rule for non-connected regions covered by
several probes. Applications to transcriptomic and ChIP-chip data of
Arabidopsis thaliana obtained with a NimbleGen tiling array highlight the
importance of a precise modeling and the region classification.
"
"  Clustering analysis by nonnegative low-rank approximations has achieved
remarkable progress in the past decade. However, most approximation approaches
in this direction are still restricted to matrix factorization. We propose a
new low-rank learning method to improve the clustering performance, which is
beyond matrix factorization. The approximation is based on a two-step bipartite
random walk through virtual cluster nodes, where the approximation is formed by
only cluster assigning probabilities. Minimizing the approximation error
measured by Kullback-Leibler divergence is equivalent to maximizing the
likelihood of a discriminative model, which endows our method with a solid
probabilistic interpretation. The optimization is implemented by a relaxed
Majorization-Minimization algorithm that is advantageous in finding good local
minima. Furthermore, we point out that the regularized algorithm with Dirichlet
prior only serves as initialization. Experimental results show that the new
method has strong performance in clustering purity for various datasets,
especially for large-scale manifold data.
"
"  We present the mixture-of-parents maximum entropy Markov model (MoP-MEMM), a
class of directed graphical models extending MEMMs. The MoP-MEMM allows
tractable incorporation of long-range dependencies between nodes by restricting
the conditional distribution of each node to be a mixture of distributions
given the parents. We show how to efficiently compute the exact marginal
posterior node distributions, regardless of the range of the dependencies. This
enables us to model non-sequential correlations present within text documents,
as well as between interconnected documents, such as hyperlinked web pages. We
apply the MoP-MEMM to a named entity recognition task and a web page
classification task. In each, our model shows significant improvement over the
basic MEMM, and is competitive with other long-range sequence models that use
approximate inference.
"
"  The GRavitational lEnsing Accuracy Testing 2008 (GREAT08) Challenge focuses
on a problem that is of crucial importance for future observations in
cosmology. The shapes of distant galaxies can be used to determine the
properties of dark energy and the nature of gravity, because light from those
galaxies is bent by gravity from the intervening dark matter. The observed
galaxy images appear distorted, although only slightly, and their shapes must
be precisely disentangled from the effects of pixelisation, convolution and
noise. The worldwide gravitational lensing community has made significant
progress in techniques to measure these distortions via the Shear TEsting
Program (STEP). Via STEP, we have run challenges within our own community, and
come to recognise that this particular image analysis problem is ideally
matched to experts in statistical inference, inverse problems and computational
learning. Thus, in order to continue the progress seen in recent years, we are
seeking an infusion of new ideas from these communities. This document details
the GREAT08 Challenge for potential participants. Please visit
http://www.great08challenge.info for the latest information.
"
"  In biological experiments researchers often have information in the form of a
graph that supplements observed numerical data. Incorporating the knowledge
contained in these graphs into an analysis of the numerical data is an
important and nontrivial task. We look at the example of metagenomic
data---data from a genomic survey of the abundance of different species of
bacteria in a sample. Here, the graph of interest is a phylogenetic tree
depicting the interspecies relationships among the bacteria species. We
illustrate that analysis of the data in a nonstandard inner-product space
effectively uses this additional graphical information and produces more
meaningful results.
"
"  Phylogenetic tree reconstruction is traditionally based on multiple sequence
alignments (MSAs) and heavily depends on the validity of this information
bottleneck. With increasing sequence divergence, the quality of MSAs decays
quickly. Alignment-free methods, on the other hand, are based on abstract
string comparisons and avoid potential alignment problems. However, in general
they are not biologically motivated and ignore our knowledge about the
evolution of sequences. Thus, it is still a major open question how to define
an evolutionary distance metric between divergent sequences that makes use of
indel information and known substitution models without the need for a multiple
alignment. Here we propose a new evolutionary distance metric to close this
gap. It uses finite-state transducers to create a biologically motivated
similarity score which models substitutions and indels, and does not depend on
a multiple sequence alignment. The sequence similarity score is defined in
analogy to pairwise alignments and additionally has the positive semi-definite
property. We describe its derivation and show in simulation studies and
real-world examples that it is more accurate in reconstructing phylogenies than
competing methods. The result is a new and accurate way of determining
evolutionary distances in and beyond the twilight zone of sequence alignments
that is suitable for large datasets.
"
"  We show a principled way of deriving online learning algorithms from a
minimax analysis. Various upper bounds on the minimax value, previously thought
to be non-constructive, are shown to yield algorithms. This allows us to
seamlessly recover known methods and to derive new ones. Our framework also
captures such ""unorthodox"" methods as Follow the Perturbed Leader and the R^2
forecaster. We emphasize that understanding the inherent complexity of the
learning problem leads to the development of algorithms.
  We define local sequential Rademacher complexities and associated algorithms
that allow us to obtain faster rates in online learning, similarly to
statistical learning theory. Based on these localized complexities we build a
general adaptive method that can take advantage of the suboptimality of the
observed sequence.
  We present a number of new algorithms, including a family of randomized
methods that use the idea of a ""random playout"". Several new versions of the
Follow-the-Perturbed-Leader algorithms are presented, as well as methods based
on the Littlestone's dimension, efficient methods for matrix completion with
trace norm, and algorithms for the problems of transductive learning and
prediction with static experts.
"
"  In recent years, spatial and spatio-temporal modeling have become an
important area of research in many fields (epidemiology, environmental studies,
disease mapping). In this work we propose different spatial models to study
hospital recruitment, including some potentially explicative variables.
Interest is on the distribution per geographical unit of the ratio between the
number of patients living in this geographical unit and the population in the
same unit. Models considered are within the framework of Bayesian Latent
Gaussian models. Our response variable is assumed to follow a binomial
distribution, with logit link, whose parameters are the population in the
geographical unit and the corresponding relative risk. The structured additive
predictor accounts for effects of various covariates in an additive way,
including smoothing functions of the covariates (for example spatial effect),
linear effect of covariates. To approximate posterior marginals, which not
available in closed form, we use integrated nested Laplace approximations
(INLA), recently proposed for approximate Bayesian inference in latent Gaussian
models. INLA has the advantage of giving very accurate approximations and being
faster than McMC methods when the number of parameters does not exceed 6 (as it
is in our case). Model comparisons are assessed using DIC criterion.
"
"  It has been estimated that about 30% of the genes in the human genome are
regulated by microRNAs (miRNAs). These are short RNA sequences that can
down-regulate the levels of mRNAs or proteins in animals and plants. Genes
regulated by miRNAs are called targets. Typically, methods for target
prediction are based solely on sequence data and on the structure information.
In this paper we propose a Bayesian graphical modeling approach that infers the
miRNA regulatory network by integrating expression levels of miRNAs with their
potential mRNA targets and, via the prior probability model, with their
sequence/structure information. We use a directed graphical model with a
particular structure adapted to our data based on biological considerations. We
then achieve network inference using stochastic search methods for variable
selection that allow us to explore the huge model space via MCMC. A
time-dependent coefficients model is also implemented. We consider experimental
data from a study on a very well-known developmental toxicant causing neural
tube defects, hyperthermia. Some of the pairs of target gene and miRNA we
identify seem very plausible and warrant future investigation. Our proposed
method is general and can be easily applied to other types of network inference
by integrating multiple data sources.
"
"  Sequential Monte Carlo techniques are useful for state estimation in
non-linear, non-Gaussian dynamic models. These methods allow us to approximate
the joint posterior distribution using sequential importance sampling. In this
framework, the dimension of the target distribution grows with each time step,
thus it is necessary to introduce some resampling steps to ensure that the
estimates provided by the algorithm have a reasonable variance. In many
applications, we are only interested in the marginal filtering distribution
which is defined on a space of fixed dimension. We present a Sequential Monte
Carlo algorithm called the Marginal Particle Filter which operates directly on
the marginal distribution, hence avoiding having to perform importance sampling
on a space of growing dimension. Using this idea, we also derive an improved
version of the auxiliary particle filter. We show theoretic and empirical
results which demonstrate a reduction in variance over conventional particle
filtering, and present techniques for reducing the cost of the marginal
particle filter with N particles from O(N2) to O(N logN).
"
"  In many real-world problems, we are dealing with collections of
high-dimensional data, such as images, videos, text and web documents, DNA
microarray data, and more. Often, high-dimensional data lie close to
low-dimensional structures corresponding to several classes or categories the
data belongs to. In this paper, we propose and study an algorithm, called
Sparse Subspace Clustering (SSC), to cluster data points that lie in a union of
low-dimensional subspaces. The key idea is that, among infinitely many possible
representations of a data point in terms of other points, a sparse
representation corresponds to selecting a few points from the same subspace.
This motivates solving a sparse optimization program whose solution is used in
a spectral clustering framework to infer the clustering of data into subspaces.
Since solving the sparse optimization program is in general NP-hard, we
consider a convex relaxation and show that, under appropriate conditions on the
arrangement of subspaces and the distribution of data, the proposed
minimization program succeeds in recovering the desired sparse representations.
The proposed algorithm can be solved efficiently and can handle data points
near the intersections of subspaces. Another key advantage of the proposed
algorithm with respect to the state of the art is that it can deal with data
nuisances, such as noise, sparse outlying entries, and missing entries,
directly by incorporating the model of the data into the sparse optimization
program. We demonstrate the effectiveness of the proposed algorithm through
experiments on synthetic data as well as the two real-world problems of motion
segmentation and face clustering.
"
"  Many algorithms have been proposed for fitting network models with
communities, but most of them do not scale well to large networks, and often
fail on sparse networks. Here we propose a new fast pseudo-likelihood method
for fitting the stochastic block model for networks, as well as a variant that
allows for an arbitrary degree distribution by conditioning on degrees. We show
that the algorithms perform well under a range of settings, including on very
sparse networks, and illustrate on the example of a network of political blogs.
We also propose spectral clustering with perturbations, a method of independent
interest, which works well on sparse networks where regular spectral clustering
fails, and use it to provide an initial value for pseudo-likelihood. We prove
that pseudo-likelihood provides consistent estimates of the communities under a
mild condition on the starting value, for the case of a block model with two
communities.
"
"  We derive an upper bound on the local Rademacher complexity of $\ell_p$-norm
multiple kernel learning, which yields a tighter excess risk bound than global
approaches. Previous local approaches aimed at analyzed the case $p=1$ only
while our analysis covers all cases $1\leq p\leq\infty$, assuming the different
feature mappings corresponding to the different kernels to be uncorrelated. We
also show a lower bound that shows that the bound is tight, and derive
consequences regarding excess loss, namely fast convergence rates of the order
$O(n^{-\frac{\alpha}{1+\alpha}})$, where $\alpha$ is the minimum eigenvalue
decay rate of the individual kernels.
"
"  In the game of Scrabble, letter tiles are drawn uniformly at random from a
bag. The variability of possible draws as the game progresses is a source of
variation that makes it more likely for an inferior player to win a
head-to-head match against a superior player, and more difficult to determine
the true ability of a player in a tournament or contest. I propose a new format
for drawing tiles in a two-player game that allows for the same tile pattern
(though not the same board) to be replicated over multiple matches, so that a
player's result can be better compared against others, yet is indistinguishable
from the bag-based draw within a game. A large number of simulations conducted
with Scrabble software shows that the variance from the tile order in this
scheme accounts for as much variance as the different patterns of letters on
the board as the game progresses. I use these simulations as well as the
experimental design to show how much various tiles are able to affect player
scores depending on their placement in the tile seeding.
"
"  This paper addresses the problem of inferring a regular expression from a
given set of strings that resembles, as closely as possible, the regular
expression that a human expert would have written to identify the language.
This is motivated by our goal of automating the task of postmasters of an email
service who use regular expressions to describe and blacklist email spam
campaigns. Training data contains batches of messages and corresponding regular
expressions that an expert postmaster feels confident to blacklist. We model
this task as a learning problem with structured output spaces and an
appropriate loss function, derive a decoder and the resulting optimization
problem, and a report on a case study conducted with an email service.
"
"  This article presents differential equations and solution methods for the
functions of the form $Q(x) = F^{-1}(G(x))$, where $F$ and $G$ are cumulative
distribution functions. Such functions allow the direct recycling of Monte
Carlo samples from one distribution into samples from another. The method may
be developed analytically for certain special cases, and illuminate the idea
that it is a more precise form of the traditional Cornish-Fisher expansion. In
this manner the model risk of distributional risk may be assessed free of the
Monte Carlo noise associated with resampling. Examples are given of equations
for converting normal samples to Student t, and converting exponential to
hyperbolic, variance gamma and normal. In the case of the normal distribution,
the change of variables employed allows the sampling to take place to good
accuracy based on a single rational approximation over a very wide range of the
sample space. The avoidance of any branching statement is of use in optimal GPU
computations as it avoids the effect of {\it warp divergence}, and we give
examples of branch-free normal quantiles that offer performance improvements in
a GPU environment, while retaining the best precision characteristics of
well-known methods. We also offer models based on a low-probability of warp
divergence. Comparisons of new and old forms are made on the Nvidia Quadro
4000, GTX 285 and 480, and Tesla C2050 GPUs. We argue that in single-precision
mode, the change-of-variables approach offers performance competitive with the
fastest existing scheme while substantially improving precision, and that in
double-precision mode, this approach offers the most GPU-optimal Gaussian
quantile yet, and without compromise on precision for Monte Carlo applications,
working twice as fast as the CUDA 4 library function with increased precision.
"
"  In this work, we redefined two important statistics, the CLRT test (Bai
et.al., Ann. Stat. 37 (2009) 3822-3840) and the LW test (Ledoit and Wolf, Ann.
Stat. 30 (2002) 1081-1102) on identity tests for high dimensional data using
random matrix theories. Compared with existing CLRT and LW tests, the new tests
can accommodate data which has unknown means and non-Gaussian distributions.
Simulations demonstrate that the new tests have good properties in terms of
size and power. What is more, even for Gaussian data, our new tests perform
favorably in comparison to existing tests. Finally, we find the CLRT is more
sensitive to eigenvalues less than 1 while the LW test has more advantages in
relation to detecting eigenvalues larger than 1.
"
"  Discussion of ``Statistical analysis of an archeological find'' by Andrey
Feuerverger [arXiv:0804.0079]
"
"  The extremal t process was proposed in the literature for modeling spatial
extremes within a copula framework based on the extreme value limit of
elliptical t distributions (Davison, Padoan and Ribatet (2012)). A major
drawback of this max-stable model was the lack of a spectral representation
such that for instance direct simulation was infeasible. The main contribution
of this note is to propose such a spectral construction for the extremal t
process. Interestingly, the extremal Gaussian process introduced by Schlather
(2002) appears as a special case. We further highlight the role of the extremal
t process as the maximum attractor for processes with finite-dimensional
elliptical distributions. All results naturally also hold within the
multivariate domain.
"
"  In this work we show that, using the eigen-decomposition of the adjacency
matrix, we can consistently estimate feature maps for latent position graphs
with positive definite link function $\kappa$, provided that the latent
positions are i.i.d. from some distribution F. We then consider the
exploitation task of vertex classification where the link function $\kappa$
belongs to the class of universal kernels and class labels are observed for a
number of vertices tending to infinity and that the remaining vertices are to
be classified. We show that minimization of the empirical $\varphi$-risk for
some convex surrogate $\varphi$ of 0-1 loss over a class of linear classifiers
with increasing complexities yields a universally consistent classifier, that
is, a classification rule with error converging to Bayes optimal for any
distribution F.
"
"  We consider the problems of estimation and selection of parameters endowed
with a known group structure, when the groups are assumed to be sign-coherent,
that is, gathering either nonnegative, nonpositive or null parameters. To
tackle this problem, we propose the cooperative-Lasso penalty. We derive the
optimality conditions defining the cooperative-Lasso estimate for generalized
linear models, and propose an efficient active set algorithm suited to
high-dimensional problems. We study the asymptotic consistency of the estimator
in the linear regression setup and derive its irrepresentable conditions, which
are milder than the ones of the group-Lasso regarding the matching of groups
with the sparsity pattern of the true parameters. We also address the problem
of model selection in linear regression by deriving an approximation of the
degrees of freedom of the cooperative-Lasso estimator. Simulations comparing
the proposed estimator to the group and sparse group-Lasso comply with our
theoretical results, showing consistent improvements in support recovery for
sign-coherent groups. We finally propose two examples illustrating the wide
applicability of the cooperative-Lasso: first to the processing of ordinal
variables, where the penalty acts as a monotonicity prior; second to the
processing of genomic data, where the set of differentially expressed probes is
enriched by incorporating all the probes of the microarray that are related to
the corresponding genes.
"
"  We use a sample of 53 galaxy clusters at 0.03 < z < 0.1 with available masses
derived from the caustic technique and with velocity dispersions computed using
208 galaxies on average per cluster, in order to investigate the scaling
between richness, mass and velocity dispersion. A tight scaling between
richness and mass is found, with an intrinsic scatter of only 0.19 dex in mass
and with a slope one, i.e. clusters which have twice as many galaxies are twice
as massive. When richness is measured without any knowledge of the cluster mass
or linked parameters (such as r200), it can predict mass with an uncertainty of
0.29+/-0.01 dex. As a mass proxy, richness competes favourably with both direct
measurements of mass given by the caustic method, which has typically 0.14 dex
errors (vs 0.29) and X-ray luminosity, which offers a similar 0.30 dex
uncertainty. The similar performances of X-ray luminosity and richness in
predicting cluster masses has been confirmed using cluster masses derived from
velocity dispersion fixed by numerical simulations. These results suggest that
cluster masses can be reliably estimated from simple galaxy counts, at least at
the redshift and masses explored in this work. This has important applications
in the estimation of cosmological parameters from optical cluster surveys,
because in current surveys clusters detected in the optical range outnumber, by
at least one order of magnitude, those detected in X-ray. Our analysis is
robust from astrophysical and statistical perspectives. The data and code used
for the stochastic computation is distributed with the paper. [Abridged]
"
"  The HyperCarte research group wishes to offer a new cartographic tool for
spatial analysis of social data, using the potential smoothing method. The
purpose of this method is to view the spreading of phenomena's in a continuous
way, at a macroscopic scale, basing on data sampled on administrative areas. We
aim to offer an interactive tool, accessible via the Web, but guarantying the
confidentiality of data. The major difficulty is induced by the high complexity
of the calculus, working on a great amount of data. We present our solution to
such a technical challenge, and our perspectives of enhancements.
"
"  One approach to monitoring a dynamic system relies on decomposition of the
system into weakly interacting subsystems. An earlier paper introduced a notion
of weak interaction called separability, and showed that it leads to exact
propagation of marginals for prediction. This paper addresses two questions
left open by the earlier paper: can we define a notion of approximate
separability that occurs naturally in practice, and do separability and
approximate separability lead to accurate monitoring? The answer to both
questions is afirmative. The paper also analyzes the structure of approximately
separable decompositions, and provides some explanation as to why these models
perform well.
"
"  Fast approximate nearest neighbor (NN) search in large databases is becoming
popular. Several powerful learning-based formulations have been proposed
recently. However, not much attention has been paid to a more fundamental
question: how difficult is (approximate) nearest neighbor search in a given
data set? And which data properties affect the difficulty of nearest neighbor
search and how? This paper introduces the first concrete measure called
Relative Contrast that can be used to evaluate the influence of several crucial
data characteristics such as dimensionality, sparsity, and database size
simultaneously in arbitrary normed metric spaces. Moreover, we present a
theoretical analysis to prove how the difficulty measure (relative contrast)
determines/affects the complexity of Local Sensitive Hashing, a popular
approximate NN search method. Relative contrast also provides an explanation
for a family of heuristic hashing algorithms with good practical performance
based on PCA. Finally, we show that most of the previous works in measuring NN
search meaningfulness/difficulty can be derived as special asymptotic cases for
dense vectors of the proposed measure.
"
"  We present a computational method for measuring financial risk by estimating
the Value at Risk and Expected Shortfall from financial series. We have made
two assumptions: First, that the predictive distributions of the values of an
asset are conditioned by information on the way in which the variable evolves
from similar conditions, and secondly, that the underlying random processes can
be described using piecewise Gaussian processes. The performance of the method
was evaluated by using it to estimate VaR and ES for a daily data series taken
from the S&P500 index and applying a backtesting procedure recommended by the
Basel Committee on Banking Supervision. The results indicated a satisfactory
performance.
"
"  Super-resolution (SR) techniques make use of subpixel shifts between frames
in an image sequence to yield higher-resolution images. We propose an original
observation model devoted to the case of non isometric inter-frame motion as
required, for instance, in the context of airborne imaging sensors. First, we
describe how the main observation models used in the SR literature deal with
motion, and we explain why they are not suited for non isometric motion. Then,
we propose an extension of the observation model by Elad and Feuer adapted to
affine motion. This model is based on a decomposition of affine transforms into
successive shear transforms, each one efficiently implemented by row-by-row or
column-by-column 1-D affine transforms.
  We demonstrate on synthetic and real sequences that our observation model
incorporated in a SR reconstruction technique leads to better results in the
case of variable scale motions and it provides equivalent results in the case
of isometric motions.
"
"  With growing data volumes from synoptic surveys, astronomers must become more
abstracted from the discovery and introspection processes. Given the scarcity
of follow-up resources, there is a particularly sharp onus on the frameworks
that replace these human roles to provide accurate and well-calibrated
probabilistic classification catalogs. Such catalogs inform the subsequent
follow-up, allowing consumers to optimize the selection of specific sources for
further study and permitting rigorous treatment of purities and efficiencies
for population studies. Here, we describe a process to produce a probabilistic
classification catalog of variability with machine learning from a multi-epoch
photometric survey. In addition to producing accurate classifications, we show
how to estimate calibrated class probabilities, and motivate the importance of
probability calibration. We also introduce a methodology for feature-based
anomaly detection, which allows discovery of objects in the survey that do not
fit within the predefined class taxonomy. Finally, we apply these methods to
sources observed by the All Sky Automated Survey (ASAS), and unveil the
Machine-learned ASAS Classification Catalog (MACC), which is a 28-class
probabilistic classification catalog of 50,124 ASAS sources. We estimate that
MACC achieves a sub-20% classification error rate, and demonstrate that the
class posterior probabilities are reasonably calibrated. MACC classifications
compare favorably to the classifications of several previous domain-specific
ASAS papers and to the ASAS Catalog of Variable Stars, which had classified
only 24% of those sources into one of 12 science classes. The MACC is publicly
available at http://www.bigmacc.info.
"
"  Recent advances in genomics have underscored the surprising ubiquity of DNA
copy number variation (CNV). Fortunately, modern genotyping platforms also
detect CNVs with fairly high reliability. Hidden Markov models and algorithms
have played a dominant role in the interpretation of CNV data. Here we explore
CNV reconstruction via estimation with a fused-lasso penalty as suggested by
Tibshirani and Wang [Biostatistics 9 (2008) 18--29]. We mount a fresh attack on
this difficult optimization problem by the following: (a) changing the penalty
terms slightly by substituting a smooth approximation to the absolute value
function, (b) designing and implementing a new MM (majorization--minimization)
algorithm, and (c) applying a fast version of Newton's method to jointly update
all model parameters. Together these changes enable us to minimize the
fused-lasso criterion in a highly effective way. We also reframe the
reconstruction problem in terms of imputation via discrete optimization. This
approach is easier and more accurate than parameter estimation because it
relies on the fact that only a handful of possible copy number states exist at
each SNP. The dynamic programming framework has the added bonus of exploiting
information that the current fused-lasso approach ignores. The accuracy of our
imputations is comparable to that of hidden Markov models at a substantially
lower computational cost.
"
"  We investigate unsupervised pre-training of deep architectures as feature
generators for ""shallow"" classifiers. Stacked Denoising Autoencoders (SdA),
when used as feature pre-processing tools for SVM classification, can lead to
significant improvements in accuracy - however, at the price of a substantial
increase in computational cost. In this paper we create a simple algorithm
which mimics the layer by layer training of SdAs. However, in contrast to SdAs,
our algorithm requires no training through gradient descent as the parameters
can be computed in closed-form. It can be implemented in less than 20 lines of
MATLABTMand reduces the computation time from several hours to mere seconds. We
show that our feature transformation reliably improves the results of SVM
classification significantly on all our data sets - often outperforming SdAs
and even deep neural networks in three out of four deep learning benchmarks.
"
"  Over the past two decades, several consistent procedures have been designed
to infer causal conclusions from observational data. We prove that if the true
causal network might be an arbitrary, linear Gaussian network or a discrete
Bayes network, then every unambiguous causal conclusion produced by a
consistent method from non-experimental data is subject to reversal as the
sample size increases any finite number of times. That result, called the
causal flipping theorem, extends prior results to the effect that causal
discovery cannot be reliable on a given sample size. We argue that since
repeated flipping of causal conclusions is unavoidable in principle for
consistent methods, the best possible discovery methods are consistent methods
that retract their earlier conclusions no more than necessary. A series of
simulations of various methods across a wide range of sample sizes illustrates
concretely both the theorem and the principle of comparing methods in terms of
retractions.
"
"  We consider a linear regression model, with the parameter of interest a
specified linear combination of the regression parameter vector. We suppose
that, as a first step, a data-based model selection (e.g. by preliminary
hypothesis tests or minimizing AIC) is used to select a model. It is common
statistical practice to then construct a confidence interval for the parameter
of interest based on the assumption that the selected model had been given to
us a priori. This assumption is false and it can lead to a confidence interval
with poor coverage properties. We provide an easily-computed finite sample
upper bound (calculated by repeated numerical evaluation of a double integral)
to the minimum coverage probability of this confidence interval. This bound
applies for model selection by any of the following methods: minimum AIC,
minimum BIC, maximum adjusted R-squared, minimum Mallows' Cp and t-tests. The
importance of this upper bound is that it delineates general categories of
design matrices and model selection procedures for which this confidence
interval has poor coverage properties. This upper bound is shown to be a finite
sample analogue of an earlier large sample upper bound due to Kabaila and Leeb.
"
"  We propose a hybrid algorithmic strategy for complex stochastic optimization
problems, which combines the use of scenario trees from multistage stochastic
programming with machine learning techniques for learning a policy in the form
of a statistical model, in the context of constrained vector-valued decisions.
Such a policy allows one to run out-of-sample simulations over a large number
of independent scenarios, and obtain a signal on the quality of the
approximation scheme used to solve the multistage stochastic program. We
propose to apply this fast simulation technique to choose the best tree from a
set of scenario trees. A solution scheme is introduced, where several scenario
trees with random branching structure are solved in parallel, and where the
tree from which the best policy for the true problem could be learned is
ultimately retained. Numerical tests show that excellent trade-offs can be
achieved between run times and solution quality.
"
"  We derive exact computable expressions for the asymptotic distribution of the
change-point mle when a change in the mean occurred at an unknown point of a
sequence of time-ordered independent Gaussian random variables. The derivation,
which assumes that nuisance parameters such as the amount of change and
variance are known, is based on ladder heights of Gaussian random walks hitting
the half-line. We then show that the exact distribution easily extends to the
distribution of the change-point mle when a change occurs in the mean vector of
a multivariate Gaussian process. We perform simulations to examine the accuracy
of the derived distribution when nuisance parameters have to be estimated as
well as robustness of the derived distribution to deviations from Gaussianity.
Through simulations, we also compare it with the well-known conditional
distribution of the mle, which may be interpreted as a Bayesian solution to the
change-point problem. Finally, we apply the derived methodology to monthly
averages of water discharges of the Nacetinsky creek, Germany.
"
"  Supervised topic models utilize document's side information for discovering
predictive low dimensional representations of documents. Existing models apply
the likelihood-based estimation. In this paper, we present a general framework
of max-margin supervised topic models for both continuous and categorical
response variables. Our approach, the maximum entropy discrimination latent
Dirichlet allocation (MedLDA), utilizes the max-margin principle to train
supervised topic models and estimate predictive topic representations that are
arguably more suitable for prediction tasks. The general principle of MedLDA
can be applied to perform joint max-margin learning and maximum likelihood
estimation for arbitrary topic models, directed or undirected, and supervised
or unsupervised, when the supervised side information is available. We develop
efficient variational methods for posterior inference and parameter estimation,
and demonstrate qualitatively and quantitatively the advantages of MedLDA over
likelihood-based topic models on movie review and 20 Newsgroups data sets.
"
"  Clustering is the problem of separating a set of objects into groups (called
clusters) so that objects within the same cluster are more similar to each
other than to those in different clusters. Spectral clustering is a now
well-known method for clustering which utilizes the spectrum of the data
similarity matrix to perform this separation. Since the method relies on
solving an eigenvector problem, it is computationally expensive for large
datasets. To overcome this constraint, approximation methods have been
developed which aim to reduce running time while maintaining accurate
classification. In this article, we summarize and experimentally evaluate
several approximation methods for spectral clustering. From an applications
standpoint, we employ spectral clustering to solve the so-called attrition
problem, where one aims to identify from a set of employees those who are
likely to voluntarily leave the company from those who are not. Our study sheds
light on the empirical performance of existing approximate spectral clustering
methods and shows the applicability of these methods in an important business
optimization related problem.
"
"  A technique for on-line estimation of spot volatility for high-frequency data
is developed. The algorithm works directly on the transaction data and updates
the volatility estimate immediately after the occurrence of a new transaction.
Furthermore, a nonlinear market microstructure noise model is proposed that
reproduces several stylized facts of high-frequency data. A computationally
efficient particle filter is used that allows for the approximation of the
unknown efficient prices and, in combination with a recursive EM algorithm, for
the estimation of the volatility curve. We neither assume that the transaction
times are equidistant nor do we use interpolated prices. We also make a
distinction between volatility per time unit and volatility per transaction and
provide estimators for both. More precisely we use a model with random time
change where spot volatility is decomposed into spot volatility per transaction
times the trading intensity - thus highlighting the influence of trading
intensity on volatility.
"
"  High density clusters can be characterized by the connected components of a
level set $L(\lambda) = \{x:\ p(x)>\lambda\}$ of the underlying probability
density function $p$ generating the data, at some appropriate level
$\lambda\geq 0$. The complete hierarchical clustering can be characterized by a
cluster tree ${\cal T}= \bigcup_{\lambda} L(\lambda)$. In this paper, we study
the behavior of a density level set estimate $\widehat L(\lambda)$ and cluster
tree estimate $\widehat{\cal{T}}$ based on a kernel density estimator with
kernel bandwidth $h$. We define two notions of instability to measure the
variability of $\widehat L(\lambda)$ and $\widehat{\cal{T}}$ as a function of
$h$, and investigate the theoretical properties of these instability measures.
"
"  Discussion of ""A statistical analysis of multiple temperature proxies: Are
reconstructions of surface temperatures over the last 1000 years reliable?"" by
B.B. McShane and A.J. Wyner [arXiv:1104.4002]
"
"  We obtain a tight distribution-specific characterization of the sample
complexity of large-margin classification with L2 regularization: We introduce
the margin-adapted dimension, which is a simple function of the second order
statistics of the data distribution, and show distribution-specific upper and
lower bounds on the sample complexity, both governed by the margin-adapted
dimension of the data distribution. The upper bounds are universal, and the
lower bounds hold for the rich family of sub-Gaussian distributions with
independent features. We conclude that this new quantity tightly characterizes
the true sample complexity of large-margin classification. To prove the lower
bound, we develop several new tools of independent interest. These include new
connections between shattering and hardness of learning, new properties of
shattering with linear classifiers, and a new lower bound on the smallest
eigenvalue of a random Gram matrix generated by sub-Gaussian variables. Our
results can be used to quantitatively compare large margin learning to other
learning rules, and to improve the effectiveness of methods that use sample
complexity bounds, such as active learning.
"
"  In this note, we propose to use sparse methods (e.g. LASSO, Post-LASSO,
sqrt-LASSO, and Post-sqrt-LASSO) to form first-stage predictions and estimate
optimal instruments in linear instrumental variables (IV) models with many
instruments in the canonical Gaussian case. The methods apply even when the
number of instruments is much larger than the sample size. We derive asymptotic
distributions for the resulting IV estimators and provide conditions under
which these sparsity-based IV estimators are asymptotically oracle-efficient.
In simulation experiments, a sparsity-based IV estimator with a data-driven
penalty performs well compared to recently advocated many-instrument-robust
procedures. We illustrate the procedure in an empirical example using the
Angrist and Krueger (1991) schooling data.
"
"  We address the issue of edge detection in Synthetic Aperture Radar imagery.
In particular, we propose nonparametric methods for edge detection, and
numerically compare them to an alternative method that has been recently
proposed in the literature. Our results show that some of the proposed methods
display superior results and are computationally simpler than the existing
method. An application to real (not simulated) data is presented and discussed.
"
"  Motivated by a potential-outcomes perspective, the idea of principal
stratification has been widely recognized for its relevance in settings
susceptible to posttreatment selection bias such as randomized clinical trials
where treatment received can differ from treatment assigned. In one such
setting, we address subtleties involved in inference for causal effects when
using a key covariate to predict membership in latent principal strata. We show
that when treatment received can differ from treatment assigned in both study
arms, incorporating a stratum-predictive covariate can make estimates of the
""complier average causal effect"" (CACE) derive from observations in the two
treatment arms with different covariate distributions. Adopting a Bayesian
perspective and using Markov chain Monte Carlo for computation, we develop
posterior checks that characterize the extent to which incorporating the
pretreatment covariate endangers estimation of the CACE. We apply the method to
analyze a clinical trial comparing two treatments for jaw fractures in which
the study protocol allowed surgeons to overrule both possible randomized
treatment assignments based on their clinical judgment and the data contained a
key covariate (injury severity) predictive of treatment received.
"
"  Gaussian Graphical Models (GGMs) have been used to construct genetic
regulatory networks where regularization techniques are widely used since the
network inference usually falls into a high-dimension-low-sample-size scenario.
Yet, finding the right amount of regularization can be challenging, especially
in an unsupervised setting where traditional methods such as BIC or
cross-validation often do not work well. In this paper, we propose a new method
- Bootstrap Inference for Network COnstruction (BINCO) - to infer networks by
directly controlling the false discovery rates (FDRs) of the selected edges.
This method fits a mixture model for the distribution of edge selection
frequencies to estimate the FDRs, where the selection frequencies are
calculated via model aggregation. This method is applicable to a wide range of
applications beyond network construction. When we applied our proposed method
to building a gene regulatory network with microarray expression breast cancer
data, we were able to identify high-confidence edges and well-connected hub
genes that could potentially play important roles in understanding the
underlying biological processes of breast cancer.
"
"  A method for estimating the true meteor rate \lambda\ from a small number of
observed meteors n is derived. We employ Bayesian inference with a Poissonian
likelihood function. We discuss the choice of a suitable prior and propose the
adoption of Jeffreys prior, P(\lambda)=\lambda^{-0.5}, which yields an
expectation value E(\lambda) = n+0.5 for any n \geq 0. We update the ZHR meteor
activity formula accordingly, and explain how 68%- and 95%-confidence intervals
can be computed.
"
"  Cryo-electron microscopy (cryo-EM) has recently emerged as a powerful tool
for obtaining three-dimensional (3D) structures of biological macromolecules in
native states. A minimum cryo-EM image data set for deriving a meaningful
reconstruction is comprised of thousands of randomly orientated projections of
identical particles photographed with a small number of electrons. The
computation of 3D structure from 2D projections requires clustering, which aims
to enhance the signal to noise ratio in each view by grouping similarly
oriented images. Nevertheless, the prevailing clustering techniques are often
compromised by three characteristics of cryo-EM data: high noise content, high
dimensionality and large number of clusters. Moreover, since clustering
requires registering images of similar orientation into the same pixel
coordinates by 2D alignment, it is desired that the clustering algorithm can
label misaligned images as outliers. Herein, we introduce a clustering
algorithm $\gamma$-SUP to model the data with a $q$-Gaussian mixture and adopt
the minimum $\gamma$-divergence for estimation, and then use a self-updating
procedure to obtain the numerical solution. We apply $\gamma$-SUP to the
cryo-EM images of two benchmark macromolecules, RNA polymerase II and ribosome.
In the former case, simulated images were chosen to decouple clustering from
alignment to demonstrate $\gamma$-SUP is more robust to misalignment outliers
than the existing clustering methods used in the cryo-EM community. In the
latter case, the clustering of real cryo-EM data by our $\gamma$-SUP method
eliminates noise in many views to reveal true structure features of ribosome at
the projection level.
"
"  We introduce a method to learn a mixture of submodular ""shells"" in a
large-margin setting. A submodular shell is an abstract submodular function
that can be instantiated with a ground set and a set of parameters to produce a
submodular function. A mixture of such shells can then also be so instantiated
to produce a more complex submodular function. What our algorithm learns are
the mixture weights over such shells. We provide a risk bound guarantee when
learning in a large-margin structured-prediction setting using a projected
subgradient method when only approximate submodular optimization is possible
(such as with submodular function maximization). We apply this method to the
problem of multi-document summarization and produce the best results reported
so far on the widely used NIST DUC-05 through DUC-07 document summarization
corpora.
"
"  An important issue in the tomographic reconstruction of the solar poles is
the relatively rapid evolution of the polar plumes. We demonstrate that it is
possible to take into account this temporal evolution in the reconstruction.
The difficulty of this problem comes from the fact that we want a 4D
reconstruction (three spatial dimensions plus time) while we only have 3D data
(2D images plus time). To overcome this difficulty, we introduce a model that
describes polar plumes as stationary objects whose intensity varies
homogeneously with time. This assumption can be physically justified if one
accepts the stability of the magnetic structure. This model leads to a bilinear
inverse problem. We describe how to extend linear inversion methods to these
kinds of problems. Studies of simulations show the reliability of our method.
Results for SOHO/EIT data show that we are able to estimate the temporal
evolution of polar plumes in order to improve the reconstruction of the solar
poles from only one point of view. We expect further improvements from
STEREO/EUVI data when the two probes will be separated by about 60 degrees.
"
"  Given two sets of data which lead to a similar statistical conclusion, the
Simpson Paradox describes the tactic of combining these two sets and achieving
the opposite conclusion. Depending upon the given data, this may or may not
succeed. Inverse Simpson is a method of decomposing a given set of comparison
data into two disjoint sets and achieving the opposite conclusion for each one.
This is always possible; however, the statistical significance of the
conclusions does depend upon the details of the given data.
"
"  A meta-model (or a surrogate model) is the modern name for what was
traditionally called a response surface. It is intended to mimic the behaviour
of a computational model M (e.g. a finite element model in mechanics) while
being inexpensive to evaluate, in contrast to the original model which may take
hours or even days of computer processing time. In this paper various types of
meta-models that have been used in the last decade in the context of structural
reliability are reviewed. More specifically classical polynomial response
surfaces, polynomial chaos expansions and kriging are addressed. It is shown
how the need for error estimates and adaptivity in their construction has
brought this type of approaches to a high level of efficiency. A new technique
that solves the problem of the potential biasedness in the estimation of a
probability of failure through the use of meta-models is finally presented.
"
"  We present a probabilistic model of events in continuous time in which each
event triggers a Poisson process of successor events. The ensemble of observed
events is thereby modeled as a superposition of Poisson processes. Efficient
inference is feasible under this model with an EM algorithm. Moreover, the EM
algorithm can be implemented as a distributed algorithm, permitting the model
to be applied to very large datasets. We apply these techniques to the modeling
of Twitter messages and the revision history of Wikipedia.
"
"  We present a Bayesian hierarchical modeling approach to paleoclimate
reconstruction using borehole temperature profiles. The approach relies on
modeling heat conduction in solids via the heat equation with step function,
surface boundary conditions. Our analysis includes model error and assumes that
the boundary conditions are random processes. The formulation also enables
separation of measurement error and model error. We apply the analysis to data
from nine borehole temperature records from the San Rafael region in Utah. We
produce ground surface temperature histories with uncertainty estimates for the
past 400 years. We pay special attention to use of prior parameter models that
illustrate borrowing strength in a combined analysis for all nine boreholes. In
addition, we review selected sensitivity analyses.
"
"  Exact inference in the linear regression model with spike and slab priors is
often intractable. Expectation propagation (EP) can be used for approximate
inference. However, the regular sequential form of EP (R-EP) may fail to
converge in this model when the size of the training set is very small. As an
alternative, we propose a provably convergent EP algorithm (PC-EP). PC-EP is
proved to minimize an energy function which, under some constraints, is bounded
from below and whose stationary points coincide with the solution of R-EP.
Experiments with synthetic data indicate that when R-EP does not converge, the
approximation generated by PC-EP is often better. By contrast, when R-EP
converges, both methods perform similarly.
"
"  We analyze the convergence of gradient-based optimization algorithms that
base their updates on delayed stochastic gradient information. The main
application of our results is to the development of gradient-based distributed
optimization algorithms where a master node performs parameter updates while
worker nodes compute stochastic gradients based on local information in
parallel, which may give rise to delays due to asynchrony. We take motivation
from statistical problems where the size of the data is so large that it cannot
fit on one computer; with the advent of huge datasets in biology, astronomy,
and the internet, such problems are now common. Our main contribution is to
show that for smooth stochastic problems, the delays are asymptotically
negligible and we can achieve order-optimal convergence results. In application
to distributed optimization, we develop procedures that overcome communication
bottlenecks and synchronization requirements. We show $n$-node architectures
whose optimization error in stochastic problems---in spite of asynchronous
delays---scales asymptotically as $\order(1 / \sqrt{nT})$ after $T$ iterations.
This rate is known to be optimal for a distributed system with $n$ nodes even
in the absence of delays. We additionally complement our theoretical results
with numerical experiments on a statistical machine learning task.
"
"  Discussions on outstanding---positively and/or negatively---athletes are
common practice. The rapidly grown amount of collected sports data now allow to
support such discussions with state of the art statistical methodology. Given a
(multivariate) data set with collected data of athletes within a specific
sport, outstanding athletes are values on the data set boundary. In the present
paper we propose archetypal analysis to compute these extreme values. The
so-called archetypes, i.e., archetypal athletes, approximate the observations
as convex combinations. We interpret the archetypal athletes and their
characteristics, and, furthermore, the composition of all athletes based on the
archetypal athletes. The application of archetypal analysis is demonstrated on
basketball statistics and soccer skill ratings.
"
"  This paper introduces a novel, well-founded, betweenness measure, called the
Bag-of-Paths (BoP) betweenness, as well as its extension, the BoP group
betweenness, to tackle semisupervised classification problems on weighted
directed graphs. The objective of semi-supervised classification is to assign a
label to unlabeled nodes using the whole topology of the graph and the labeled
nodes at our disposal. The BoP betweenness relies on a bag-of-paths framework
assigning a Boltzmann distribution on the set of all possible paths through the
network such that long (high-cost) paths have a low probability of being picked
from the bag, while short (low-cost) paths have a high probability of being
picked. Within that context, the BoP betweenness of node j is defined as the
sum of the a posteriori probabilities that node j lies in-between two arbitrary
nodes i, k, when picking a path starting in i and ending in k. Intuitively, a
node typically receives a high betweenness if it has a large probability of
appearing on paths connecting two arbitrary nodes of the network. This quantity
can be computed in closed form by inverting a n x n matrix where n is the
number of nodes. For the group betweenness, the paths are constrained to start
and end in nodes within the same class, therefore defining a group betweenness
for each class. Unlabeled nodes are then classified according to the class
showing the highest group betweenness. Experiments on various real-world data
sets show that BoP group betweenness outperforms all the tested state
of-the-art methods. The benefit of the BoP betweenness is particularly
noticeable when only a few labeled nodes are available.
"
"  We study a new class of codes for lossy compression with the squared-error
distortion criterion, designed using the statistical framework of
high-dimensional linear regression. Codewords are linear combinations of
subsets of columns of a design matrix. Called a Sparse Superposition or Sparse
Regression codebook, this structure is motivated by an analogous construction
proposed recently by Barron and Joseph for communication over an AWGN channel.
For i.i.d Gaussian sources and minimum-distance encoding, we show that such a
code can attain the Shannon rate-distortion function with the optimal error
exponent, for all distortions below a specified value. It is also shown that
sparse regression codes are robust in the following sense: a codebook designed
to compress an i.i.d Gaussian source of variance $\sigma^2$ with
(squared-error) distortion $D$ can compress any ergodic source of variance less
than $\sigma^2$ to within distortion $D$. Thus the sparse regression ensemble
retains many of the good covering properties of the i.i.d random Gaussian
ensemble, while having having a compact representation in terms of a matrix
whose size is a low-order polynomial in the block-length.
"
"  In this paper, we unify the Markov theory of a variety of different types of
graphs used in graphical Markov models by introducing the class of loopless
mixed graphs, and show that all independence models induced by $m$-separation
on such graphs are compositional graphoids. We focus in particular on the
subclass of ribbonless graphs which as special cases include undirected graphs,
bidirected graphs, and directed acyclic graphs, as well as ancestral graphs and
summary graphs. We define maximality of such graphs as well as a pairwise and a
global Markov property. We prove that the global and pairwise Markov properties
of a maximal ribbonless graph are equivalent for any independence model that is
a compositional graphoid.
"
"  This paper presents Sparse Partitioning, a Bayesian method for identifying
predictors that either individually or in combination with others affect a
response variable. The method is designed for regression problems involving
binary or tertiary predictors and allows the number of predictors to exceed the
size of the sample, two properties which make it well suited for association
studies. Sparse Partitioning differs from other regression methods by placing
no restrictions on how the predictors may influence the response. To compensate
for this generality, Sparse Partitioning implements a novel way of exploring
the model space. It searches for high posterior probability partitions of the
predictor set, where each partition defines groups of predictors that jointly
influence the response. The result is a robust method that requires no prior
knowledge of the true predictor--response relationship. Testing on simulated
data suggests Sparse Partitioning will typically match the performance of an
existing method on a data set which obeys the existing method's model
assumptions. When these assumptions are violated, Sparse Partitioning will
generally offer superior performance.
"
"  Imaging in clinical oncology trials provides a wealth of information that
contributes to the drug development process, especially in early phase studies.
This paper focuses on kinetic modeling in DCE-MRI, inspired by mixed-effects
models that are frequently used in the analysis of clinical trials. Instead of
summarizing each scanning session as a single kinetic parameter -- such as
median $\ktrans$ across all voxels in the tumor ROI -- we propose to analyze
all voxel time courses from all scans and across all subjects simultaneously in
a single model. The kinetic parameters from the usual non-linear regression
model are decomposed into unique components associated with factors from the
longitudinal study; e.g., treatment, patient and voxel effects. A Bayesian
hierarchical model provides the framework in order to construct a data model, a
parameter model, as well as prior distributions. The posterior distribution of
the kinetic parameters is estimated using Markov chain Monte Carlo (MCMC)
methods. Hypothesis testing at the study level for an overall treatment effect
is straightforward and the patient- and voxel-level parameters capture random
effects that provide additional information at various levels of resolution to
allow a thorough evaluation of the clinical trial. The proposed method is
validated with a breast cancer study, where the subjects were imaged before and
after two cycles of chemotherapy, demonstrating the clinical potential of this
method to longitudinal oncology studies.
"
"  The L1-regularized maximum likelihood estimation problem has recently become
a topic of great interest within the machine learning, statistics, and
optimization communities as a method for producing sparse inverse covariance
estimators. In this paper, a proximal gradient method (G-ISTA) for performing
L1-regularized covariance matrix estimation is presented. Although numerous
algorithms have been proposed for solving this problem, this simple proximal
gradient method is found to have attractive theoretical and numerical
properties. G-ISTA has a linear rate of convergence, resulting in an O(log e)
iteration complexity to reach a tolerance of e. This paper gives eigenvalue
bounds for the G-ISTA iterates, providing a closed-form linear convergence
rate. The rate is shown to be closely related to the condition number of the
optimal point. Numerical convergence results and timing comparisons for the
proposed method are presented. G-ISTA is shown to perform very well, especially
when the optimal point is well-conditioned.
"
"  I describe ongoing work developing Bayesian methods for flexible modeling of
arrival time series data without binning, aiming to improve detection and
measurement of X-ray and gamma-ray pulsars, and of pulses in gamma-ray bursts.
The methods use parametric and semiparametric Poisson point process models for
the event rate, and by design have close connections to conventional
frequentist methods currently used in time-domain astronomy.
"
"  The Joint United Nations Programme on HIV/AIDS (UNAIDS) has developed the
Estimation and Projection Package (EPP) for making national estimates and
short-term projections of HIV prevalence based on observed prevalence trends at
antenatal clinics. Assessing the uncertainty about its estimates and
projections is important for informed policy decision making, and we propose
the use of Bayesian melding for this purpose. Prevalence data and other
information about the EPP model's input parameters are used to derive a
probabilistic HIV prevalence projection, namely a probability distribution over
a set of future prevalence trajectories. We relate antenatal clinic prevalence
to population prevalence and account for variability between clinics using a
random effects model. Predictive intervals for clinic prevalence are derived
for checking the model. We discuss predictions given by the EPP model and the
results of the Bayesian melding procedure for Uganda, where prevalence peaked
at around 28% in 1990; the 95% prediction interval for 2010 ranges from 2% to
7%.
"
"  While statistics focusses on hypothesis testing and on estimating (properties
of) the true sampling distribution, in machine learning the performance of
learning algorithms on future data is the primary issue. In this paper we
bridge the gap with a general principle (PHI) that identifies hypotheses with
best predictive performance. This includes predictive point and interval
estimation, simple and composite hypothesis testing, (mixture) model selection,
and others as special cases. For concrete instantiations we will recover
well-known methods, variations thereof, and new ones. PHI nicely justifies,
reconciles, and blends (a reparametrization invariant variation of) MAP, ML,
MDL, and moment estimation. One particular feature of PHI is that it can
genuinely deal with nested hypotheses.
"
"  The fundamental equations that model turbulent flow do not provide much
insight into the size and shape of observed turbulent structures. We
investigate the efficient and accurate representation of structures in
two-dimensional turbulence by applying statistical models directly to the
simulated vorticity field. Rather than extract the coherent portion of the
image from the background variation, as in the classical signal-plus-noise
model, we present a model for individual vortices using the non-decimated
discrete wavelet transform. A template image, supplied by the user, provides
the features to be extracted from the vorticity field. By transforming the
vortex template into the wavelet domain, specific characteristics present in
the template, such as size and symmetry, are broken down into components
associated with spatial frequencies. Multivariate multiple linear regression is
used to fit the vortex template to the vorticity field in the wavelet domain.
Since all levels of the template decomposition may be used to model each level
in the field decomposition, the resulting model need not be identical to the
template. Application to a vortex census algorithm that records quantities of
interest (such as size, peak amplitude, circulation, etc.) as the vorticity
field evolves is given. The multiresolution census algorithm extracts coherent
structures of all shapes and sizes in simulated vorticity fields and is able to
reproduce known physical scaling laws when processing a set of voriticity
fields that evolve over time.
"
"  In this paper, we consider the problem of ""hyper-sparse aggregation"". Namely,
given a dictionary $F = \{f_1, ..., f_M \}$ of functions, we look for an
optimal aggregation algorithm that writes $\tilde f = \sum_{j=1}^M \theta_j
f_j$ with as many zero coefficients $\theta_j$ as possible. This problem is of
particular interest when $F$ contains many irrelevant functions that should not
appear in $\tilde{f}$. We provide an exact oracle inequality for $\tilde f$,
where only two coefficients are non-zero, that entails $\tilde f$ to be an
optimal aggregation algorithm. Since selectors are suboptimal aggregation
procedures, this proves that 2 is the minimal number of elements of $F$
required for the construction of an optimal aggregation procedures in every
situations. A simulated example of this algorithm is proposed on a dictionary
obtained using LARS, for the problem of selection of the regularization
parameter of the LASSO. We also give an example of use of aggregation to
achieve minimax adaptation over anisotropic Besov spaces, which was not
previously known in minimax theory (in regression on a random design).
"
"  We have measured the dissimilarities among several printed characters of a
single page in the Gutenberg 42-line bible and we prove statistically the
existence of several different matrices from which the metal types where
constructed. This is in contrast with the prevailing theory, which states that
only one matrix per character was used in the printing process of Gutenberg's
greatest work.
  The main mathematical tool for this purpose is cluster analysis, combined
with a statistical test for outliers. We carry out the research with two
letters, i and a. In the first case, an exact clustering method is employed; in
the second, with more specimens to be classified, we resort to an approximate
agglomerative clustering method.
  The results show that the letters form clusters according to their shape,
with significant shape differences among clusters, and allow to conclude, with
a very small probability of error, that indeed the metal types used to print
them were cast from several different matrices.
  Mathematics Subject Classification: 62H30
"
"  This paper deals with the problems of consistence and strong consistence of
the maximum likelihood estimators of the mean and variance of the drift
fractional Brownian motions observed at discrete time instants. A central limit
theorem for these estimators is also obtained by using the Malliavin calculus.
"
"  We introduce a new nearest-prototype classifier, the prototype vector machine
(PVM). It arises from a combinatorial optimization problem which we cast as a
variant of the set cover problem. We propose two algorithms for approximating
its solution. The PVM selects a relatively small number of representative
points which can then be used for classification. It contains 1-NN as a special
case. The method is compatible with any dissimilarity measure, making it
amenable to situations in which the data are not embedded in an underlying
feature space or in which using a non-Euclidean metric is desirable. Indeed, we
demonstrate on the much studied ZIP code data how the PVM can reap the benefits
of a problem-specific metric. In this example, the PVM outperforms the highly
successful 1-NN with tangent distance, and does so retaining fewer than half of
the data points. This example highlights the strengths of the PVM in yielding a
low-error, highly interpretable model. Additionally, we apply the PVM to a
protein classification problem in which a kernel-based distance is used.
"
"  Markov networks (MNs) are a powerful way to compactly represent a joint
probability distribution, but most MN structure learning methods are very slow,
due to the high cost of evaluating candidates structures. Dependency networks
(DNs) represent a probability distribution as a set of conditional probability
distributions. DNs are very fast to learn, but the conditional distributions
may be inconsistent with each other and few inference algorithms support DNs.
In this paper, we present a closed-form method for converting a DN into an MN,
allowing us to enjoy both the efficiency of DN learning and the convenience of
the MN representation. When the DN is consistent, this conversion is exact. For
inconsistent DNs, we present averaging methods that significantly improve the
approximation. In experiments on 12 standard datasets, our methods are orders
of magnitude faster than and often more accurate than combining conditional
distributions using weight learning.
"
"  The class of chain event graph models is a generalisation of the class of
discrete Bayesian networks, retaining most of the structural advantages of the
Bayesian network for model interrogation, propagation and learning, while more
naturally encoding asymmetric state spaces and the order in which events
happen. In this paper we demonstrate how with complete sampling, conjugate
closed form model selection based on product Dirichlet priors is possible, and
prove that suitable homogeneity assumptions characterise the product Dirichlet
prior on this class of models. We demonstrate our techniques using two
educational examples.
"
"  Simulating sample correlation matrices is important in many areas of
statistics. Approaches such as generating Gaussian data and finding their
sample correlation matrix or generating random uniform $[-1,1]$ deviates as
pairwise correlations both have drawbacks. We develop an algorithm for adding
noise, in a highly controlled manner, to general correlation matrices. In many
instances, our method yields results which are superior to those obtained by
simply simulating Gaussian data. Moreover, we demonstrate how our general
algorithm can be tailored to a number of different correlation models. Using
our results with a few different applications, we show that simulating
correlation matrices can help assess statistical methodology.
"
"  Signal recovery is one of the key techniques of Compressive sensing (CS). It
reconstructs the original signal from the linear sub-Nyquist measurements.
Classical methods exploit the sparsity in one domain to formulate the L0 norm
optimization. Recent investigation shows that some signals are sparse in
multiple domains. To further improve the signal reconstruction performance, we
can exploit this multi-sparsity to generate a new convex programming model. The
latter is formulated with multiple sparsity constraints in multiple domains and
the linear measurement fitting constraint. It improves signal recovery
performance by additional a priori information. Since some EMG signals exhibit
sparsity both in time and frequency domains, we take them as example in
numerical experiments. Results show that the newly proposed method achieves
better performance for multi-sparse signals.
"
"  Genomic regions (or loci) displaying outstanding correlation with some
environmental variables are likely to be under selection and this is the
rationale of recent methods of identifying selected loci and retrieving
functional information about them. To be efficient, such methods need to be
able to disentangle the potential effect of environmental variables from the
confounding effect of population history. For the routine analysis of
genome-wide datasets, one also needs fast inference and model selection
algorithms. We propose a method based on an explicit spatial model which is an
instance of spatial generalized linear mixed model (SGLMM). For inference, we
make use of the INLA-SPDE theoretical and computational framework developed by
Rue et al. (2009) and Lindgren et al (2011). The method we propose allows one
to quantify the correlation between genotypes and environmental variables. It
works for the most common types of genetic markers, obtained either at the
individual or at the population level. Analyzing simulated data produced under
a geostatistical model then under an explicit model of selection, we show that
the method is efficient. We also re-analyze a dataset relative to nineteen pine
weevils (Hylobius abietis}) populations across Europe. The method proposed
appears also as a statistically sound alternative to the Mantel tests for
testing the association between genetic and environmental variables.
"
"  Inverse optimal control, also known as inverse reinforcement learning, is the
problem of recovering an unknown reward function in a Markov decision process
from expert demonstrations of the optimal policy. We introduce a probabilistic
inverse optimal control algorithm that scales gracefully with task
dimensionality, and is suitable for large, continuous domains where even
computing a full policy is impractical. By using a local approximation of the
reward function, our method can also drop the assumption that the
demonstrations are globally optimal, requiring only local optimality. This
allows it to learn from examples that are unsuitable for prior methods.
"
"  Large graphs are natural mathematical models for describing the structure of
the data in a wide variety of fields, such as web mining, social networks,
information retrieval, biological networks, etc. For all these applications,
automatic tools are required to get a synthetic view of the graph and to reach
a good understanding of the underlying problem. In particular, discovering
groups of tightly connected vertices and understanding the relations between
those groups is very important in practice. This paper shows how a kernel
version of the batch Self Organizing Map can be used to achieve these goals via
kernels derived from the Laplacian matrix of the graph, especially when it is
used in conjunction with more classical methods based on the spectral analysis
of the graph. The proposed method is used to explore the structure of a
medieval social network modeled through a weighted graph that has been directly
built from a large corpus of agrarian contracts.
"
"  We consider the task of opportunistic channel access in a primary system
composed of independent Gilbert-Elliot channels where the secondary (or
opportunistic) user does not dispose of a priori information regarding the
statistical characteristics of the system. It is shown that this problem may be
cast into the framework of model-based learning in a specific class of
Partially Observed Markov Decision Processes (POMDPs) for which we introduce an
algorithm aimed at striking an optimal tradeoff between the exploration (or
estimation) and exploitation requirements. We provide finite horizon regret
bounds for this algorithm as well as a numerical evaluation of its performance
in the single channel model as well as in the case of stochastically identical
channels.
"
"  The effort to identify genes with periodic expression during the cell cycle
from genome-wide microarray time series data has been ongoing for a decade.
However, the lack of rigorous modeling of periodic expression as well as the
lack of a comprehensive model for integrating information across genes and
experiments has impaired the effort for the accurate identification of
periodically expressed genes. To address the problem, we introduce a Bayesian
model to integrate multiple independent microarray data sets from three recent
genome-wide cell cycle studies on fission yeast. A hierarchical model was used
for data integration. In order to facilitate an efficient Monte Carlo sampling
from the joint posterior distribution, we develop a novel Metropolis--Hastings
group move. A surprising finding from our integrated analysis is that more than
40% of the genes in fission yeast are significantly periodically expressed,
greatly enhancing the reported 10--15% of the genes in the current literature.
It calls for a reconsideration of the periodically expressed gene detection
problem.
"
"  We propose a novel reformulation of the stochastic optimal control problem as
an approximate inference problem, demonstrating, that such a interpretation
leads to new practical methods for the original problem. In particular we
characterise a novel class of iterative solutions to the stochastic optimal
control problem based on a natural relaxation of the exact dual formulation.
These theoretical insights are applied to the Reinforcement Learning problem
where they lead to new model free, off policy methods for discrete and
continuous problems.
"
"  We use the expectation of the range of an arithmetic Brownian motion and the
method of moments on the daily high, low, opening and closing prices to
estimate the volatility of the stock price. The daily price jump at the opening
is considered to be the result of the unobserved evolution of an after-hours
virtual trading day.The annualized volatility is used to calculate
Black-Scholes prices for European options, and a trading strategy is devised to
profit when these prices differ flagrantly from the market prices.
"
"  ""Asymptotic formulae for likelihood-based tests of new physics"" presents a
mathematical formalism for a new approximation for hypothesis testing in high
energy physics. The approximations are designed to greatly reduce the
computational burden for such problems. We seek to test the conditions under
which the approximations described remain valid. To do so, we perform parallel
calculations for a range of scenarios and compare the full calculation to the
approximations to determine the limits and robustness of the approximation. We
compare this approximation against values calculated with the Collie framework,
which for our analysis we assume produces true values.
"
"  We propose a probabilistic formulation that enables sequential detection of
multiple change points in a network setting. We present a class of sequential
detection rules for certain functionals of change points (minimum among a
subset), and prove their asymptotic optimality properties in terms of expected
detection delay time. Drawing from graphical model formalism, the sequential
detection rules can be implemented by a computationally efficient
message-passing protocol which may scale up linearly in network size and in
waiting time. The effectiveness of our inference algorithm is demonstrated by
simulations.
"
"  We discuss the use of multivariate kernel smoothing methods to date
manuscripts dating from the 11th to the 15th centuries, in the English county
of Essex. The dataset consists of some 3300 dated and 5000 undated manuscripts,
and the former are used as a training sample for imputing dates for the latter.
It is assumed that two manuscripts that are ``close'', in a sense that may be
defined by a vector of measures of distance for documents, will have close
dates. Using this approach, statistical ideas are used to assess
``similarity'', by smoothing among distance measures, and thus to estimate
dates for the 5000 undated manuscripts by reference to the dated ones.
"
"  We consider a linear feeder connecting multiple distributed loads and
generators to the sub-station. Voltage is controlled directly at the
sub-station, however, voltage down the line shifts up or down, in particular
depending on if the feeder operates in the power export regime or power import
regime. Starting from this finite element description of the feeder, assuming
that the consumption/generation is distributed heterogeneously along the
feeder, and following the asymptotic homogenization approach, we derive simple
low-parametric ODE model of the feeder. We also explain how the homogeneous ODE
modeling is generalized to account for other distributed effects, e.g. for
inverter based and voltage dependent control of reactive power. The resulting
system of the DistFlow-ODEs, relating homogenized voltage to flows of real and
reactive power along the lines, admits computationally efficient analysis in
terms of the minimal number of the feeder line ""media"" parameters, such as the
ratio of the inductance-to-resistance densities. Exploring the space of the
media and control parameters allows us to test and juxtapose different measures
of the system performance, in particular expressed in terms of the voltage drop
along the feeder, power import/export from the feeder line as the whole, power
losses within the feeder, and critical (with respect to possible voltage
collapse) length of the feeder. Our most surprising funding relates to
performance of a feeder rich on PhotoVoltaic (PV) systems during a sunny day.
We observe that if the feeder is sufficiently long the DistFlow-ODEs may have
multiple stable solutions. The multiplicity may mean troubles for successful
recovery of the feeder after a very short, few periods long, fault at the head
of the line.
"
"  I introduce a general, Bayesian method for modelling univariate time series
data assumed to be drawn from a continuous, stochastic process. The method
accommodates arbitrary temporal sampling, and takes into account measurement
uncertainties for arbitrary error models (not just Gaussian) on both the time
and signal variables. Any model for the deterministic component of the
variation of the signal with time is supported, as is any model of the
stochastic component on the signal and time variables. Models illustrated here
are constant and sinusoidal models for the signal mean combined with a Gaussian
stochastic component, as well as a purely stochastic model, the
Ornstein-Uhlenbeck process. The posterior probability distribution over model
parameters is determined via Monte Carlo sampling. Models are compared using
the ""cross-validation likelihood"", in which the posterior-averaged likelihood
for different partitions of the data are combined. In principle this is more
robust to changes in the prior than is the evidence (the prior-averaged
likelihood). The method is demonstrated by applying it to the light curves of
11 ultra cool dwarf stars, claimed by a previous study to show statistically
significant variability. This is reassessed here by calculating the
cross-validation likelihood for various time series models, including a null
hypothesis of no variability beyond the error bars. 10 of 11 light curves are
confirmed as being significantly variable, and one of these seems to be
periodic, with two plausible periods identified. Another object is best
described by the Ornstein-Uhlenbeck process, a conclusion which is obviously
limited to the set of models actually tested.
"
"  A variable screening procedure via correlation learning was proposed Fan and
Lv (2008) to reduce dimensionality in sparse ultra-high dimensional models.
Even when the true model is linear, the marginal regression can be highly
nonlinear. To address this issue, we further extend the correlation learning to
marginal nonparametric learning. Our nonparametric independence screening is
called NIS, a specific member of the sure independence screening. Several
closely related variable screening procedures are proposed. Under the
nonparametric additive models, it is shown that under some mild technical
conditions, the proposed independence screening methods enjoy a sure screening
property. The extent to which the dimensionality can be reduced by independence
screening is also explicitly quantified. As a methodological extension, an
iterative nonparametric independence screening (INIS) is also proposed to
enhance the finite sample performance for fitting sparse additive models. The
simulation results and a real data analysis demonstrate that the proposed
procedure works well with moderate sample size and large dimension and performs
better than competing methods.
"
"  In many practical applications of clustering, the objects to be clustered
evolve over time, and a clustering result is desired at each time step. In such
applications, evolutionary clustering typically outperforms traditional static
clustering by producing clustering results that reflect long-term trends while
being robust to short-term variations. Several evolutionary clustering
algorithms have recently been proposed, often by adding a temporal smoothness
penalty to the cost function of a static clustering method. In this paper, we
introduce a different approach to evolutionary clustering by accurately
tracking the time-varying proximities between objects followed by static
clustering. We present an evolutionary clustering framework that adaptively
estimates the optimal smoothing parameter using shrinkage estimation, a
statistical approach that improves a naive estimate using additional
information. The proposed framework can be used to extend a variety of static
clustering algorithms, including hierarchical, k-means, and spectral
clustering, into evolutionary clustering algorithms. Experiments on synthetic
and real data sets indicate that the proposed framework outperforms static
clustering and existing evolutionary clustering algorithms in many scenarios.
"
"  A concentration graph associated with a random vector is an undirected graph
where each vertex corresponds to one random variable in the vector. The absence
of an edge between any pair of vertices (or variables) is equivalent to full
conditional independence between these two variables given all the other
variables. In the multivariate Gaussian case, the absence of an edge
corresponds to a zero coefficient in the precision matrix, which is the inverse
of the covariance matrix. It is well known that this concentration graph
represents some of the conditional independencies in the distribution of the
associated random vector. These conditional independencies correspond to the
""separations"" or absence of edges in that graph. In this paper we assume that
there are no other independencies present in the probability distribution than
those represented by the graph. This property is called the perfect
Markovianity of the probability distribution with respect to the associated
concentration graph. We prove in this paper that this particular concentration
graph, the one associated with a perfect Markov distribution, can be determined
by only conditioning on a limited number of variables. We demonstrate that this
number is equal to the maximum size of the minimal separators in the
concentration graph.
"
"  We investigate a robust penalized logistic regression algorithm based on a
minimum distance criterion. Influential outliers are often associated with the
explosion of parameter vector estimates, but in the context of standard
logistic regression, the bias due to outliers always causes the parameter
vector to implode, that is shrink towards the zero vector. Thus, using
LASSO-like penalties to perform variable selection in the presence of outliers
can result in missed detections of relevant covariates. We show that by
choosing a minimum distance criterion together with an Elastic Net penalty, we
can simultaneously find a parsimonious model and avoid estimation implosion
even in the presence of many outliers in the important small $n$ large $p$
situation. Implementation using an MM algorithm is described and performance
evaluated.
"
"  In many fields observations are performed irregularly along time, due to
either measurement limitations or lack of a constant immanent rate. While
discrete-time Markov models (as Dynamic Bayesian Networks) introduce either
inefficient computation or an information loss to reasoning about such
processes, continuous-time Markov models assume either a discrete state space
(as Continuous-Time Bayesian Networks), or a flat continuous state space (as
stochastic differential equations). To address these problems, we present a new
modeling class called Irregular-Time Bayesian Networks (ITBNs), generalizing
Dynamic Bayesian Networks, allowing substantially more compact representations,
and increasing the expressivity of the temporal dynamics. In addition, a
globally optimal solution is guaranteed when learning temporal systems,
provided that they are fully observed at the same irregularly spaced
time-points, and a semiparametric subclass of ITBNs is introduced to allow
further adaptation to the irregular nature of the available data.
"
"  In this study, two-state Markov switching multinomial logit models are
proposed for statistical modeling of accident injury severities. These models
assume Markov switching in time between two unobserved states of roadway
safety. The states are distinct, in the sense that in different states accident
severity outcomes are generated by separate multinomial logit processes. To
demonstrate the applicability of the approach presented herein, two-state
Markov switching multinomial logit models are estimated for severity outcomes
of accidents occurring on Indiana roads over a four-year time interval.
Bayesian inference methods and Markov Chain Monte Carlo (MCMC) simulations are
used for model estimation. The estimated Markov switching models result in a
superior statistical fit relative to the standard (single-state) multinomial
logit models. It is found that the more frequent state of roadway safety is
correlated with better weather conditions. The less frequent state is found to
be correlated with adverse weather conditions.
"
"  We propose a model to analyze citation growth and influences of fitness
(competitiveness) factors in an evolving citation network. Applying the
proposed method to modeling citations to papers and scholars in the InfoVis
2004 data, a benchmark collection about a 31-year history of information
visualization, leads to findings consistent with citation distributions in
general and observations of the domain in particular. Fitness variables based
on prior impacts and the time factor have significant influences on citation
outcomes. We find considerably large effect sizes from the fitness modeling,
which suggest inevitable bias in citation analysis due to these factors. While
raw citation scores offer little insight into the growth of InfoVis,
normalization of the scores by influences of time and prior fitness offers a
reasonable depiction of the field's development. The analysis demonstrates the
proposed model's ability to produce results consistent with observed data and
to support meaningful comparison of citation scores over time.
"
"  Nonnegative Matrix Factorization (NMF) has been continuously evolving in
several areas like pattern recognition and information retrieval methods. It
factorizes a matrix into a product of 2 low-rank non-negative matrices that
will define parts-based, and linear representation of nonnegative data.
Recently, Graph regularized NMF (GrNMF) is proposed to find a compact
representation,which uncovers the hidden semantics and simultaneously respects
the intrinsic geometric structure. In GNMF, an affinity graph is constructed
from the original data space to encode the geometrical information. In this
paper, we propose a novel idea which engages a Multiple Kernel Learning
approach into refining the graph structure that reflects the factorization of
the matrix and the new data space. The GrNMF is improved by utilizing the graph
refined by the kernel learning, and then a novel kernel learning method is
introduced under the GrNMF framework. Our approach shows encouraging results of
the proposed algorithm in comparison to the state-of-the-art clustering
algorithms like NMF, GrNMF, SVD etc.
"
"  In this paper, we first demonstrate that b-bit minwise hashing, whose
estimators are positive definite kernels, can be naturally integrated with
learning algorithms such as SVM and logistic regression. We adopt a simple
scheme to transform the nonlinear (resemblance) kernel into linear (inner
product) kernel; and hence large-scale problems can be solved extremely
efficiently. Our method provides a simple effective solution to large-scale
learning in massive and extremely high-dimensional datasets, especially when
data do not fit in memory.
  We then compare b-bit minwise hashing with the Vowpal Wabbit (VW) algorithm
(which is related the Count-Min (CM) sketch). Interestingly, VW has the same
variances as random projections. Our theoretical and empirical comparisons
illustrate that usually $b$-bit minwise hashing is significantly more accurate
(at the same storage) than VW (and random projections) in binary data.
Furthermore, $b$-bit minwise hashing can be combined with VW to achieve further
improvements in terms of training speed, especially when $b$ is large.
"
"  Information theoretic active learning has been widely studied for
probabilistic models. For simple regression an optimal myopic policy is easily
tractable. However, for other tasks and with more complex models, such as
classification with nonparametric models, the optimal solution is harder to
compute. Current approaches make approximations to achieve tractability. We
propose an approach that expresses information gain in terms of predictive
entropies, and apply this method to the Gaussian Process Classifier (GPC). Our
approach makes minimal approximations to the full information theoretic
objective. Our experimental performance compares favourably to many popular
active learning algorithms, and has equal or lower computational complexity. We
compare well to decision theoretic approaches also, which are privy to more
information and require much more computational time. Secondly, by developing
further a reformulation of binary preference learning to a classification
problem, we extend our algorithm to Gaussian Process preference learning.
"
"  Translating potential disease biomarkers between multi-species 'omics'
experiments is a new direction in biomedical research. The existing methods are
limited to simple experimental setups such as basic healthy-diseased
comparisons. Most of these methods also require an a priori matching of the
variables (e.g., genes or metabolites) between the species. However, many
experiments have a complicated multi-way experimental design often involving
irregularly-sampled time-series measurements, and for instance metabolites do
not always have known matchings between organisms. We introduce a Bayesian
modelling framework for translating between multiple species the results from
'omics' experiments having a complex multi-way, time-series experimental
design. The underlying assumption is that the unknown matching can be inferred
from the response of the variables to multiple covariates including time.
"
"  A framework for causal inference from two-level factorial designs is
proposed. The framework utilizes the concept of potential outcomes that lies at
the center stage of causal inference and extends Neyman's repeated sampling
approach for estimation of causal effects and randomization tests based on
Fisher's sharp null hypothesis to the case of 2-level factorial experiments.
The framework allows for statistical inference from a finite population,
permits definition and estimation of estimands other than ""average factorial
effects"" and leads to more flexible inference procedures than those based on
ordinary least squares estimation from a linear model.
"
"  The main purpose of this paper is to provide an asymptotically optimal test.
The proposed statistic is of Neyman-Pearson-type when the parameters are
estimated with a particular kind of estimators. It is shown that the proposed
estimators enable us to achieve this end. Two particular cases, AR(1) and ARCH
models were studied and the asymptotic power function was derived.
"
"  In this paper we develop a randomized block-coordinate descent method for
minimizing the sum of a smooth and a simple nonsmooth block-separable convex
function and prove that it obtains an $\epsilon$-accurate solution with
probability at least $1-\rho$ in at most $O(\tfrac{n}{\epsilon} \log
\tfrac{1}{\rho})$ iterations, where $n$ is the number of blocks. For strongly
convex functions the method converges linearly. This extends recent results of
Nesterov [Efficiency of coordinate descent methods on huge-scale optimization
problems, CORE Discussion Paper #2010/2], which cover the smooth case, to
composite minimization, while at the same time improving the complexity by the
factor of 4 and removing $\epsilon$ from the logarithmic term. More
importantly, in contrast with the aforementioned work in which the author
achieves the results by applying the method to a regularized version of the
objective function with an unknown scaling factor, we show that this is not
necessary, thus achieving true iteration complexity bounds. In the smooth case
we also allow for arbitrary probability vectors and non-Euclidean norms.
Finally, we demonstrate numerically that the algorithm is able to solve
huge-scale $\ell_1$-regularized least squares and support vector machine
problems with a billion variables.
"
"  A bivariate ensemble model output statistics (EMOS) technique for the
postprocessing of ensemble forecasts of two-dimensional wind vectors is
proposed, where the postprocessed probabilistic forecast takes the form of a
bivariate normal probability density function. The postprocessed means and
variances of the wind vector components are linearly bias-corrected versions of
the ensemble means and ensemble variances, respectively, and the conditional
correlation between the wind components is represented by a trigonometric
function of the ensemble mean wind direction. In a case study on 48-hour
forecasts of wind vectors over the North American Pacific Northwest with the
University of Washington Mesoscale Ensemble, the bivariate EMOS density
forecasts were calibrated and sharp, and showed considerable improvement over
the raw ensemble and reference forecasts, including ensemble copula coupling.
"
"  Standardization, a common approach for controlling confounding in
population-studies or data from disease registries, is defined to be a weighted
average of stratum specific rates. Typically, discussions on the construction
of a particular standardized rate regard the strata as fixed, and focus on the
considerations that affect the specification of weights. Each year the data
from the SEER cancer registries are analyzed using a weighting procedure
referred to as ``direct standardization for age.'' To evaluate the performance
of direct standardization, we define a general class of standardization
operators. We regard a particular standardized rate to be the output of an
operator and a given data set. Based on the functional form of the operators,
we define a subclass of standardization operators that controls for confounding
by measured risk factors. Using the fundamental disease probability paradigm
for inference, we establish the conclusions that can be drawn from year-to-year
contrasts of standardized rates produced by these operators in the presence of
unmeasured cancer risk factors. These conclusions take the form of falsifying
specific assumptions about the conditional probabilities of disease given all
the risk factors (both measured and unmeasured), and the conditional
probabilities of the unmeasured risk factors given the measured risk factors.
We show the one-to-one correspondence between these falsifications and the
inferences made from the contrasts of directly standardized rates reported each
year in the Annual Report to the Nation on the Status of Cancer.
"
"  Suppose you're on a game show, and you're given the choice of three doors:
Behind one door is a car; behind the others, goats. You pick a door, say No. 1,
and the host, who knows what's behind the doors, opens another door, say No. 3,
which has a goat. He then says to you, ``Do you want to pick door No. 2?'' Is
it to your advantage to switch your choice? The answer is ``yes'' but the
literature offers many reasons why this is the correct answer. The present
paper argues that the most common reasoning found in introductory statistics
texts, depending on making a number of ``obvious'' or ``natural'' assumptions
and then computing a conditional probability, is a classical example of
solution driven science. The best reason to switch is to be found in von
Neumann's minimax theorem from game theory, rather than in Bayes' theorem.
"
"  We present and implement two algorithms for analytic asymptotic evaluation of
the marginal likelihood of data given a Bayesian network with hidden nodes. As
shown by previous work, this evaluation is particularly hard for latent
Bayesian network models, namely networks that include hidden variables, where
asymptotic approximation deviates from the standard BIC score. Our algorithms
solve two central difficulties in asymptotic evaluation of marginal likelihood
integrals, namely, evaluation of regular dimensionality drop for latent
Bayesian network models and computation of non-standard approximation formulas
for singular statistics for these models. The presented algorithms are
implemented in Matlab and Maple and their usage is demonstrated for marginal
likelihood approximations for Bayesian networks with hidden variables.
"
"  Directed acyclic graphs (DAGs) are commonly used to represent causal
relationships among random variables in graphical models. Applications of these
models arise in the study of physical, as well as biological systems, where
directed edges between nodes represent the influence of components of the
system on each other. The general problem of estimating DAGs from observed data
is computationally NP-hard, Moreover two directed graphs may be observationally
equivalent. When the nodes exhibit a natural ordering, the problem of
estimating directed graphs reduces to the problem of estimating the structure
of the network. In this paper, we propose a penalized likelihood approach that
directly estimates the adjacency matrix of DAGs. Both lasso and adaptive lasso
penalties are considered and an efficient algorithm is proposed for estimation
of high dimensional DAGs. We study variable selection consistency of the two
penalties when the number of variables grows to infinity with the sample size.
We show that although lasso can only consistently estimate the true network
under stringent assumptions, adaptive lasso achieves this task under mild
regularity conditions. The performance of the proposed methods is compared to
alternative methods in simulated, as well as real, data examples.
"
"  Learning the network structure of a large graph is computationally demanding,
and dynamically monitoring the network over time for any changes in structure
threatens to be more challenging still. This paper presents a two-stage method
for anomaly detection in dynamic graphs: the first stage uses simple, conjugate
Bayesian models for discrete time counting processes to track the pairwise
links of all nodes in the graph to assess normality of behavior; the second
stage applies standard network inference tools on a greatly reduced subset of
potentially anomalous nodes. The utility of the method is demonstrated on
simulated and real data sets.
"
"  Complex systems in nature and in society are often represented as networks,
describing the rich set of interactions between objects of interest. Many
deterministic and probabilistic clustering methods have been developed to
analyze such structures. Given a network, almost all of them partition the
vertices into disjoint clusters, according to their connection profile.
However, recent studies have shown that these techniques were too restrictive
and that most of the existing networks contained overlapping clusters. To
tackle this issue, we present in this paper the Overlapping Stochastic Block
Model. Our approach allows the vertices to belong to multiple clusters, and, to
some extent, generalizes the well-known Stochastic Block Model [Nowicki and
Snijders (2001)]. We show that the model is generically identifiable within
classes of equivalence and we propose an approximate inference procedure, based
on global and local variational techniques. Using toy data sets as well as the
French Political Blogosphere network and the transcriptional network of
Saccharomyces cerevisiae, we compare our work with other approaches.
"
"  Given a Gaussian Markov random field, we consider the problem of selecting a
subset of variables to observe which minimizes the total expected squared
prediction error of the unobserved variables. We first show that finding an
exact solution is NP-hard even for a restricted class of Gaussian Markov random
fields, called Gaussian free fields, which arise in semi-supervised learning
and computer vision. We then give a simple greedy approximation algorithm for
Gaussian free fields on arbitrary graphs. Finally, we give a message passing
algorithm for general Gaussian Markov random fields on bounded tree-width
graphs.
"
"  In practical machine learning systems, graph based data representation has
been widely used in various learning paradigms, ranging from unsupervised
clustering to supervised classification. Besides those applications with
natural graph or network structure data, such as social network analysis and
relational learning, many other applications often involve a critical step in
converting data vectors to an adjacency graph. In particular, a sparse subgraph
extracted from the original graph is often required due to both theoretic and
practical needs. Previous study clearly shows that the performance of different
learning algorithms, e.g., clustering and classification, benefits from such
sparse subgraphs with balanced node connectivity. However, the existing graph
construction methods are either computationally expensive or with
unsatisfactory performance. In this paper, we utilize a scalable method called
auction algorithm and its parallel extension to recover a sparse yet nearly
balanced subgraph with significantly reduced computational cost. Empirical
study and comparison with the state-ofart approaches clearly demonstrate the
superiority of the proposed method in both efficiency and accuracy.
"
"  Many statistical issues arise in the analysis of Particle Physics
experiments. We give a brief introduction to Particle Physics, before
describing the techniques used by Particle Physicists for dealing with
statistical problems, and also some of the open statistical questions.
"
"  This paper introduces a class of k-nearest neighbor ($k$-NN) estimators
called bipartite plug-in (BPI) estimators for estimating integrals of
non-linear functions of a probability density, such as Shannon entropy and
R\'enyi entropy. The density is assumed to be smooth, have bounded support, and
be uniformly bounded from below on this set. Unlike previous $k$-NN estimators
of non-linear density functionals, the proposed estimator uses data-splitting
and boundary correction to achieve lower mean square error. Specifically, we
assume that $T$ i.i.d. samples ${X}_i \in \mathbb{R}^d$ from the density are
split into two pieces of cardinality $M$ and $N$ respectively, with $M$ samples
used for computing a k-nearest-neighbor density estimate and the remaining $N$
samples used for empirical estimation of the integral of the density
functional. By studying the statistical properties of k-NN balls, explicit
rates for the bias and variance of the BPI estimator are derived in terms of
the sample size, the dimension of the samples and the underlying probability
distribution. Based on these results, it is possible to specify optimal choice
of tuning parameters $M/T$, $k$ for maximizing the rate of decrease of the mean
square error (MSE). The resultant optimized BPI estimator converges faster and
achieves lower mean squared error than previous $k$-NN entropy estimators. In
addition, a central limit theorem is established for the BPI estimator that
allows us to specify tight asymptotic confidence intervals.
"
"  The problem of clustering is considered, for the case when each data point is
a sample generated by a stationary ergodic process. We propose a very natural
asymptotic notion of consistency, and show that simple consistent algorithms
exist, under most general non-parametric assumptions. The notion of consistency
is as follows: two samples should be put into the same cluster if and only if
they were generated by the same distribution. With this notion of consistency,
clustering generalizes such classical statistical problems as homogeneity
testing and process classification. We show that, for the case of a known
number of clusters, consistency can be achieved under the only assumption that
the joint distribution of the data is stationary ergodic (no parametric or
Markovian assumptions, no assumptions of independence, neither between nor
within the samples). If the number of clusters is unknown, consistency can be
achieved under appropriate assumptions on the mixing rates of the processes.
(again, no parametric or independence assumptions). In both cases we give
examples of simple (at most quadratic in each argument) algorithms which are
consistent.
"
"  In this paper we study output coding for multi-label prediction. For a
multi-label output coding to be discriminative, it is important that codewords
for different label vectors are significantly different from each other. In the
meantime, unlike in traditional coding theory, codewords in output coding are
to be predicted from the input, so it is also critical to have a predictable
label encoding.
  To find output codes that are both discriminative and predictable, we first
propose a max-margin formulation that naturally captures these two properties.
We then convert it to a metric learning formulation, but with an exponentially
large number of constraints as commonly encountered in structured prediction
problems. Without a label structure for tractable inference, we use
overgenerating (i.e., relaxation) techniques combined with the cutting plane
method for optimization.
  In our empirical study, the proposed output coding scheme outperforms a
variety of existing multi-label prediction methods for image, text and music
classification.
"
"  We present a Dirichlet process mixture model over discrete incomplete
rankings and study two Gibbs sampling inference techniques for estimating
posterior clusterings. The first approach uses a slice sampling subcomponent
for estimating cluster parameters. The second approach marginalizes out several
cluster parameters by taking advantage of approximations to the conditional
posteriors. We empirically demonstrate (1) the effectiveness of this
approximation for improving convergence, (2) the benefits of the Dirichlet
process model over alternative clustering techniques for ranked data, and (3)
the applicability of the approach to exploring large realworld ranking
datasets.
"
"  This paper addresses the problem of identifying a lower dimensional space
where observed data can be sparsely represented. This under-complete dictionary
learning task can be formulated as a blind separation problem of sparse sources
linearly mixed with an unknown orthogonal mixing matrix. This issue is
formulated in a Bayesian framework. First, the unknown sparse sources are
modeled as Bernoulli-Gaussian processes. To promote sparsity, a weighted
mixture of an atom at zero and a Gaussian distribution is proposed as prior
distribution for the unobserved sources. A non-informative prior distribution
defined on an appropriate Stiefel manifold is elected for the mixing matrix.
The Bayesian inference on the unknown parameters is conducted using a Markov
chain Monte Carlo (MCMC) method. A partially collapsed Gibbs sampler is
designed to generate samples asymptotically distributed according to the joint
posterior distribution of the unknown model parameters and hyperparameters.
These samples are then used to approximate the joint maximum a posteriori
estimator of the sources and mixing matrix. Simulations conducted on synthetic
data are reported to illustrate the performance of the method for recovering
sparse representations. An application to sparse coding on under-complete
dictionary is finally investigated.
"
"  We propose an image deconvolution algorithm when the data is contaminated by
Poisson noise. The image to restore is assumed to be sparsely represented in a
dictionary of waveforms such as the wavelet or curvelet transforms. Our key
contributions are: First, we handle the Poisson noise properly by using the
Anscombe variance stabilizing transform leading to a {\it non-linear}
degradation equation with additive Gaussian noise. Second, the deconvolution
problem is formulated as the minimization of a convex functional with a
data-fidelity term reflecting the noise properties, and a non-smooth
sparsity-promoting penalties over the image representation coefficients (e.g.
$\ell_1$-norm). Third, a fast iterative backward-forward splitting algorithm is
proposed to solve the minimization problem. We derive existence and uniqueness
conditions of the solution, and establish convergence of the iterative
algorithm. Finally, a GCV-based model selection procedure is proposed to
objectively select the regularization parameter. Experimental results are
carried out to show the striking benefits gained from taking into account the
Poisson statistics of the noise. These results also suggest that using
sparse-domain regularization may be tractable in many deconvolution
applications with Poisson noise such as astronomy and microscopy.
"
"  In the applications related to airborne radars, simulation has always played
an important role. This is mainly because of the two fold reason of the
unavailability of desired data and the difficulty associated with the
collection of data under controlled environment. A simple example will be
regarding the collection of pure multipolar radar data. Even after phenomenal
development in the field of radar hardware design and signal processing, till
now the collection of pure multipolar data is a challenge for the radar system
designers. Till very recently, the power of computer simulation of radar signal
return was available to a very selected few. This was because of the heavy cost
associated with some of the main line electro magnetic (EM) simulators for
radar signal simulation, and secondly because many such EM simulators are for
restricted marketting. However, because of the fast progress made in the field
of EM simulation, many of the current generic EM simulators can be used to
simulate radar returns from realistic targets. The current article expounds the
steps towards generating a synthetic aperture radar (SAR) image database of
ground targets, using a eneric EM g simulator. It also demonstrates by the help
of some example images, the quality of the SAR mage generated i using a general
purpose EM simulator.
"
"  Microarray time course (MTC) gene expression data are commonly collected to
study the dynamic nature of biological processes. One important problem is to
identify genes that show different expression profiles over time and pathways
that are perturbed during a given biological process. While methods are
available to identify the genes with differential expression levels over time,
there is a lack of methods that can incorporate the pathway information in
identifying the pathways being modified/activated during a biological process.
In this paper we develop a hidden spatial-temporal Markov random field
(hstMRF)-based method for identifying genes and subnetworks that are related to
biological processes, where the dependency of the differential expression
patterns of genes on the networks are modeled over time and over the network of
pathways. Simulation studies indicated that the method is quite effective in
identifying genes and modified subnetworks and has higher sensitivity than the
commonly used procedures that do not use the pathway structure or time
dependency information, with similar false discovery rates. Application to a
microarray gene expression study of systemic inflammation in humans identified
a core set of genes on the KEGG pathways that show clear differential
expression patterns over time. In addition, the method confirmed that the
TOLL-like signaling pathway plays an important role in immune response to
endotoxins.
"
"  Inferring the causal structure that links n observables is usually based upon
detecting statistical dependences and choosing simple graphs that make the
joint measure Markovian. Here we argue why causal inference is also possible
when only single observations are present.
  We develop a theory how to generate causal graphs explaining similarities
between single objects. To this end, we replace the notion of conditional
stochastic independence in the causal Markov condition with the vanishing of
conditional algorithmic mutual information and describe the corresponding
causal inference rules.
  We explain why a consistent reformulation of causal inference in terms of
algorithmic complexity implies a new inference principle that takes into
account also the complexity of conditional probability densities, making it
possible to select among Markov equivalent causal graphs. This insight provides
a theoretical foundation of a heuristic principle proposed in earlier work.
  We also discuss how to replace Kolmogorov complexity with decidable
complexity criteria. This can be seen as an algorithmic analog of replacing the
empirically undecidable question of statistical independence with practical
independence tests that are based on implicit or explicit assumptions on the
underlying distribution.
"
"  A two-sex Basic Reproduction Number (BRN) is used to investigate the
conditions under which the Human Immunodeficiency Virus (HIV) may spread
through heterosexual contacts in Sub-Saharan Africa. (The BRN is the expected
number of new infections generated by one infected individual; the disease
spreads if the BRN is larger than 1). A simple analytical expression for the
BRN is derived on the basis of recent data on survival rates, transmission
probabilities, and levels of sexual activity. Baseline results show that in the
population at large (characterized by equal numbers of men and women) the BRN
is larger than 1 if every year each person has 82 sexual contacts with
different partners. the BRN is also larger than 1 for commercial sex workers
(CSWs) and their clients (two populations of different sizes) if each CSW has
about 256 clients per year and each client visits one CSW every two weeks. A
sensitivity analysis explores the effect on the BRN of a doubling (or a
halving) of the transmission probabilities. Implications and extensions are
discussed.
"
"  In most papers establishing consistency for learning algorithms it is assumed
that the observations used for training are realizations of an i.i.d. process.
In this paper we go far beyond this classical framework by showing that support
vector machines (SVMs) essentially only require that the data-generating
process satisfies a certain law of large numbers. We then consider the
learnability of SVMs for $\a$-mixing (not necessarily stationary) processes for
both classification and regression, where for the latter we explicitly allow
unbounded noise.
"
"  Estimation of the level set of a function (i.e., regions where the function
exceeds some value) is an important problem with applications in digital
elevation mapping, medical imaging, astronomy, etc. In many applications, the
function of interest is not observed directly. Rather, it is acquired through
(linear) projection measurements, such as tomographic projections,
interferometric measurements, coded-aperture measurements, and random
projections associated with compressed sensing. This paper describes a new
methodology for rapid and accurate estimation of the level set from such
projection measurements. The key defining characteristic of the proposed
method, called the projective level set estimator, is its ability to estimate
the level set from projection measurements without an intermediate
reconstruction step. This leads to significantly faster computation relative to
heuristic ""plug-in"" methods that first estimate the function, typically with an
iterative algorithm, and then threshold the result. The paper also includes a
rigorous theoretical analysis of the proposed method, which utilizes the recent
results from the non-asymptotic theory of random matrices results from the
literature on concentration of measure and characterizes the estimator's
performance in terms of geometry of the measurement operator and 1-norm of the
discretized function.
"
"  Estimating statistical models within sensor networks requires distributed
algorithms, in which both data and computation are distributed across the nodes
of the network. We propose a general approach for distributed learning based on
combining local estimators defined by pseudo-likelihood components,
encompassing a number of combination methods, and provide both theoretical and
experimental analysis. We show that simple linear combination or max-voting
methods, when combined with second-order information, are statistically
competitive with more advanced and costly joint optimization. Our algorithms
have many attractive properties including low communication and computational
cost and ""any-time"" behavior.
"
"  The recent proliferation of richly structured probabilistic models raises the
question of how to automatically determine an appropriate model for a dataset.
We investigate this question for a space of matrix decomposition models which
can express a variety of widely used models from unsupervised learning. To
enable model selection, we organize these models into a context-free grammar
which generates a wide variety of structures through the compositional
application of a few simple rules. We use our grammar to generically and
efficiently infer latent components and estimate predictive likelihood for
nearly 2500 structures using a small toolbox of reusable algorithms. Using a
greedy search over our grammar, we automatically choose the decomposition
structure from raw data by evaluating only a small fraction of all models. The
proposed method typically finds the correct structure for synthetic data and
backs off gracefully to simpler models under heavy noise. It learns sensible
structures for datasets as diverse as image patches, motion capture, 20
Questions, and U.S. Senate votes, all using exactly the same code.
"
"  We present a new model for the electricity spot price dynamics, which is able
to capture seasonality, low-frequency dynamics and the extreme spikes in the
market. Instead of the usual purely deterministic trend we introduce a
non-stationary independent increments process for the low-frequency dynamics,
and model the large fluctuations by a non-Gaussian stable CARMA process. The
model allows for analytic futures prices, and we apply these to model and
estimate the whole market consistently. Besides standard parameter estimation,
an estimation procedure is suggested, where we fit the non-stationary trend
using futures data with long time until delivery, and a robust $L^1$-filter to
find the states of the CARMA process. The procedure also involves the empirical
and theoretical risk premiums which -- as a by-product -- are also estimated.
We apply this procedure to data from the German electricity exchange EEX, where
we split the empirical analysis into base load and peak load prices. We find an
overall negative risk premium for the base load futures contracts, except for
contracts close to delivery, where a small positive risk premium is detected.
The peak load contracts, on the other hand, show a clear positive risk premium,
when they are close to delivery, while the contracts in the longer end also
have a negative premium.
"
"  We present a novel approach to the formulation and the resolution of sparse
Linear Discriminant Analysis (LDA). Our proposal, is based on penalized Optimal
Scoring. It has an exact equivalence with penalized LDA, contrary to the
multi-class approaches based on the regression of class indicator that have
been proposed so far. Sparsity is obtained thanks to a group-Lasso penalty that
selects the same features in all discriminant directions. Our experiments
demonstrate that this approach generates extremely parsimonious models without
compromising prediction performances. Besides prediction, the resulting sparse
discriminant directions are also amenable to low-dimensional representations of
data. Our algorithm is highly efficient for medium to large number of
variables, and is thus particularly well suited to the analysis of gene
expression data.
"
"  This manuscript develops the theory of agglomerative clustering with Bregman
divergences. Geometric smoothing techniques are developed to deal with
degenerate clusters. To allow for cluster models based on exponential families
with overcomplete representations, Bregman divergences are developed for
nondifferentiable convex functions.
"
"  Information-Geometric Optimization (IGO) is a unified framework of stochastic
algorithms for optimization problems. Given a family of probability
distributions, IGO turns the original optimization problem into a new
maximization problem on the parameter space of the probability distributions.
IGO updates the parameter of the probability distribution along the natural
gradient, taken with respect to the Fisher metric on the parameter manifold,
aiming at maximizing an adaptive transform of the objective function. IGO
recovers several known algorithms as particular instances: for the family of
Bernoulli distributions IGO recovers PBIL, for the family of Gaussian
distributions the pure rank-mu CMA-ES update is recovered, and for exponential
families in expectation parametrization the cross-entropy/ML method is
recovered. This article provides a theoretical justification for the IGO
framework, by proving that any step size not greater than 1 guarantees monotone
improvement over the course of optimization, in terms of q-quantile values of
the objective function f. The range of admissible step sizes is independent of
f and its domain. We extend the result to cover the case of different step
sizes for blocks of the parameters in the IGO algorithm. Moreover, we prove
that expected fitness improves over time when fitness-proportional selection is
applied, in which case the RPP algorithm is recovered.
"
"  We improve recently published results about resources of Restricted Boltzmann
Machines (RBM) and Deep Belief Networks (DBN) required to make them Universal
Approximators. We show that any distribution p on the set of binary vectors of
length n can be arbitrarily well approximated by an RBM with k-1 hidden units,
where k is the minimal number of pairs of binary vectors differing in only one
entry such that their union contains the support set of p. In important cases
this number is half of the cardinality of the support set of p. We construct a
DBN with 2^n/2(n-b), b ~ log(n), hidden layers of width n that is capable of
approximating any distribution on {0,1}^n arbitrarily well. This confirms a
conjecture presented by Le Roux and Bengio 2010.
"
"  Graphs and networks provide a canonical representation of relational data,
with massive network data sets becoming increasingly prevalent across a variety
of scientific fields. Although tools from mathematics and computer science have
been eagerly adopted by practitioners in the service of network inference, they
do not yet comprise a unified and coherent framework for the statistical
analysis of large-scale network data. This paper serves as both an introduction
to the topic and a first step toward formal inference procedures. We develop
and illustrate our arguments using the example of hypothesis testing for
network structure. We invoke a generalized likelihood ratio framework and use
it to highlight the growing number of topics in this area that require strong
contributions from statistical science. We frame our discussion in the context
of previous work from across a variety of disciplines, and conclude by
outlining fundamental statistical challenges whose solutions will in turn serve
to advance the science of network inference.
"
"  Evaluating the overall ability of players in the National Hockey League (NHL)
is a difficult task. Existing methods such as the famous ""plus/minus"" statistic
have many shortcomings. Standard linear regression methods work well when
player substitutions are relatively uncommon and scoring events are relatively
common, such as in basketball, but as neither of these conditions exists for
hockey, we use an approach that embraces the unique characteristics of the
sport. We model the scoring rate for each team as its own semi-Markov process,
with hazard functions for each process that depend on the players on the ice.
This method yields offensive and defensive player ability ratings which take
into account quality of teammates and opponents, the game situation, and other
desired factors, that themselves have a meaningful interpretation in terms of
game outcomes. Additionally, since the number of parameters in this model can
be quite large, we make use of two different shrinkage methods depending on the
question of interest: full Bayesian hierarchical models that partially pool
parameters according to player position, and penalized maximum likelihood
estimation to select a smaller number of parameters that stand out as being
substantially different from average. We apply the model to all five-on-five
(full-strength) situations for games in five NHL seasons.
"
"  In applications ranging from communications to genetics, signals can be
modeled as lying in a union of subspaces. Under this model, signal coefficients
that lie in certain subspaces are active or inactive together. The potential
subspaces are known in advance, but the particular set of subspaces that are
active (i.e., in the signal support) must be learned from measurements. We show
that exploiting knowledge of subspaces can further reduce the number of
measurements required for exact signal recovery, and derive universal bounds
for the number of measurements needed. The bound is universal in the sense that
it only depends on the number of subspaces under consideration, and their
orientation relative to each other. The particulars of the subspaces (e.g.,
compositions, dimensions, extents, overlaps, etc.) does not affect the results
we obtain. In the process, we derive sample complexity bounds for the special
case of the group lasso with overlapping groups (the latent group lasso), which
is used in a variety of applications. Finally, we also show that wavelet
transform coefficients of images can be modeled as lying in groups, and hence
can be efficiently recovered using group lasso methods.
"
"  Inferring the structure of gene regulatory networks (GRN) from gene
expression data has many applications, from the elucidation of complex
biological processes to the identification of potential drug targets. It is
however a notoriously difficult problem, for which the many existing methods
reach limited accuracy. In this paper, we formulate GRN inference as a sparse
regression problem and investigate the performance of a popular feature
selection method, least angle regression (LARS) combined with stability
selection. We introduce a novel, robust and accurate scoring technique for
stability selection, which improves the performance of feature selection with
LARS. The resulting method, which we call TIGRESS (Trustful Inference of Gene
REgulation using Stability Selection), was ranked among the top methods in the
DREAM5 gene network reconstruction challenge. We investigate in depth the
influence of the various parameters of the method and show that a fine
parameter tuning can lead to significant improvements and state-of-the-art
performance for GRN inference. TIGRESS reaches state-of-the-art performance on
benchmark data. This study confirms the potential of feature selection
techniques for GRN inference. Code and data are available on
http://cbio.ensmp.fr/~ahaury. Running TIGRESS online is possible on
GenePattern: http://www.broadinstitute.org/cancer/software/genepattern/.
"
"  The deployment of improvised explosive devices (IEDs) along major roadways
has been a favoured strategy of insurgents in recent war zones, both for the
ability to cause damage to targets along roadways at minimal cost, but also as
a means of controlling the flow of traffic and causing additional expense to
opposing forces. Among other related approaches (which we discuss), the
adversarial problem has an analogue in the Canadian Traveller Problem, wherein
a stretch of road is blocked with some independent probability, and the state
of the road is only discovered once the traveller reaches one of the
intersections that bound this stretch of road. We discuss the implementation of
ideas from social network analysis, namely the notion of ""betweenness
centrality"", and how this can be adapted to the notion of deployment of IEDs
with the aid of Generalized Linear Models (GLMs): namely, how we can model the
probability of an IED deployment in terms of the increased effort due to
Canadian betweenness, how we can include expert judgement on the probability of
a deployment, and how we can extend the approach to estimation and updating
over several time steps.
"
"  Structured learning is appropriate when predicting structured outputs such as
trees, graphs, or sequences. Most prior work requires the training set to
consist of complete trees, graphs or sequences. Specifying such detailed ground
truth can be tedious or infeasible for large outputs. Our main contribution is
a large margin formulation that makes structured learning from only partially
annotated data possible. The resulting optimization problem is non-convex, yet
can be efficiently solve by concave-convex procedure (CCCP) with novel speedup
strategies. We apply our method to a challenging tracking-by-assignment problem
of a variable number of divisible objects. On this benchmark, using only 25% of
a full annotation we achieve a performance comparable to a model learned with a
full annotation. Finally, we offer a unifying perspective of previous work
using the hinge, ramp, or max loss for structured learning, followed by an
empirical comparison on their practical performance.
"
"  In this paper, we develop a logistic regression model to estimate the
probability that a particular shot in an NHL game will result in a goal, and
use the results to evaluate the performance of NHL skaters, goalies, and teams.
We weight each shot based on the estimated probabilities obtained from our
model, call this statistic ""weighted shots"", and use advanced statistics based
on weighted shots as the basis of our evaluation. We also analyze whether
advanced statistics based on weighted shots outperform traditional statistics
as an indicator of future performance of skaters, goalies, and teams. In
general, statistics based on weighted shots perform well, but not better than
traditional statistics. We conclude that weighted shots should not be viewed as
a replacement for those statistics, but can be used in conjunction with those
statistics. Finally, we use weighted shots as the dependent variable in an
adjusted plus-minus model. The results are estimates of each player's offensive
and defensive contribution to his team's weighted shots during even strength,
power play, and short handed situations, independent of the strength of his
teammates, the strength of his opponents, and the zone in which his shifts
begin.
"
"  Motivation: Although principal component analysis is frequently applied to
reduce the dimensionality of matrix data, the method is sensitive to noise and
bias and has difficulty with comparability and interpretation. These issues are
addressed by improving the fidelity to the study design. Principal axes and the
components for variables are found through the arrangement of the training data
set, and the centers of data are found according to the design. By using both
the axes and the center, components for an observation that belong to various
studies can be separately estimated. Both of the components for variables and
observations are scaled to a unit length, which enables relationships to be
seen between them.
  Results: Analyses in transcriptome studies showed an improvement in the
separation of experimental groups and in robustness to bias and noise. Unknown
samples were appropriately classified on predetermined axes. These axes well
reflected the study design, and this facilitated the interpretation. Together,
the introduced concepts resulted in improved generality and objectivity in the
analytical results, with the ability to locate hidden structures in the data.
"
"  The variation in DNA copy number carries information on the modalities of
genome evolution and misregulation of DNA replication in cancer cells; its
study can be helpful to localize tumor suppressor genes, distinguish different
populations of cancerous cell, as well identify genomic variations responsible
for disease phenotypes. A number of different high throughput technologies can
be used to identify copy number variable sites, and the literature documents
multiple effective algorithms. We focus here on the specific problem of
detecting regions where variation in copy number is relatively common in the
sample at hand: this encompasses the cases of copy number polymorphisms,
related samples, technical replicates, and cancerous sub-populations from the
same individual. We present an algorithm based on regularization approaches
with significant computational advantages and competitive accuracy. We
illustrate its applicability with simulated and real data sets.
"
"  We consider the evaluation of laboratory practice through the comparison of
measurements made by participating metrology laboratories when the measurement
procedures are considered to have both fixed effects (the residual error due to
unrecognised sources of error) and random effects (drawn from a distribution of
known variance after correction for all known systematic errors). We show that,
when estimating the participant fixed effects, the random effects described can
be ignored. We also derive the adjustment to the variance estimates of the
participant fixed effects due to these random effects.
"
"  Modeling viral dynamics in HIV/AIDS studies has resulted in a deep
understanding of pathogenesis of HIV infection from which novel antiviral
treatment guidance and strategies have been derived. Viral dynamics models
based on nonlinear differential equations have been proposed and well developed
over the past few decades. However, it is quite challenging to use experimental
or clinical data to estimate the unknown parameters (both constant and
time-varying parameters) in complex nonlinear differential equation models.
Therefore, investigators usually fix some parameter values, from the literature
or by experience, to obtain only parameter estimates of interest from clinical
or experimental data. However, when such prior information is not available, it
is desirable to determine all the parameter estimates from data. In this paper
we intend to combine the newly developed approaches, a multi-stage
smoothing-based (MSSB) method and the spline-enhanced nonlinear least squares
(SNLS) approach, to estimate all HIV viral dynamic parameters in a nonlinear
differential equation model. In particular, to the best of our knowledge, this
is the first attempt to propose a comparatively thorough procedure, accounting
for both efficiency and accuracy, to rigorously estimate all key kinetic
parameters in a nonlinear differential equation model of HIV dynamics from
clinical data. These parameters include the proliferation rate and death rate
of uninfected HIV-targeted cells, the average number of virions produced by an
infected cell, and the infection rate which is related to the antiviral
treatment effect and is time-varying. To validate the estimation methods, we
verified the identifiability of the HIV viral dynamic model and performed
simulation studies.
"
"  It is shown that under certain circumstances in particular for small datasets
the recently proposed citation impact indicators I3(6PR) and R(6,k) behave
inconsistently when additional papers or citations are taken into
consideration. Three simple examples are presented, in which the indicators
fluctuate strongly and the ranking of scientists in the evaluated group is
sometimes completely mixed up by minor changes in the data base. The erratic
behavior is traced to the specific way in which weights are attributed to the
six percentile rank classes, specifically for the tied papers. For 100
percentile rank classes the effects will be less serious. For the 6 classes it
is demonstrated that a different way of assigning weights avoids these
problems, although the non-linearity of the weights for the different
percentile rank classes can still lead to (much less frequent) changes in the
ranking. This behavior is not undesired, because it can be used to correct for
differences in citation behavior in different fields. Remaining deviations from
the theoretical value R(6,k) = 1.91 can be avoided by a new scoring rule, the
fractional scoring. Previously proposed consistency criteria are amended by
another property of strict independence which a performance indicator should
aim at.
"
"  This paper explores unsupervised learning of parsing models along two
directions. First, which models are identifiable from infinite data? We use a
general technique for numerically checking identifiability based on the rank of
a Jacobian matrix, and apply it to several standard constituency and dependency
parsing models. Second, for identifiable models, how do we estimate the
parameters efficiently? EM suffers from local optima, while recent work using
spectral methods cannot be directly applied since the topology of the parse
tree varies across sentences. We develop a strategy, unmixing, which deals with
this additional complexity for restricted classes of parsing models.
"
"  Constrained clustering has been well-studied for algorithms such as $K$-means
and hierarchical clustering. However, how to satisfy many constraints in these
algorithmic settings has been shown to be intractable. One alternative to
encode many constraints is to use spectral clustering, which remains a
developing area. In this paper, we propose a flexible framework for constrained
spectral clustering. In contrast to some previous efforts that implicitly
encode Must-Link and Cannot-Link constraints by modifying the graph Laplacian
or constraining the underlying eigenspace, we present a more natural and
principled formulation, which explicitly encodes the constraints as part of a
constrained optimization problem. Our method offers several practical
advantages: it can encode the degree of belief in Must-Link and Cannot-Link
constraints; it guarantees to lower-bound how well the given constraints are
satisfied using a user-specified threshold; it can be solved deterministically
in polynomial time through generalized eigendecomposition. Furthermore, by
inheriting the objective function from spectral clustering and encoding the
constraints explicitly, much of the existing analysis of unconstrained spectral
clustering techniques remains valid for our formulation. We validate the
effectiveness of our approach by empirical results on both artificial and real
datasets. We also demonstrate an innovative use of encoding large number of
constraints: transfer learning via constraints.
"
"  We propose a method to infer causal structures containing both discrete and
continuous variables. The idea is to select causal hypotheses for which the
conditional density of every variable, given its causes, becomes smooth. We
define a family of smooth densities and conditional densities by second order
exponential models, i.e., by maximizing conditional entropy subject to first
and second statistical moments. If some of the variables take only values in
proper subsets of R^n, these conditionals can induce different families of
joint distributions even for Markov-equivalent graphs.
  We consider the case of one binary and one real-valued variable where the
method can distinguish between cause and effect. Using this example, we
describe that sometimes a causal hypothesis must be rejected because
P(effect|cause) and P(cause) share algorithmic information (which is untypical
if they are chosen independently). This way, our method is in the same spirit
as faithfulness-based causal inference because it also rejects non-generic
mutual adjustments among DAG-parameters.
"
"  It has been shown that one can accommodate data (Bayes) and constraints
(MaxEnt) in one method, the method of Maximum (relative) Entropy (ME) (Giffin
2007). In this paper we show a complex agent based example of inference with
two different forms of information; moments and data. In this example, several
agents each receive partial information about a system in the form of data. In
addition, each agent agrees or is informed that there are certain global
constraints on the system that are always true. The agents are then asked to
make inferences about the entire system. The system becomes more complex as we
add agents and allow them to share information. This system can have a
geometrical form, such as a crystal structure. The shape may dictate how the
agents are able to share information, such as sharing with nearest neighbors.
This method can be used to model many systems where the agents or cells have
local or partial information but must adhere to some global rules. This could
also illustrate how the agents evolve and could illuminate emergent behavior of
the system.
"
"  Neural spike trains, which are sequences of very brief jumps in voltage
across the cell membrane, were one of the motivating applications for the
development of point process methodology. Early work required the assumption of
stationarity, but contemporary experiments often use time-varying stimuli and
produce time-varying neural responses. More recently, many statistical methods
have been developed for nonstationary neural point process data. There has also
been much interest in identifying synchrony, meaning events across two or more
neurons that are nearly simultaneous at the time scale of the recordings. A
natural statistical approach is to discretize time, using short time bins, and
to introduce loglinear models for dependency among neurons, but previous use of
loglinear modeling technology has assumed stationarity. We introduce a succinct
yet powerful class of time-varying loglinear models by (a) allowing
individual-neuron effects (main effects) to involve time-varying intensities;
(b) also allowing the individual-neuron effects to involve autocovariation
effects (history effects) due to past spiking, (c) assuming excess synchrony
effects (interaction effects) do not depend on history, and (d) assuming all
effects vary smoothly across time.
"
"  Sparse estimation methods are aimed at using or obtaining parsimonious
representations of data or models. They were first dedicated to linear variable
selection but numerous extensions have now emerged such as structured sparsity
or kernel selection. It turns out that many of the related estimation problems
can be cast as convex optimization problems by regularizing the empirical risk
with appropriate non-smooth norms. The goal of this paper is to present from a
general perspective optimization tools and techniques dedicated to such
sparsity-inducing penalties. We cover proximal methods, block-coordinate
descent, reweighted $\ell_2$-penalized techniques, working-set and homotopy
methods, as well as non-convex formulations and extensions, and provide an
extensive set of experiments to compare various algorithms from a computational
point of view.
"
"  Giant impacts by comets and asteroids have probably had an important
influence on terrestrial biological evolution. We know of around 180 high
velocity impact craters on the Earth with ages up to 2400Myr and diameters up
to 300km. Some studies have identified a periodicity in their age distribution,
with periods ranging from 13 to 50Myr. It has further been claimed that such
periods may be causally linked to a periodic motion of the solar system through
the Galactic plane. However, many of these studies suffer from methodological
problems, for example misinterpretation of p-values, overestimation of
significance in the periodogram or a failure to consider plausible alternative
models. Here I develop a Bayesian method for this problem in which impacts are
treated as a stochastic phenomenon. Models for the time variation of the impact
probability are defined and the evidence for them in the geological record is
compared using Bayes factors. This probabilistic approach obviates the need for
ad hoc statistics, and also makes explicit use of the age uncertainties. I find
strong evidence for a monotonic decrease in the recorded impact rate going back
in time over the past 250Myr for craters larger than 5km. The same is found for
the past 150Myr when craters with upper age limits are included. This is
consistent with a crater preservation/discovery bias modulating an otherwise
constant impact rate. The set of craters larger than 35km (so less affected by
erosion and infilling) and younger than 400Myr are best explained by a constant
impact probability model. A periodic variation in the cratering rate is
strongly disfavoured in all data sets. There is also no evidence for a
periodicity superimposed on a constant rate or trend, although this more
complex signal would be harder to distinguish.
"
"  In spectral clustering, one defines a similarity matrix for a collection of
data points, transforms the matrix to get the Laplacian matrix, finds the
eigenvectors of the Laplacian matrix, and obtains a partition of the data using
the leading eigenvectors. The last step is sometimes referred to as rounding,
where one needs to decide how many leading eigenvectors to use, to determine
the number of clusters, and to partition the data points. In this paper, we
propose a novel method for rounding. The method differs from previous methods
in three ways. First, we relax the assumption that the number of clusters
equals the number of eigenvectors used. Second, when deciding the number of
leading eigenvectors to use, we not only rely on information contained in the
leading eigenvectors themselves, but also use subsequent eigenvectors. Third,
our method is model-based and solves all the three subproblems of rounding
using a class of graphical models called latent tree models. We evaluate our
method on both synthetic and real-world data. The results show that our method
works correctly in the ideal case where between-clusters similarity is 0, and
degrades gracefully as one moves away from the ideal case.
"
"  As more and more network-structured data sets are available, the statistical
analysis of valued graphs has become common place. Looking for a latent
structure is one of the many strategies used to better understand the behavior
of a network. Several methods already exist for the binary case. We present a
model-based strategy to uncover groups of nodes in valued graphs. This
framework can be used for a wide span of parametric random graphs models and
allows to include covariates. Variational tools allow us to achieve approximate
maximum likelihood estimation of the parameters of these models. We provide a
simulation study showing that our estimation method performs well over a broad
range of situations. We apply this method to analyze host--parasite interaction
networks in forest ecosystems.
"
"  Maximum Variance Unfolding is one of the main methods for (nonlinear)
dimensionality reduction. We study its large sample limit, providing specific
rates of convergence under standard assumptions. We find that it is consistent
when the underlying submanifold is isometric to a convex subset, and we provide
some simple examples where it fails to be consistent.
"
"  Interest in the analysis of networks has grown rapidly in the new millennium.
Consequently, we promote renewed attention to a certain methodological approach
introduced in 1974. Over the succeeding decade, this
two-stage--double-standardization and hierarchical clustering
(single-linkage-like)--procedure was applied to a wide variety of weighted,
directed networks of a socioeconomic nature, frequently revealing the presence
of ``hubs''. These were, typically--in the numerous instances studied of
migration flows between geographic subdivisions within
nations--``cosmopolitan/non-provincial'' areas, a prototypical example being
the French capital, Paris. Such locations emit and absorb people broadly across
their respective nations. Additionally, the two-stage procedure--which ``might
very well be the most successful application of cluster analysis'' (R. C.
Dubes, 1985)--detected many (physically or socially) isolated, functional
groups (regions) of areas, such as the southern islands, Shikoku and Kyushu, of
Japan, the Italian islands of Sardinia and Sicily, and the New England region
of the United States. Further, we discuss a (complementary) approach developed
in 1976, in which the max-flow/min-cut theorem was applied to
raw/non-standardized (interindustry, as well as migration) flows.
"
"  Nature has evolved many molecular machines such as kinesin, myosin, and the
rotary flagellar motor powered by an ion current from the mitochondria. Direct
observation of the step-like motion of these machines with time series from
novel experimental assays has recently become possible. These time series are
corrupted by molecular and experimental noise that requires removal, but
classical signal processing is of limited use for recovering such step-like
dynamics. This paper reports simple, novel Bayesian filters that are robust to
step-like dynamics in noise, and introduce an L1-regularized, global filter
whose sparse solution can be rapidly obtained by standard convex optimization
methods. We show these techniques outperforming classical filters on simulated
time series in terms of their ability to accurately recover the underlying step
dynamics. To show the techniques in action, we extract step-like speed
transitions from Rhodobacter sphaeroides flagellar motor time series. Code
implementing these algorithms available from
http://www.eng.ox.ac.uk/samp/members/max/software/.
"
"  Including covariates in loglinear models of population registers improves
population size estimates for two reasons. First, it is possible to take
heterogeneity of inclusion probabilities over the levels of a covariate into
account; and second, it allows subdivision of the estimated population by the
levels of the covariates, giving insight into characteristics of individuals
that are not included in any of the registers. The issue of whether or not
marginalizing the full table of registers by covariates over one or more
covariates leaves the estimated population size estimate invariant is
intimately related to collapsibility of contingency tables [Biometrika 70
(1983) 567-578]. We show that, with information from two registers, population
size invariance is equivalent to the simultaneous collapsibility of each margin
consisting of one register and the covariates. We give a short path
characterization of the loglinear model which describes when marginalizing over
a covariate leads to different population size estimates. Covariates that are
collapsible are called passive, to distinguish them from covariates that are
not collapsible and are termed active. We make the case that it can be useful
to include passive covariates within the estimation model, because they allow a
finer description of the population in terms of these covariates. As an example
we discuss the estimation of the population size of people born in the Middle
East but residing in the Netherlands.
"
"  There is substantial empirical and climatological evidence that precipitation
extremes have become more extreme during the twentieth century, and that this
trend is likely to continue as global warming becomes more intense. However,
understanding these issues is limited by a fundamental issue of spatial
scaling: most evidence of past trends comes from rain gauge data, whereas
trends into the future are produced by climate models, which rely on gridded
aggregates. To study this further, we fit the Generalized Extreme Value (GEV)
distribution to the right tail of the distribution of both rain gauge and
gridded events. The results of this modeling exercise confirm that return
values computed from rain gauge data are typically higher than those computed
from gridded data; however, the size of the difference is somewhat surprising,
with the rain gauge data exhibiting return values sometimes two or three times
that of the gridded data. The main contribution of this paper is the
development of a family of regression relationships between the two sets of
return values that also take spatial variations into account. Based on these
results, we now believe it is possible to project future changes in
precipitation extremes at the point-location level based on results from
climate models.
"
"  A problem of practical significance is the analysis of large, spatially
distributed data sets. The problem is more challenging for variables that
follow non-Gaussian distributions. We show that the spatial correlations
between variables can be captured by interactions between ""spins"". The spins
represent multilevel discretizations of the initial field with respect to a
number of pre-defined thresholds. The spatial dependence between the ""spins"" is
imposed by means of short-range interactions. We present two approaches,
inspired by the Ising and Potts models, that generate conditional simulations
from samples with missing data. The simulations of the ""spin system"" are forced
to respect locally the sample values and the system statistics globally. We
compare the two approaches in terms of their ability to reproduce the sample
statistical properties, to predict data at unsampled locations, as well as in
terms of their computational complexity. We discuss the impact of relevant
simulation parameters, such as the domain size, the number of discretization
levels, and the initial conditions.
"
"  In this paper, we develop a general approach for probabilistic estimation and
optimization. An explicit formula and a computational approach are established
for controlling the reliability of probabilistic estimation based on a mixed
criterion of absolute and relative errors. By employing the Chernoff-Hoeffding
bound and the concept of sampling, the minimization of a probabilistic function
is transformed into an optimization problem amenable for gradient descendent
algorithms.
"
"  Bayesian models offer great flexibility for clustering
applications---Bayesian nonparametrics can be used for modeling infinite
mixtures, and hierarchical Bayesian models can be utilized for sharing clusters
across multiple data sets. For the most part, such flexibility is lacking in
classical clustering methods such as k-means. In this paper, we revisit the
k-means clustering algorithm from a Bayesian nonparametric viewpoint. Inspired
by the asymptotic connection between k-means and mixtures of Gaussians, we show
that a Gibbs sampling algorithm for the Dirichlet process mixture approaches a
hard clustering algorithm in the limit, and further that the resulting
algorithm monotonically minimizes an elegant underlying k-means-like clustering
objective that includes a penalty for the number of clusters. We generalize
this analysis to the case of clustering multiple data sets through a similar
asymptotic argument with the hierarchical Dirichlet process. We also discuss
further extensions that highlight the benefits of our analysis: i) a spectral
relaxation involving thresholded eigenvectors, and ii) a normalized cut graph
clustering algorithm that does not fix the number of clusters in the graph.
"
"  This paper presents a novel theoretical study of the general problem of
multiple source adaptation using the notion of Renyi divergence. Our results
build on our previous work [12], but significantly broaden the scope of that
work in several directions. We extend previous multiple source loss guarantees
based on distribution weighted combinations to arbitrary target distributions
P, not necessarily mixtures of the source distributions, analyze both known and
unknown target distribution cases, and prove a lower bound. We further extend
our bounds to deal with the case where the learner receives an approximate
distribution for each source instead of the exact one, and show that similar
loss guarantees can be achieved depending on the divergence between the
approximate and true distributions. We also analyze the case where the labeling
functions of the source domains are somewhat different. Finally, we report the
results of experiments with both an artificial data set and a sentiment
analysis task, showing the performance benefits of the distribution weighted
combinations and the quality of our bounds based on the Renyi divergence.
"
"  Matching cells over time has long been the most difficult step in cell
tracking. In this paper, we approach this problem by recasting it as a
classification problem. We construct a feature set for each cell, and compute a
feature difference vector between a cell in the current frame and a cell in a
previous frame. Then we determine whether the two cells represent the same cell
over time by training decision trees as our binary classifiers. With the output
of decision trees, we are able to formulate an assignment problem for our cell
association task and solve it using a modified version of the Hungarian
algorithm.
"
"  Most learning methods with rank or sparsity constraints use convex
relaxations, which lead to optimization with the nuclear norm or the
$\ell_1$-norm. However, several important learning applications cannot benefit
from this approach as they feature these convex norms as constraints in
addition to the non-convex rank and sparsity constraints. In this setting, we
derive efficient sparse projections onto the simplex and its extension, and
illustrate how to use them to solve high-dimensional learning problems in
quantum tomography, sparse density estimation and portfolio selection with
non-convex constraints.
"
"  Probabilistic models are conceptually powerful tools for finding structure in
data, but their practical effectiveness is often limited by our ability to
perform inference in them. Exact inference is frequently intractable, so
approximate inference is often performed using Markov chain Monte Carlo (MCMC).
To achieve the best possible results from MCMC, we want to efficiently simulate
many steps of a rapidly mixing Markov chain which leaves the target
distribution invariant. Of particular interest in this regard is how to take
advantage of multi-core computing to speed up MCMC-based inference, both to
improve mixing and to distribute the computational load. In this paper, we
present a parallelizable Markov chain Monte Carlo algorithm for efficiently
sampling from continuous probability distributions that can take advantage of
hundreds of cores. This method shares information between parallel Markov
chains to build a scale-mixture of Gaussians approximation to the density
function of the target distribution. We combine this approximation with a
recent method known as elliptical slice sampling to create a Markov chain with
no step-size parameters that can mix rapidly without requiring gradient or
curvature computations.
"
"  We present results of a forecast initiated following assimilation of
observations for week Week 50 (i.e. the forecast begins December 16, 2012) of
the 2012-2013 influenza season for municipalities in the United States. The
forecast was made on December 21, 2012. Results from forecasts initiated the
three previous weeks (Weeks 47-49) are also presented. Also results from
forecasts generated with an SIRS model without absolute humidity forcing (no
AH) are shown.
"
"  We present an algorithm, AROFAC2, which detects the (CP-)rank of a degree 3
tensor and calculates its factorization into rank-one components. We provide
generative conditions for the algorithm to work and demonstrate on both
synthetic and real world data that AROFAC2 is a potentially outperforming
alternative to the gold standard PARAFAC over which it has the advantages that
it can intrinsically detect the true rank, avoids spurious components, and is
stable with respect to outliers and non-Gaussian noise.
"
"  Wind energy production is very sensitive to instantaneous wind speed
fluctuations. Thus rapid variation of wind speed due to changes in the local
meteorological conditions can lead to electrical power variations of the order
of the nominal power output. In small grids, as they exist for example on some
islands in the French West Indies, such fluctuations can cause instabilities in
case of intermediate power shortages. To palliate these difficulties, it is
essential to identify and characterize the wind speed distributions. This
allows to anticipate the eventuality of power shortage or power surge.
Therefore, it is of interest to categorize wind speed fluctuations into
distinct classes and to estimate the probability of a distribution to belong to
a class. This paper presents a method for classifying wind speed histograms by
estimating a finite mixture of Dirichlet distributions. The SAEM algorithm that
we use provides a fine distinction between wind speed distribution classes.
It's a new nonparametric method for wind speed sequences classification.
However, we show that the wind speed distributions in each class correspond to
specific Gram- Charlier densities.
"
"  We consider the problem of efficient on-line anomaly detection in computer
network traffic. The problem is approached statistically, as that of sequential
(quickest) changepoint detection. A multi-cyclic setting of quickest change
detection is a natural fit for this problem. We propose a novel score-based
multi-cyclic detection algorithm. The algorithm is based on the so-called
Shiryaev-Roberts procedure. This procedure is as easy to employ in practice and
as computationally inexpensive as the popular Cumulative Sum chart and the
Exponentially Weighted Moving Average scheme. The likelihood ratio based
Shiryaev-Roberts procedure has appealing optimality properties, particularly it
is exactly optimal in a multi-cyclic setting geared to detect a change
occurring at a far time horizon. It is therefore expected that an intrusion
detection algorithm based on the Shiryaev-Roberts procedure will perform better
than other detection schemes. This is confirmed experimentally for real traces.
We also discuss the possibility of complementing our anomaly detection
algorithm with a spectral-signature intrusion detection system with false alarm
filtering and true attack confirmation capability, so as to obtain a
synergistic system.
"
"  We are pleased to present a Special Section on Statistics and Astronomy in
this issue of the The Annals of Applied Statistics. Astronomy is an
observational rather than experimental science; as a result, astronomical data
sets both small and large present particularly challenging problems to analysts
who must make the best of whatever the sky offers their instruments. The
resulting statistical problems have enormous diversity. In one problem, one may
have to carefully quantify uncertainty in a hard-won, sparse data set; in
another, the sheer volume of data may forbid a formally optimal analysis,
requiring judicious balancing of model sophistication, approximations, and
clever algorithms. Often the data bear a complex relationship to the underlying
phenomenon producing them, much in the manner of inverse problems.
"
"  We propose a model-based vulnerability index of the population from Uruguay
to vector-borne diseases. We have available measurements of a set of variables
in the census tract level of the 19 Departmental capitals of Uruguay. In
particular, we propose an index that combines different sources of information
via a set of micro-environmental indicators and geographical location in the
country. Our index is based on a new class of spatially hierarchical factor
models that explicitly account for the different levels of hierarchy in the
country, such as census tracts within the city level, and cities in the country
level. We compare our approach with that obtained when data are aggregated in
the city level. We show that our proposal outperforms current and standard
approaches, which fail to properly account for discrepancies in the region
sizes, for example, number of census tracts. We also show that data aggregation
can seriously affect the estimation of the cities vulnerability rankings under
benchmark models.
"
"  In manifold learning, algorithms based on graph Laplacians constructed from
data have received considerable attention both in practical applications and
theoretical analysis. In particular, the convergence of graph Laplacians
obtained from sampled data to certain continuous operators has become an active
research topic recently. Most of the existing work has been done under the
assumption that the data is sampled from a manifold without boundary or that
the functions of interests are evaluated at a point away from the boundary.
However, the question of boundary behavior is of considerable practical and
theoretical interest. In this paper we provide an analysis of the behavior of
graph Laplacians at a point near or on the boundary, discuss their convergence
rates and their implications and provide some numerical results. It turns out
that while points near the boundary occupy only a small part of the total
volume of a manifold, the behavior of graph Laplacian there has different
scaling properties from its behavior elsewhere on the manifold, with global
effects on the whole manifold, an observation with potentially important
implications for the general problem of learning on manifolds.
"
"  Motivated by value function estimation in reinforcement learning, we study
statistical linear inverse problems, i.e., problems where the coefficients of a
linear system to be solved are observed in noise. We consider penalized
estimators, where performance is evaluated using a matrix-weighted two-norm of
the defect of the estimator measured with respect to the true, unknown
coefficients. Two objective functions are considered depending whether the
error of the defect measured with respect to the noisy coefficients is squared
or unsquared. We propose simple, yet novel and theoretically well-founded
data-dependent choices for the regularization parameters for both cases that
avoid data-splitting. A distinguishing feature of our analysis is that we
derive deterministic error bounds in terms of the error of the coefficients,
thus allowing the complete separation of the analysis of the stochastic
properties of these errors. We show that our results lead to new insights and
bounds for linear value function estimation in reinforcement learning.
"
"  Climate modelers generally require meteorological information on regular
grids, but monitoring stations are, in practice, sited irregularly. Thus, there
is a need to produce public data records that interpolate available data to a
high density grid, which can then be used to generate meteorological maps at a
broad range of spatial and temporal scales. In addition to point predictions,
quantifications of uncertainty are also needed. One way to accomplish this is
to provide multiple simulations of the relevant meteorological quantities
conditional on the observed data taking into account the various uncertainties
in predicting a space-time process at locations with no monitoring data. Using
a high-quality dataset of minute-by-minute measurements of atmospheric pressure
in north-central Oklahoma, this work describes a statistical approach to
carrying out these conditional simulations. Based on observations at 11
stations, conditional simulations were produced at two other sites with
monitoring stations. The resulting point predictions are very accurate and the
multiple simulations produce well-calibrated prediction uncertainties for
temporal changes in atmospheric pressure but are substantially overconservative
for the uncertainties in the predictions of (undifferenced) pressure.
"
"  Democratic societies are built around the principle of free and fair
elections, that each citizen's vote should count equal. National elections can
be regarded as large-scale social experiments, where people are grouped into
usually large numbers of electoral districts and vote according to their
preferences. The large number of samples implies certain statistical
consequences for the polling results which can be used to identify election
irregularities. Using a suitable data collapse, we find that vote distributions
of elections with alleged fraud show a kurtosis of hundred times more than
normal elections on certain levels of data aggregation. As an example we show
that reported irregularities in recent Russian elections are indeed well
explained by systematic ballot stuffing and develop a parametric model
quantifying to which extent fraudulent mechanisms are present. We show that if
specific statistical properties are present in an election, the results do not
represent the will of the people. We formulate a parametric test detecting
these statistical properties in election results. Remarkably, this technique
produces similar outcomes irrespective of the data resolution and thus allows
for cross-country comparisons.
"
"  In the information system research, a question of particular interest is to
interpret and to predict the probability of a firm to adopt a new technology
such that market promotions are targeted to only those firms that were more
likely to adopt the technology. Typically, there exists significant difference
between the observed number of ``adopters'' and ``nonadopters,'' which is
usually coded as binary response. A critical issue involved in modeling such
binary response data is the appropriate choice of link functions in a
regression model. In this paper we introduce a new flexible skewed link
function for modeling binary response data based on the generalized extreme
value (GEV) distribution. We show how the proposed GEV links provide more
flexible and improved skewed link regression models than the existing skewed
links, especially when dealing with imbalance between the observed number of
0's and 1's in a data. The flexibility of the proposed model is illustrated
through simulated data sets and a billing data set of the electronic payments
system adoption from a Fortune 100 company in 2005.
"
"  Bayesian structure learning is the NP-hard problem of discovering a Bayesian
network that optimally represents a given set of training data. In this paper
we study the computational worst-case complexity of exact Bayesian structure
learning under graph theoretic restrictions on the super-structure. The
super-structure (a concept introduced by Perrier, Imoto, and Miyano, JMLR 2008)
is an undirected graph that contains as subgraphs the skeletons of solution
networks. Our results apply to several variants of score-based Bayesian
structure learning where the score of a network decomposes into local scores of
its nodes. Results: We show that exact Bayesian structure learning can be
carried out in non-uniform polynomial time if the super-structure has bounded
treewidth and in linear time if in addition the super-structure has bounded
maximum degree. We complement this with a number of hardness results. We show
that both restrictions (treewidth and degree) are essential and cannot be
dropped without loosing uniform polynomial time tractability (subject to a
complexity-theoretic assumption). Furthermore, we show that the restrictions
remain essential if we do not search for a globally optimal network but we aim
to improve a given network by means of at most k arc additions, arc deletions,
or arc reversals (k-neighborhood local search).
"
"  The problem of ranking a set of objects given some measure of similarity is
one of the most basic in machine learning. Recently Agarwal proposed a method
based on techniques in semi-supervised learning utilizing the graph Laplacian.
In this work we consider a novel application of this technique to ranking
binary choice data and apply it specifically to ranking US Senators by their
ideology.
"
"  Networks have in recent years emerged as an invaluable tool for describing
and quantifying complex systems in many branches of science. Recent studies
suggest that networks often exhibit hierarchical organization, where vertices
divide into groups that further subdivide into groups of groups, and so forth
over multiple scales. In many cases these groups are found to correspond to
known functional units, such as ecological niches in food webs, modules in
biochemical networks (protein interaction networks, metabolic networks, or
genetic regulatory networks), or communities in social networks. Here we
present a general technique for inferring hierarchical structure from network
data and demonstrate that the existence of hierarchy can simultaneously explain
and quantitatively reproduce many commonly observed topological properties of
networks, such as right-skewed degree distributions, high clustering
coefficients, and short path lengths. We further show that knowledge of
hierarchical structure can be used to predict missing connections in partially
known networks with high accuracy, and for more general network structures than
competing techniques. Taken together, our results suggest that hierarchy is a
central organizing principle of complex networks, capable of offering insight
into many network phenomena.
"
"  In medical risk modeling, typical data are ""scarce"": they have relatively
small number of training instances (N), censoring, and high dimensionality (M).
We show that the problem may be effectively simplified by reducing it to
bipartite ranking, and introduce new bipartite ranking algorithm, Smooth Rank,
for robust learning on scarce data. The algorithm is based on ensemble learning
with unsupervised aggregation of predictors. The advantage of our approach is
confirmed in comparison with two ""gold standard"" risk modeling methods on 10
real life survival analysis datasets, where the new approach has the best
results on all but two datasets with the largest ratio N/M. For systematic
study of the effects of data scarcity on modeling by all three methods, we
conducted two types of computational experiments: on real life data with
randomly drawn training sets of different sizes, and on artificial data with
increasing number of features. Both experiments demonstrated that Smooth Rank
has critical advantage over the popular methods on the scarce data; it does not
suffer from overfitting where other methods do.
"
"  Multiple kernel learning (MKL), structured sparsity, and multi-task learning
have recently received considerable attention. In this paper, we show how
different MKL algorithms can be understood as applications of either
regularization on the kernel weights or block-norm-based regularization, which
is more common in structured sparsity and multi-task learning. We show that
these two regularization strategies can be systematically mapped to each other
through a concave conjugate operation. When the kernel-weight-based regularizer
is separable into components, we can naturally consider a generative
probabilistic model behind MKL. Based on this model, we propose learning
algorithms for the kernel weights through the maximization of marginal
likelihood. We show through numerical experiments that $\ell_2$-norm MKL and
Elastic-net MKL achieve comparable accuracy to uniform kernel combination.
Although uniform kernel combination might be preferable from its simplicity,
$\ell_2$-norm MKL and Elastic-net MKL can learn the usefulness of the
information sources represented as kernels. In particular, Elastic-net MKL
achieves sparsity in the kernel weights.
"
"  Gaussian processes (GPs) provide a probabilistic nonparametric representation
of functions in regression, classification, and other problems. Unfortunately,
exact learning with GPs is intractable for large datasets. A variety of
approximate GP methods have been proposed that essentially map the large
dataset into a small set of basis points. Among them, two state-of-the-art
methods are sparse pseudo-input Gaussian process (SPGP) (Snelson and
Ghahramani, 2006) and variablesigma GP (VSGP) Walder et al. (2008), which
generalizes SPGP and allows each basis point to have its own length scale.
However, VSGP was only derived for regression. In this paper, we propose a new
sparse GP framework that uses expectation propagation to directly approximate
general GP likelihoods using a sparse and smooth basis. It includes both SPGP
and VSGP for regression as special cases. Plus as an EP algorithm, it inherits
the ability to process data online. As a particular choice of approximating
family, we blur each basis point with a Gaussian distribution that has a full
covariance matrix representing the data distribution around that basis point;
as a result, we can summarize local data manifold information with a small set
of basis points. Our experiments demonstrate that this framework outperforms
previous GP classification methods on benchmark datasets in terms of minimizing
divergence to the non-sparse GP solution as well as lower misclassification
rate.
"
"  High-dimensional tensors or multi-way data are becoming prevalent in areas
such as biomedical imaging, chemometrics, networking and bibliometrics.
Traditional approaches to finding lower dimensional representations of tensor
data include flattening the data and applying matrix factorizations such as
principal components analysis (PCA) or employing tensor decompositions such as
the CANDECOMP / PARAFAC (CP) and Tucker decompositions. The former can lose
important structure in the data, while the latter Higher-Order PCA (HOPCA)
methods can be problematic in high-dimensions with many irrelevant features. We
introduce frameworks for sparse tensor factorizations or Sparse HOPCA based on
heuristic algorithmic approaches and by solving penalized optimization problems
related to the CP decomposition. Extensions of these approaches lead to methods
for general regularized tensor factorizations, multi-way Functional HOPCA and
generalizations of HOPCA for structured data. We illustrate the utility of our
methods for dimension reduction, feature selection, and signal recovery on
simulated data and multi-dimensional microarrays and functional MRIs.
"
"  A commonly used characteristic of statistical dependence of adjacency
relations in real networks, the clustering coefficient, evaluates chances that
two neighbours of a given vertex are adjacent. An extension is obtained by
considering conditional probabilities that two randomly chosen vertices are
adjacent given that they have r common neighbours. We denote such probabilities
cl(r) and call r-> cl(r) the clustering function.
  We compare clustering functions of several networks having non-negligible
clustering coefficient. They show similar patterns and surprising regularity.
We establish a first order asymptotic (as the number of vertices tends to
infinity) of the clustering function of related random intersection graph
models admitting nonvanishing clustering coefficient and asymptotic degree
distribution having a finite second moment.
"
"  This paper presents a new approach to conditional inference, based on the
simulation of samples conditioned by a statistics of the data. Also an explicit
expression for the approximation of the conditional likelihood of long runs of
the sample given the observed statistics is provided. It is shown that when the
conditioning statistics is sufficient for a given parameter, the approximating
density is still invariant with respect to the parameter. A new
Rao-Blackwellisation procedure is proposed and simulation shows that Lehmann
Scheff\'{e} Theorem is valid for this approximation. Conditional inference for
exponential families with nuisance parameter is also studied, leading to Monte
carlo tests. Finally the estimation of the parameter of interest through
conditional likelihood is considered. Comparison with the parametric bootstrap
method is discussed.
"
"  The renewable energies prediction and particularly global radiation
forecasting is a challenge studied by a growing number of research teams. This
paper proposes an original technique to model the insolation time series based
on combining Artificial Neural Network (ANN) and Auto-Regressive and Moving
Average (ARMA) model. While ANN by its non-linear nature is effective to
predict cloudy days, ARMA techniques are more dedicated to sunny days without
cloud occurrences. Thus, three hybrids models are suggested: the first proposes
simply to use ARMA for 6 months in spring and summer and to use an optimized
ANN for the other part of the year; the second model is equivalent to the first
but with a seasonal learning; the last model depends on the error occurred the
previous hour. These models were used to forecast the hourly global radiation
for five places in Mediterranean area. The forecasting performance was compared
among several models: the 3 above mentioned models, the best ANN and ARMA for
each location. In the best configuration, the coupling of ANN and ARMA allows
an improvement of more than 1%, with a maximum in autumn (3.4%) and a minimum
in winter (0.9%) where ANN alone is the best.
"
"  We study the distributions of the LASSO, SCAD, and thresholding estimators,
in finite samples and in the large-sample limit. The asymptotic distributions
are derived for both the case where the estimators are tuned to perform
consistent model selection and for the case where the estimators are tuned to
perform conservative model selection. Our findings complement those of Knight
and Fu (2000) and Fan and Li (2001). We show that the distributions are
typically highly nonnormal regardless of how the estimator is tuned, and that
this property persists in large samples. The uniform convergence rate of these
estimators is also obtained, and is shown to be slower than 1/root(n) in case
the estimator is tuned to perform consistent model selection. An impossibility
result regarding estimation of the estimators' distribution function is also
provided.
"
"  It is well known that correlations in microarray data represent a serious
nuisance deteriorating the performance of gene selection procedures. This paper
is intended to demonstrate that the correlation structure of microarray data
provides a rich source of useful information. We discuss distinct correlation
substructures revealed in microarray gene expression data by an appropriate
ordering of genes. These substructures include stochastic proportionality of
expression signals in a large percentage of all gene pairs, negative
correlations hidden in ordered gene triples, and a long sequence of weakly
dependent random variables associated with ordered pairs of genes. The reported
striking regularities are of general biological interest and they also have
far-reaching implications for theory and practice of statistical methods of
microarray data analysis. We illustrate the latter point with a method for
testing differential expression of nonoverlapping gene pairs. While designed
for testing a different null hypothesis, this method provides an order of
magnitude more accurate control of type 1 error rate compared to conventional
methods of individual gene expression profiling. In addition, this method is
robust to the technical noise. Quantitative inference of the correlation
structure has the potential to extend the analysis of microarray data far
beyond currently practiced methods.
"
"  Semisupervised methods inevitably invoke some assumption that links the
marginal distribution of the features to the regression function of the label.
Most commonly, the cluster or manifold assumptions are used which imply that
the regression function is smooth over high-density clusters or manifolds
supporting the data. A generalization of these assumptions is that the
regression function is smooth with respect to some density sensitive distance.
This motivates the use of a density based metric for semisupervised learning.
We analyze this setting and make the following contributions - (a) we propose a
semi-supervised learner that uses a density-sensitive kernel and show that it
provides better performance than any supervised learner if the density support
set has a small condition number and (b) we show that it is possible to adapt
to the degree of semi-supervisedness using data-dependent choice of a parameter
that controls sensitivity of the distance metric to the density. This ensures
that the semisupervised learner never performs worse than a supervised learner
even if the assumptions fail to hold.
"
"  Sentiment analysis is a new area in text analytics where it focuses on the
analysis and understanding of the emotions from the text patterns. This new
form of analysis has been widely adopted in customer relation management
especially in the context of complaint management. With increasing level of
interest in this technology, more and more companies are adopting it and using
it to champion their marketing efforts. However, sentiment analysis using
twitter has remained extremely difficult to manage due to the sampling bias. In
this paper, we will discuss about the application of using reweighting
techniques in conjunction with online sentiment divisions to predict the vote
percentage that individual candidate will receive. There will be in depth
discussion about the various aspects using sentiment analysis to predict
outcomes as well as the potential pitfalls in the estimation due to the
anonymous nature of the internet.
"
"  A Support Vector Method for multivariate performance measures was recently
introduced by Joachims (2005). The underlying optimization problem is currently
solved using cutting plane methods such as SVM-Perf and BMRM. One can show that
these algorithms converge to an eta accurate solution in O(1/Lambda*e)
iterations, where lambda is the trade-off parameter between the regularizer and
the loss function. We present a smoothing strategy for multivariate performance
scores, in particular precision/recall break-even point and ROCArea. When
combined with Nesterov's accelerated gradient algorithm our smoothing strategy
yields an optimization algorithm which converges to an eta accurate solution in
O(min{1/e,1/sqrt(lambda*e)}) iterations. Furthermore, the cost per iteration of
our scheme is the same as that of SVM-Perf and BMRM. Empirical evaluation on a
number of publicly available datasets shows that our method converges
significantly faster than cutting plane methods without sacrificing
generalization ability.
"
"  Much recent work has concerned sparse approximations to speed up the Gaussian
process regression from the unfavorable O(n3) scaling in computational time to
O(nm2). Thus far, work has concentrated on models with one covariance function.
However, in many practical situations additive models with multiple covariance
functions may perform better, since the data may contain both long and short
length-scale phenomena. The long length-scales can be captured with global
sparse approximations, such as fully independent conditional (FIC), and the
short length-scales can be modeled naturally by covariance functions with
compact support (CS). CS covariance functions lead to naturally sparse
covariance matrices, which are computationally cheaper to handle than full
covariance matrices. In this paper, we propose a new sparse Gaussian process
model with two additive components: FIC for the long length-scales and CS
covariance function for the short length-scales. We give theoretical and
experimental results and show that under certain conditions the proposed model
has the same computational complexity as FIC. We also compare the model
performance of the proposed model to additive models approximated by fully and
partially independent conditional (PIC). We use real data sets and show that
our model outperforms FIC and PIC approximations for data sets with two
additive phenomena.
"
"  This article presents a form of bi-cross-validation (BCV) for choosing the
rank in outer product models, especially the singular value decomposition (SVD)
and the nonnegative matrix factorization (NMF). Instead of leaving out a set of
rows of the data matrix, we leave out a set of rows and a set of columns, and
then predict the left out entries by low rank operations on the retained data.
We prove a self-consistency result expressing the prediction error as a
residual from a low rank approximation. Random matrix theory and some empirical
results suggest that smaller hold-out sets lead to more over-fitting, while
larger ones are more prone to under-fitting. In simulated examples we find that
a method leaving out half the rows and half the columns performs well.
"
"  In many recent applications, data is plentiful. By now, we have a rather
clear understanding of how more data can be used to improve the accuracy of
learning algorithms. Recently, there has been a growing interest in
understanding how more data can be leveraged to reduce the required training
runtime. In this paper, we study the runtime of learning as a function of the
number of available training examples, and underscore the main high-level
techniques. We provide some initial positive results showing that the runtime
can decrease exponentially while only requiring a polynomial growth of the
number of examples, and spell-out several interesting open problems.
"
"  The most popular tool used in the industry for monitoring a process is the
Shewhart control chart. The major disadvantage of the Shewhart control chart is
that it is not very efficient in detecting small process average shifts. To
increase the sensitivity of Shewhart control charts to small shifts additional
supplementary runs rules has been suggested. In this paper we introduce and
study the modified r/m control chart which has an improved sensitivity to small
and moderate process average shifts as compared with the standard Shewhart
X-bar control chart and corresponding control charts proposed recently in the
literature.
"
"  Many inference problems involving questions of optimality ask for the maximum
or the minimum of a finite set of unknown quantities. This technical report
derives the first two posterior moments of the maximum of two correlated
Gaussian variables and the first two posterior moments of the two generating
variables (corresponding to Gaussian approximations minimizing relative
entropy). It is shown how this can be used to build a heuristic approximation
to the maximum relationship over a finite set of Gaussian variables, allowing
approximate inference by Expectation Propagation on such quantities.
"
"  Many of you reading these words will have been attracted by the discussion
paper [McShane and Wyner (2011)], in which case, this may be the first, but
hopefully not the last, time you will have read anything in a statistics
journal. I would like to take this opportunity to discuss the review process in
our journal and to make some comments about the role of statistics and
uncertainty assessment in paleoclimatology and the broader debate about climate
change.
"
"  Directed graphs have asymmetric connections, yet the current graph clustering
methodologies cannot identify the potentially global structure of these
asymmetries. We give a spectral algorithm called di-sim that builds on a dual
measure of similarity that correspond to how a node (i) sends and (ii) receives
edges. Using di-sim, we analyze the global asymmetries in the networks of Enron
emails, political blogs, and the c elegans neural connectome. In each example,
a small subset of nodes have persistent asymmetries; these nodes send edges
with one cluster, but receive edges with another cluster. Previous approaches
would have assigned these asymmetric nodes to only one cluster, failing to
identify their sending/receiving asymmetries. Regularization and ""projection""
are two steps of di-sim that are essential for spectral clustering algorithms
to work in practice. The theoretical results show that these steps make the
algorithm weakly consistent under the degree corrected Stochastic
co-Blockmodel, a model that generalizes the Stochastic Blockmodel to allow for
both (i) degree heterogeneity and (ii) the global asymmetries that we intend to
detect. The theoretical results make no assumptions on the smallest degree
nodes. Instead, the theorem requires that the average degree grows sufficiently
fast and that the weak consistency only applies to the subset of the nodes with
sufficiently large leverage scores. The results results also apply to bipartite
graphs.
"
"  We show that the predictability of letters in written English texts depends
strongly on their position in the word. The first letters are usually the least
easy to predict. This agrees with the intuitive notion that words are well
defined subunits in written languages, with much weaker correlations across
these units than within them. It implies that the average entropy of a letter
deep inside a word is roughly 4 times smaller than the entropy of the first
letter.
"
"  We consider supervised learning problems where the features are embedded in a
graph, such as gene expressions in a gene network. In this context, it is of
much interest to automatically select a subgraph with few connected components;
by exploiting prior knowledge, one can indeed improve the prediction
performance or obtain results that are easier to interpret. Regularization or
penalty functions for selecting features in graphs have recently been proposed,
but they raise new algorithmic challenges. For example, they typically require
solving a combinatorially hard selection problem among all connected subgraphs.
In this paper, we propose computationally feasible strategies to select a
sparse and well-connected subset of features sitting on a directed acyclic
graph (DAG). We introduce structured sparsity penalties over paths on a DAG
called ""path coding"" penalties. Unlike existing regularization functions that
model long-range interactions between features in a graph, path coding
penalties are tractable. The penalties and their proximal operators involve
path selection problems, which we efficiently solve by leveraging network flow
optimization. We experimentally show on synthetic, image, and genomic data that
our approach is scalable and leads to more connected subgraphs than other
regularization functions for graphs.
"
"  Loopy and generalized belief propagation are popular algorithms for
approximate inference in Markov random fields and Bayesian networks. Fixed
points of these algorithms correspond to extrema of the Bethe and Kikuchi free
energy. However, belief propagation does not always converge, which explains
the need for approaches that explicitly minimize the Kikuchi/Bethe free energy,
such as CCCP and UPS. Here we describe a class of algorithms that solves this
typically nonconvex constrained minimization of the Kikuchi free energy through
a sequence of convex constrained minimizations of upper bounds on the Kikuchi
free energy. Intuitively one would expect tighter bounds to lead to faster
algorithms, which is indeed convincingly demonstrated in our simulations.
Several ideas are applied to obtain tight convex bounds that yield dramatic
speed-ups over CCCP.
"
"  The goal of imitation learning is for an apprentice to learn how to behave in
a stochastic environment by observing a mentor demonstrating the correct
behavior. Accurate prior knowledge about the correct behavior can reduce the
need for demonstrations from the mentor. We present a novel approach to
encoding prior knowledge about the correct behavior, where we assume that this
prior knowledge takes the form of a Markov Decision Process (MDP) that is used
by the apprentice as a rough and imperfect model of the mentor's behavior.
Specifically, taking a Bayesian approach, we treat the value of a policy in
this modeling MDP as the log prior probability of the policy. In other words,
we assume a priori that the mentor's behavior is likely to be a high value
policy in the modeling MDP, though quite possibly different from the optimal
policy. We describe an efficient algorithm that, given a modeling MDP and a set
of demonstrations by a mentor, provably converges to a stationary point of the
log posterior of the mentor's policy, where the posterior is computed with
respect to the ""value based"" prior. We also present empirical evidence that
this prior does in fact speed learning of the mentor's policy, and is an
improvement in our experiments over similar previous methods.
"
"  International migration is now a significant driver of population change
across Europe but the methods available to estimate its true impact upon
sub-national areas remain inconsistent, constrained by inadequate systems of
measurement and data capture. In the absence of a population register for
England, official statistics on immigration and emigration are derived from a
combination of survey and census sources. This paper demonstrates how
administrative data systems such as those which capture registrations of recent
migrants with a local doctor, National Insurance Number registrations by
workers from abroad and the registration of foreign students for higher
education, can provide data to better understand patterns and trends in
international migration. The paper proposes a model for the estimation of
immigration at a local level, integrating existing national estimates from the
Office for National Statistics with data from these administrative sources. The
model attempts to circumvent conceptual differences between datasets through
the use of proportional distributions rather than absolute migrant counts in
the estimation process. The model methodology and the results it produces
provide alternative estimates of immigration for consideration by the Office
for National Statistics as it develops its own programme of improvement to
sub-national migration statistics.
"
"  This paper presents a general iterative bias correction procedure for
regression smoothers. This bias reduction schema is shown to correspond
operationally to the $L_2$ Boosting algorithm and provides a new statistical
interpretation for $L_2$ Boosting. We analyze the behavior of the Boosting
algorithm applied to common smoothers $S$ which we show depend on the spectrum
of $I-S$. We present examples of common smoother for which Boosting generates a
divergent sequence. The statistical interpretation suggest combining algorithm
with an appropriate stopping rule for the iterative procedure. Finally we
illustrate the practical finite sample performances of the iterative smoother
via a simulation study. simulations.
"
"  We propose dimension reduction methods for sparse, high-dimensional
multivariate response regression models. Both the number of responses and that
of the predictors may exceed the sample size. Sometimes viewed as
complementary, predictor selection and rank reduction are the most popular
strategies for obtaining lower-dimensional approximations of the parameter
matrix in such models. We show in this article that important gains in
prediction accuracy can be obtained by considering them jointly. We motivate a
new class of sparse multivariate regression models, in which the coefficient
matrix has low rank and zero rows or can be well approximated by such a matrix.
Next, we introduce estimators that are based on penalized least squares, with
novel penalties that impose simultaneous row and rank restrictions on the
coefficient matrix. We prove that these estimators indeed adapt to the unknown
matrix sparsity and have fast rates of convergence. We support our theoretical
results with an extensive simulation study and two data analyses.
"
"  Hutter (2007) recently introduced the loss rank principle (LoRP) as a
generalpurpose principle for model selection. The LoRP enjoys many attractive
properties and deserves further investigations. The LoRP has been well-studied
for regression framework in Hutter and Tran (2010). In this paper, we study the
LoRP for classification framework, and develop it further for model selection
problems in unsupervised learning where the main interest is to describe the
associations between input measurements, like cluster analysis or graphical
modelling. Theoretical properties and simulation studies are presented.
"
"  We develop a nested hierarchical Dirichlet process (nHDP) for hierarchical
topic modeling. The nHDP is a generalization of the nested Chinese restaurant
process (nCRP) that allows each word to follow its own path to a topic node
according to a document-specific distribution on a shared tree. This alleviates
the rigid, single-path formulation of the nCRP, allowing a document to more
easily express thematic borrowings as a random effect. We derive a stochastic
variational inference algorithm for the model, in addition to a greedy subtree
selection method for each document, which allows for efficient inference using
massive collections of text documents. We demonstrate our algorithm on 1.8
million documents from The New York Times and 3.3 million documents from
Wikipedia.
"
"  We compute the expected value of the Kullback-Leibler divergence to various
fundamental statistical models with respect to canonical priors on the
probability simplex. We obtain closed formulas for the expected model
approximation errors, depending on the dimension of the models and the
cardinalities of their sample spaces. For the uniform prior, the expected
divergence from any model containing the uniform distribution is bounded by a
constant $1-\gamma$, and for the models that we consider, this bound is
approached if the state space is very large and the models' dimension does not
grow too fast. For Dirichlet priors the expected divergence is bounded in a
similar way, if the concentration parameters take reasonable values. These
results serve as reference values for more complicated statistical models.
"
"  The ability to quantitatively assess the health of an ecosystem is often of
great interest to those tasked with monitoring and conserving ecosystems. For
decades, research in this area has relied upon multimetric indices of various
forms. Although indices may be numbers, many are constructed based on
procedures that are highly qualitative in nature, thus limiting the
quantitative rigour of the practical interpretations made from these indices.
The statistical modelling approach to construct the latent health factor index
(LHFI) was recently developed to express ecological data, collected to
construct conventional multimetric health indices, in a rigorous quantitative
model that integrates qualitative features of ecosystem health and preconceived
ecological relationships among such features. This hierarchical modelling
approach allows (a) statistical inference of health for observed sites and (b)
prediction of health for unobserved sites, all accompanied by formal
uncertainty statements. Thus far, the LHFI approach has been demonstrated and
validated on freshwater ecosystems. The goal of this paper is to adapt this
approach to modelling estuarine ecosystem health, particularly that of the
previously unassessed system in Richibucto in New Brunswick, Canada. Field data
correspond to biotic health metrics that constitute the AZTI marine biotic
index (AMBI) and abiotic predictors preconceived to influence biota. We also
briefly discuss related LHFI research involving additional metrics that form
the infaunal trophic index (ITI). Our paper is the first to construct a
scientifically sensible model to rigorously identify the collective explanatory
capacity of salinity, distance downstream, channel depth, and silt-clay content
--- all regarded a priori as qualitatively important abiotic drivers ---
towards site health in the Richibucto ecosystem.
"
"  Unsupervised models can provide supplementary soft constraints to help
classify new target data under the assumption that similar objects in the
target set are more likely to share the same class label. Such models can also
help detect possible differences between training and target distributions,
which is useful in applications where concept drift may take place. This paper
describes a Bayesian framework that takes as input class labels from existing
classifiers (designed based on labeled data from the source domain), as well as
cluster labels from a cluster ensemble operating solely on the target data to
be classified, and yields a consensus labeling of the target data. This
framework is particularly useful when the statistics of the target data drift
or change from those of the training data. We also show that the proposed
framework is privacy-aware and allows performing distributed learning when
data/models have sharing restrictions. Experiments show that our framework can
yield superior results to those provided by applying classifier ensembles only.
"
"  Support Vector Machines are a widely used classification technique. They are
computationally efficient and provide excellent predictions even for
high-dimensional data. Moreover, Support Vector Machines are very flexible due
to the incorporation of kernel functions. The latter allow to model
nonlinearity, but also to deal with nonnumerical data such as protein strings.
However, Support Vector Machines can suffer a lot from unclean data containing,
for example, outliers or mislabeled observations. Although several outlier
detection schemes have been proposed in the literature, the selection of
outliers versus nonoutliers is often rather ad hoc and does not provide much
insight in the data. In robust multivariate statistics outlier maps are quite
popular tools to assess the quality of data under consideration. They provide a
visual representation of the data depicting several types of outliers. This
paper proposes an outlier map designed for Support Vector Machine
classification. The Stahel--Donoho outlyingness measure from multivariate
statistics is extended to an arbitrary kernel space. A trimmed version of
Support Vector Machines is defined trimming part of the samples with largest
outlyingness. Based on this classifier, an outlier map is constructed
visualizing data in any type of high-dimensional kernel space. The outlier map
is illustrated on 4 biological examples showing its use in exploratory data
analysis.
"
"  A common situation in filtering where classical Kalman filtering does not
perform particularly well is tracking in the presence of propagating outliers.
This calls for robustness understood in a distributional sense, i.e.; we
enlarge the distribution assumptions made in the ideal model by suitable
neighborhoods. Based on optimality results for distributional-robust Kalman
filtering from Ruckdeschel[01,10], we propose new robust recursive filters and
smoothers designed for this purpose as well as specialized versions for
non-propagating outliers. We apply these procedures in the context of a GPS
problem arising in the car industry. To better understand these filters, we
study their behavior at stylized outlier patterns (for which they are not
designed) and compare them to other approaches for the tracking problem.
Finally, in a simulation study we discuss efficiency of our procedures in
comparison to competitors.
"
"  We study the problem of estimating a manifold from random samples. In
particular, we consider piecewise constant and piecewise linear estimators
induced by k-means and k-flats, and analyze their performance. We extend
previous results for k-means in two separate directions. First, we provide new
results for k-means reconstruction on manifolds and, secondly, we prove
reconstruction bounds for higher-order approximation (k-flats), for which no
known results were previously available. While the results for k-means are
novel, some of the technical tools are well-established in the literature. In
the case of k-flats, both the results and the mathematical tools are new.
"
"  Data analysis and data mining are concerned with unsupervised pattern finding
and structure determination in data sets. ""Structure"" can be understood as
symmetry and a range of symmetries are expressed by hierarchy. Such symmetries
directly point to invariants, that pinpoint intrinsic properties of the data
and of the background empirical domain of interest. We review many aspects of
hierarchy here, including ultrametric topology, generalized ultrametric,
linkages with lattices and other discrete algebraic structures and with p-adic
number representations. By focusing on symmetries in data we have a powerful
means of structuring and analyzing massive, high dimensional data stores. We
illustrate the powerfulness of hierarchical clustering in case studies in
chemistry and finance, and we provide pointers to other published case studies.
"
"  Instead of the 'bag-of-words' representation, in the quantitative profile
approach to spam filtering and email categorization, an email is represented by
an m-dimensional vector of numbers, with m fixed in advance. Inspired by Sroufe
et al. [Sroufe, P., Phithakkitnukoon, S., Dantu, R., and Cangussu, J. (2010).
Email shape analysis. In \emph{LNCS}, 5935, pp. 18-29] two instances of
quantitative profiles are considered: line profile and character profile.
Performance of these profiles is studied on the TREC 2007, CEAS 2008 and a
private corpuses. At low computational costs, the two quantitative profiles
achieve performance that is at least comparable to that of heuristic rules and
naive Bayes.
"
"  We show how a retailer can estimate the optimal price of a new product using
observed transaction prices from online second-price auction experiments. For
this purpose we propose a Bayesian P\'olya tree approach which, given the
limited nature of the data, requires a specially tailored implementation.
Avoiding the need for a priori parametric assumptions, the P\'olya tree
approach allows for flexible inference of the valuation distribution, leading
to more robust estimation of optimal price than competing parametric
approaches. In collaboration with an online jewelry retailer, we illustrate how
our methodology can be combined with managerial prior knowledge to estimate the
profit maximizing price of a new jewelry product.
"
"  We propose a nonparametric approach to link prediction in large-scale dynamic
networks. Our model uses graph-based features of pairs of nodes as well as
those of their local neighborhoods to predict whether those nodes will be
linked at each time step. The model allows for different types of evolution in
different parts of the graph (e.g, growing or shrinking communities). We focus
on large-scale graphs and present an implementation of our model that makes use
of locality-sensitive hashing to allow it to be scaled to large problems.
Experiments with simulated data as well as five real-world dynamic graphs show
that we outperform the state of the art, especially when sharp fluctuations or
nonlinearities are present. We also establish theoretical properties of our
estimator, in particular consistency and weak convergence, the latter making
use of an elaboration of Stein's method for dependency graphs.
"
"  Wikipedia (WP) as a collaborative, dynamical system of humans is an
appropriate subject of social studies. Each single action of the members of
this society, i.e. editors, is well recorded and accessible. Using the
cumulative data of 34 Wikipedias in different languages, we try to characterize
and find the universalities and differences in temporal activity patterns of
editors. Based on this data, we estimate the geographical distribution of
editors for each WP in the globe. Furthermore we also clarify the differences
among different groups of WPs, which originate in the variance of cultural and
social features of the communities of editors.
"
"  For two or more classes (or types) of points, nearest neighbor contingency
tables (NNCTs) are constructed using nearest neighbor (NN) frequencies and are
used in testing spatial segregation of the classes. Pielou's test of
independence, Dixon's cell-specific, class-specific, and overall tests are the
tests based on NNCTs (i.e., they are NNCT-tests). These tests are designed and
intended for use under the null pattern of random labeling (RL) of completely
mapped data. However, it has been shown that Pielou's test is not appropriate
for testing segregation against the RL pattern while Dixon's tests are. In this
article, we compare Pielou's and Dixon's NNCT-tests; introduce the one-sided
versions of Pielou's test; extend the use of NNCT-tests for testing complete
spatial randomness (CSR) of points from two or more classes (which is called
\emph{CSR independence}, henceforth). We assess the finite sample performance
of the tests by an extensive Monte Carlo simulation study and demonstrate that
Dixon's tests are also appropriate for testing CSR independence; but Pielou's
test and the corresponding one-sided versions are liberal for testing CSR
independence or RL. Furthermore, we show that Pielou's tests are only
appropriate when the NNCT is based on a random sample of (base, NN) pairs. We
also prove the consistency of the tests under their appropriate null
hypotheses. Moreover, we investigate the edge (or boundary) effects on the
NNCT-tests and compare the buffer zone and toroidal edge correction methods for
these tests. We illustrate the tests on a real life and an artificial data set.
"
"  User authentication and intrusion detection differ from standard
classification problems in that while we have data generated from legitimate
users, impostor or intrusion data is scarce or non-existent. We review existing
techniques for dealing with this problem and propose a novel alternative based
on a principled statistical decision-making view point. We examine the
technique on a toy problem and validate it on complex real-world data from an
RFID based access control system. The results indicate that it can
significantly outperform the classical world model approach. The method could
be more generally useful in other decision-making scenarios where there is a
lack of adversary data.
"
"  The asymptotic behavior of estimates and information criteria in linear
models are studied in the context of hierarchically correlated sampling units.
The work is motivated by biological data collected on species where
autocorrelation is based on the species' genealogical tree. Hierarchical
autocorrelation is also found in many other kinds of data, such as from
microarray experiments or human languages. Similar correlation also arises in
ANOVA models with nested effects. I show that the best linear unbiased
estimators are almost surely convergent but may not be consistent for some
parameters such as the intercept and lineage effects, in the context of
Brownian motion evolution on the genealogical tree. For the purpose of model
selection I show that the usual BIC does not provide an appropriate
approximation to the posterior probability of a model. To correct for this, an
effective sample size is introduced for parameters that are inconsistently
estimated. For biological studies, this work implies that tree-aware sampling
design is desirable; adding more sampling units may not help ancestral
reconstruction and only strong lineage effects may be detected with high power.
"
"  Oriental ink painting, called Sumi-e, is one of the most appealing painting
styles that has attracted artists around the world. Major challenges in
computer-based Sumi-e simulation are to abstract complex scene information and
draw smooth and natural brush strokes. To automatically find such strokes, we
propose to model the brush as a reinforcement learning agent, and learn desired
brush-trajectories by maximizing the sum of rewards in the policy search
framework. We also provide elaborate design of actions, states, and rewards
tailored for a Sumi-e agent. The effectiveness of our proposed approach is
demonstrated through simulated Sumi-e experiments.
"
"  Motivated by the unceasing interest in hidden Markov models (HMMs), this
paper re-examines hidden path inference in these models, using primarily a
risk-based framework. While the most common maximum a posteriori (MAP), or
Viterbi, path estimator and the minimum error, or Posterior Decoder (PD), have
long been around, other path estimators, or decoders, have been either only
hinted at or applied more recently and in dedicated applications generally
unfamiliar to the statistical learning community. Over a decade ago, however, a
family of algorithmically defined decoders aiming to hybridize the two standard
ones was proposed (Brushe et al., 1998). The present paper gives a careful
analysis of this hybridization approach, identifies several problems and issues
with it and other previously proposed approaches, and proposes practical
resolutions of those. Furthermore, simple modifications of the classical
criteria for hidden path recognition are shown to lead to a new class of
decoders. Dynamic programming algorithms to compute these decoders in the usual
forward-backward manner are presented. A particularly interesting subclass of
such estimators can be also viewed as hybrids of the MAP and PD estimators.
Similar to previously proposed MAP-PD hybrids, the new class is parameterized
by a small number of tunable parameters. Unlike their algorithmic predecessors,
the new risk-based decoders are more clearly interpretable, and, most
importantly, work ""out of the box"" in practice, which is demonstrated on some
real bioinformatics tasks and data. Some further generalizations and
applications are discussed in conclusion.
"
"  Gene regulatory networks are collections of genes that interact with one
other and with other substances in the cell. By measuring gene expression over
time using high-throughput technologies, it may be possible to reverse
engineer, or infer, the structure of the gene network involved in a particular
cellular process. These gene expression data typically have a high
dimensionality and a limited number of biological replicates and time points.
Due to these issues and the complexity of biological systems, the problem of
reverse engineering networks from gene expression data demands a specialized
suite of statistical tools and methodologies. We propose a non-standard
adaptation of a simulation-based approach known as Approximate Bayesian
Computing based on Markov chain Monte Carlo sampling. This approach is
particularly well suited for the inference of gene regulatory networks from
longitudinal data. The performance of this approach is investigated via
simulations and using longitudinal expression data from a genetic repair system
in Escherichia coli.
"
"  Association testing aims to discover the underlying relationship between
genotypes (usually Single Nucleotide Polymorphisms, or SNPs) and phenotypes
(attributes, or traits). The typically large data sets used in association
testing often contain missing values. Standard statistical methods either
impute the missing values using relatively simple assumptions, or delete them,
or both, which can generate biased results. Here we describe the Bayesian
hierarchical model BAMD (Bayesian Association with Missing Data). BAMD is a
Gibbs sampler, in which missing values are multiply imputed based upon all of
the available information in the data set. We estimate the parameters and prove
that updating one SNP at each iteration preserves the ergodic property of the
Markov chain, and at the same time improves computational speed. We also
implement a model selection option in BAMD, which enables potential detection
of SNP interactions. Simulations show that unbiased estimates of SNP effects
are recovered with missing genotype data. Also, we validate associations
between SNPs and a carbon isotope discrimination phenotype that were previously
reported using a family based method, and discover an additional SNP associated
with the trait. BAMD is available as an R-package from
http://cran.r-project.org/package=BAMD
"
"  Multi-target tracking is mainly challenged by the nonlinearity present in the
measurement equation, and the difficulty in fast and accurate data association.
To overcome these challenges, the present paper introduces a grid-based model
in which the state captures target signal strengths on a known spatial grid
(TSSG). This model leads to \emph{linear} state and measurement equations,
which bypass data association and can afford state estimation via
sparsity-aware Kalman filtering (KF). Leveraging the grid-induced sparsity of
the novel model, two types of sparsity-cognizant TSSG-KF trackers are
developed: one effects sparsity through $\ell_1$-norm regularization, and the
other invokes sparsity as an extra measurement. Iterative extended KF and
Gauss-Newton algorithms are developed for reduced-complexity tracking, along
with accurate error covariance updates for assessing performance of the
resultant sparsity-aware state estimators. Based on TSSG state estimates, more
informative target position and track estimates can be obtained in a follow-up
step, ensuring that track association and position estimation errors do not
propagate back into TSSG state estimates. The novel TSSG trackers do not
require knowing the number of targets or their signal strengths, and exhibit
considerably lower complexity than the benchmark hidden Markov model filter,
especially for a large number of targets. Numerical simulations demonstrate
that sparsity-cognizant trackers enjoy improved root mean-square error
performance at reduced complexity when compared to their sparsity-agnostic
counterparts.
"
"  In this paper, I construct a new test of conditional moment inequalities,
which is based on studentized kernel estimates of moment functions with many
different values of the bandwidth parameter. The test automatically adapts to
the unknown smoothness of moment functions and has uniformly correct asymptotic
size. The test has high power in a large class of models with conditional
moment inequalities. Some existing tests have nontrivial power against
n^{-1/2}-local alternatives in a certain class of these models whereas my
method only allows for nontrivial testing against (n/\log n)^{-1/2}-local
alternatives in this class. There exist, however, other classes of models with
conditional moment inequalities where the mentioned tests have much lower power
in comparison with the test developed in this paper.
"
"  Design of experiments is a branch of statistics that aims to identify
efficient procedures for planning experiments in order to optimize knowledge
discovery. Network inference is a subfield of systems biology devoted to the
identification of biochemical networks from experimental data. Common to both
areas of research is their focus on the maximization of information gathered
from experimentation. The goal of this paper is to establish a connection
between these two areas coming from the common use of polynomial models and
techniques from computational algebra.
"
"  Omitted variable bias can affect treatment effect estimates obtained from
observational data due to the lack of random assignment to treatment groups.
Sensitivity analyses adjust these estimates to quantify the impact of potential
omitted variables. This paper presents methods of sensitivity analysis to
adjust interval estimates of treatment effect---both the point estimate and
standard error---obtained using multiple linear regression. Central to our
approach is what we term benchmarking, the use of data to establish reference
points for speculation about omitted confounders. The method adapts to
treatment effects that may differ by subgroup, to scenarios involving omission
of multiple variables, and to combinations of covariance adjustment with
propensity score stratification. We illustrate it using data from an
influential study of health outcomes of patients admitted to critical care.
"
"  We present a method for the reconstruction of networks, based on the order of
nodes visited by a stochastic branching process. Our algorithm reconstructs a
network of minimal size that ensures consistency with the data. Crucially, we
show that global consistency with the data can be achieved through purely local
considerations, inferring the neighbourhood of each node in turn. The
optimisation problem solved for each individual node can be reduced to a Set
Covering Problem, which is known to be NP-hard but can be approximated well in
practice. We then extend our approach to account for noisy data, based on the
Minimum Description Length principle. We demonstrate our algorithms on
synthetic data, generated by an SIR-like epidemiological model.
"
"  We propose an enhanced peer-review process where the reviewers are encouraged
to truthfully disclose their reviews. We start by modelling that process using
a Bayesian model where the uncertainty regarding the quality of the manuscript
is taken into account. After that, we introduce a scoring function to evaluate
the reported reviews. Under mild assumptions, we show that reviewers strictly
maximize their expected scores by telling the truth. We also show how those
scores can be used in order to reach consensus.
"
"  Observational time series data often exhibit both cyclic temporal trends and
autocorrelation and may also depend on covariates. As such, there is a need for
flexible regression models that are able to capture these trends and model any
residual autocorrelation simultaneously. Modelling the autocorrelation in the
residuals leads to more realistic forecasts than an assumption of independence.
In this paper we propose a method which combines spline-based semi-parametric
regression modelling with the modelling of auto-regressive errors.
  The method is applied to a simulated data set in order to show its efficacy
and to ultrafine particle number concentration in Helsinki, Finland, to show
its use in real world problems.
"
"  We critique the analysis by A. Feuerverger of an archaeological find
[arXiv:0804.0079] that has been alleged by some to be the tomb of Jesus of
Nazareth. We show that his analysis rests on six faulty assumptions that have
been severely criticized by historians, archaeologists, and scholars in related
disciplines. We summarize the results of an alternative computation using
Bayes' theorem that estimates a probability of less than 2% that the Talpiot
tomb belongs to Jesus of Nazareth.
"
"  We address the issue of knots selection for Gaussian predictive process
methodology. Predictive process approximation provides an effective solution to
the cubic order computational complexity of Gaussian process models. This
approximation crucially depends on a set of points, called knots, at which the
original process is retained, while the rest is approximated via a
deterministic extrapolation. Knots should be few in number to keep the
computational complexity low, but provide a good coverage of the process domain
to limit approximation error. We present theoretical calculations to show that
coverage must be judged by the canonical metric of the Gaussian process. This
necessitates having in place a knots selection algorithm that automatically
adapts to the changes in the canonical metric affected by changes in the
parameter values controlling the Gaussian process covariance function. We
present an algorithm toward this by employing an incomplete Cholesky
factorization with pivoting and dynamic stopping. Although these concepts
already exist in the literature, our contribution lies in unifying them into a
fast algorithm and in using computable error bounds to finesse implementation
of the predictive process approximation. The resulting adaptive predictive
process offers a substantial automatization of Guassian process model fitting,
especially for Bayesian applications where thousands of values of the
covariance parameters are to be explored.
"
"  Direction of arrival (DOA) estimation is a classical problem in signal
processing with many practical applications. Its research has recently been
advanced owing to the development of methods based on sparse signal
reconstruction. While these methods have shown advantages over conventional
ones, there are still difficulties in practical situations where true DOAs are
not on the discretized sampling grid. To deal with such an off-grid DOA
estimation problem, this paper studies an off-grid model that takes into
account effects of the off-grid DOAs and has a smaller modeling error. An
iterative algorithm is developed based on the off-grid model from a Bayesian
perspective while joint sparsity among different snapshots is exploited by
assuming a Laplace prior for signals at all snapshots. The new approach applies
to both single snapshot and multi-snapshot cases. Numerical simulations show
that the proposed algorithm has improved accuracy in terms of mean squared
estimation error. The algorithm can maintain high estimation accuracy even
under a very coarse sampling grid.
"
"  In this paper we present a fully Bayesian latent variable model which
exploits conditional nonlinear(in)-dependence structures to learn an efficient
latent representation. The latent space is factorized to represent shared and
private information from multiple views of the data. In contrast to previous
approaches, we introduce a relaxation to the discrete segmentation and allow
for a ""softly"" shared latent space. Further, Bayesian techniques allow us to
automatically estimate the dimensionality of the latent spaces. The model is
capable of capturing structure underlying extremely high dimensional spaces.
This is illustrated by modelling unprocessed images with tenths of thousands of
pixels. This also allows us to directly generate novel images from the trained
model by sampling from the discovered latent spaces. We also demonstrate the
model by prediction of human pose in an ambiguous setting. Our Bayesian
framework allows us to perform disambiguation in a principled manner by
including latent space priors which incorporate the dynamic nature of the data.
"
"  We examine the problem of learning a probabilistic model for melody directly
from musical sequences belonging to the same genre. This is a challenging task
as one needs to capture not only the rich temporal structure evident in music,
but also the complex statistical dependencies among different music components.
To address this problem we introduce the Variable-gram Topic Model, which
couples the latent topic formalism with a systematic model for contextual
information. We evaluate the model on next-step prediction. Additionally, we
present a novel way of model evaluation, where we directly compare model
samples with data sequences using the Maximum Mean Discrepancy of string
kernels, to assess how close is the model distribution to the data
distribution. We show that the model has the highest performance under both
evaluation measures when compared to LDA, the Topic Bigram and related
non-topic models.
"
"  Recently, sparsity-based algorithms are proposed for super-resolution
spectrum estimation. However, to achieve adequately high resolution in
real-world signal analysis, the dictionary atoms have to be close to each other
in frequency, thereby resulting in a coherent design. The popular convex
compressed sensing methods break down in presence of high coherence and large
noise. We propose a new regularization approach to handle model collinearity
and obtain parsimonious frequency selection simultaneously. It takes advantage
of the pairing structure of sine and cosine atoms in the frequency dictionary.
A probabilistic spectrum screening is also developed for fast computation in
high dimensions. A data-resampling version of high-dimensional Bayesian
Information Criterion is used to determine the regularization parameters.
Experiments show the efficacy and efficiency of the proposed algorithms in
challenging situations with small sample size, high frequency resolution, and
low signal-to-noise ratio.
"
"  Diffusion tensor imaging (DTI) is a novel modality of magnetic resonance
imaging that allows noninvasive mapping of the brain's white matter. A
particular map derived from DTI measurements is a map of water principal
diffusion directions, which are proxies for neural fiber directions. We
consider a study in which diffusion direction maps were acquired for two groups
of subjects. The objective of the analysis is to find regions of the brain in
which the corresponding diffusion directions differ between the groups. This is
attained by first computing a test statistic for the difference in direction at
every brain location using a Watson model for directional data. Interesting
locations are subsequently selected with control of the false discovery rate.
More accurate modeling of the null distribution is obtained using an empirical
null density based on the empirical distribution of the test statistics across
the brain. Further, substantial improvements in power are achieved by local
spatial averaging of the test statistic map. Although the focus is on one
particular study and imaging technology, the proposed inference methods can be
applied to other large scale simultaneous hypothesis testing problems with a
continuous underlying spatial structure.
"
"  We consider the problem of classification when inputs correspond to sets of
vectors. This setting occurs in many problems such as the classification of
pieces of mail containing several pages, of web sites with several sections or
of images that have been pre-segmented into smaller regions. We propose
generalizations of the restricted Boltzmann machine (RBM) that are appropriate
in this context and explore how to incorporate different assumptions about the
relationship between the input sets and the target class within the RBM. In
experiments on standard multiple-instance learning datasets, we demonstrate the
competitiveness of approaches based on RBMs and apply the proposed variants to
the problem of incoming mail classification.
"
"  We consider the detection of activations over graphs under Gaussian noise,
where signals are piece-wise constant over the graph. Despite the wide
applicability of such a detection algorithm, there has been little success in
the development of computationally feasible methods with proveable theoretical
guarantees for general graph topologies. We cast this as a hypothesis testing
problem, and first provide a universal necessary condition for asymptotic
distinguishability of the null and alternative hypotheses. We then introduce
the spanning tree wavelet basis over graphs, a localized basis that reflects
the topology of the graph, and prove that for any spanning tree, this approach
can distinguish null from alternative in a low signal-to-noise regime. Lastly,
we improve on this result and show that using the uniform spanning tree in the
basis construction yields a randomized test with stronger theoretical
guarantees that in many cases matches our necessary conditions. Specifically,
we obtain near-optimal performance in edge transitive graphs, $k$-nearest
neighbor graphs, and $\epsilon$-graphs.
"
"  The performance of Orthogonal Matching Pursuit (OMP) for variable selection
is analyzed for random designs. When contrasted with the deterministic case,
since the performance is here measured after averaging over the distribution of
the design matrix, one can have far less stringent sparsity constraints on the
coefficient vector. We demonstrate that for exact sparse vectors, the
performance of the OMP is similar to known results on the Lasso algorithm
[\textit{IEEE Trans. Inform. Theory} \textbf{55} (2009) 2183--2202]. Moreover,
variable selection under a more relaxed sparsity assumption on the coefficient
vector, whereby one has only control on the $\ell_1$ norm of the smaller
coefficients, is also analyzed. As a consequence of these results, we also show
that the coefficient estimate satisfies strong oracle type inequalities.
"
"  Discovering causal relationships is a hard task, often hindered by the need
for intervention, and often requiring large amounts of data to resolve
statistical uncertainty. However, humans quickly arrive at useful causal
relationships. One possible reason is that humans extrapolate from past
experience to new, unseen situations: that is, they encode beliefs over causal
invariances, allowing for sound generalization from the observations they
obtain from directly acting in the world.
  Here we outline a Bayesian model of causal induction where beliefs over
competing causal hypotheses are modeled using probability trees. Based on this
model, we illustrate why, in the general case, we need interventions plus
constraints on our causal hypotheses in order to extract causal information
from our experience.
"
"  In many instances, information on engineering systems can be obtained through
measurements, monitoring or direct observations of system performances and can
be used to update the system reliability estimate. In structural reliability
analysis, such information is expressed either by inequalities (e.g. for the
observation that no defect is present) or by equalities (e.g. for quantitative
measurements of system characteristics). When information Z is of the equality
type, the a-priori probability of Z is zero and most structural reliability
methods (SRM) are not directly applicable to the computation of the updated
reliability. Hitherto, the computation of the reliability of engineering
systems conditional on equality information was performed through first- and
second order approximations. In this paper, it is shown how equality
information can be transformed into inequality information, which enables
reliability updating by solving a standard structural system reliability
problem. This approach enables the use of any SRM, including those based on
simulation, for reliability updating with equality information. It is
demonstrated on three numerical examples, including an application to fatigue
reliability.
"
"  Online learning algorithms are designed to learn even when their input is
generated by an adversary. The widely-accepted formal definition of an online
algorithm's ability to learn is the game-theoretic notion of regret. We argue
that the standard definition of regret becomes inadequate if the adversary is
allowed to adapt to the online algorithm's actions. We define the alternative
notion of policy regret, which attempts to provide a more meaningful way to
measure an online algorithm's performance against adaptive adversaries.
Focusing on the online bandit setting, we show that no bandit algorithm can
guarantee a sublinear policy regret against an adaptive adversary with
unbounded memory. On the other hand, if the adversary's memory is bounded, we
present a general technique that converts any bandit algorithm with a sublinear
regret bound into an algorithm with a sublinear policy regret bound. We extend
this result to other variants of regret, such as switching regret, internal
regret, and swap regret.
"
"  We show that the existence of a computationally efficient calibration
algorithm, with a low weak calibration rate, would imply the existence of an
efficient algorithm for computing approximate Nash equilibria - thus implying
the unlikely conclusion that every problem in PPAD is solvable in polynomial
time.
"
"  Inferring the functional specificity of brain regions from functional
Magnetic Resonance Images (fMRI) data is a challenging statistical problem.
While the General Linear Model (GLM) remains the standard approach for brain
mapping, supervised learning techniques (a.k.a.} decoding) have proven to be
useful to capture multivariate statistical effects distributed across voxels
and brain regions. Up to now, much effort has been made to improve decoding by
incorporating prior knowledge in the form of a particular regularization term.
In this paper we demonstrate that further improvement can be made by accounting
for non-linearities using a ranking approach rather than the commonly used
least-square regression. Through simulation, we compare the recovery properties
of our approach to linear models commonly used in fMRI based decoding. We
demonstrate the superiority of ranking with a real fMRI dataset.
"
"  We consider the problem of learning classifiers for labeled data that has
been distributed across several nodes. Our goal is to find a single classifier,
with small approximation error, across all datasets while minimizing the
communication between nodes. This setting models real-world communication
bottlenecks in the processing of massive distributed datasets. We present
several very general sampling-based solutions as well as some two-way protocols
which have a provable exponential speed-up over any one-way protocol. We focus
on core problems for noiseless data distributed across two or more nodes. The
techniques we introduce are reminiscent of active learning, but rather than
actively probing labels, nodes actively communicate with each other, each node
simultaneously learning the important data from another node.
"
"  We develop dependent hierarchical normalized random measures and apply them
to dynamic topic modeling. The dependency arises via superposition, subsampling
and point transition on the underlying Poisson processes of these measures. The
measures used include normalised generalised Gamma processes that demonstrate
power law properties, unlike Dirichlet processes used previously in dynamic
topic modeling. Inference for the model includes adapting a recently developed
slice sampler to directly manipulate the underlying Poisson process.
Experiments performed on news, blogs, academic and Twitter collections
demonstrate the technique gives superior perplexity over a number of previous
models.
"
"  CD8 T cells are specialized immune cells that play an important role in the
regulation of antiviral immune response and the generation of protective
immunity. In this paper we investigate the differentiation of memory CD8 T
cells in the immune response using a short time course microarray experiment.
Structurally, this experiment is similar to many in that it involves
measurements taken on independent samples, in one biological group, at a small
number of irregularly spaced time points, and exhibiting patterns of temporal
nonstationarity. To analyze this CD8 T-cell experiment, we develop a
hierarchical state space model so that we can: (1) detect temporally
differentially expressed genes, (2) identify the direction of successive
changes over time, and (3) assess the magnitude of successive changes over
time. We incorporate hidden Markov models into our model to utilize the
information embedded in the time series and set up the proposed hierarchical
state space model in an empirical Bayes framework to utilize the population
information from the large-scale data. Analysis of the CD8 T-cell experiment
using the proposed model results in biologically meaningful findings. Temporal
patterns involved in the differentiation of memory CD8 T cells are summarized
separately and performance of the proposed model is illustrated in a simulation
study.
"
"  HIV dynamic studies have contributed significantly to the understanding of
HIV pathogenesis and antiviral treatment strategies for AIDS patients.
Establishing the relationship of virologic responses with clinical factors and
covariates during long-term antiretroviral (ARV) therapy is important to the
development of effective treatments. Medication adherence is an important
predictor of the effectiveness of ARV treatment, but an appropriate determinant
of adherence rate based on medication event monitoring system (MEMS) data is
critical to predict virologic outcomes. The primary objective of this paper is
to investigate the effects of a number of summary determinants of MEMS
adherence rates on virologic response measured repeatedly over time in
HIV-infected patients. We developed a mechanism-based differential equation
model with consideration of drug adherence, interacted by virus susceptibility
to drug and baseline characteristics, to characterize the long-term virologic
responses after initiation of therapy. This model fully integrates viral load,
MEMS adherence, drug resistance and baseline covariates into the data analysis.
In this study we employed the proposed model and associated Bayesian nonlinear
mixed-effects modeling approach to assess how to efficiently use the MEMS
adherence data for prediction of virologic response, and to evaluate the
predicting power of each summary metric of the MEMS adherence rates.
"
"  The original contributions of this paper are twofold: a new understanding of
the influence of noise on the eigenvectors of the graph Laplacian of a set of
image patches, and an algorithm to estimate a denoised set of patches from a
noisy image. The algorithm relies on the following two observations: (1) the
low-index eigenvectors of the diffusion, or graph Laplacian, operators are very
robust to random perturbations of the weights and random changes in the
connections of the patch-graph; and (2) patches extracted from smooth regions
of the image are organized along smooth low-dimensional structures in the
patch-set, and therefore can be reconstructed with few eigenvectors.
Experiments demonstrate that our denoising algorithm outperforms the denoising
gold-standards.
"
"  We develop statistically based methods to detect single nucleotide DNA
mutations in next generation sequencing data. Sequencing generates counts of
the number of times each base was observed at hundreds of thousands to billions
of genome positions in each sample. Using these counts to detect mutations is
challenging because mutations may have very low prevalence and sequencing error
rates vary dramatically by genome position. The discreteness of sequencing data
also creates a difficult multiple testing problem: current false discovery rate
methods are designed for continuous data, and work poorly, if at all, on
discrete data. We show that a simple randomization technique lets us use
continuous false discovery rate methods on discrete data. Our approach is a
useful way to estimate false discovery rates for any collection of discrete
test statistics, and is hence not limited to sequencing data. We then use an
empirical Bayes model to capture different sources of variation in sequencing
error rates. The resulting method outperforms existing detection approaches on
example data sets.
"
"  We propose a novel algorithm to solve the expectation propagation relaxation
of Bayesian inference for continuous-variable graphical models. In contrast to
most previous algorithms, our method is provably convergent. By marrying
convergent EP ideas from (Opper&Winther 05) with covariance decoupling
techniques (Wipf&Nagarajan 08, Nickisch&Seeger 09), it runs at least an order
of magnitude faster than the most commonly used EP solver.
"
"  The goal of temporal alignment is to establish time correspondence between
two sequences, which has many applications in a variety of areas such as speech
processing, bioinformatics, computer vision, and computer graphics. In this
paper, we propose a novel temporal alignment method called least-squares
dynamic time warping (LSDTW). LSDTW finds an alignment that maximizes
statistical dependency between sequences, measured by a squared-loss variant of
mutual information. The benefit of this novel information-theoretic formulation
is that LSDTW can align sequences with different lengths, different
dimensionality, high non-linearity, and non-Gaussianity in a computationally
efficient manner. In addition, model parameters such as an initial alignment
matrix can be systematically optimized by cross-validation. We demonstrate the
usefulness of LSDTW through experiments on synthetic and real-world Kinect
action recognition datasets.
"
"  In this paper we consider the problem of Gaussian process classifier (GPC)
model selection with different Leave-One-Out (LOO) Cross Validation (CV) based
optimization criteria and provide a practical algorithm using LOO predictive
distributions with such criteria to select hyperparameters. Apart from the
standard average negative logarithm of predictive probability (NLP), we also
consider smoothed versions of criteria such as F-measure and Weighted Error
Rate (WER), which are useful for handling imbalanced data. Unlike the
regression case, LOO predictive distributions for the classifier case are
intractable. We use approximate LOO predictive distributions arrived from
Expectation Propagation (EP) approximation. We conduct experiments on several
real world benchmark datasets. When the NLP criterion is used for optimizing
the hyperparameters, the predictive approaches show better or comparable NLP
generalization performance with existing GPC approaches. On the other hand,
when the F-measure criterion is used, the F-measure generalization performance
improves significantly on several datasets. Overall, the EP-based predictive
algorithm comes out as an excellent choice for GP classifier model selection
with different optimization criteria.
"
"  A comparative study of the application of Gaussian Mixture Model (GMM) and
Radial Basis Function (RBF) in biometric recognition of voice has been carried
out and presented. The application of machine learning techniques to biometric
authentication and recognition problems has gained a widespread acceptance. In
this research, a GMM model was trained, using Expectation Maximization (EM)
algorithm, on a dataset containing 10 classes of vowels and the model was used
to predict the appropriate classes using a validation dataset. For experimental
validity, the model was compared to the performance of two different versions
of RBF model using the same learning and validation datasets. The results
showed very close recognition accuracy between the GMM and the standard RBF
model, but with GMM performing better than the standard RBF by less than 1% and
the two models outperformed similar models reported in literature. The DTREG
version of RBF outperformed the other two models by producing 94.8% recognition
accuracy. In terms of recognition time, the standard RBF was found to be the
fastest among the three models.
"
"  Scalability of statistical estimators is of increasing importance in modern
applications and dimension reduction is often used to extract relevant
information from data. A variety of popular dimension reduction approaches can
be framed as symmetric generalized eigendecomposition problems. In this paper
we outline how taking into account the low rank structure assumption implicit
in these dimension reduction approaches provides both computational and
statistical advantages. We adapt recent randomized low-rank approximation
algorithms to provide efficient solutions to three dimension reduction methods:
Principal Component Analysis (PCA), Sliced Inverse Regression (SIR), and
Localized Sliced Inverse Regression (LSIR). A key observation in this paper is
that randomization serves a dual role, improving both computational and
statistical performance. This point is highlighted in our experiments on real
and simulated data.
"
"  We compute exact values respectively bounds of ""distances"" - in the sense of
(transforms of) power divergences and relative entropy - between two
discrete-time Galton-Watson branching processes with immigration GWI for which
the offspring as well as the immigration is arbitrarily Poisson-distributed
(leading to arbitrary type of criticality). Implications for asymptotic
distinguishability behaviour in terms of contiguity and entire separation of
the involved GWI are given, too. Furthermore, we determine the corresponding
limit quantities for the context in which the two GWI converge to Feller-type
branching diffusion processes, as the time-lags between observations tend to
zero. Some applications to (static random environment like) Bayesian decision
making and Neyman-Pearson testing are presented as well.
"
"  Bayesian Inference is a powerful approach to data analysis that is based
almost entirely on probability theory. In this approach, probabilities model
{\it uncertainty} rather than randomness or variability. This thesis is
composed of a series of papers that have been published in various astronomical
journals during the years 2005-2008. The unifying thread running through the
papers is the use of Bayesian Inference to solve underdetermined inverse
problems in astrophysics. Firstly, a methodology is developed to solve a
question in gravitational lens inversion - using the observed images of
gravitational lens systems to reconstruct the undistorted source profile and
the mass profile of the lensing galaxy. A similar technique is also applied to
the task of inferring the number and frequency of modes of oscillation of a
star from the time series observations that are used in the field of
asteroseismology. For these complex problems, many of the required calculations
cannot be done analytically, and so Markov Chain Monte Carlo algorithms have
been used. Finally, probabilistic reasoning is applied to a controversial
question in astrobiology: does the fact that life formed quite soon after the
Earth constitute evidence that the formation of life is quite probable, given
the right macroscopic conditions?
"
"  We consider the restless Markov bandit problem, in which the state of each
arm evolves according to a Markov process independently of the learner's
actions. We suggest an algorithm that after $T$ steps achieves
$\tilde{O}(\sqrt{T})$ regret with respect to the best policy that knows the
distributions of all arms. No assumptions on the Markov chains are made except
that they are irreducible. In addition, we show that index-based policies are
necessarily suboptimal for the considered problem.
"
"  Astrometric surveys provide the opportunity to measure the absolute
magnitudes of large numbers of stars, but only if the individual line-of-sight
extinctions are known. Unfortunately, extinction is highly degenerate with
stellar effective temperature when estimated from broad band optical/infrared
photometry. To address this problem, I introduce a Bayesian method for
estimating the intrinsic parameters of a star and its line-of-sight extinction.
It uses both photometry and parallaxes in a self-consistent manner in order to
provide a non-parametric posterior probability distribution over the
parameters. The method makes explicit use of domain knowledge by employing the
Hertzsprung--Russell Diagram (HRD) to constrain solutions and to ensure that
they respect stellar physics. I first demonstrate this method by using it to
estimate effective temperature and extinction from BVJHK data for a set of
artificially reddened Hipparcos stars, for which accurate effective
temperatures have been estimated from high resolution spectroscopy. Using just
the four colours, we see the expected strong degeneracy (positive correlation)
between the temperature and extinction. Introducing the parallax, apparent
magnitude and the HRD reduces this degeneracy and improves both the precision
(reduces the error bars) and the accuracy of the parameter estimates, the
latter by about 35%. The resulting accuracy is about 200K in temperature and
0.2mag in extinction. I then apply the method to estimate these parameters and
absolute magnitudes for some 47000 F,G,K Hipparcos stars which have been
cross-matched with 2MASS. The method can easily be extended to incorporate the
estimation of other parameters, in particular metallicity and surface gravity,
making it particularly suitable for the analysis of the 10^9 stars from Gaia.
"
"  In this article, we derive concentration inequalities for the
cross-validation estimate of the generalization error for empirical risk
minimizers. In the general setting, we prove sanity-check bounds in the spirit
of \cite{KR99} \textquotedblleft\textit{bounds showing that the worst-case
error of this estimate is not much worse that of training error estimate}
\textquotedblright . General loss functions and class of predictors with finite
VC-dimension are considered. We closely follow the formalism introduced by
\cite{DUD03} to cover a large variety of cross-validation procedures including
leave-one-out cross-validation, $k$% -fold cross-validation, hold-out
cross-validation (or split sample), and the leave-$\upsilon$-out
cross-validation.
  In particular, we focus on proving the consistency of the various
cross-validation procedures. We point out the interest of each cross-validation
procedure in terms of rate of convergence. An estimation curve with transition
phases depending on the cross-validation procedure and not only on the
percentage of observations in the test sample gives a simple rule on how to
choose the cross-validation. An interesting consequence is that the size of the
test sample is not required to grow to infinity for the consistency of the
cross-validation procedure.
"
"  Drug discovery is the process of identifying compounds which have potentially
meaningful biological activity. A major challenge that arises is that the
number of compounds to search over can be quite large, sometimes numbering in
the millions, making experimental testing intractable. For this reason
computational methods are employed to filter out those compounds which do not
exhibit strong biological activity. This filtering step, also called virtual
screening reduces the search space, allowing for the remaining compounds to be
experimentally tested. In this paper we propose several novel approaches to the
problem of virtual screening based on Canonical Correlation Analysis (CCA) and
on a kernel-based extension. Spectral learning ideas motivate our proposed new
method called Indefinite Kernel CCA (IKCCA). We show the strong performance of
this approach both for a toy problem as well as using real world data with
dramatic improvements in predictive accuracy of virtual screening over an
existing methodology.
"
"  The aim of this study was to estimate the number of new cells and neurons
added to the dentate gyrus across the lifespan, and to compare the rate of
age-associated decline in neurogenesis across species. Data from mice (Mus
musculus), rats (Rattus norvegicus), lesser hedgehog tenrecs (Echinops
telfairi), macaques (Macaca mulatta), marmosets (Callithrix jacchus), tree
shrews (Tupaia belangeri), and humans (Homo sapiens) were extracted from twenty
one data sets published in fourteen different papers. ANOVA, exponential,
Weibull, and power models were fit to the data to determine which best
described the relationship between age and neurogenesis. Exponential models
provided a suitable fit and were used to estimate the relevant parameters. The
rate of decrease of neurogenesis correlated with species longevity r = 0.769, p
= 0.043), but not body mass or basal metabolic rate. Of all the cells added
postnatally to the mouse dentate gyrus, only 8.5% (95% CI = 1.0% to 14.7%) of
these will be added after middle age. In addition, only 5.7% (95% CI = 0.7% to
9.9%) of the existing cell population turns over from middle age onwards. Thus,
relatively few new cells are added for much of an animal's life, and only a
proportion of these will mature into functional neurons.
"
"  Spontaneous brain activity, as observed in functional neuroimaging, has been
shown to display reproducible structure that expresses brain architecture and
carries markers of brain pathologies. An important view of modern neuroscience
is that such large-scale structure of coherent activity reflects modularity
properties of brain connectivity graphs. However, to date, there has been no
demonstration that the limited and noisy data available in spontaneous activity
observations could be used to learn full-brain probabilistic models that
generalize to new data. Learning such models entails two main challenges: i)
modeling full brain connectivity is a difficult estimation problem that faces
the curse of dimensionality and ii) variability between subjects, coupled with
the variability of functional signals between experimental runs, makes the use
of multiple datasets challenging. We describe subject-level brain functional
connectivity structure as a multivariate Gaussian process and introduce a new
strategy to estimate it from group data, by imposing a common structure on the
graphical model in the population. We show that individual models learned from
functional Magnetic Resonance Imaging (fMRI) data using this population prior
generalize better to unseen data than models based on alternative
regularization schemes. To our knowledge, this is the first report of a
cross-validated model of spontaneous brain activity. Finally, we use the
estimated graphical model to explore the large-scale characteristics of
functional architecture and show for the first time that known cognitive
networks appear as the integrated communities of functional connectivity graph.
"
"  Graphical models use graphs to compactly capture stochastic dependencies
amongst a collection of random variables. Inference over graphical models
corresponds to finding marginal probability distributions given joint
probability distributions. In general, this is computationally intractable,
which has led to a quest for finding efficient approximate inference
algorithms. We propose a framework for generalized inference over graphical
models that can be used as a wrapper for improving the estimates of approximate
inference algorithms. Instead of applying an inference algorithm to the
original graph, we apply the inference algorithm to a block-graph, defined as a
graph in which the nodes are non-overlapping clusters of nodes from the
original graph. This results in marginal estimates of a cluster of nodes, which
we further marginalize to get the marginal estimates of each node. Our proposed
block-graph construction algorithm is simple, efficient, and motivated by the
observation that approximate inference is more accurate on graphs with longer
cycles. We present extensive numerical simulations that illustrate our
block-graph framework with a variety of inference algorithms (e.g., those in
the libDAI software package). These simulations show the improvements provided
by our framework.
"
"  This paper develops a theory for group Lasso using a concept called strong
group sparsity. Our result shows that group Lasso is superior to standard Lasso
for strongly group-sparse signals. This provides a convincing theoretical
justification for using group sparse regularization when the underlying group
structure is consistent with the data. Moreover, the theory predicts some
limitations of the group Lasso formulation that are confirmed by simulation
studies.
"
"  In the present article we derive an explicit expression for the trun- cated
mean and variance for the multivariate normal distribution with ar- bitrary
rectangular double truncation. We use the moment generating ap- proach of
Tallis (1961) and extend it to general {\mu}, {\Sigma} and all combinations of
truncation. As part of the solution we also give a formula for the bivari- ate
marginal density of truncated multinormal variates. We also prove an invariance
property of some elements of the inverse covariance after trunca- tion.
Computer algorithms for computing the truncated mean, variance and the
bivariate marginal probabilities for doubly truncated multivariate normal
variates have been written in R and are presented along with three examples.
"
"  We present a system that enables rapid model experimentation for tera-scale
machine learning with trillions of non-zero features, billions of training
examples, and millions of parameters. Our contribution to the literature is a
new method (SA L-BFGS) for changing batch L-BFGS to perform in near real-time
by using statistical tools to balance the contributions of previous weights,
old training examples, and new training examples to achieve fast convergence
with few iterations. The result is, to our knowledge, the most scalable and
flexible linear learning system reported in the literature, beating standard
practice with the current best system (Vowpal Wabbit and AllReduce). Using the
KDD Cup 2012 data set from Tencent, Inc. we provide experimental results to
verify the performance of this method.
"
"  The Sample Compression Conjecture of Littlestone & Warmuth has remained
unsolved for over two decades. This paper presents a systematic geometric
investigation of the compression of finite maximum concept classes. Simple
arrangements of hyperplanes in Hyperbolic space, and Piecewise-Linear
hyperplane arrangements, are shown to represent maximum classes, generalizing
the corresponding Euclidean result. A main result is that PL arrangements can
be swept by a moving hyperplane to unlabeled d-compress any finite maximum
class, forming a peeling scheme as conjectured by Kuzmin & Warmuth. A corollary
is that some d-maximal classes cannot be embedded into any maximum class of VC
dimension d+k, for any constant k. The construction of the PL sweeping involves
Pachner moves on the one-inclusion graph, corresponding to moves of a
hyperplane across the intersection of d other hyperplanes. This extends the
well known Pachner moves for triangulations to cubical complexes.
"
"  We present nonparametric techniques for constructing and verifying density
estimates from high-dimensional data whose irregular dependence structure
cannot be modelled by parametric multivariate distributions. A low-dimensional
representation of the data is critical in such situations because of the curse
of dimensionality. Our proposed methodology consists of three main parts: (1)
data reparameterization via dimensionality reduction, wherein the data are
mapped into a space where standard techniques can be used for density
estimation and simulation; (2) inverse mapping, in which simulated points are
mapped back to the high-dimensional input space; and (3) verification, in which
the quality of the estimate is assessed by comparing simulated samples with the
observed data. These approaches are illustrated via an exploration of the
spatial variability of tropical cyclones in the North Atlantic; each datum in
this case is an entire hurricane trajectory. We conclude the paper with a
discussion of extending the methods to model the relationship between TC
variability and climatic variables.
"
"  In this paper, we study two general classes of optimization algorithms for
kernel methods with convex loss function and quadratic norm regularization, and
analyze their convergence. The first approach, based on fixed-point iterations,
is simple to implement and analyze, and can be easily parallelized. The second,
based on coordinate descent, exploits the structure of additively separable
loss functions to compute solutions of line searches in closed form. Instances
of these general classes of algorithms are already incorporated into state of
the art machine learning software for large scale problems. We start from a
solution characterization of the regularized problem, obtained using
sub-differential calculus and resolvents of monotone operators, that holds for
general convex loss functions regardless of differentiability. The two
methodologies described in the paper can be regarded as instances of non-linear
Jacobi and Gauss-Seidel algorithms, and are both well-suited to solve large
scale problems.
"
"  This paper introduces a probability density estimator based on Green's
function identities. A density model is constructed under the sole assumption
that the probability density is differentiable. The method is implemented as a
binary likelihood estimator for classification purposes, so issues such as
mis-modeling and overtraining are also discussed. The identity behind the
density estimator can be interpreted as a real-valued, non-scalar kernel method
which is able to reconstruct differentiable density functions.
"
"  Our article considers the class of recently developed stochastic models that
combine claims payments and incurred losses information into a coherent
reserving methodology. In particular, we develop a family of Heirarchical
Bayesian Paid-Incurred-Claims models, combining the claims reserving models of
Hertig et al. (1985) and Gogol et al. (1993). In the process we extend the
independent log-normal model of Merz et al. (2010) by incorporating different
dependence structures using a Data-Augmented mixture Copula Paid-Incurred
claims model.
  The utility and influence of incorporating both payment and incurred losses
into estimating of the full predictive distribution of the outstanding loss
liabilities and the resulting reserves is demonstrated in the following cases:
(i) an independent payment (P) data model; (ii) the independent
Payment-Incurred Claims (PIC) data model of Merz et al. (2010); (iii) a novel
dependent lag-year telescoping block diagonal Gaussian Copula PIC data model
incorporating conjugacy via transformation; (iv) a novel data-augmented mixture
Archimedean copula dependent PIC data model.
  Inference in such models is developed via a class of adaptive Markov chain
Monte Carlo sampling algorithms. These incorporate a data-augmentation
framework utilized to efficiently evaluate the likelihood for the copula based
PIC model in the loss reserving triangles. The adaptation strategy is based on
representing a positive definite covariance matrix by the exponential of a
symmetric matrix as proposed by Leonard et al. (1992).
"
"  Traditional approaches to Bayes net structure learning typically assume
little regularity in graph structure other than sparseness. However, in many
cases, we expect more systematicity: variables in real-world systems often
group into classes that predict the kinds of probabilistic dependencies they
participate in. Here we capture this form of prior knowledge in a hierarchical
Bayesian framework, and exploit it to enable structure learning and type
discovery from small datasets. Specifically, we present a nonparametric
generative model for directed acyclic graphs as a prior for Bayes net structure
learning. Our model assumes that variables come in one or more classes and that
the prior probability of an edge existing between two variables is a function
only of their classes. We derive an MCMC algorithm for simultaneous inference
of the number of classes, the class assignments of variables, and the Bayes net
structure over variables. For several realistic, sparse datasets, we show that
the bias towards systematicity of connections provided by our model yields more
accurate learned networks than a traditional, uniform prior approach, and that
the classes found by our model are appropriate.
"
"  We study the problem of structured output learning from a regression
perspective. We first provide a general formulation of the kernel dependency
estimation (KDE) problem using operator-valued kernels. We show that some of
the existing formulations of this problem are special cases of our framework.
We then propose a covariance-based operator-valued kernel that allows us to
take into account the structure of the kernel feature space. This kernel
operates on the output space and encodes the interactions between the outputs
without any reference to the input space. To address this issue, we introduce a
variant of our KDE method based on the conditional covariance operator that in
addition to the correlation between the outputs takes into account the effects
of the input variables. Finally, we evaluate the performance of our KDE
approach using both covariance and conditional covariance kernels on two
structured output problems, and compare it to the state-of-the-art kernel-based
structured output regression methods.
"
"  Anatomical shape differences in cortical structures in the brain can be
associated with various neuropsychiatric and neuro-developmental diseases or
disorders. Labeled Cortical Distance Map (LCDM), can be a powerful tool to
quantize such morphometric differences. In this article, we investigate various
issues regarding the analysis of LCDM distances in relation to morphometry. The
length of the LCDM distance vector provides the number of voxels (approximately
a multiple of volume (in mm^3)); median, mode, range, and variance of LCDM
distances are all suggestive of size, thickness, and shape differences. However
these measures provide a crude summary based on LCDM distances which may convey
much more information about the tissue in question. To utilize more of this
information, we pool (merge) the LCDM distances from subjects in the same group
or condition. The statistical methodology we employ require normality and
within and between sample independence. We demonstrate that the violation of
these assumptions have mild influence on the tests. We specify the types of
alternatives the parametric and nonparametric tests are more sensitive for. We
also show that the pooled LCDM distances provide powerful results for group
differences in distribution, left-right morphometric asymmetry of the tissues,
and variation of LCDM distances. As an illustrative example, we use gray matter
(GM) tissue of ventral medial prefrontal cortices (VMPFCs) from subjects with
major depressive disorder, subjects at high risk, and control subjects. We find
significant evidence that VMPFCs of subjects with depressive disorders are
different in shape compared to those of normal subjects.
"
"  Recently, machine learning algorithms have successfully entered large-scale
real-world industrial applications (e.g. search engines and email spam
filters). Here, the CPU cost during test time must be budgeted and accounted
for. In this paper, we address the challenge of balancing the test-time cost
and the classifier accuracy in a principled fashion. The test-time cost of a
classifier is often dominated by the computation required for feature
extraction-which can vary drastically across eatures. We decrease this
extraction time by constructing a tree of classifiers, through which test
inputs traverse along individual paths. Each path extracts different features
and is optimized for a specific sub-partition of the input space. By only
computing features for inputs that benefit from them the most, our cost
sensitive tree of classifiers can match the high accuracies of the current
state-of-the-art at a small fraction of the computational cost.
"
"  We investigate the organization of traffic flow on preexisting uni- and
bidirectional ant trails. Our investigations comprise a theoretical as well as
an empirical part. We propose minimal models of uni- and bi-directional traffic
flow implemented as cellular automata. Using these models, the spatio-temporal
organization of ants on the trail is studied. Based on this, some unusual flow
characteristics which differ from those known from other traffic systems, like
vehicular traffic or pedestrians dynamics, are found. The theoretical
investigations are supplemented by an empirical study of bidirectional traffic
on a trail of Leptogenys processionalis. Finally, we discuss some plausible
implications of our observations from the perspective of flow optimization.
"
"  Large datasets with interactions between objects are common to numerous
scientific fields (i.e. social science, internet, biology...). The interactions
naturally define a graph and a common way to explore or summarize such dataset
is graph clustering. Most techniques for clustering graph vertices just use the
topology of connections ignoring informations in the vertices features. In this
paper, we provide a clustering algorithm exploiting both types of data based on
a statistical model with latent structure characterizing each vertex both by a
vector of features as well as by its connectivity. We perform simulations to
compare our algorithm with existing approaches, and also evaluate our method
with real datasets based on hyper-textual documents. We find that our algorithm
successfully exploits whatever information is found both in the connectivity
pattern and in the features.
"
"  We compare calcium ion signaling ($\mathrm {Ca}^{2+}$) between two exposures;
the data are present as movies, or, more prosaically, time series of images.
This paper describes novel uses of singular value decompositions (SVD) and
weighted versions of them (WSVD) to extract the signals from such movies, in a
way that is semi-automatic and tuned closely to the actual data and their many
complexities. These complexities include the following. First, the images
themselves are of no interest: all interest focuses on the behavior of
individual cells across time, and thus, the cells need to be segmented in an
automated manner. Second, the cells themselves have 100$+$ pixels, so that they
form 100$+$ curves measured over time, so that data compression is required to
extract the features of these curves. Third, some of the pixels in some of the
cells are subject to image saturation due to bit depth limits, and this
saturation needs to be accounted for if one is to normalize the images in a
reasonably unbiased manner. Finally, the $\mathrm {Ca}^{2+}$ signals have
oscillations or waves that vary with time and these signals need to be
extracted. Thus, our aim is to show how to use multiple weighted and standard
singular value decompositions to detect, extract and clarify the $\mathrm
{Ca}^{2+}$ signals. Our signal extraction methods then lead to simple although
finely focused statistical methods to compare $\mathrm {Ca}^{2+}$ signals
across experimental conditions.
"
"  Using recent results on the occurrence times of a string of symbols in a
stochastic process with mixing properties, we present a new method for the
search of rare words in biological sequences generally modelled by a Markov
chain. We obtain a bound on the error between the distribution of the number of
occurrences of a word in a sequence (under a Markov model) and its Poisson
approximation. A global bound is already given by a Chen-Stein method. Our
approach, the psi-mixing method, gives local bounds. Since we only need the
error in the tails of distribution, the global uniform bound of Chen-Stein is
too large and it is a better way to consider local bounds. We search for two
thresholds on the number of occurrences from which we can regard the studied
word as an over-represented or an under-represented one. A biological role is
suggested for these over- or under-represented words. Our method gives such
thresholds for a panel of words much broader than the Chen-Stein method.
Comparing the methods, we observe a better accuracy for the psi-mixing method
for the bound of the tails of distribution. We also present the software PANOW
(available at http://stat.genopole.cnrs.fr/software/panowdir/) dedicated to the
computation of the error term and the thresholds for a studied word.
"
"  We study the behavior of bivariate empirical copula process
$\mathbb{G}_n(\cdot,\cdot)$ on pavements $[0,k_n/n]^2$ of $[0,1]^2,$ where
$k_n$ is a sequence of positive constants fulfilling some conditions. We
provide a upper bound for the strong approximation of
$\mathbb{G}_n(\cdot,\cdot)$ by a Gaussian process when $k_n/n \searrow \gamma$
as $n\rightarrow \infty,$ where $0 \leq \gamma \leq 1.$
"
"  Assume that cause-effect relationships between variables can be described as
a directed acyclic graph and the corresponding linear structural equation
model.We consider the identification problem of total effects in the presence
of latent variables and selection bias between a treatment variable and a
response variable. Pearl and his colleagues provided the back door criterion,
the front door criterion (Pearl, 2000) and the conditional instrumental
variable method (Brito and Pearl, 2002) as identifiability criteria for total
effects in the presence of latent variables, but not in the presence of
selection bias. In order to solve this problem, we propose new graphical
identifiability criteria for total effects based on the identifiable factor
models. The results of this paper are useful to identify total effects in
observational studies and provide a new viewpoint to the identification
conditions of factor models.
"
"  We propose a new prior distribution for classical (nonhierarchical) logistic
regression models, constructed by first scaling all nonbinary variables to have
mean 0 and standard deviation 0.5, and then placing independent Student-$t$
prior distributions on the coefficients. As a default choice, we recommend the
Cauchy distribution with center 0 and scale 2.5, which in the simplest setting
is a longer-tailed version of the distribution attained by assuming one-half
additional success and one-half additional failure in a logistic regression.
Cross-validation on a corpus of datasets shows the Cauchy class of prior
distributions to outperform existing implementations of Gaussian and Laplace
priors. We recommend this prior distribution as a default choice for routine
applied use. It has the advantage of always giving answers, even when there is
complete separation in logistic regression (a common problem, even when the
sample size is large and the number of predictors is small), and also
automatically applying more shrinkage to higher-order interactions. This can be
useful in routine data analysis as well as in automated procedures such as
chained equations for missing-data imputation. We implement a procedure to fit
generalized linear models in R with the Student-$t$ prior distribution by
incorporating an approximate EM algorithm into the usual iteratively weighted
least squares. We illustrate with several applications, including a series of
logistic regressions predicting voting preferences, a small bioassay
experiment, and an imputation model for a public health data set.
"
"  We present a general probabilistic perspective on Gaussian filtering and
smoothing. This allows us to show that common approaches to Gaussian
filtering/smoothing can be distinguished solely by their methods of
computing/approximating the means and covariances of joint probabilities. This
implies that novel filters and smoothers can be derived straightforwardly by
providing methods for computing these moments. Based on this insight, we derive
the cubature Kalman smoother and propose a novel robust filtering and smoothing
algorithm based on Gibbs sampling.
"
"  This paper develops a new multivariate control charting method for vector
autocorrelated and serially correlated processes. The main idea is to propose a
Bayesian multivariate local level model, which is a generalization of the
Shewhart-Deming model for autocorrelated processes, in order to provide the
predictive error distribution of the process and then to apply a univariate
modified EWMA control chart to the logarithm of the Bayes' factors of the
predictive error density versus the target error density. The resulting chart
is proposed as capable to deal with both the non-normality and the
autocorrelation structure of the log Bayes' factors. The new control charting
scheme is general in application and it has the advantage to control
simultaneously not only the process mean vector and the dispersion covariance
matrix, but also the entire target distribution of the process. Two examples of
London metal exchange data and of production time series data illustrate the
capabilities of the new control chart.
"
"  Many applications require the ability to judge uncertainty of time-series
forecasts. Uncertainty is often specified as point-wise error bars around a
mean or median forecast. Due to temporal dependencies, such a method obscures
some information. We would ideally have a way to query the posterior
probability of the entire time-series given the predictive variables, or at a
minimum, be able to draw samples from this distribution. We use a Bayesian
dictionary learning algorithm to statistically generate an ensemble of
forecasts. We show that the algorithm performs as well as a physics-based
ensemble method for temperature forecasts for Houston. We conclude that the
method shows promise for scenario forecasting where physics-based methods are
absent.
"
"  We consider the problem of Probably Approximate Correct (PAC) learning of a
binary classifier from noisy labeled examples acquired from multiple annotators
(each characterized by a respective classification noise rate). First, we
consider the complete information scenario, where the learner knows the noise
rates of all the annotators. For this scenario, we derive sample complexity
bound for the Minimum Disagreement Algorithm (MDA) on the number of labeled
examples to be obtained from each annotator. Next, we consider the incomplete
information scenario, where each annotator is strategic and holds the
respective noise rate as a private information. For this scenario, we design a
cost optimal procurement auction mechanism along the lines of Myerson's optimal
auction design framework in a non-trivial manner. This mechanism satisfies
incentive compatibility property, thereby facilitating the learner to elicit
true noise rates of all the annotators.
"
"  In this paper we propose a novel Bayesian methodology for Value-at-Risk
computation based on parametric Product Partition Models. Value-at-Risk is a
standard tool to measure and control the market risk of an asset or a
portfolio, and it is also required for regulatory purposes. Its popularity is
partly due to the fact that it is an easily understood measure of risk. The use
of Product Partition Models allows us to remain in a Normal setting even in
presence of outlying points, and to obtain a closed-form expression for
Value-at-Risk computation. We present and compare two different scenarios: a
product partition structure on the vector of means and a product partition
structure on the vector of variances. We apply our methodology to an Italian
stock market data set from Mib30. The numerical results clearly show that
Product Partition Models can be successfully exploited in order to quantify
market risk exposure. The obtained Value-at-Risk estimates are in full
agreement with Maximum Likelihood approaches, but our methodology provides
richer information about the clustering structure of the data and the presence
of outlying points.
"
"  Rapid research progress in genotyping techniques have allowed large
genome-wide association studies. Existing methods often focus on determining
associations between single loci and a specific phenotype. However, a
particular phenotype is usually the result of complex relationships between
multiple loci and the environment. In this paper, we describe a two-stage
method for detecting epistasis by combining the traditionally used single-locus
search with a search for multiway interactions. Our method is based on an
extended version of Fisher's exact test. To perform this test, a Markov chain
is constructed on the space of multidimensional contingency tables using the
elements of a Markov basis as moves. We test our method on simulated data and
compare it to a two-stage logistic regression method and to a fully Bayesian
method, showing that we are able to detect the interacting loci when other
methods fail to do so. Finally, we apply our method to a genome-wide data set
consisting of 685 dogs and identify epistasis associated with canine hair
length for four pairs of SNPs.
"
"  We present an new sequential Monte Carlo sampler for coalescent based
Bayesian hierarchical clustering. Our model is appropriate for modeling
non-i.i.d. data and offers a substantial reduction of computational cost when
compared to the original sampler without resorting to approximations. We also
propose a quadratic complexity approximation that in practice shows almost no
loss in performance compared to its counterpart. We show that as a byproduct of
our formulation, we obtain a greedy algorithm that exhibits performance
improvement over other greedy algorithms, particularly in small data sets. In
order to exploit the correlation structure of the data, we describe how to
incorporate Gaussian process priors in the model as a flexible way to model
non-i.i.d. data. Results on artificial and real data show significant
improvements over closely related approaches.
"
"  Research in examining the equity of service accessibility has emerged as
economic and social equity advocates recognized that where people live
influences their opportunities for economic development, access to quality
health care and political participation. In this research paper service
accessibility equity is concerned with where and when services have been and
are accessed by different groups of people, identified by location or
underlying socioeconomic variables. Using new statistical methods for modeling
spatial-temporal data, this paper estimates demographic association patterns to
financial service accessibility varying over a large geographic area (Georgia)
and over a period of 13 years. The underlying model is a space--time varying
coefficient model including both separable space and time varying coefficients
and space--time interaction terms. The model is extended to a multilevel
response where the varying coefficients account for both the within- and
between-variability. We introduce an inference procedure for assessing the
shape of the varying regression coefficients using confidence bands.
"
"  We consider the empirical risk minimization problem for linear supervised
learning, with regularization by structured sparsity-inducing norms. These are
defined as sums of Euclidean norms on certain subsets of variables, extending
the usual $\ell_1$-norm and the group $\ell_1$-norm by allowing the subsets to
overlap. This leads to a specific set of allowed nonzero patterns for the
solutions of such problems. We first explore the relationship between the
groups defining the norm and the resulting nonzero patterns, providing both
forward and backward algorithms to go back and forth from groups to patterns.
This allows the design of norms adapted to specific prior knowledge expressed
in terms of nonzero patterns. We also present an efficient active set
algorithm, and analyze the consistency of variable selection for least-squares
linear regression in low and high-dimensional settings.
"
"  This work measures the evolution of the iron content in galaxy clusters by a
rigorous analysis of the data of 130 clusters at 0.1<z<1.3. This task is made
difficult by a) the low signal-to-noise ratio of abundance measurements and the
upper limits, b) possible selection effects, c) boundaries in the parameter
space, d) non-Gaussian errors, e) the intrinsic variety of the objects studied,
and f) abundance systematics. We introduce a Bayesian model to address all
these issues at the same time, thus allowing cross-talk (covariance). On
simulated data, the Bayesian fit recovers the input enrichment history, unlike
in standard analysis. After accounting for a possible dependence on X-ray
temperature, for metal abundance systematics, and for the intrinsic variety of
studied objects, we found that the present-day metal content is not reached
either at high or at low redshifts, but gradually over time: iron abundance
increases by a factor 1.5 in the 7 Gyr sampled by the data. Therefore, feedback
in metal abundance does not end at high redshift. Evolution is established with
a moderate amount of evidence, 19 to 1 odds against faster or slower metal
enrichment histories. We quantify, for the first time, the intrinsic spread in
metal abundance, 18+/-3 %, after correcting for the effect of evolution, X-ray
temperature, and metal abundance systematics. Finally, we also present an
analytic approximation of the X-ray temperature and metal abundance likelihood
functions, which are useful for other regression fitting involving these
parameters. The data for the 130 clusters and code used for the stochastic
computation are provided with the paper.
"
"  Most conventional Reinforcement Learning (RL) algorithms aim to optimize
decision-making rules in terms of the expected returns. However, especially for
risk management purposes, other risk-sensitive criteria such as the
value-at-risk or the expected shortfall are sometimes preferred in real
applications. Here, we describe a parametric method for estimating density of
the returns, which allows us to handle various criteria in a unified manner. We
first extend the Bellman equation for the conditional expected return to cover
a conditional probability density of the returns. Then we derive an extension
of the TD-learning algorithm for estimating the return densities in an unknown
environment. As test instances, several parametric density estimation
algorithms are presented for the Gaussian, Laplace, and skewed Laplace
distributions. We show that these algorithms lead to risk-sensitive as well as
robust RL paradigms through numerical experiments.
"
"  Training a Support Vector Machine (SVM) requires the solution of a quadratic
programming problem (QP) whose computational complexity becomes prohibitively
expensive for large scale datasets. Traditional optimization methods cannot be
directly applied in these cases, mainly due to memory restrictions.
  By adopting a slightly different objective function and under mild conditions
on the kernel used within the model, efficient algorithms to train SVMs have
been devised under the name of Core Vector Machines (CVMs). This framework
exploits the equivalence of the resulting learning problem with the task of
building a Minimal Enclosing Ball (MEB) problem in a feature space, where data
is implicitly embedded by a kernel function.
  In this paper, we improve on the CVM approach by proposing two novel methods
to build SVMs based on the Frank-Wolfe algorithm, recently revisited as a fast
method to approximate the solution of a MEB problem. In contrast to CVMs, our
algorithms do not require to compute the solutions of a sequence of
increasingly complex QPs and are defined by using only analytic optimization
steps. Experiments on a large collection of datasets show that our methods
scale better than CVMs in most cases, sometimes at the price of a slightly
lower accuracy. As CVMs, the proposed methods can be easily extended to machine
learning problems other than binary classification. However, effective
classifiers are also obtained using kernels which do not satisfy the condition
required by CVMs and can thus be used for a wider set of problems.
"
"  A stratified sampling plan to audit health insurance claims is offered. The
stratification is by dollar amount of the claim. The plan is representative in
the sense that with high probability for each stratum, the difference in the
average dollar amount of the claim in the sample and the average dollar amount
in the population, is ``small.'' Several notions of ``small'' are presented.
The plan then yields a relatively small total sample size with the property
that the overall average dollar amount in the sample is close to the average
dollar amount in the population. Three different estimators and corresponding
lower confidence bounds for over (under) payments are studied.
"
"  Learning a Bayesian network structure from data is an NP-hard problem and
thus exact algorithms are feasible only for small data sets. Therefore, network
structures for larger networks are usually learned with various heuristics.
Another approach to scaling up the structure learning is local learning. In
local learning, the modeler has one or more target variables that are of
special interest; he wants to learn the structure near the target variables and
is not interested in the rest of the variables. In this paper, we present a
score-based local learning algorithm called SLL. We conjecture that our
algorithm is theoretically sound in the sense that it is optimal in the limit
of large sample size. Empirical results suggest that SLL is competitive when
compared to the constraint-based HITON algorithm. We also study the prospects
of constructing the network structure for the whole node set based on local
results by presenting two algorithms and comparing them to several heuristics.
"
"  This paper describes computationally efficient approaches and associated
theoretical performance guarantees for the detection of known targets and
anomalies from few projection measurements of the underlying signals. The
proposed approaches accommodate signals of different strengths contaminated by
a colored Gaussian background, and perform detection without reconstructing the
underlying signals from the observations. The theoretical performance bounds of
the target detector highlight fundamental tradeoffs among the number of
measurements collected, amount of background signal present, signal-to-noise
ratio, and similarity among potential targets coming from a known dictionary.
The anomaly detector is designed to control the number of false discoveries.
The proposed approach does not depend on a known sparse representation of
targets; rather, the theoretical performance bounds exploit the structure of a
known dictionary of targets and the distance preservation property of the
measurement matrix. Simulation experiments illustrate the practicality and
effectiveness of the proposed approaches.
"
"  The Statistical Process Control (SPC) and the Automated Process Control (APC)
have a common goal: achieve optimal product quality by controlling variations
in the process. The work in this paper will present a developed integration
methodology of the APC in the SPC which is based on discretization of the
transfer functions relating to each component of the process. We proposed on
the one hand, a new control rule which is based on a system of first order. In
the other hand, we showed how to establish control charts to a process of the
type AR (1). Using simulation experiments, we showed that the proposed control
rule reduced variability by comparing it with that proposed in literature.
"
"  We consider composite loss functions for multiclass prediction comprising a
proper (i.e., Fisher-consistent) loss over probability distributions and an
inverse link function. We establish conditions for their (strong) convexity and
explore the implications. We also show how the separation of concerns afforded
by using this composite representation allows for the design of families of
losses with the same Bayes risk.
"
"  Minimum mean squared error (MMSE) estimators of signals from samples
corrupted by jitter (timing noise) and additive noise are nonlinear, even when
the signal prior and additive noise have normal distributions. This paper
develops a stochastic algorithm based on Gibbs sampling and slice sampling to
approximate the optimal MMSE estimator in this Bayesian formulation.
Simulations demonstrate that this nonlinear algorithm can improve significantly
upon the linear MMSE estimator, as well as the EM algorithm approximation to
the maximum likelihood (ML) estimator used in classical estimation. Effective
off-chip post-processing to mitigate jitter enables greater jitter to be
tolerated, potentially reducing on-chip ADC power consumption.
"
"  Mathematical models could be helpful in assisting the Indian Government's new
initiative of issuing biometric cards to its citizens. In this note, we look
into the role of mathematical models in estimating the missing, non-enumerated
population numbers, estimating annual numbers of cards required by age, gender
and regions in India. The linkage between National Population Register and
biometric cards is also highlighted. See technical Appendices. There are other
scientific issues, namely, electronic, data storage management, identity
verification etc, which we do not address in this paper.
"
"  The paper proposes a new covariance estimator for large covariance matrices
when the variables have a natural ordering. Using the Cholesky decomposition of
the inverse, we impose a banded structure on the Cholesky factor, and select
the bandwidth adaptively for each row of the Cholesky factor, using a novel
penalty we call nested Lasso. This structure has more flexibility than regular
banding, but, unlike regular Lasso applied to the entries of the Cholesky
factor, results in a sparse estimator for the inverse of the covariance matrix.
An iterative algorithm for solving the optimization problem is developed. The
estimator is compared to a number of other covariance estimators and is shown
to do best, both in simulations and on a real data example. Simulations show
that the margin by which the estimator outperforms its competitors tends to
increase with dimension.
"
"  Despite its shortcomings, cross-level or ecological inference remains a
necessary part of some areas of quantitative inference, including in United
States voting rights litigation. Ecological inference suffers from a lack of
identification that, most agree, is best addressed by incorporating
individual-level data into the model. In this paper we test the limits of such
an incorporation by attempting it in the context of drawing inferences about
racial voting patterns using a combination of an exit poll and precinct-level
ecological data; accurate information about racial voting patterns is needed to
assess triggers in voting rights laws that can determine the composition of
United States legislative bodies. Specifically, we extend and study a hybrid
model that addresses two-way tables of arbitrary dimension. We apply the hybrid
model to an exit poll we administered in the City of Boston in 2008. Using the
resulting data as well as simulation, we compare the performance of a pure
ecological estimator, pure survey estimators using various sampling schemes and
our hybrid. We conclude that the hybrid estimator offers substantial benefits
by enabling substantive inferences about voting patterns not practicably
available without its use.
"
"  We show how to construct the best linear unbiased predictor (BLUP) for the
continuation of a curve in a spline-function model. We assume that the entire
curve is drawn from some smooth random process and that the curve is given up
to some cut point. We demonstrate how to compute the BLUP efficiently.
Confidence bands for the BLUP are discussed. Finally, we apply the proposed
BLUP to real-world call center data. Specifically, we forecast the continuation
of both the call arrival counts and the workload process at the call center of
a commercial bank.
"
"  We develop a Bayesian nonparametric approach to a general family of latent
class problems in which individuals can belong simultaneously to multiple
classes and where each class can be exhibited multiple times by an individual.
We introduce a combinatorial stochastic process known as the negative binomial
process (NBP) as an infinite-dimensional prior appropriate for such problems.
We show that the NBP is conjugate to the beta process, and we characterize the
posterior distribution under the beta-negative binomial process (BNBP) and
hierarchical models based on the BNBP (the HBNBP). We study the asymptotic
properties of the BNBP and develop a three-parameter extension of the BNBP that
exhibits power-law behavior. We derive MCMC algorithms for posterior inference
under the HBNBP, and we present experiments using these algorithms in the
domains of image segmentation, object recognition, and document analysis.
"
"  The theory of cointegration has been a leading theory in econometrics with
powerful applications to macroeconomics during the last decades. On the other
hand the theory of phase synchronization for weakly coupled complex oscillators
has been one of the leading theories in physics for many years with many
applications to different areas of science. For example, in neuroscience phase
synchronization is regarded as essential for functional coupling of different
brain regions. In an abstract sense both theories describe the dynamic
fluctuation around some equilibrium. In this paper, we point out that there
exists a very close connection between both theories. Apart from phase jumps, a
stochastic version of the Kuramoto equations can be approximated by a
cointegrated system of difference equations. As one consequence, the rich
theory on statistical inference for cointegrated systems can immediately be
applied for statistical inference on phase synchronization based on empirical
data. This includes tests for phase synchronization, tests for unidirectional
coupling and the identification of the equilibrium from data including phase
shifts. We study two examples on a unidirectionally coupled R\""ossler-Lorenz
system and on electrochemical oscillators. The methods from cointegration may
also be used to investigate phase synchronization in complex networks.
Conversely, there are many interesting results on phase synchronization which
may inspire new research on cointegration.
"
"  We consider the high-dimensional heteroscedastic regression model, where the
mean and the log variance are modeled as a linear combination of input
variables. Existing literature on high-dimensional linear regres- sion models
has largely ignored non-constant error variances, even though they commonly
occur in a variety of applications ranging from biostatis- tics to finance. In
this paper we study a class of non-convex penalized pseudolikelihood estimators
for both the mean and variance parameters. We show that the Heteroscedastic
Iterative Penalized Pseudolikelihood Optimizer (HIPPO) achieves the oracle
property, that is, we prove that the rates of convergence are the same as if
the true model was known. We demonstrate numerical properties of the procedure
on a simulation study and real world data.
"
"  In this paper we propose a new nonparametric approach to interacting failing
systems (FS), that is systems whose probability of failure is not negligible in
a fixed time horizon, a typical example being firms and financial bonds. The
main purpose when studying a FS is to calculate the probability of default and
the distribution of the number of failures that may occur during the
observation period. A model used to study a failing system is defined default
model. In particular, we present a general recursive model constructed by the
means of inter- acting urns. After introducing the theoretical model and its
properties we show a first application to credit risk modeling, showing how to
assess the idiosyncratic probability of default of an obligor and the joint
probability of failure of a set of obligors in a portfolio of risks, that are
divided into reliability classes.
"
"  In this paper we develop a Bayesian procedure for estimating multivariate
stochastic volatility (MSV) using state space models. A multiplicative model
based on inverted Wishart and multivariate singular beta distributions is
proposed for the evolution of the volatility, and a flexible sequential
volatility updating is employed. Being computationally fast, the resulting
estimation procedure is particularly suitable for on-line forecasting. Three
performance measures are discussed in the context of model selection: the
log-likelihood criterion, the mean of standardized one-step forecast errors,
and sequential Bayes factors. Finally, the proposed methods are applied to a
data set comprising eight exchange rates vis-a-vis the US dollar.
"
"  Agricultural research has been profited by technical advances such as
automation, data mining. Today, data mining is used in a vast areas and many
off-the-shelf data mining system products and domain specific data mining
application soft wares are available, but data mining in agricultural soil
datasets is a relatively a young research field. The large amounts of data that
are nowadays virtually harvested along with the crops have to be analyzed and
should be used to their full extent. This research aims at analysis of soil
dataset using data mining techniques. It focuses on classification of soil
using various algorithms available. Another important purpose is to predict
untested attributes using regression technique, and implementation of automated
soil sample classification.
"
"  We develop a highly scalable optimization method called ""hierarchical
group-thresholding"" for solving a multi-task regression model with complex
structured sparsity constraints on both input and output spaces. Despite the
recent emergence of several efficient optimization algorithms for tackling
complex sparsity-inducing regularizers, true scalability in practical
high-dimensional problems where a huge amount (e.g., millions) of sparsity
patterns need to be enforced remains an open challenge, because all existing
algorithms must deal with ALL such patterns exhaustively in every iteration,
which is computationally prohibitive. Our proposed algorithm addresses the
scalability problem by screening out multiple groups of coefficients
simultaneously and systematically. We employ a hierarchical tree representation
of group constraints to accelerate the process of removing irrelevant
constraints by taking advantage of the inclusion relationships between group
sparsities, thereby avoiding dealing with all constraints in every optimization
step, and necessitating optimization operation only on a small number of
outstanding coefficients. In our experiments, we demonstrate the efficiency of
our method on simulation datasets, and in an application of detecting genetic
variants associated with gene expression traits.
"
"  Problems in machine learning (ML) can involve noisy input data, and ML
classification methods have reached limiting accuracies when based on standard
ML data sets consisting of feature vectors and their classes. Greater accuracy
will require incorporation of prior structural information on data into
learning. We study methods to regularize feature vectors (unsupervised
regularization methods), analogous to supervised regularization for estimating
functions in ML. We study regularization (denoising) of ML feature vectors
using Tikhonov and other regularization methods for functions on ${\bf R}^n$. A
feature vector ${\bf x}=(x_1,\ldots,x_n)=\{x_q\}_{q=1}^n$ is viewed as a
function of its index $q$, and smoothed using prior information on its
structure. This can involve a penalty functional on feature vectors analogous
to those in statistical learning, or use of proximity (e.g. graph) structure on
the set of indices. Such feature vector regularization inherits a property from
function denoising on ${\bf R}^n$, in that accuracy is non-monotonic in the
denoising (regularization) parameter $\alpha$. Under some assumptions about the
noise level and the data structure, we show that the best reconstruction
accuracy also occurs at a finite positive $\alpha$ in index spaces with graph
structures. We adapt two standard function denoising methods used on ${\bf
R}^n$, local averaging and kernel regression. In general the index space can be
any discrete set with a notion of proximity, e.g. a metric space, a subset of
${\bf R}^n$, or a graph/network, with feature vectors as functions with some
notion of continuity. We show this improves feature vector recovery, and thus
the subsequent classification or regression done on them. We give an example in
gene expression analysis for cancer classification with the genome as an index
space and network structure based protein-protein interactions.
"
"  In the last few years, many different performance measures have been
introduced to overcome the weakness of the most natural metric, the Accuracy.
Among them, Matthews Correlation Coefficient has recently gained popularity
among researchers not only in machine learning but also in several application
fields such as bioinformatics. Nonetheless, further novel functions are being
proposed in literature. We show that Confusion Entropy, a recently introduced
classifier performance measure for multi-class problems, has a strong
(monotone) relation with the multi-class generalization of a classical metric,
the Matthews Correlation Coefficient. Computational evidence in support of the
claim is provided, together with an outline of the theoretical explanation.
"
"  Recent research has documented a significant rise in the volatility (e.g.,
expected squared change) of individual incomes in the U.S. since the 1970s.
Existing measures of this trend abstract from individual heterogeneity,
effectively estimating an increase in average volatility. We decompose this
increase in average volatility and find that it is far from representative of
the experience of most people: there has been no systematic rise in volatility
for the vast majority of individuals. The rise in average volatility has been
driven almost entirely by a sharp rise in the income volatility of those
expected to have the most volatile incomes, identified ex-ante by large income
changes in the past. We document that the self-employed and those who
self-identify as risk-tolerant are much more likely to have such volatile
incomes; these groups have experienced much larger increases in income
volatility than the population at large. These results color the policy
implications one might draw from the rise in average volatility. While the
basic results are apparent from PSID summary statistics, providing a complete
characterization of the dynamics of the volatility distribution is a
methodological challenge. We resolve these difficulties with a Markovian
hierarchical Dirichlet process that builds on work from the non-parametric
Bayesian statistics literature.
"
"  In a recent article, I showed that in several academic disciplines in Italy,
professors display a paucity of last names that cannot be explained by
unbiased, random, hiring processes. I suggested that this scarcity of last
names could be related to the prevalence of nepotistic hires, i.e., professors
engaging in illegal practices to have their relatives hired as academics. My
findings have recently been questioned through repeat analysis to the United
Kingdom university system. Ferlazzo & Sdoia found that several disciplines in
this system also display a scarcity of last names, and that a similar scarcity
is found when analyzing the first (given) names of Italian professors. Here I
show that the scarcity of first names in Italian disciplines is completely
explained by uneven male/female representation, while the scarcity of last
names in United Kingdom academia is due to discipline-specific immigration.
However, these factors cannot explain the scarcity of last names in Italian
disciplines. Geographic and demographic considerations -- proposed as a
possible explanation of my findings -- appear to have no significant effect:
after correcting for these factors, the scarcity of last names remains highly
significant in several disciplines, and there is a marked trend from north to
south, with a higher likelihood of nepotism in the south and in Sicily.
Moreover, I show that in several Italian disciplines positions tend to be
inherited as with last names (i.e., from father to son, but not from mother to
daughter). Taken together, these results strenghten the case for nepotism,
highlighting that statistical tests cannot be applied to a dataset without
carefully considering the characteristics of the data and critically
interpreting the results.
"
"  A general methodology to study public opinion inspired from information and
complexity theories is outlined. It is based on probabilistic data extracted
from opinion polls. It gives a quantitative information-theoretic explanation
of high job approval of Greek Prime Minister Mr. Constantinos Karamanlis
(2004-2007), while the same time series of polls conducted by the company
Metron Analysis showed that his party New Democracy (abbr. ND) was slightly
higher than the opposition party of PASOK -party leader Mr. George Papandreou.
It is seen that the same mathematical model applies to the case of the
popularity of President Clinton between January 1998 and February 1999,
according to a previous study, although the present work extends the
investigation to concepts as complexity and Fisher information, quantifying the
organization of public opinion data.
"
"  The purpose of this paper is to investigate and develop methods for analysis
of multi-center randomized clinical trials which only rely on the randomization
process as a basis of inference. Our motivation is prompted by the fact that
most current statistical procedures used in the analysis of randomized
multi-center studies are model based. The randomization feature of the trials
is usually ignored. An important characteristic of model based analysis is that
it is straightforward to model covariates. Nevertheless, in nearly all model
based analyses, the effects due to different centers and, in general, the
design of the clinical trials are ignored. An alternative to a model based
analysis is to have analyses guided by the design of the trial. Our development
of design based methods allows the incorporation of centers as well as other
features of the trial design. The methods make use of conditioning on the
ancillary statistics in the sample space generated by the randomization
process. We have investigated the power of the methods and have found that, in
the presence of center variation, there is a significant increase in power. The
methods have been extended to group sequential trials with similar increases in
power.
"
"  Kernel smoothing represents a useful approach in the graduation of mortality
rates. Though there exist several options for performing kernel smoothing in
statistical software packages, there have been very few contributions to date
that have focused on applications of these techniques in the graduation
context. Also, although it has been shown that the use of a variable or
adaptive smoothing parameter, based on the further information provided by the
exposed to the risk of death, provides additional benefits, specific
computational tools for this approach are essentially absent. Furthermore,
little attention has been given to providing methods in available software for
any kind of subsequent analysis with respect to the graduated mortality rates.
To facilitate analyses in the field, the R package DBKGrad is introduced. Among
the available kernel approaches, it considers a recent discrete beta kernel
estimator, in both its fixed and adaptive variants. In this approach, boundary
bias is automatically reduced and age is pragmatically considered as a discrete
variable. The bandwidth, fixed or adaptive, is allowed to be manually given by
the user or selected by cross-validation. Pointwise confidence intervals, for
each considered age, are also provided. An application to mortality rates from
the Sicily Region (Italy) for the year 2008 is also presented to exemplify the
use of the package.
"
"  Many real-world problems encountered in several disciplines deal with the
modeling of time-series containing different underlying dynamical regimes, for
which probabilistic approaches are very often employed. In this paper we
describe several such approaches in the common framework of graphical models.
We give a unified overview of models previously introduced in the literature,
which is simpler and more comprehensive than previous descriptions and enables
us to highlight commonalities and differences among models that were not
observed in the past. In addition, we present several new models and inference
routines, which are naturally derived within this unified viewpoint.
"
"  We consider the problem of predicting as well as the best linear combination
of d given functions in least squares regression, and variants of this problem
including constraints on the parameters of the linear combination. When the
input distribution is known, there already exists an algorithm having an
expected excess risk of order d/n, where n is the size of the training data.
Without this strong assumption, standard results often contain a multiplicative
log n factor, and require some additional assumptions like uniform boundedness
of the d-dimensional input representation and exponential moments of the
output. This work provides new risk bounds for the ridge estimator and the
ordinary least squares estimator, and their variants. It also provides
shrinkage procedures with convergence rate d/n (i.e., without the logarithmic
factor) in expectation and in deviations, under various assumptions. The key
common surprising factor of these results is the absence of exponential moment
condition on the output distribution while achieving exponential deviations.
All risk bounds are obtained through a PAC-Bayesian analysis on truncated
differences of losses. Finally, we show that some of these results are not
particular to the least squares loss, but can be generalized to similar
strongly convex loss functions.
"
"  This paper investigates a fully unsupervised statistical method for edge
preserving image restoration and compression using a spatial decomposition
scheme. Smoothed maximum likelihood is used for local estimation of edge pixels
from mixture parametric models of local templates. For the complementary smooth
part the traditional L2-variational problem is solved in the Fourier domain
with Thin Plate Spline (TPS) regularization. It is well known that naive
Fourier compression of the whole image fails to restore a piece-wise smooth
noisy image satisfactorily due to Gibbs phenomenon. Images are interpreted as
relative frequency histograms of samples from bi-variate densities where the
sample sizes might be unknown. The set of discontinuities is assumed to be
completely unsupervised Lebesgue-null, compact subset of the plane in the
continuous formulation of the problem. Proposed spatial decomposition uses a
widely used topological concept, partition of unity. The decision on edge pixel
neighborhoods are made based on the multiple testing procedure of Holms.
Statistical summary of the final output is decomposed into two layers of
information extraction, one for the subset of edge pixels and the other for the
smooth region. Robustness is also demonstrated by applying the technique on
noisy degradation of clean images.
"
"  We introduce a machine learning model to predict atomization energies of a
diverse set of organic molecules, based on nuclear charges and atomic positions
only. The problem of solving the molecular Schr\""odinger equation is mapped
onto a non-linear statistical regression problem of reduced complexity.
Regression models are trained on and compared to atomization energies computed
with hybrid density-functional theory. Cross-validation over more than seven
thousand small organic molecules yields a mean absolute error of ~10 kcal/mol.
Applicability is demonstrated for the prediction of molecular atomization
potential energy curves.
"
"  In conventional supervised pattern recognition tasks, model selection is
typically accomplished by minimizing the classification error rate on a set of
so-called development data, subject to ground-truth labeling by human experts
or some other means. In the context of speech processing systems and other
large-scale practical applications, however, such labeled development data are
typically costly and difficult to obtain. This article proposes an alternative
semi-supervised framework for likelihood-based model selection that leverages
unlabeled data by using trained classifiers representing each model to
automatically generate putative labels. The errors that result from this
automatic labeling are shown to be amenable to results from robust statistics,
which in turn provide for minimax-optimal censored likelihood ratio tests that
recover the nonparametric sign test as a limiting case. This approach is then
validated experimentally using a state-of-the-art automatic speech recognition
system to select between candidate word pronunciations using unlabeled speech
data that only potentially contain instances of the words under test. Results
provide supporting evidence for the utility of this approach, and suggest that
it may also find use in other applications of machine learning.
"
"  A relatively recent advance in cognitive neuroscience has been multi-voxel
pattern analysis (MVPA), which enables researchers to decode brain states
and/or the type of information represented in the brain during a cognitive
operation. MVPA methods utilize machine learning algorithms to distinguish
among types of information or cognitive states represented in the brain, based
on distributed patterns of neural activity. In the current investigation, we
propose a new approach for representation of neural data for pattern analysis,
namely a Mesh Learning Model. In this approach, at each time instant, a star
mesh is formed around each voxel, such that the voxel corresponding to the
center node is surrounded by its p-nearest neighbors. The arc weights of each
mesh are estimated from the voxel intensity values by least squares method. The
estimated arc weights of all the meshes, called Mesh Arc Descriptors (MADs),
are then used to train a classifier, such as Neural Networks, k-Nearest
Neighbor, Na\""ive Bayes and Support Vector Machines. The proposed Mesh Model
was tested on neuroimaging data acquired via functional magnetic resonance
imaging (fMRI) during a recognition memory experiment using categorized word
lists, employing a previously established experimental paradigm (\""Oztekin &
Badre, 2011). Results suggest that the proposed Mesh Learning approach can
provide an effective algorithm for pattern analysis of brain activity during
cognitive processing.
"
"  We propose a novel approach for distributed statistical detection of
change-points in high-volume network traffic. We consider more specifically the
task of detecting and identifying the targets of Distributed Denial of Service
(DDoS) attacks. The proposed algorithm, called DTopRank, performs distributed
network anomaly detection by aggregating the partial information gathered in a
set of network monitors. In order to address massive data while limiting the
communication overhead within the network, the approach combines record
filtering at the monitor level and a nonparametric rank test for doubly
censored time series at the central decision site. The performance of the
DTopRank algorithm is illustrated both on synthetic data as well as from a
traffic trace provided by a major Internet service provider.
"
"  Graphical modelling has a long history in statistics as a tool for the
analysis of multivariate data, starting from Wright's path analysis and Gibbs'
applications to statistical physics at the beginning of the last century. In
its modern form, it was pioneered by Lauritzen and Wermuth and Pearl in the
1980s, and has since found applications in fields as diverse as bioinformatics,
customer satisfaction surveys and weather forecasts.
  Genetics and systems biology are unique among these fields in the dimension
of the data sets they study, which often contain several hundreds of variables
and only a few tens or hundreds of observations. This raises problems in both
computational complexity and the statistical significance of the resulting
networks, collectively known as the ""curse of dimensionality"". Furthermore, the
data themselves are difficult to model correctly due to the limited
understanding of the underlying mechanisms. In the following, we will
illustrate how such challenges affect practical graphical modelling and some
possible solutions.
"
"  Modern Internet services, such as those at Google, Yahoo!, and Amazon, handle
billions of requests per day on clusters of thousands of computers. Because
these services operate under strict performance requirements, a statistical
understanding of their performance is of great practical interest. Such
services are modeled by networks of queues, where each queue models one of the
computers in the system. A key challenge is that the data are incomplete,
because recording detailed information about every request to a heavily used
system can require unacceptable overhead. In this paper we develop a Bayesian
perspective on queueing models in which the arrival and departure times that
are not observed are treated as latent variables. Underlying this viewpoint is
the observation that a queueing model defines a deterministic transformation
between the data and a set of independent variables called the service times.
With this viewpoint in hand, we sample from the posterior distribution over
missing data and model parameters using Markov chain Monte Carlo. We evaluate
our framework on data from a benchmark Web application. We also present a
simple technique for selection among nested queueing models. We are unaware of
any previous work that considers inference in networks of queues in the
presence of missing data.
"
"  Suppose a given observation matrix can be decomposed as the sum of a low-rank
matrix and a sparse matrix (outliers), and the goal is to recover these
individual components from the observed sum. Such additive decompositions have
applications in a variety of numerical problems including system
identification, latent variable graphical modeling, and principal components
analysis. We study conditions under which recovering such a decomposition is
possible via a combination of $\ell_1$ norm and trace norm minimization. We are
specifically interested in the question of how many outliers are allowed so
that convex programming can still achieve accurate recovery, and we obtain
stronger recovery guarantees than previous studies. Moreover, we do not assume
that the spatial pattern of outliers is random, which stands in contrast to
related analyses under such assumptions via matrix completion.
"
"  We propose a semiparametric approach, named nonparanormal skeptic, for
estimating high dimensional undirected graphical models. In terms of modeling,
we consider the nonparanormal family proposed by Liu et al (2009). In terms of
estimation, we exploit nonparametric rank-based correlation coefficient
estimators including the Spearman's rho and Kendall's tau. In high dimensional
settings, we prove that the nonparanormal skeptic achieves the optimal
parametric rate of convergence in both graph and parameter estimation. This
result suggests that the nonparanormal graphical models are a safe replacement
of the Gaussian graphical models, even when the data are Gaussian.
"
"  Given a set of mixtures, blind source separation attempts to retrieve the
source signals without or with very little information of the the mixing
process. We present a geometric approach for blind separation of nonnegative
linear mixtures termed {\em facet component analysis} (FCA). The approach is
based on facet identification of the underlying cone structure of the data.
Earlier works focus on recovering the cone by locating its vertices (vertex
component analysis or VCA) based on a mutual sparsity condition which requires
each source signal to possess a stand-alone peak in its spectrum. We formulate
alternative conditions so that enough data points fall on the facets of a cone
instead of accumulating around the vertices. To find a regime of unique
solvability, we make use of both geometric and density properties of the data
points, and develop an efficient facet identification method by combining data
classification and linear regression. For noisy data, we show that denoising
methods may be employed, such as the total variation technique in imaging
processing, and principle component analysis. We show computational results on
nuclear magnetic resonance spectroscopic data to substantiate our method.
"
"  We consider distributed state estimation in a wireless sensor network without
a fusion center. Each sensor performs a global estimation task---based on the
past and current measurements of all sensors---using only local processing and
local communications with its neighbors. In this estimation task, the joint
(all-sensors) likelihood function (JLF) plays a central role as it epitomizes
the measurements of all sensors. We propose a distributed method for computing,
at each sensor, an approximation of the JLF by means of consensus algorithms.
This ""likelihood consensus"" method is applicable if the local likelihood
functions of the various sensors (viewed as conditional probability density
functions of the local measurements) belong to the exponential family of
distributions. We then use the likelihood consensus method to implement a
distributed particle filter and a distributed Gaussian particle filter. Each
sensor runs a local particle filter, or a local Gaussian particle filter, that
computes a global state estimate. The weight update in each local (Gaussian)
particle filter employs the JLF, which is obtained through the likelihood
consensus scheme. For the distributed Gaussian particle filter, the number of
particles can be significantly reduced by means of an additional consensus
scheme. Simulation results are presented to assess the performance of the
proposed distributed particle filters for a multiple target tracking problem.
"
"  In microarray experiments, it is often of interest to identify genes which
have a pre-specified gene expression profile with respect to time. Methods
available in the literature are, however, typically not stringent enough in
identifying such genes, particularly when the profile requires equivalence of
gene expression levels at certain time points. In this paper, the authors
introduce a new methodology, called gene profiling, that uses simultaneous
differential and equivalent gene expression level testing to rank genes
according to a pre-specified gene expression profile. Gene profiling treats the
vector of true gene expression levels as a linear combination of appropriate
vectors, i.e., vectors that give the required criteria for the profile. This
gene-profile model is fitted to the data and the resultant parameter estimates
are summarized in a single test statistic that is then used to rank the genes.
The theoretical underpinnings of gene profiling (equivalence testing,
intersection-union tests) are discussed in this paper, and the gene profiling
methodology is applied to our motivating stem cell experiment.
"
"  Multidimensional scaling (MDS) is a class of projective algorithms
traditionally used in Euclidean space to produce two- or three-dimensional
visualizations of datasets of multidimensional points or point distances. More
recently however, several authors have pointed out that for certain datasets,
hyperbolic target space may provide a better fit than Euclidean space.
  In this paper we develop PD-MDS, a metric MDS algorithm designed specifically
for the Poincare disk (PD) model of the hyperbolic plane. Emphasizing the
importance of proceeding from first principles in spite of the availability of
various black box optimizers, our construction is based on an elementary
hyperbolic line search and reveals numerous particulars that need to be
carefully addressed when implementing this as well as more sophisticated
iterative optimization methods in a hyperbolic space model.
"
"  Bayesian priors offer a compact yet general means of incorporating domain
knowledge into many learning tasks. The correctness of the Bayesian analysis
and inference, however, largely depends on accuracy and correctness of these
priors. PAC-Bayesian methods overcome this problem by providing bounds that
hold regardless of the correctness of the prior distribution. This paper
introduces the first PAC-Bayesian bound for the batch reinforcement learning
problem with function approximation. We show how this bound can be used to
perform model-selection in a transfer learning scenario. Our empirical results
confirm that PAC-Bayesian policy evaluation is able to leverage prior
distributions when they are informative and, unlike standard Bayesian RL
approaches, ignore them when they are misleading.
"
"  High-throughput sequencing allows the detection and quantification of
frequencies of somatic single nucleotide variants (SNV) in heterogeneous tumor
cell populations. In some cases, the evolutionary history and population
frequency of the subclonal lineages of tumor cells present in the sample can be
reconstructed from these SNV frequency measurements. However, automated methods
to do this reconstruction are not available and the conditions under which
reconstruction is possible have not been described.
  We describe the conditions under which the evolutionary history can be
uniquely reconstructed from SNV frequencies from single or multiple samples
from the tumor population and we introduce a new statistical model, PhyloSub,
that infers the phylogeny and genotype of the major subclonal lineages
represented in the population of cancer cells. It uses a Bayesian nonparametric
prior over trees that groups SNVs into major subclonal lineages and
automatically estimates the number of lineages and their ancestry. We sample
from the joint posterior distribution over trees to identify evolutionary
histories and cell population frequencies that have the highest probability of
generating the observed SNV frequency data. When multiple phylogenies are
consistent with a given set of SNV frequencies, PhyloSub represents the
uncertainty in the tumor phylogeny using a partial order plot. Experiments on a
simulated dataset and two real datasets comprising tumor samples from acute
myeloid leukemia and chronic lymphocytic leukemia patients demonstrate that
PhyloSub can infer both linear (or chain) and branching lineages and its
inferences are in good agreement with ground truth, where it is available.
"
"  Hierarchical structure is ubiquitous in data across many domains. There are
many hierarchical clustering methods, frequently used by domain experts, which
strive to discover this structure. However, most of these methods limit
discoverable hierarchies to those with binary branching structure. This
limitation, while computationally convenient, is often undesirable. In this
paper we explore a Bayesian hierarchical clustering algorithm that can produce
trees with arbitrary branching structure at each node, known as rose trees. We
interpret these trees as mixtures over partitions of a data set, and use a
computationally efficient, greedy agglomerative algorithm to find the rose
trees which have high marginal likelihood given the data. Lastly, we perform
experiments which demonstrate that rose trees are better models of data than
the typical binary trees returned by other hierarchical clustering algorithms.
"
"  Optical phase measurement is a simple example of a quantum--limited
measurement problem with important applications in metrology such as
gravitational wave detection. The formulation of optimal strategies for such
measurements is an important test-bed for the development of robust statistical
methods for instrument evaluation. However, the class of possible distributions
exhibits extreme pathologies not commonly encountered in conventional
statistical analysis. To overcome these difficulties we reformulate the basic
variational problem of optimal phase measurement within a Bayesian paradigm and
employ the Shannon information as a robust figure of merit. Single-mode
performance bounds are discussed, and we invoke a general theorem that reduces
the problem of finding the multi-mode performance bounds to the bounding of a
single integral, without need of the central limit theorem.
"
"  Logistic Gaussian process (LGP) priors provide a flexible alternative for
modelling unknown densities. The smoothness properties of the density estimates
can be controlled through the prior covariance structure of the LGP, but the
challenge is the analytically intractable inference. In this paper, we present
approximate Bayesian inference for LGP density estimation in a grid using
Laplace's method to integrate over the non-Gaussian posterior distribution of
latent function values and to determine the covariance function parameters with
type-II maximum a posteriori (MAP) estimation. We demonstrate that Laplace's
method with MAP is sufficiently fast for practical interactive visualisation of
1D and 2D densities. Our experiments with simulated and real 1D data sets show
that the estimation accuracy is close to a Markov chain Monte Carlo
approximation and state-of-the-art hierarchical infinite Gaussian mixture
models. We also construct a reduced-rank approximation to speed up the
computations for dense 2D grids, and demonstrate density regression with the
proposed Laplace approach.
"
"  In this paper we study the covering numbers of the space of convex and
uniformly bounded functions in multi-dimension. We find optimal upper and lower
bounds for the $\epsilon$-covering number of $\C([a, b]^d, B)$, in the
$L_p$-metric, $1 \le p < \infty$, in terms of the relevant constants, where $d
\geq 1$, $a < b \in \mathbb{R}$, $B>0$, and $\C([a,b]^d, B)$ denotes the set of
all convex functions on $[a, b]^d$ that are uniformly bounded by $B$. We
summarize previously known results on covering numbers for convex functions and
also provide alternate proofs of some known results. Our results have direct
implications in the study of rates of convergence of empirical minimization
procedures as well as optimal convergence rates in the numerous convexity
constrained function estimation problems.
"
"  The influence of DNA cis-regulatory elements on a gene's expression has been
intensively studied. However, little is known about expressions driven by
trans-acting DNA hotspots. DNA hotspots harboring copy number aberrations are
recognized to be important in cancer as they influence multiple genes on a
global scale. The challenge in detecting trans-effects is mainly due to the
computational difficulty in detecting weak and sparse trans-acting signals
amidst co-occuring passenger events. We propose an integrative approach to
learn a sparse interaction network of DNA copy-number regions with their
downstream targets in a breast cancer dataset. Information from this network
helps distinguish copy-number driven from copy-number independent expression
changes on a global scale. Our result further delineates cis- and trans-effects
in a breast cancer dataset, for which important oncogenes such as ESR1 and
ERBB2 appear to be highly copy-number dependent. Further, our model is shown to
be efficient and in terms of goodness of fit no worse than other state-of the
art predictors and network reconstruction models using both simulated and real
data.
"
"  Latent feature models are widely used to decompose data into a small number
of components. Bayesian nonparametric variants of these models, which use the
Indian buffet process (IBP) as a prior over latent features, allow the number
of features to be determined from the data. We present a generalization of the
IBP, the distance dependent Indian buffet process (dd-IBP), for modeling
non-exchangeable data. It relies on distances defined between data points,
biasing nearby data to share more features. The choice of distance measure
allows for many kinds of dependencies, including temporal and spatial. Further,
the original IBP is a special case of the dd-IBP. In this paper, we develop the
dd-IBP and theoretically characterize its feature-sharing properties. We derive
a Markov chain Monte Carlo sampler for a linear Gaussian model with a dd-IBP
prior and study its performance on several non-exchangeable data sets.
"
"  We develop a theory of local asymptotic normality in the quantum domain based
on a novel quantum analogue of the log-likelihood ratio. This formulation is
applicable to any quantum statistical model satisfying a mild smoothness
condition. As an application, we prove the asymptotic achievability of the
Holevo bound for the local shift parameter.
"
"  Graph embedding techniques are useful to characterize spectral signature
relations for hyperspectral images. However, such images consists of disjoint
classes due to spatial details that are often ignored by existing graph
computing tools. Robust parameter estimation is a challenge for kernel
functions that compute such graphs. Finding a corresponding high quality
coordinate system to map signature relations remains an open research question.
We answer positively on these challenges by first proposing a kernel function
of spatial and spectral information in computing neighborhood graphs. Secondly,
the study exploits the force field interpretation from mechanics and devise a
unifying nonlinear graph embedding framework. The generalized framework leads
to novel unsupervised multidimensional artificial field embedding techniques
that rely on the simple additive assumption of pair-dependent attraction and
repulsion functions. The formulations capture long range and short range
distance related effects often associated with living organisms and help to
establish algorithmic properties that mimic mutual behavior for the purpose of
dimensionality reduction. The main benefits from the proposed models includes
the ability to preserve the local topology of data and produce quality
visualizations i.e. maintaining disjoint meaningful neighborhoods. As part of
evaluation, visualization, gradient field trajectories, and semisupervised
classification experiments are conducted for image scenes acquired by multiple
sensors at various spatial resolutions over different types of objects. The
results demonstrate the superiority of the proposed embedding framework over
various widely used methods.
"
"  Despite its simplicity, the naive Bayes classifier has surprised machine
learning researchers by exhibiting good performance on a variety of learning
problems. Encouraged by these results, researchers have looked to overcome
naive Bayes primary weakness - attribute independence - and improve the
performance of the algorithm. This paper presents a locally weighted version of
naive Bayes that relaxes the independence assumption by learning local models
at prediction time. Experimental results show that locally weighted naive Bayes
rarely degrades accuracy compared to standard naive Bayes and, in many cases,
improves accuracy dramatically. The main advantage of this method compared to
other techniques for enhancing naive Bayes is its conceptual and computational
simplicity.
"
"  In many real-world networks, nodes have class labels, attributes, or
variables that affect the network's topology. If the topology of the network is
known but the labels of the nodes are hidden, we would like to select a small
subset of nodes such that, if we knew their labels, we could accurately predict
the labels of all the other nodes. We develop an active learning algorithm for
this problem which uses information-theoretic techniques to choose which nodes
to explore. We test our algorithm on networks from three different domains: a
social network, a network of English words that appear adjacently in a novel,
and a marine food web. Our algorithm makes no initial assumptions about how the
groups connect, and performs well even when faced with quite general types of
network structure. In particular, we do not assume that nodes of the same class
are more likely to be connected to each other---only that they connect to the
rest of the network in similar ways.
"
"  Drawing a random sample of ballots to conduct a risk-limiting audit generally
requires knowing how the ballots cast in an election are organized into groups,
for instance, how many containers of ballots there are in all and how many
ballots are in each container. A list of the ballot group identifiers along
with number of ballots in each group is called a ballot manifest. What if the
ballot manifest is not accurate? Surprisingly, even if ballots are known to be
missing from the manifest, it is not necessary to make worst-case assumptions
about those ballots--for instance, to adjust the margin by the number of
missing ballots--to ensure that the audit remains conservative. Rather, it
suffices to make worst-case assumptions about the individual randomly selected
ballots that the audit cannot find. This observation provides a simple
modification to some risk-limiting audit procedures that makes them
automatically become more conservative if the ballot manifest has errors. The
modification--phantoms to evil zombies (~2EZ)--requires only an upper bound on
the total number of ballots cast. ~2EZ makes the audit P-value stochastically
larger than it would be had the manifest been accurate, automatically requiring
more than enough ballots to be audited to offset the manifest errors. This
ensures that the true risk limit remains smaller than the nominal risk limit.
On the other hand, if the manifest is in fact accurate and the upper bound on
the total number of ballots equals the total according to the manifest, ~2EZ
has no effect at all on the number of ballots audited nor on the true risk
limit.
"
"  We propose a method for post-processing an ensemble of multivariate forecasts
in order to obtain a joint predictive distribution of weather. Our method
utilizes existing univariate post-processing techniques, in this case ensemble
Bayesian model averaging (BMA), to obtain estimated marginal distributions.
However, implementing these methods individually offers no information
regarding the joint distribution. To correct this, we propose the use of a
Gaussian copula, which offers a simple procedure for recovering the dependence
that is lost in the estimation of the ensemble BMA marginals. Our method is
applied to 48-h forecasts of a set of five weather quantities using the
8-member University of Washington mesoscale ensemble. We show that our method
recovers many well-understood dependencies between weather quantities and
subsequently improves calibration and sharpness over both the raw ensemble and
a method which does not incorporate joint distributional information.
"
"  Despite the overwhelming success of the existing Social Networking Services
(SNS), their centralized ownership and control have led to serious concerns in
user privacy, censorship vulnerability and operational robustness of these
services. To overcome these limitations, Distributed Social Networks (DSN) have
recently been proposed and implemented. Under these new DSN architectures, no
single party possesses the full knowledge of the entire social network. While
this approach solves the above problems, the lack of global knowledge for the
DSN nodes makes it much more challenging to support some common but critical
SNS services like friends discovery and community detection. In this paper, we
tackle the problem of community detection for a given user under the constraint
of limited local topology information as imposed by common DSN architectures.
By considering the Personalized Page Rank (PPR) approach as an ink spilling
process, we justify its applicability for decentralized community detection
using limited local topology information.Our proposed PPR-based solution has a
wide range of applications such as friends recommendation, targeted
advertisement, automated social relationship labeling and sybil defense. Using
data collected from a large-scale SNS in practice, we demonstrate our adapted
version of PPR can significantly outperform the basic PR as well as two other
commonly used heuristics. The inclusion of a few manually labeled friends in
the Escape Vector (EV) can boost the performance considerably (64.97% relative
improvement in terms of Area Under the ROC Curve (AUC)).
"
"  We investigate a family of poisoning attacks against Support Vector Machines
(SVM). Such attacks inject specially crafted training data that increases the
SVM's test error. Central to the motivation for these attacks is the fact that
most learning algorithms assume that their training data comes from a natural
or well-behaved distribution. However, this assumption does not generally hold
in security-sensitive settings. As we demonstrate, an intelligent adversary
can, to some extent, predict the change of the SVM's decision function due to
malicious input and use this ability to construct malicious data. The proposed
attack uses a gradient ascent strategy in which the gradient is computed based
on properties of the SVM's optimal solution. This method can be kernelized and
enables the attack to be constructed in the input space even for non-linear
kernels. We experimentally demonstrate that our gradient ascent procedure
reliably identifies good local maxima of the non-convex validation error
surface, which significantly increases the classifier's test error.
"
"  Given a convex optimization problem and its dual, there are many possible
first-order algorithms. In this paper, we show the equivalence between mirror
descent algorithms and algorithms generalizing the conditional gradient method.
This is done through convex duality, and implies notably that for certain
problems, such as for supervised machine learning problems with non-smooth
losses or problems regularized by non-smooth regularizers, the primal
subgradient method and the dual conditional gradient method are formally
equivalent. The dual interpretation leads to a form of line search for mirror
descent, as well as guarantees of convergence for primal-dual certificates.
"
"  The success of kernel-based learning methods depend on the choice of kernel.
Recently, kernel learning methods have been proposed that use data to select
the most appropriate kernel, usually by combining a set of base kernels. We
introduce a new algorithm for kernel learning that combines a {\em continuous
set of base kernels}, without the common step of discretizing the space of base
kernels. We demonstrate that our new method achieves state-of-the-art
performance across a variety of real-world datasets. Furthermore, we explicitly
demonstrate the importance of combining the right dictionary of kernels, which
is problematic for methods based on a finite set of base kernels chosen a
priori. Our method is not the first approach to work with continuously
parameterized kernels. However, we show that our method requires substantially
less computation than previous such approaches, and so is more amenable to
multiple dimensional parameterizations of base kernels, which we demonstrate.
"
"  We present a flexible branching process model for cell population dynamics in
synchrony/time-series experiments used to study important cellular processes.
Its formulation is constructive, based on an accounting of the unique cohorts
in the population as they arise and evolve over time, allowing it to be written
in closed form. The model can attribute effects to subsets of the population,
providing flexibility not available using the models historically applied to
these populations. It provides a tool for in silico synchronization of the
population and can be used to deconvolve population-level experimental
measurements, such as temporal expression profiles. It also allows for the
direct comparison of assay measurements made from multiple experiments. The
model can be fit either to budding index or DNA content measurements, or both,
and is easily adaptable to new forms of data. The ability to use DNA content
data makes the model applicable to almost any organism. We describe the model
and illustrate its utility and flexibility in a study of cell cycle progression
in the yeast Saccharomyces cerevisiae.
"
"  It is difficult to find the optimal sparse solution of a manifold learning
based dimensionality reduction algorithm. The lasso or the elastic net
penalized manifold learning based dimensionality reduction is not directly a
lasso penalized least square problem and thus the least angle regression (LARS)
(Efron et al. \cite{LARS}), one of the most popular algorithms in sparse
learning, cannot be applied. Therefore, most current approaches take indirect
ways or have strict settings, which can be inconvenient for applications. In
this paper, we proposed the manifold elastic net or MEN for short. MEN
incorporates the merits of both the manifold learning based dimensionality
reduction and the sparse learning based dimensionality reduction. By using a
series of equivalent transformations, we show MEN is equivalent to the lasso
penalized least square problem and thus LARS is adopted to obtain the optimal
sparse solution of MEN. In particular, MEN has the following advantages for
subsequent classification: 1) the local geometry of samples is well preserved
for low dimensional data representation, 2) both the margin maximization and
the classification error minimization are considered for sparse projection
calculation, 3) the projection matrix of MEN improves the parsimony in
computation, 4) the elastic net penalty reduces the over-fitting problem, and
5) the projection matrix of MEN can be interpreted psychologically and
physiologically. Experimental evidence on face recognition over various popular
datasets suggests that MEN is superior to top level dimensionality reduction
algorithms.
"
"  The increasing availability of longitudinal student achievement data has
heightened interest among researchers, educators and policy makers in using
these data to evaluate educational inputs, as well as for school and possibly
teacher accountability. Researchers have developed elaborate ""value-added
models"" of these longitudinal data to estimate the effects of educational
inputs (e.g., teachers or schools) on student achievement while using prior
achievement to adjust for nonrandom assignment of students to schools and
classes. A challenge to such modeling efforts is the extensive numbers of
students with incomplete records and the tendency for those students to be
lower achieving. These conditions create the potential for results to be
sensitive to violations of the assumption that data are missing at random,
which is commonly used when estimating model parameters. The current study
extends recent value-added modeling approaches for longitudinal student
achievement data Lockwood et al. [J. Educ. Behav. Statist. 32 (2007) 125--150]
to allow data to be missing not at random via random effects selection and
pattern mixture models, and applies those methods to data from a large urban
school district to estimate effects of elementary school mathematics teachers.
We find that allowing the data to be missing not at random has little impact on
estimated teacher effects. The robustness of estimated teacher effects to the
missing data assumptions appears to result from both the relatively small
impact of model specification on estimated student effects compared with the
large variability in teacher effects and the downweighting of scores from
students with incomplete data.
"
"  The processes influencing animal movement and resource selection are complex
and varied. Past efforts to model behavioral changes over time used Bayesian
statistical models with variable parameter space, such as reversible-jump
Markov chain Monte Carlo approaches, which are computationally demanding and
inaccessible to many practitioners. We present a continuous-time discrete-space
(CTDS) model of animal movement that can be fit using standard generalized
linear modeling (GLM) methods. This CTDS approach allows for the joint modeling
of location-based as well as directional drivers of movement. Changing behavior
over time is modeled using a varying-coefficient framework which maintains the
computational simplicity of a GLM approach, and variable selection is
accomplished using a group lasso penalty. We apply our approach to a study of
two mountain lions (Puma concolor) in Colorado, USA.
"
"  Mixtures of linear mixed models (MLMMs) are useful for clustering grouped
data and can be estimated by likelihood maximization through the EM algorithm.
The conventional approach to determining a suitable number of components is to
compare different mixture models using penalized log-likelihood criteria such
as BIC.We propose fitting MLMMs with variational methods which can perform
parameter estimation and model selection simultaneously. A variational
approximation is described where the variational lower bound and parameter
updates are in closed form, allowing fast evaluation. A new variational greedy
algorithm is developed for model selection and learning of the mixture
components. This approach allows an automatic initialization of the algorithm
and returns a plausible number of mixture components automatically. In cases of
weak identifiability of certain model parameters, we use hierarchical centering
to reparametrize the model and show empirically that there is a gain in
efficiency by variational algorithms similar to that in MCMC algorithms.
Related to this, we prove that the approximate rate of convergence of
variational algorithms by Gaussian approximation is equal to that of the
corresponding Gibbs sampler which suggests that reparametrizations can lead to
improved convergence in variational algorithms as well.
"
"  In several recent publications, Bettencourt, West and collaborators claim
that properties of cities such as gross economic production, personal income,
numbers of patents filed, number of crimes committed, etc., show super-linear
power-scaling with total population, while measures of resource use show
sub-linear power-law scaling. Re-analysis of the gross economic production and
personal income for cities in the United States, however, shows that the data
cannot distinguish between power laws and other functional forms, including
logarithmic growth, and that size predicts relatively little of the variation
between cities. The striking appearance of scaling in previous work is largely
artifact of using extensive quantities (city-wide totals) rather than intensive
ones (per-capita rates). The remaining dependence of productivity on city size
is explained by concentration of specialist service industries, with high
value-added per worker, in larger cities, in accordance with the long-standing
economic notion of the ""hierarchy of central places"".
"
"  This paper examines the problem of ranking a collection of objects using
pairwise comparisons (rankings of two objects). In general, the ranking of $n$
objects can be identified by standard sorting methods using $n log_2 n$
pairwise comparisons. We are interested in natural situations in which
relationships among the objects may allow for ranking using far fewer pairwise
comparisons. Specifically, we assume that the objects can be embedded into a
$d$-dimensional Euclidean space and that the rankings reflect their relative
distances from a common reference point in $R^d$. We show that under this
assumption the number of possible rankings grows like $n^{2d}$ and demonstrate
an algorithm that can identify a randomly selected ranking using just slightly
more than $d log n$ adaptively selected pairwise comparisons, on average. If
instead the comparisons are chosen at random, then almost all pairwise
comparisons must be made in order to identify any ranking. In addition, we
propose a robust, error-tolerant algorithm that only requires that the pairwise
comparisons are probably correct. Experimental studies with synthetic and real
datasets support the conclusions of our theoretical analysis.
"
"  We consider the problem of learning Bayesian network classifiers that
maximize the marginover a set of classification variables. We find that this
problem is harder for Bayesian networks than for undirected graphical models
like maximum margin Markov networks. The main difficulty is that the parameters
in a Bayesian network must satisfy additional normalization constraints that an
undirected graphical model need not respect. These additional constraints
complicate the optimization task. Nevertheless, we derive an effective training
algorithm that solves the maximum margin training problem for a range of
Bayesian network topologies, and converges to an approximate solution for
arbitrary network topologies. Experimental results show that the method can
demonstrate improved generalization performance over Markov networks when the
directed graphical structure encodes relevant knowledge. In practice, the
training technique allows one to combine prior knowledge expressed as a
directed (causal) model with state of the art discriminative learning methods.
"
"  Recent literature has shown that the control of False Discovery Rate (FDR)
for distributed detection in wireless sensor networks (WSNs) can provide
substantial improvement in detection performance over conventional design
methodologies. In this paper, we further investigate system design issues in
FDR based distributed detection. We demonstrate that improved system design may
be achieved by employing the Kolmogorov-Smirnov distance metric instead of the
deflection coefficient, as originally proposed in Ray&VarshneyAES11. We also
analyze the performance of FDR based distributed detection in the presence of
Byzantines. Byzantines are malicious sensors which send falsified information
to the Fusion Center (FC) to deteriorate system performance. We provide
analytical and simulation results on the global detection probability as a
function of the fraction of Byzantines in the network. It is observed that the
detection performance degrades considerably when the fraction of Byzantines is
large. Hence, we propose an adaptive algorithm at the FC which learns the
Byzantines' behavior over time and changes the FDR parameter to overcome the
loss in detection performance. Detailed simulation results are provided to
demonstrate the robustness of the proposed adaptive algorithm to Byzantine
attacks in WSNs.
"
"  Bayesian learning is often hampered by large computational expense. As a
powerful generalization of popular belief propagation, expectation propagation
(EP) efficiently approximates the exact Bayesian computation. Nevertheless, EP
can be sensitive to outliers and suffer from divergence for difficult cases. To
address this issue, we propose a new approximate inference approach, relaxed
expectation propagation (REP). It relaxes the moment matching requirement of
expectation propagation by adding a relaxation factor into the KL minimization.
We penalize this relaxation with a $l_1$ penalty. As a result, when two
distributions in the relaxed KL divergence are similar, the relaxation factor
will be penalized to zero and, therefore, we obtain the original moment
matching; In the presence of outliers, these two distributions are
significantly different and the relaxation factor will be used to reduce the
contribution of the outlier. Based on this penalized KL minimization, REP is
robust to outliers and can greatly improve the posterior approximation quality
over EP. To examine the effectiveness of REP, we apply it to Gaussian process
classification, a task known to be suitable to EP. Our classification results
on synthetic and UCI benchmark datasets demonstrate significant improvement of
REP over EP and Power EP--in terms of algorithmic stability, estimation
accuracy and predictive performance.
"
"  The correlation coefficient between stocks depends on price history and
includes information on hierarchical structure in financial markets. It is
useful for portfolio selection and estimation of risk. I introduce the Life
Time of Correlation between stocks prices to know how far we should investigate
the price history to obtain the optimal durability of correlation. I carry out
my research on emerging (Poland) and established markets (in the USA, Great
Britain and Germany). Other methods, including the Minimum Spanning Trees, tree
half-life, decomposition of correlations and the Epps effect are also
discussed.
"
"  We consider the problem of learning the structure of Ising models (pairwise
binary Markov random fields) from i.i.d. samples. While several methods have
been proposed to accomplish this task, their relative merits and limitations
remain somewhat obscure. By analyzing a number of concrete examples, we show
that low-complexity algorithms often fail when the Markov random field develops
long-range correlations. More precisely, this phenomenon appears to be related
to the Ising model phase transition (although it does not coincide with it).
"
"  We investigate different ways of generating approximate solutions to the
pairwise Markov random field (MRF) selection problem. We focus mainly on the
inverse Ising problem, but discuss also the somewhat related inverse Gaussian
problem because both types of MRF are suitable for inference tasks with the
belief propagation algorithm (BP) under certain conditions. Our approach
consists in to take a Bethe mean-field solution obtained with a maximum
spanning tree (MST) of pairwise mutual information, referred to as the
\emph{Bethe reference point}, for further perturbation procedures. We consider
three different ways following this idea: in the first one, we select and
calibrate iteratively the optimal links to be added starting from the Bethe
reference point; the second one is based on the observation that the natural
gradient can be computed analytically at the Bethe point; in the third one,
assuming no local field and using low temperature expansion we develop a dual
loop joint model based on a well chosen fundamental cycle basis. We indeed
identify a subclass of planar models, which we refer to as \emph{Bethe-dual
graph models}, having possibly many loops, but characterized by a singly
connected dual factor graph, for which the partition function and the linear
response can be computed exactly in respectively O(N) and $O(N^2)$ operations,
thanks to a dual weight propagation (DWP) message passing procedure that we set
up. When restricted to this subclass of models, the inverse Ising problem being
convex, becomes tractable at any temperature. Experimental tests on various
datasets with refined $L_0$ or $L_1$ regularization procedures indicate that
these approaches may be competitive and useful alternatives to existing ones.
"
"  We derive an optimal strategy in the popular Deal or No Deal game show.
Q-learning quantifies the continuation value inherent in sequential decision
making and we use this to analyze contestants risky choices. Given their
choices and optimal strategy, we invert to find implied bounds on their levels
of risk aversion. In risky decision making, previous empirical evidence has
suggested that past outcomes affect future choices and that contestants have
time-varying risk aversion. We demonstrate that the strategies of two players
(Suzanne and Frank) from the European version of the game are consistent with
constant risk aversion levels except for their last risk-seeking choice.
"
"  Unbalanced data arises in many learning tasks such as clustering of
multi-class data, hierarchical divisive clustering and semisupervised learning.
Graph-based approaches are popular tools for these problems. Graph construction
is an important aspect of graph-based learning. We show that graph-based
algorithms can fail for unbalanced data for many popular graphs such as k-NN,
\epsilon-neighborhood and full-RBF graphs. We propose a novel graph
construction technique that encodes global statistical information into node
degrees through a ranking scheme. The rank of a data sample is an estimate of
its p-value and is proportional to the total number of data samples with
smaller density. This ranking scheme serves as a surrogate for density; can be
reliably estimated; and indicates whether a data sample is close to
valleys/modes. This rank-modulated degree(RMD) scheme is able to significantly
sparsify the graph near valleys and provides an adaptive way to cope with
unbalanced data. We then theoretically justify our method through limit cut
analysis. Unsupervised and semi-supervised experiments on synthetic and real
data sets demonstrate the superiority of our method.
"
"  We consider data consisting of photon counts of diffracted x-ray radiation as
a function of the angle of diffraction. The problem is to determine the
positions, powers and shapes of the relevant peaks. An additional difficulty is
that the power of the peaks is to be measured from a baseline which itself must
be identified. Most methods of de-noising data of this kind do not explicitly
take into account the modality of the final estimate. The residual-based
procedure we propose uses the so-called taut string method, which minimizes the
number of peaks subject to a tube constraint on the integrated data. The
baseline is identified by combining the result of the taut string with an
estimate of the first derivative of the baseline obtained using a weighted
smoothing spline. Finally, each individual peak is expressed as the finite sum
of kernels chosen from a parametric family.
"
"  Post-election audits use the discrepancy between machine counts and a hand
tally of votes in a random sample of precincts to infer whether error affected
the electoral outcome. The maximum relative overstatement of pairwise margins
(MRO) quantifies that discrepancy. The electoral outcome a full hand tally
shows must agree with the apparent outcome if the MRO is less than 1. This
condition is sharper than previous ones when there are more than two candidates
or when voters may vote for more than one candidate. For the 2006 U.S. Senate
race in Minnesota, a test using MRO gives a $P$-value of 4.05% for the
hypothesis that a full hand tally would find a different winner, less than half
the value Stark [Ann. Appl. Statist. 2 (2008) 550--581] finds.
"
"  Comonotonicity had been a extreme case of dependency between random
variables. This article consider an extension of single life model under
multiple dependent decrement causes to the case of comonotonic group-life.
"
"  With ever-increasing available data, predicting individuals' preferences and
helping them locate the most relevant information has become a pressing need.
Understanding and predicting preferences is also important from a fundamental
point of view, as part of what has been called a ""new"" computational social
science. Here, we propose a novel approach based on stochastic block models,
which have been developed by sociologists as plausible models of complex
networks of social interactions. Our model is in the spirit of predicting
individuals' preferences based on the preferences of others but, rather than
fitting a particular model, we rely on a Bayesian approach that samples over
the ensemble of all possible models. We show that our approach is considerably
more accurate than leading recommender algorithms, with major relative
improvements between 38% and 99% over industry-level algorithms. Besides, our
approach sheds light on decision-making processes by identifying groups of
individuals that have consistently similar preferences, and enabling the
analysis of the characteristics of those groups.
"
"  We propose the Bayesian bridge estimator for regularized regression and
classification. Two key mixture representations for the Bayesian bridge model
are developed: (1) a scale mixture of normals with respect to an alpha-stable
random variable; and (2) a mixture of Bartlett--Fejer kernels (or triangle
densities) with respect to a two-component mixture of gamma random variables.
Both lead to MCMC methods for posterior simulation, and these methods turn out
to have complementary domains of maximum efficiency. The first representation
is a well known result due to West (1987), and is the better choice for
collinear design matrices. The second representation is new, and is more
efficient for orthogonal problems, largely because it avoids the need to deal
with exponentially tilted stable random variables. It also provides insight
into the multimodality of the joint posterior distribution, a feature of the
bridge model that is notably absent under ridge or lasso-type priors. We prove
a theorem that extends this representation to a wider class of densities
representable as scale mixtures of betas, and provide an explicit inversion
formula for the mixing distribution. The connections with slice sampling and
scale mixtures of normals are explored. On the practical side, we find that the
Bayesian bridge model outperforms its classical cousin in estimation and
prediction across a variety of data sets, both simulated and real. We also show
that the MCMC for fitting the bridge model exhibits excellent mixing
properties, particularly for the global scale parameter. This makes for a
favorable contrast with analogous MCMC algorithms for other sparse Bayesian
models. All methods described in this paper are implemented in the R package
BayesBridge. An extensive set of simulation results are provided in two
supplemental files.
"
"  Vocal tract resonance characteristics in acoustic speech signals are
classically tracked using frame-by-frame point estimates of formant frequencies
followed by candidate selection and smoothing using dynamic programming methods
that minimize ad hoc cost functions. The goal of the current work is to provide
both point estimates and associated uncertainties of center frequencies and
bandwidths in a statistically principled state-space framework. Extended Kalman
(K) algorithms take advantage of a linearized mapping to infer formant and
antiformant parameters from frame-based estimates of autoregressive moving
average (ARMA) cepstral coefficients. Error analysis of KARMA, WaveSurfer, and
Praat is accomplished in the all-pole case using a manually marked formant
database and synthesized speech waveforms. KARMA formant tracks exhibit lower
overall root-mean-square error relative to the two benchmark algorithms, with
third formant tracking more challenging. Antiformant tracking performance of
KARMA is illustrated using synthesized and spoken nasal phonemes. The
simultaneous tracking of uncertainty levels enables practitioners to recognize
time-varying confidence in parameters of interest and adjust algorithmic
settings accordingly.
"
"  Unbinned maximum likelihood is a common procedure for parameter estimation.
After parameters have been estimated, it is crucial to know whether the fit
model adequately describes the experimental data. Univariate Goodness of Fit
procedures have been thoroughly analyzed. In multi-dimensions, Goodness of Fit
test powers have rarely been studied on realistic problems. There is no
definitive answer to regarding which method is better. Test performance is
strictly related to specific analysis characteristics. In this work, a review
of multi-variate Goodness of Fit techniques is presented.
"
"  Item response theory (IRT) models are a class of statistical models used to
describe the response behaviors of individuals to a set of items having a
certain number of options. They are adopted by researchers in social science,
particularly in the analysis of performance or attitudinal data, in psychology,
education, medicine, marketing and other fields where the aim is to measure
latent constructs. Most IRT analyses use parametric models that rely on
assumptions that often are not satisfied. In such cases, a nonparametric
approach might be preferable; nevertheless, there are not many software
applications allowing to use that. To address this gap, this paper presents the
R package KernSmoothIRT. It implements kernel smoothing for the estimation of
option characteristic curves, and adds several plotting and analytical tools to
evaluate the whole test/questionnaire, the items, and the subjects. In order to
show the package's capabilities, two real datasets are used, one employing
multiple-choice responses, and the other scaled responses.
"
"  Using a nonparametric function estimation methodology, we present a
comparative analysis of the WMAP 1-, 3-, 5-, and 7-year data releases for the
CMB angular power spectrum with respect to the following key questions: (a) How
well is the power spectrum determined by the data alone? (b) How well is the
$\Lambda$CDM model supported by a model-independent, data-driven analysis? (c)
What are the realistic uncertainties on peak/dip locations and heights? Our
results show that the height of the power spectrum is well determined by data
alone for multipole l approximately less than 546 (1-year), 667 (3-year), 804
(5-year), and 842 (7-year data). We show that parametric fits based on the
$\Lambda$CDM model are remarkably close to our nonparametric fits in
$l$-regions where data are sufficiently precise. In contrast, the power
spectrum for an H$\Lambda$CDM model gets progressively pushed away from our
nonparametric fit as data quality improves with successive data realizations,
suggesting incompatibility of this particular cosmological model with respect
to the WMAP data sets. We present uncertainties on peak/dip locations and
heights at the 95% ($2 \sigma$) level of confidence, and show how these
uncertainties translate into hyperbolic ""bands"" on the acoustic scale ($l_A$)
and peak shift ($\phi_m$) parameters. Based on the confidence set for the
7-year data, we argue that the low-l up-turn in the CMB power spectrum cannot
be ruled out at any confidence level in excess of about 10% ($\approx 0.12
\sigma$). Additional outcomes of this work are a numerical formulation for
minimization of a noise-weighted risk function subject to monotonicity
constraints, a prescription for obtaining nonparametric fits that are closer to
cosmological expectations on smoothness, and a method for sampling
cosmologically meaningful power spectrum variations from the confidence set of
a nonparametric fit.
"
"  The optimum interval method for finding an upper limit of a one-dimensionally
distributed signal in the presence of an unknown background is extended to the
case of high statistics. There is also some discussion of how the method can be
extended to the multiple dimensional case.
"
"  This chapter provides a overview of Bayesian inference, mostly emphasising
that it is a universal method for summarising uncertainty and making estimates
and predictions using probability statements conditional on observed data and
an assumed model (Gelman 2008). The Bayesian perspective is thus applicable to
all aspects of statistical inference, while being open to the incorporation of
information items resulting from earlier experiments and from expert opinions.
We provide here the basic elements of Bayesian analysis when considered for
standard models, refering to Marin and Robert (2007) and to Robert (2007) for
book-length entries.1 In the following, we refrain from embarking upon
philosophical discussions about the nature of knowledge (see, e.g., Robert
2007, Chapter 10), opting instead for a mathematically sound presentation of an
eminently practical statistical methodology. We indeed believe that the most
convincing arguments for adopting a Bayesian version of data analyses are in
the versatility of this tool and in the large range of existing applications,
rather than in those polemical arguments.
"
"  Multivariate machine learning methods are increasingly used to analyze
neuroimaging data, often replacing more traditional ""mass univariate""
techniques that fit data one voxel at a time. In the functional magnetic
resonance imaging (fMRI) literature, this has led to broad application of
""off-the-shelf"" classification and regression methods. These generic approaches
allow investigators to use ready-made algorithms to accurately decode
perceptual, cognitive, or behavioral states from distributed patterns of neural
activity. However, when applied to correlated whole-brain fMRI data these
methods suffer from coefficient instability, are sensitive to outliers, and
yield dense solutions that are hard to interpret without arbitrary
thresholding. Here, we develop variants of the the Graph-constrained Elastic
Net (GraphNet), ..., we (1) extend GraphNet to include robust loss functions
that confer insensitivity to outliers, (2) equip them with ""adaptive"" penalties
that asymptotically guarantee correct variable selection, and (3) develop a
novel sparse structured Support Vector GraphNet classifier (SVGN). When applied
to previously published data, these efficient whole-brain methods significantly
improved classification accuracy over previously reported VOI-based analyses on
the same data while discovering task-related regions not documented in the
original VOI approach. Critically, GraphNet estimates generalize well to
out-of-sample data collected more than three years later on the same task but
with different subjects and stimuli. By enabling robust and efficient selection
of important voxels from whole-brain data taken over multiple time points
(>100,000 ""features""), these methods enable data-driven selection of brain
areas that accurately predict single-trial behavior within and across
individuals.
"
"  This paper describes a graph visualization methodology based on hierarchical
maximal modularity clustering, with interactive and significant coarsening and
refining possibilities. An application of this method to HIV epidemic analysis
in Cuba is outlined.
"
"  Inference is an integral part of probabilistic topic models, but is often
non-trivial to derive an efficient algorithm for a specific model. It is even
much more challenging when we want to find a fast inference algorithm which
always yields sparse latent representations of documents. In this article, we
introduce a simple framework for inference in probabilistic topic models,
denoted by FW. This framework is general and flexible enough to be easily
adapted to mixture models. It has a linear convergence rate, offers an easy way
to incorporate prior knowledge, and provides us an easy way to directly trade
off sparsity against quality and time. We demonstrate the goodness and
flexibility of FW over existing inference methods by a number of tasks.
Finally, we show how inference in topic models with nonconjugate priors can be
done efficiently.
"
"  This paper concerns with the sensor management problem in collocated
Multiple-Input Multiple-Output (MIMO) radars. After deriving the Cramer-Rao
Lower Bound (CRLB) as a performance measure, the antenna allocation problem is
formulated as a standard Semi-definite Programming (SDP) for the single-target
case. In addition, for multiple unresolved target scenarios, a sampling-based
algorithm is proposed to deal with the non-convexity of the cost function.
Simulations confirm the superiority of the localization results under the
optimal structure.
"
"  Multi-armed bandit problems are the most basic examples of sequential
decision problems with an exploration-exploitation trade-off. This is the
balance between staying with the option that gave highest payoffs in the past
and exploring new options that might give higher payoffs in the future.
Although the study of bandit problems dates back to the Thirties,
exploration-exploitation trade-offs arise in several modern applications, such
as ad placement, website optimization, and packet routing. Mathematically, a
multi-armed bandit is defined by the payoff process associated with each
option. In this survey, we focus on two extreme cases in which the analysis of
regret is particularly simple and elegant: i.i.d. payoffs and adversarial
payoffs. Besides the basic setting of finitely many actions, we also analyze
some of the most important variants and extensions, such as the contextual
bandit model.
"
"  Balancing a rare and serious possibility against a more common and less
serious one is a familiar problem in many situations, such as the prediction of
rare diseases. The relative costs of forecasting errors can be used for any
prediction method that gives an estimated probability of a future event. The
probability at which the likely cost (defined as cost x probability) of a
possible false negative is exactly equal to that of a possible false positive
gives the relevant cutpoint and all subjects with probability of disease
greater than this have a positive test result. All standard methods of logistic
regression will give the log-odds and hence the predicted probability of a
positive outcome for every subject:
"
"  This paper constructs dynamical models and estimation algorithms for the
concentration of target molecules in a fluid flow using an array of novel
biosensors. Each biosensor is constructed out of protein molecules embedded in
a synthetic cell membrane. The concentration evolves according to an
advection-diffusion partial differential equation which is coupled with
chemical reaction equations on the biosensor surface. By using averaging theory
methods and the divergence theorem, an approximate model is constructed that
describes the asymptotic behaviour of the concentration as a system of ordinary
differential equations. The estimate of target molecules is then obtained by
solving a nonlinear least squares problem. It is shown that the estimator is
strongly consistent and asymptotically normal. An explicit expression is
obtained for the asymptotic variance of the estimation error. As an example,
the results are illustrated for a novel biosensor built out of protein
molecules.
"
"  This paper proposes a technique for the unsupervised detection and tracking
of arbitrary objects in videos. It is intended to reduce the need for detection
and localization methods tailored to specific object types and serve as a
general framework applicable to videos with varied objects, backgrounds, and
image qualities. The technique uses a dependent Dirichlet process mixture
(DDPM) known as the Generalized Polya Urn (GPUDDPM) to model image pixel data
that can be easily and efficiently extracted from the regions in a video that
represent objects. This paper describes a specific implementation of the model
using spatial and color pixel data extracted via frame differencing and gives
two algorithms for performing inference in the model to accomplish detection
and tracking. This technique is demonstrated on multiple synthetic and
benchmark video datasets that illustrate its ability to, without modification,
detect and track objects with diverse physical characteristics moving over
non-uniform backgrounds and through occlusion.
"
"  Network tomography has been regarded as one of the most promising
methodologies for performance evaluation and diagnosis of the massive and
decentralized Internet. This paper proposes a new estimation approach for
solving a class of inverse problems in network tomography, based on marginal
distributions of a sequence of one-dimensional linear projections of the
observed data. We give a general identifiability result for the proposed method
and study the design issue of these one dimensional projections in terms of
statistical efficiency. We show that for a simple Gaussian tomography model,
there is an optimal set of one-dimensional projections such that the estimator
obtained from these projections is asymptotically as efficient as the maximum
likelihood estimator based on the joint distribution of the observed data. For
practical applications, we carry out simulation studies of the proposed method
for two instances of network tomography. The first is for traffic demand
tomography using a Gaussian Origin-Destination traffic model with a power
relation between its mean and variance, and the second is for network delay
tomography where the link delays are to be estimated from the end-to-end path
delays. We compare estimators obtained from our method and that obtained from
using the joint distribution and other lower dimensional projections, and show
that in both cases, the proposed method yields satisfactory results.
"
"  Even though clustering trajectory data attracted considerable attention in
the last few years, most of prior work assumed that moving objects can move
freely in an euclidean space and did not consider the eventual presence of an
underlying road network and its influence on evaluating the similarity between
trajectories. In this paper, we present two approaches to clustering
network-constrained trajectory data. The first approach discovers clusters of
trajectories that traveled along the same parts of the road network. The second
approach is segment-oriented and aims to group together road segments based on
trajectories that they have in common. Both approaches use a graph model to
depict the interactions between observations w.r.t. their similarity and
cluster this similarity graph using a community detection algorithm. We also
present experimental results obtained on synthetic data to showcase our
propositions.
"
"  Transactional network data can be thought of as a list of one-to-many
communications(e.g., email) between nodes in a social network. Most social
network models convert this type of data into binary relations between pairs of
nodes. We develop a latent mixed membership model capable of modeling richer
forms of transactional network data, including relations between more than two
nodes. The model can cluster nodes and predict transactions. The block-model
nature of the model implies that groups can be characterized in very general
ways. This flexible notion of group structure enables discovery of rich
structure in transactional networks. Estimation and inference are accomplished
via a variational EM algorithm. Simulations indicate that the learning
algorithm can recover the correct generative model. Interesting structure is
discovered in the Enron email dataset and another dataset extracted from the
Reddit website. Analysis of the Reddit data is facilitated by a novel
performance measure for comparing two soft clusterings. The new model is
superior at discovering mixed membership in groups and in predicting
transactions.
"
"  Motivated by problems of anomaly detection, this paper implements the
Neyman-Pearson paradigm to deal with asymmetric errors in binary classification
with a convex loss. Given a finite collection of classifiers, we combine them
and obtain a new classifier that satisfies simultaneously the two following
properties with high probability: (i) its probability of type I error is below
a pre-specified level and (ii), it has probability of type II error close to
the minimum possible. The proposed classifier is obtained by solving an
optimization problem with an empirical objective and an empirical constraint.
New techniques to handle such problems are developed and have consequences on
chance constrained programming.
"
"  This paper studies the partial estimation of Gaussian graphical models from
high-dimensional empirical observations. We derive a convex formulation for
this problem using $\ell_1$-regularized maximum-likelihood estimation, which
can be solved via a block coordinate descent algorithm. Statistical estimation
performance can be established for our method. The proposed approach has
competitive empirical performance compared to existing methods, as demonstrated
by various experiments on synthetic and real datasets.
"
"  We introduce a data-driven order reduction method for nonlinear control
systems, drawing on recent progress in machine learning and statistical
dimensionality reduction. The method rests on the assumption that the nonlinear
system behaves linearly when lifted into a high (or infinite) dimensional
feature space where balanced truncation may be carried out implicitly. This
leads to a nonlinear reduction map which can be combined with a representation
of the system belonging to a reproducing kernel Hilbert space to give a closed,
reduced order dynamical system which captures the essential input-output
characteristics of the original model. Empirical simulations illustrating the
approach are also provided.
"
"  We extend the mixtures of Gaussians (MOG) model to the projected mixture of
Gaussians (PMOG) model. In the PMOG model, we assume that q dimensional input
data points z_i are projected by a q dimensional vector w into 1-D variables
u_i. The projected variables u_i are assumed to follow a 1-D MOG model. In the
PMOG model, we maximize the likelihood of observing u_i to find both the model
parameters for the 1-D MOG as well as the projection vector w. First, we derive
an EM algorithm for estimating the PMOG model. Next, we show how the PMOG model
can be applied to the problem of blind source separation (BSS). In contrast to
conventional BSS where an objective function based on an approximation to
differential entropy is minimized, PMOG based BSS simply minimizes the
differential entropy of projected sources by fitting a flexible MOG model in
the projected 1-D space while simultaneously optimizing the projection vector
w. The advantage of PMOG over conventional BSS algorithms is the more flexible
fitting of non-Gaussian source densities without assuming near-Gaussianity (as
in conventional BSS) and still retaining computational feasibility.
"
"  Spatial Independent Component Analysis (ICA) is an increasingly used
data-driven method to analyze functional Magnetic Resonance Imaging (fMRI)
data. To date, it has been used to extract sets of mutually correlated brain
regions without prior information on the time course of these regions. Some of
these sets of regions, interpreted as functional networks, have recently been
used to provide markers of brain diseases and open the road to paradigm-free
population comparisons. Such group studies raise the question of modeling
subject variability within ICA: how can the patterns representative of a group
be modeled and estimated via ICA for reliable inter-group comparisons? In this
paper, we propose a hierarchical model for patterns in multi-subject fMRI
datasets, akin to mixed-effect group models used in linear-model-based
analysis. We introduce an estimation procedure, CanICA (Canonical ICA), based
on i) probabilistic dimension reduction of the individual data, ii) canonical
correlation analysis to identify a data subspace common to the group iii)
ICA-based pattern extraction. In addition, we introduce a procedure based on
cross-validation to quantify the stability of ICA patterns at the level of the
group. We compare our method with state-of-the-art multi-subject fMRI ICA
methods and show that the features extracted using our procedure are more
reproducible at the group level on two datasets of 12 healthy controls: a
resting-state and a functional localizer study.
"
"  Using the LRT statistic, a model R^2 is proposed for the generalized linear
mixed model for assessing the association between the correlated outcomes and
fixed effects. The R^2 compares the full model to a null model with all fixed
effects deleted.
"
"  Network models are widely used to represent relational information among
interacting units and the structural implications of these relations. Recently,
social network studies have focused a great deal of attention on random graph
models of networks whose nodes represent individual social actors and whose
edges represent a specified relationship between the actors. Most inference for
social network models assumes that the presence or absence of all possible
links is observed, that the information is completely reliable, and that there
are no measurement (e.g., recording) errors. This is clearly not true in
practice, as much network data is collected though sample surveys. In addition
even if a census of a population is attempted, individuals and links between
individuals are missed (i.e., do not appear in the recorded data). In this
paper we develop the conceptual and computational theory for inference based on
sampled network information. We first review forms of network sampling designs
used in practice. We consider inference from the likelihood framework, and
develop a typology of network data that reflects their treatment within this
frame. We then develop inference for social network models based on information
from adaptive network designs. We motivate and illustrate these ideas by
analyzing the effect of link-tracing sampling designs on a collaboration
network.
"
"  We present a new approach to the analysis of time symmetry in light curves,
such as those in the x-ray at the center of the Scorpius X-1 occultation
debate. Our method uses a new parameterization for such events (the bilogistic
event profile) and provides a clear, physically relevant characterization of
each event's key features. We also demonstrate a Markov Chain Monte Carlo
algorithm to carry out this analysis, including a novel independence chain
configuration for the estimation of each event's location in the light curve.
These tools are applied to the Scorpius X-1 light curves presented in Chang et
al. (2007), providing additional evidence based on the time series that the
events detected thus far are most likely not occultations by TNOs.
"
"  Natural image statistics exhibit hierarchical dependencies across multiple
scales. Representing such prior knowledge in non-factorial latent tree models
can boost performance of image denoising, inpainting, deconvolution or
reconstruction substantially, beyond standard factorial ""sparse"" methodology.
We derive a large scale approximate Bayesian inference algorithm for linear
models with non-factorial (latent tree-structured) scale mixture priors.
Experimental results on a range of denoising and inpainting problems
demonstrate substantially improved performance compared to MAP estimation or to
inference with factorial priors.
"
"  A DNA palindrome is a segment of double-stranded DNA sequence with inver-
sion symmetry which may form secondary structures conferring significant
biolog- ical functions ranging from RNA transcription to DNA replication. To
test if the clusters of DNA palindromes distribute randomly is an interesting
bioinformatic problem, where the occurrence rate of the DNA palindromes is a
key estimator for setting up a test. The most commonly used statistics for
estimating the occur- rence rate for scan statistics is the average rate.
However, in our simulation, the average rate may double the null occurrence
rate of DNA palindromes due to hot spot regions of 3000 bp's in a herpes virus
genome. Here, we propose a formula to estimate the occurrence rate through an
analytic derivation under a Markov assumption on DNA sequence. Our simulation
study shows that the performance of this method has improved the accuracy and
robustness against hot spots, as compared to the commonly used average rate. In
addition, we derived analytical formula for the moment-generating functions of
various statistics under a Markov model, enabling further calculations of
p-values.
"
"  Prototype methods seek a minimal subset of samples that can serve as a
distillation or condensed view of a data set. As the size of modern data sets
grows, being able to present a domain specialist with a short list of
""representative"" samples chosen from the data set is of increasing
interpretative value. While much recent statistical research has been focused
on producing sparse-in-the-variables methods, this paper aims at achieving
sparsity in the samples. We discuss a method for selecting prototypes in the
classification setting (in which the samples fall into known discrete
categories). Our method of focus is derived from three basic properties that we
believe a good prototype set should satisfy. This intuition is translated into
a set cover optimization problem, which we solve approximately using standard
approaches. While prototype selection is usually viewed as purely a means
toward building an efficient classifier, in this paper we emphasize the
inherent value of having a set of prototypical elements. That said, by using
the nearest-neighbor rule on the set of prototypes, we can of course discuss
our method as a classifier as well.
"
"  We develop a simple test for deviations from power law tails, which is based
on the asymptotic properties of the empirical distribution function. We use
this test to answer the question whether great natural disasters, financial
crashes or electricity price spikes should be classified as dragon kings or
'only' as black swans.
"
"  Bayesian learning in undirected graphical models|computing posterior
distributions over parameters and predictive quantities is exceptionally
difficult. We conjecture that for general undirected models, there are no
tractable MCMC (Markov Chain Monte Carlo) schemes giving the correct
equilibrium distribution over parameters. While this intractability, due to the
partition function, is familiar to those performing parameter optimisation,
Bayesian learning of posterior distributions over undirected model parameters
has been unexplored and poses novel challenges. we propose several approximate
MCMC schemes and test on fully observed binary models (Boltzmann machines) for
a small coronary heart disease data set and larger artificial systems. While
approximations must perform well on the model, their interaction with the
sampling scheme is also important. Samplers based on variational mean- field
approximations generally performed poorly, more advanced methods using loopy
propagation, brief sampling and stochastic dynamics lead to acceptable
parameter posteriors. Finally, we demonstrate these techniques on a Markov
random field with hidden variables.
"
"  Contemporary global optimization algorithms are based on local measures of
utility, rather than a probability measure over location and value of the
optimum. They thus attempt to collect low function values, not to learn about
the optimum. The reason for the absence of probabilistic global optimizers is
that the corresponding inference problem is intractable in several ways. This
paper develops desiderata for probabilistic optimization algorithms, then
presents a concrete algorithm which addresses each of the computational
intractabilities with a sequence of approximations and explicitly adresses the
decision problem of maximizing information gain from each evaluation.
"
"  We discuss a general notion of ""sparsity structure"" and associated recoveries
of a sparse signal from its linear image of reduced dimension possibly
corrupted with noise. Our approach allows for unified treatment of (a) the
""usual sparsity"" and ""usual $\ell_1$ recovery,"" (b) block-sparsity with
possibly overlapping blocks and associated block-$\ell_1$ recovery, and (c)
low-rank-oriented recovery by nuclear norm minimization. The proposed recovery
routines are natural extensions of the usual $\ell_1$ minimization used in
Compressed Sensing. Specifically we present nullspace-type sufficient
conditions for the recovery to be precise on sparse signals in the noiseless
case. Then we derive error bounds for imperfect (nearly sparse signal, presence
of observation noise, etc.) recovery under these conditions. In all of these
cases, we present efficiently verifiable sufficient conditions for the validity
of the associated nullspace properties.
"
"  We consider the problem of learning a linear factor model. We propose a
regularized form of principal component analysis (PCA) and demonstrate through
experiments with synthetic and real data the superiority of resulting estimates
to those produced by pre-existing factor analysis approaches. We also establish
theoretical results that explain how our algorithm corrects the biases induced
by conventional approaches. An important feature of our algorithm is that its
computational requirements are similar to those of PCA, which enjoys wide use
in large part due to its efficiency.
"
"  To quantify the randomness of Markov trajectories with fixed initial and
final states, Ekroot and Cover proposed a closed-form expression for the
entropy of trajectories of an irreducible finite state Markov chain. Numerous
applications, including the study of random walks on graphs, require the
computation of the entropy of Markov trajectories conditioned on a set of
intermediate states. However, the expression of Ekroot and Cover does not allow
for computing this quantity. In this paper, we propose a method to compute the
entropy of conditional Markov trajectories through a transformation of the
original Markov chain into a Markov chain that exhibits the desired conditional
distribution of trajectories. Moreover, we express the entropy of Markov
trajectories - a global quantity - as a linear combination of local entropies
associated with the Markov chain states.
"
"  A key problem in statistical modeling is model selection, how to choose a
model at an appropriate level of complexity. This problem appears in many
settings, most prominently in choosing the number ofclusters in mixture models
or the number of factors in factor analysis. In this tutorial we describe
Bayesian nonparametric methods, a class of methods that side-steps this issue
by allowing the data to determine the complexity of the model. This tutorial is
a high-level introduction to Bayesian nonparametric methods and contains
several examples of their application.
"
"  Effective sensor scheduling requires the consideration of long-term effects
and thus optimization over long time horizons. Determining the optimal sensor
schedule, however, is equivalent to solving a binary integer program, which is
computationally demanding for long time horizons and many sensors. For linear
Gaussian systems, two efficient multi-step sensor scheduling approaches are
proposed in this paper. The first approach determines approximate but close to
optimal sensor schedules via convex optimization. The second approach combines
convex optimization with a \BB search for efficiently determining the optimal
sensor schedule.
"
"  Recent studies in the field of human vision science suggest that the human
responses to the stimuli on a visual display are non-deterministic. People may
attend to different locations on the same visual input at the same time. Based
on this knowledge, we propose a new stochastic model of visual attention by
introducing a dynamic Bayesian network to predict the likelihood of where
humans typically focus on a video scene. The proposed model is composed of a
dynamic Bayesian network with 4 layers. Our model provides a framework that
simulates and combines the visual saliency response and the cognitive state of
a person to estimate the most probable attended regions. Sample-based inference
with Markov chain Monte-Carlo based particle filter and stream processing with
multi-core processors enable us to estimate human visual attention in near real
time. Experimental results have demonstrated that our model performs
significantly better in predicting human visual attention compared to the
previous deterministic models.
"
"  In this manuscript we introduce a generalisation of the log-Normal
distribution that is inspired by a modification of the Kaypten multiplicative
process using the $q$-product of Borges [Physica A \textbf{340}, 95 (2004)].
Depending on the value of q the distribution increases the tail for small (when
$q<1$) or large (when $q>1$) values of the variable upon analysis. The usual
log-Normal distribution is retrieved when $q=1$. The main statistical features
of this distribution are presented as well as a related random number
generators and tables of quantiles of the Kolmogorov-Smirnov. Lastly, we
illustrate the application of this distribution studying the adjustment of a
set of variables of biological and financial origin.
"
"  Large-scale multiple testing tasks often exhibit dependence, and leveraging
the dependence between individual tests is still one challenging and important
problem in statistics. With recent advances in graphical models, it is feasible
to use them to perform multiple testing under dependence. We propose a multiple
testing procedure which is based on a Markov-random-field-coupled mixture
model. The ground truth of hypotheses is represented by a latent binary Markov
random field, and the observed test statistics appear as the coupled mixture
variables. The parameters in our model can be automatically learned by a novel
EM algorithm. We use an MCMC algorithm to infer the posterior probability that
each hypothesis is null (termed local index of significance), and the false
discovery rate can be controlled accordingly. Simulations show that the
numerical performance of multiple testing can be improved substantially by
using our procedure. We apply the procedure to a real-world genome-wide
association study on breast cancer, and we identify several SNPs with strong
association evidence.
"
"  A simple linear algebraic explanation of the algorithm in ""A Spectral
Algorithm for Learning Hidden Markov Models"" (COLT 2009). Most of the content
is in Figure 2; the text just makes everything precise in four nearly-trivial
claims.
"
"  We present asymptotic and finite-sample results on the use of stochastic
blockmodels for the analysis of network data. We show that the fraction of
misclassified network nodes converges in probability to zero under maximum
likelihood fitting when the number of classes is allowed to grow as the root of
the network size and the average network degree grows at least
poly-logarithmically in this size. We also establish finite-sample confidence
bounds on maximum-likelihood blockmodel parameter estimates from data
comprising independent Bernoulli random variates; these results hold uniformly
over class assignment. We provide simulations verifying the conditions
sufficient for our results, and conclude by fitting a logit parameterization of
a stochastic blockmodel with covariates to a network data example comprising a
collection of Facebook profiles, resulting in block estimates that reveal
residual structure.
"
"  The United Nations (UN) Population Division is considering producing
probabilistic projections for the total fertility rate (TFR) using the Bayesian
hierarchical model of Alkema et al. (2011), which produces predictive
distributions of TFR for individual countries. The UN is interested in
publishing probabilistic projections for aggregates of countries, such as
regions and trading blocs. This requires joint probabilistic projections of
future country-specific TFRs, taking account of the correlations between them.
We propose an extension of the Bayesian hierarchical model that allows for
probabilistic projection of TFR for any set of countries. We model the
correlation between country forecast errors as a linear function of time
invariant covariates, namely whether the countries are contiguous, whether they
had a common colonizer after 1945, and whether they are in the same UN region.
The resulting correlation model is incorporated into the Bayesian hierarchical
model's error distribution. We produce predictive distributions of TFR for
1990-2010 for each of the UN's primary regions. We find that the proportions of
the observed values that fall within the prediction intervals from our method
are closer to their nominal levels than those produced by the current model.
Our results suggest that a significant proportion of the correlation between
forecast errors for TFR in different countries is due to countries' geographic
proximity to one another, and that if this correlation is accounted for, the
quality of probabilitistic projections of TFR for regions and other aggregates
is improved.
"
"  Most prior work on active learning of classifiers has focused on sequentially
selecting one unlabeled example at a time to be labeled in order to reduce the
overall labeling effort. In many scenarios, however, it is desirable to label
an entire batch of examples at once, for example, when labels can be acquired
in parallel. This motivates us to study batch active learning, which
iteratively selects batches of $k>1$ examples to be labeled. We propose a novel
batch active learning method that leverages the availability of high-quality
and efficient sequential active-learning policies by attempting to approximate
their behavior when applied for $k$ steps. Specifically, our algorithm first
uses Monte-Carlo simulation to estimate the distribution of unlabeled examples
selected by a sequential policy over $k$ step executions. The algorithm then
attempts to select a set of $k$ examples that best matches this distribution,
leading to a combinatorial optimization problem that we term ""bounded
coordinated matching"". While we show this problem is NP-hard in general, we
give an efficient greedy solution, which inherits approximation bounds from
supermodular minimization theory. Our experimental results on eight benchmark
datasets show that the proposed approach is highly effective
"
"  Mean-field variational inference is a method for approximate Bayesian
posterior inference. It approximates a full posterior distribution with a
factorized set of distributions by maximizing a lower bound on the marginal
likelihood. This requires the ability to integrate a sum of terms in the log
joint likelihood using this factorized distribution. Often not all integrals
are in closed form, which is typically handled by using a lower bound. We
present an alternative algorithm based on stochastic optimization that allows
for direct optimization of the variational lower bound. This method uses
control variates to reduce the variance of the stochastic search gradient, in
which existing lower bounds can play an important role. We demonstrate the
approach on two non-conjugate models: logistic regression and an approximation
to the HDP.
"
"  The problem of optimizing unknown costly-to-evaluate functions has been
studied for a long time in the context of Bayesian Optimization. Algorithms in
this field aim to find the optimizer of the function by asking only a few
function evaluations at locations carefully selected based on a posterior
model. In this paper, we assume the unknown function is Lipschitz continuous.
Leveraging the Lipschitz property, we propose an algorithm with a distinct
exploration phase followed by an exploitation phase. The exploration phase aims
to select samples that shrink the search space as much as possible. The
exploitation phase then focuses on the reduced search space and selects samples
closest to the optimizer. Considering the Expected Improvement (EI) as a
baseline, we empirically show that the proposed algorithm significantly
outperforms EI.
"
"  The paper focuses on minimum mean square error (MMSE) Bayesian estimation for
a Gaussian source impaired by additive Middleton's Class-A impulsive noise. In
addition to the optimal Bayesian estimator, the paper considers also the
soft-limiter and the blanker, which are two popular suboptimal estimators
characterized by very low complexity. The MMSE-optimum thresholds for such
suboptimal estimators are obtained by practical iterative algorithms with fast
convergence. The paper derives also the optimal thresholds according to a
maximum-SNR (MSNR) criterion, and establishes connections with the MMSE
criterion. Furthermore, closed form analytic expressions are derived for the
MSE and the SNR of all the suboptimal estimators, which perfectly match
simulation results. Noteworthy, these results can be applied to characterize
the receiving performance of any multicarrier system impaired by a
Gaussian-mixture noise, such as asymmetric digital subscriber lines (ADSL) and
power-line communications (PLC).
"
"  An information-geometric approach to sensor management is introduced that is
based on following geodesic curves in a manifold of possible sensor
configurations. This perspective arises by observing that, given a parameter
estimation problem to be addressed through management of sensor assets, any
particular sensor configuration corresponds to a Riemannian metric on the
parameter manifold. With this perspective, managing sensors involves navigation
on the space of all Riemannian metrics on the parameter manifold, which is
itself a Riemannian manifold. Existing work assumes the metric on the parameter
manifold is one that, in statistical terms, corresponds to a Jeffreys prior on
the parameter to be estimated. It is observed that informative priors, as arise
in sensor management, can also be accommodated. Given an initial sensor
configuration, the trajectory along which to move in sensor configuration space
to gather most information is seen to be locally defined by the geodesic
structure of this manifold. Further, divergences based on Fisher and Shannon
information lead to the same Riemannian metric and geodesics.
"
"  Four decades after their invention, quasi-Newton methods are still state of
the art in unconstrained numerical optimization. Although not usually
interpreted thus, these are learning algorithms that fit a local quadratic
approximation to the objective function. We show that many, including the most
popular, quasi-Newton methods can be interpreted as approximations of Bayesian
linear regression under varying prior assumptions. This new notion elucidates
some shortcomings of classical algorithms, and lights the way to a novel
nonparametric quasi-Newton method, which is able to make more efficient use of
available information at computational cost similar to its predecessors.
"
"  We consider the bridge linear regression modeling, which can produce a sparse
or non-sparse model. A crucial point in the model building process is the
selection of adjusted parameters including a regularization parameter and a
tuning parameter in bridge regression models. The choice of the adjusted
parameters can be viewed as a model selection and evaluation problem. We
propose a model selection criterion for evaluating bridge regression models in
terms of Bayesian approach. This selection criterion enables us to select the
adjusted parameters objectively. We investigate the effectiveness of our
proposed modeling strategy through some numerical examples.
"
"  Monte Carlo methods can provide accurate p-value estimates of word counting
test statistics and are easy to implement. They are especially attractive when
an asymptotic theory is absent or when either the search sequence or the word
pattern is too short for the application of asymptotic formulae. Naive direct
Monte Carlo is undesirable for the estimation of small probabilities because
the associated rare events of interest are seldom generated. We propose instead
efficient importance sampling algorithms that use controlled insertion of the
desired word patterns on randomly generated sequences. The implementation is
illustrated on word patterns of biological interest: Palindromes and inverted
repeats, patterns arising from position specific weight matrices and
co-occurrences of pairs of motifs.
"
"  The T-wave of an electrocardiogram (ECG) represents the ventricular
repolarization that is critical in restoration of the heart muscle to a
pre-contractile state prior to the next beat. Alterations in the T-wave reflect
various cardiac conditions; and links between abnormal (prolonged) ventricular
repolarization and malignant arrhythmias have been documented. Cardiac safety
testing prior to approval of any new drug currently relies on two points of the
ECG waveform: onset of the Q-wave and termination of the T-wave; and only a few
beats are measured. Using functional data analysis, a statistical approach
extracts a common shape for each subject (reference curve) from a sequence of
beats, and then models the deviation of each curve in the sequence from that
reference curve as a four-dimensional vector. The representation can be used to
distinguish differences between beats or to model shape changes in a subject's
T-wave over time. This model provides physically interpretable parameters
characterizing T-wave shape, and is robust to the determination of the endpoint
of the T-wave. Thus, this dimension reduction methodology offers the strong
potential for definition of more robust and more informative biomarkers of
cardiac abnormalities than the QT (or QT corrected) interval in current use.
"
"  We introduce a new family of estimators for unnormalized statistical models.
Our family of estimators is parameterized by two nonlinear functions and uses a
single sample from an auxiliary distribution, generalizing Maximum Likelihood
Monte Carlo estimation of Geyer and Thompson (1992). The family is such that we
can estimate the partition function like any other parameter in the model. The
estimation is done by optimizing an algebraically simple, well defined
objective function, which allows for the use of dedicated optimization methods.
We establish consistency of the estimator family and give an expression for the
asymptotic covariance matrix, which enables us to further analyze the influence
of the nonlinearities and the auxiliary density on estimation performance. Some
estimators in our family are particularly stable for a wide range of auxiliary
densities. Interestingly, a specific choice of the nonlinearity establishes a
connection between density estimation and classification by nonlinear logistic
regression. Finally, the optimal amount of auxiliary samples relative to the
given amount of the data is considered from the perspective of computational
efficiency.
"
"  Topic models provide a useful method for dimensionality reduction and
exploratory data analysis in large text corpora. Most approaches to topic model
inference have been based on a maximum likelihood objective. Efficient
algorithms exist that approximate this objective, but they have no provable
guarantees. Recently, algorithms have been introduced that provide provable
bounds, but these algorithms are not practical because they are inefficient and
not robust to violations of model assumptions. In this paper we present an
algorithm for topic model inference that is both provable and practical. The
algorithm produces results comparable to the best MCMC implementations while
running orders of magnitude faster.
"
"  Spatial Independent Components Analysis (ICA) is increasingly used in the
context of functional Magnetic Resonance Imaging (fMRI) to study cognition and
brain pathologies. Salient features present in some of the extracted
Independent Components (ICs) can be interpreted as brain networks, but the
segmentation of the corresponding regions from ICs is still ill-controlled.
Here we propose a new ICA-based procedure for extraction of sparse features
from fMRI datasets. Specifically, we introduce a new thresholding procedure
that controls the deviation from isotropy in the ICA mixing model. Unlike
current heuristics, our procedure guarantees an exact, possibly conservative,
level of specificity in feature detection. We evaluate the sensitivity and
specificity of the method on synthetic and fMRI data and show that it
outperforms state-of-the-art approaches.
"
"  We propose a new image denoising algorithm when the data is contaminated by a
Poisson noise. As in the Non-Local Means filter, the proposed algorithm is
based on a weighted linear combination of the bserved image. But in contract to
the latter where the weights are defined by a Gaussian kernel, we propose to
choose them in an optimal way. First some ""oracle"" weights are defined by
minimizing a very tight upper bound of the Mean Square Error. For a practical
application the weights are estimated from the observed image. We prove that
the proposed filter converges at the usual optimal rate to the true image.
Simulation results are presented to compare the performance of the presented
filter with conventional filtering methods.
"
"  Creating a loyal customer base is one of the most important, and at the same
time, most difficult tasks a company faces. Creating loyalty online (e-loyalty)
is especially difficult since customers can ``switch'' to a competitor with the
click of a mouse. In this paper we investigate e-loyalty in online auctions.
Using a unique data set of over 30,000 auctions from one of the main
consumer-to-consumer online auction houses, we propose a novel measure of
e-loyalty via the associated network of transactions between bidders and
sellers. Using a bipartite network of bidder and seller nodes, two nodes are
linked when a bidder purchases from a seller and the number of repeat-purchases
determines the strength of that link. We employ ideas from functional principal
component analysis to derive, from this network, the loyalty distribution which
measures the perceived loyalty of every individual seller, and associated
loyalty scores which summarize this distribution in a parsimonious way. We then
investigate the effect of loyalty on the outcome of an auction. In doing so, we
are confronted with several statistical challenges in that standard statistical
models lead to a misrepresentation of the data and a violation of the model
assumptions. The reason is that loyalty networks result in an extreme
clustering of the data, with few high-volume sellers accounting for most of the
individual transactions. We investigate several remedies to the clustering
problem and conclude that loyalty networks consist of very distinct segments
that can best be understood individually.
"
"  Hidden Markov Models (HMMs) can be accurately approximated using
co-occurrence frequencies of pairs and triples of observations by using a fast
spectral method in contrast to the usual slow methods like EM or Gibbs
sampling. We provide a new spectral method which significantly reduces the
number of model parameters that need to be estimated, and generates a sample
complexity that does not depend on the size of the observation vocabulary. We
present an elementary proof giving bounds on the relative accuracy of
probability estimates from our model. (Correlaries show our bounds can be
weakened to provide either L1 bounds or KL bounds which provide easier direct
comparisons to previous work.) Our theorem uses conditions that are checkable
from the data, instead of putting conditions on the unobservable Markov
transition matrix.
"
"  In this paper, we propose a new method remMap -- REgularized Multivariate
regression for identifying MAster Predictors -- for fitting multivariate
response regression models under the high-dimension-low-sample-size setting.
remMap is motivated by investigating the regulatory relationships among
different biological molecules based on multiple types of high dimensional
genomic data. Particularly, we are interested in studying the influence of DNA
copy number alterations on RNA transcript levels. For this purpose, we model
the dependence of the RNA expression levels on DNA copy numbers through
multivariate linear regressions and utilize proper regularizations to deal with
the high dimensionality as well as to incorporate desired network structures.
Criteria for selecting the tuning parameters are also discussed. The
performance of the proposed method is illustrated through extensive simulation
studies. Finally, remMap is applied to a breast cancer study, in which genome
wide RNA transcript levels and DNA copy numbers were measured for 172 tumor
samples. We identify a tran-hub region in cytoband 17q12-q21, whose
amplification influences the RNA expression levels of more than 30 unlinked
genes. These findings may lead to a better understanding of breast cancer
pathology.
"
"  An important quality aspect of official statistics produced by national
statistical institutes is comparability over time. To maintain uninterrupted
time series, surveys conducted by national statistical institutes are often
kept unchanged as long as possible. To improve the quality or efficiency of a
survey process, however, it remains inevitable to adjust methods or redesign
this process from time to time. Adjustments in the survey process generally
affect survey characteristics such as response bias and therefore have a
systematic effect on the parameter estimates of a sample survey. Therefore, it
is important that the effects of a survey redesign on the estimated series are
explained and quantified. In this paper a structural time series model is
applied to estimate discontinuities in series of the Dutch survey on social
participation and environmental consciousness due to a redesign of the
underlying survey process.
"
"  Experiment shows that art students prefer abstract art to monkey art in about
two-third of the cases. Since the number is above 50%, some argue that abstract
art is different and better than animal art. I compare this result with figure
skating competitions, where on average 73% of judges prefer gold medalist to
silver medalist. This means that the difference between abstract artists and
animal artists is less than the difference between gold and silver medalists.
"
"  The steep rise in availability and usage of high-throughput technologies in
biology brought with it a clear need for methods to control the False Discovery
Rate (FDR) in multiple tests. Benjamini and Hochberg (BH) introduced in 1995 a
simple procedure and proved that it provided a bound on the expected value,
$\mathit{FDR}\leq q$. Since then, many authors tried to improve the BH bound,
with one approach being designing adaptive procedures, which aim at estimating
the number of true null hypothesis in order to get a better FDR bound. Our two
main rigorous results are the following: (i) a theorem that provides a bound on
the FDR for adaptive procedures that use any estimator for the number of true
hypotheses ($m_0$), (ii) a theorem that proves a monotonicity property of
general BH-like procedures, both for the case where the hypotheses are
independent. We also propose two improved procedures for which we prove FDR
control for the independent case, and demonstrate their advantages over several
available bounds, on simulated data and on a large number of gene expression
data sets. Both applications are simple and involve a similar amount of
computation as the original BH procedure. We compare the performance of our
proposed procedures with BH and other procedures and find that in most cases we
get more power for the same level of statistical significance.
"
"  We extend a recent synchronization analysis of exact finite-state sources to
nonexact sources for which synchronization occurs only asymptotically. Although
the proof methods are quite different, the primary results remain the same. We
find that an observer's average uncertainty in the source state vanishes
exponentially fast and, as a consequence, an observer's average uncertainty in
predicting future output converges exponentially fast to the source entropy
rate.
"
"  The ECME algorithm has proven to be an effective way of accelerating the EM
algorithm for many problems. Recognising the limitation of using prefixed
acceleration subspace in ECME, we propose the new Dynamic ECME (DECME)
algorithm which allows the acceleration subspace to be chosen dynamically. Our
investigation of an inefficient special case of DECME, the classical Successive
Overrelaxation (SOR) method, leads to an efficient, simple, and widely
applicable DECME implementation, called DECME_v1. The fast convergence of
DECME_v1 is established by the theoretical result that, in a small
neighbourhood of the maximum likelihood estimate (MLE), DECME_v1 is equivalent
to a conjugate direction method. Numerical results show that DECME_v1 and its
two variants are very stable and often converge faster than EM by a factor of
one hundred in terms of number of iterations and a factor of thirty in terms of
CPU time when EM is very slow.
"
"  We show how binary classification methods developed to work on i.i.d. data
can be used for solving statistical problems that are seemingly unrelated to
classification and concern highly-dependent time series. Specifically, the
problems of time-series clustering, homogeneity testing and the three-sample
problem are addressed. The algorithms that we construct for solving these
problems are based on a new metric between time-series distributions, which can
be evaluated using binary classification methods. Universal consistency of the
proposed algorithms is proven under most general assumptions. The theoretical
results are illustrated with experiments on synthetic and real-world data.
"
"  Recent advances in topic models have explored complicated structured
distributions to represent topic correlation. For example, the pachinko
allocation model (PAM) captures arbitrary, nested, and possibly sparse
correlations between topics using a directed acyclic graph (DAG). While PAM
provides more flexibility and greater expressive power than previous models
like latent Dirichlet allocation (LDA), it is also more difficult to determine
the appropriate topic structure for a specific dataset. In this paper, we
propose a nonparametric Bayesian prior for PAM based on a variant of the
hierarchical Dirichlet process (HDP). Although the HDP can capture topic
correlations defined by nested data structure, it does not automatically
discover such correlations from unstructured data. By assuming an HDP-based
prior for PAM, we are able to learn both the number of topics and how the
topics are correlated. We evaluate our model on synthetic and real-world text
datasets, and show that nonparametric PAM achieves performance matching the
best of PAM without manually tuning the number of topics.
"
"  The use of community detection algorithms is explored within the framework of
cover song identification, i.e. the automatic detection of different audio
renditions of the same underlying musical piece. Until now, this task has been
posed as a typical query-by-example task, where one submits a query song and
the system retrieves a list of possible matches ranked by their similarity to
the query. In this work, we propose a new approach which uses song communities
to provide more relevant answers to a given query. Starting from the output of
a state-of-the-art system, songs are embedded in a complex weighted network
whose links represent similarity (related musical content). Communities inside
the network are then recognized as groups of covers and this information is
used to enhance the results of the system. In particular, we show that this
approach increases both the coherence and the accuracy of the system.
Furthermore, we provide insight into the internal organization of individual
cover song communities, showing that there is a tendency for the original song
to be central within the community. We postulate that the methods and results
presented here could be relevant to other query-by-example tasks.
"
"  One of the main necessities for population geneticists is the availability of
statistical tools that enable to accept or reject the neutral Wright-Fisher
model with high power. A number of statistical tests have been developed to
detect specific deviations from the null frequency spectrum in different
directions (i.e., Tajima's D, Fu and Li's F and D test, Fay and Wu's H).
Recently, a general framework was proposed to generate all neutrality tests
that are linear functions of the frequency spectrum. In this framework, a
family of optimal tests was developed to have almost maximum power against a
specific alternative evolutionary scenario. Following these developments, in
this paper we provide a thorough discussion of linear and nonlinear neutrality
tests. First, we present the general framework for linear tests and emphasize
the importance of the property of scalability with the sample size (that is,
the results of the tests should not depend on the sample size), which, if
missing, can guide to errors in data interpretation. The motivation and
structure of linear optimal tests are discussed. In a further generalization,
we develop a general framework for nonlinear neutrality tests and we derive
nonlinear optimal tests for polynomials of any degree in the frequency
spectrum.
"
"  Inspired by the hierarchical hidden Markov models (HHMM), we present the
hierarchical semi-Markov conditional random field (HSCRF), a generalisation of
embedded undirectedMarkov chains tomodel complex hierarchical, nestedMarkov
processes. It is parameterised in a discriminative framework and has polynomial
time algorithms for learning and inference. Importantly, we consider
partiallysupervised learning and propose algorithms for generalised
partially-supervised learning and constrained inference. We demonstrate the
HSCRF in two applications: (i) recognising human activities of daily living
(ADLs) from indoor surveillance cameras, and (ii) noun-phrase chunking. We show
that the HSCRF is capable of learning rich hierarchical models with reasonable
accuracy in both fully and partially observed data cases.
"
"  This survey contains statistics on elections in Russia published in different
places and available online. This data is discussed from the viewpoint of
statistical model selection. The current version is updated including the
materials up to July, 2020 voting on constitutional changes, Belarus 2020
elections and papers that appeared in 2020; most of the data are not consistent
with the assumption of fair elections.
"
"  Profile likelihood intervals of large quantiles in Extreme Value
distributions provide a good way to estimate these parameters of interest since
they take into account the asymmetry of the likelihood surface in the case of
small and moderate sample sizes; however they are seldom used in practice. In
contrast, maximum likelihood asymptotic (mla) intervals are commonly used
without respect to sample size. It is shown here that profile likelihood
intervals actually are a good alternative for the estimation of quantiles for
sample sizes $25 \leq n\leq 100$ of block maxima, since they presented adequate
coverage frequencies in contrast to the poor coverage frequencies of mla
intervals for these sample sizes, which also tended to underestimate the
quantile and therefore might be a dangerous statistical practice.
  In addition, maximum likelihood estimation can present problems when Weibull
models are considered for moderate or small sample sizes due to singularities
of the corresponding density function when the shape parameter is smaller than
one. These estimation problems can be traced to the commonly used continuous
approximation to the likelihood function and could be avoided by using the
exact or correct likelihood function, at least for the settings considered
here. A rainfall data example is presented to exemplify the suggested
inferential procedure based on the analyses of profile likelihoods.
"
"  Clustering evaluation measures are frequently used to evaluate the
performance of algorithms. However, most measures are not properly normalized
and ignore some information in the inherent structure of clusterings. We model
the relation between two clusterings as a bipartite graph and propose a general
component-based decomposition formula based on the components of the graph.
Most existing measures are examples of this formula. In order to satisfy
consistency in the component, we further propose a split-merge framework for
comparing clusterings of different data sets. Our framework gives measures that
are conditionally normalized, and it can make use of data point information,
such as feature vectors and pairwise distances. We use an entropy-based
instance of the framework and a coreference resolution data set to demonstrate
empirically the utility of our framework over other measures.
"
"  Cluster-Weighted Modeling (CWM) is a flexible mixture approach for modeling
the joint probability of data coming from a heterogeneous population as a
weighted sum of the products of marginal distributions and conditional
distributions. In this paper, we introduce a wide family of Cluster Weighted
models in which the conditional distributions are assumed to belong to the
exponential family with canonical links which will be referred to as
Generalized Linear Gaussian Cluster Weighted Models. Moreover, we show that, in
a suitable sense, mixtures of generalized linear models can be considered as
nested in Generalized Linear Gaussian Cluster Weighted Models. The proposal is
illustrated through many numerical studies based on both simulated and real
data sets.
"
"  We present a new statistical learning paradigm for Boltzmann machines based
on a new inference principle we have proposed: the latent maximum entropy
principle (LME). LME is different both from Jaynes maximum entropy principle
and from standard maximum likelihood estimation.We demonstrate the LME
principle BY deriving new algorithms for Boltzmann machine parameter
estimation, and show how robust and fast new variant of the EM algorithm can be
developed.Our experiments show that estimation based on LME generally yields
better results than maximum likelihood estimation, particularly when inferring
hidden units from small amounts of data.
"
"  SOBA is an approach to election verification that provides observers with
justifiably high confidence that the reported results of an election are
consistent with an audit trail (""ballots""), which can be paper or electronic.
SOBA combines three ideas: (1) publishing cast vote records (CVRs) separately
for each contest, so that anyone can verify that each reported contest outcome
is correct, if the CVRs reflect voters' intentions with sufficient accuracy;
(2) shrouding a mapping between ballots and the CVRs for those ballots to
prevent the loss of privacy that could occur otherwise; (3) assessing the
accuracy with which the CVRs reflect voters' intentions for a collection of
contests while simultaneously assessing the integrity of the shrouded mapping
between ballots and CVRs by comparing randomly selected ballots to the CVRs
that purport to represent them. Step (1) is related to work by the Humboldt
County Election Transparency Project, but publishing CVRs separately for
individual contests rather than images of entire ballots preserves privacy.
Step (2) requires a cryptographic commitment from elections officials.
Observers participate in step (3), which relies on the ""super-simple
simultaneous single-ballot risk-limiting audit."" Step (3) is designed to reveal
relatively few ballots if the shrouded mapping is proper and the CVRs
accurately reflect voter intent. But if the reported outcomes of the contests
differ from the outcomes that a full hand count would show, step (3) is
guaranteed to have a large chance of requiring all the ballots to be counted by
hand, thereby limiting the risk that an incorrect outcome will become official
and final.
"
"  We provide faster algorithms for the problem of Gaussian summation, which
occurs in many machine learning methods. We develop two new extensions - an
O(Dp) Taylor expansion for the Gaussian kernel with rigorous error bounds and a
new error control scheme integrating any arbitrary approximation method -
within the best discretealgorithmic framework using adaptive hierarchical data
structures. We rigorously evaluate these techniques empirically in the context
of optimal bandwidth selection in kernel density estimation, revealing the
strengths and weaknesses of current state-of-the-art approaches for the first
time. Our results demonstrate that the new error control scheme yields improved
performance, whereas the series expansion approach is only effective in low
dimensions (five or less).
"
"  Wireless sensor networks are composed of distributed sensors that can be used
for signal detection or classification. The likelihood functions of the
hypotheses are often not known in advance, and decision rules have to be
learned via supervised learning. A specific such algorithm is Fisher
discriminant analysis (FDA), the classification accuracy of which has been
previously studied in the context of wireless sensor networks. Previous work,
however, does not take into account the communication protocol or battery
lifetime of the sensor networks; in this paper we extend the existing studies
by proposing a model that captures the relationship between battery lifetime
and classification accuracy. In order to do so we combine the FDA with a model
that captures the dynamics of the Carrier-Sense Multiple-Access (CSMA)
algorithm, the random-access algorithm used to regulate communications in
sensor networks. This allows us to study the interaction between the
classification accuracy, battery lifetime and effort put towards learning, as
well as the impact of the back-off rates of CSMA on the accuracy. We
characterize the tradeoff between the length of the training stage and
accuracy, and show that accuracy is non-monotone in the back-off rate due to
changes in the training sample size and overfitting.
"
"  This paper introduces a new model and methodology for estimating the ability
of NBA players. The main idea is to directly measure how good a player is by
comparing how their team performs when they are on the court as opposed to when
they are off it. This is achieved in a such a way as to control for the
changing abilities of the other players on court at different times during a
match. The new method uses multiple seasons' data in a structured way to
estimate player ability in an isolated season, measuring separately defensive
and offensive merit as well as combining these to give an overall rating. The
use of game statistics in predicting player ability will be considered. Results
using data from the 2008/9 season suggest that LeBron James, who won the NBA
MVP award, was the best overall player. The best defensive player was Lamar
Odom and the best rookie was Russell Westbrook, neither of whom won an NBA
award that season. The results further indicate that whilst the
frequently-reported game statistics provide some information on offensive
ability, they do not perform well in the prediction of defensive ability.
"
"  In recent years, a rich variety of shrinkage priors have been proposed that
have great promise in addressing massive regression problems. In general, these
new priors can be expressed as scale mixtures of normals, but have more complex
forms and better properties than traditional Cauchy and double exponential
priors. We first propose a new class of normal scale mixtures through a novel
generalized beta distribution that encompasses many interesting priors as
special cases. This encompassing framework should prove useful in comparing
competing priors, considering properties and revealing close connections. We
then develop a class of variational Bayes approximations through the new
hierarchy presented that will scale more efficiently to the types of truly
massive data sets that are now encountered routinely.
"
"  Spatial interaction patterns such as segregation and association can be
tested using nearest neighbor contingency tables (NNCTs). We introduce new
cell-specific (or pairwise) and overall segregation tests and determine their
asymptotic distributions. In particular, we demonstrate that cell-specific
tests enjoy asymptotic normality, while overall tests have chi-square
distributions asymptotically. We also perform an extensive Monte Carlo
simulation study to compare the finite sample performance of the tests in terms
of empirical size and power. In addition to the cell-specific tests as post-hoc
tests for overall tests, we discuss one-class-versus-rest type of NNCT-tests
after an overall test yields significant interaction. We also introduce the
concepts of total, strong, and partial segregation/association to label levels
of these patterns. We compare these new tests with the existing NNCT-tests in
literature with simulations as well and illustrate the NNCT-tests on an
ecological data set.
"
"  Like mean, quantile and variance, mode is also an important measure of
central tendency and data summary. Many practical questions often focus on
""Which element (gene or file or signal) occurs most often or is the most
typical among all elements in a network?"". In such cases mode regression
provides a convenient summary of how the regressors affect the conditional mode
and is totally different from other regression models based on conditional mean
or conditional quantile or conditional variance. Some inference methods have
been used for mode regression but none of them from the Bayesian perspective.
This paper introduces Bayesian mode regression by exploring three different
approaches. We start from a parametric Bayesian model by employing a likelihood
function that is based on a mode uniform distribution. It is shown that
irrespective of the original distribution of the data, the use of this special
uniform distribution is a very natural and effective way for Bayesian mode
regression. Posterior estimates based on this parametric likelihood, even under
misspecification, are consistent and asymptotically normal. We then develop a
nonparametric Bayesian model by using Dirichlet process (DP) mixtures of mode
uniform distributions and finally we explore Bayesian empirical likelihood mode
regression by taking empirical likelihood into a Bayesian framework. The paper
also demonstrates that a variety of improper priors for the unknown model
parameters yield a proper joint posterior. The proposed approach is illustrated
using simulated datasets and a real data set.
"
"  We propose an extensive framework for additive regression models for
correlated functional responses, allowing for multiple partially nested or
crossed functional random effects with flexible correlation structures for,
e.g., spatial, temporal, or longitudinal functional data. Additionally, our
framework includes linear and nonlinear effects of functional and scalar
covariates that may vary smoothly over the index of the functional response. It
accommodates densely or sparsely observed functional responses and predictors
which may be observed with additional error and includes both spline-based and
functional principal component-based terms. Estimation and inference in this
framework is based on standard additive mixed models, allowing us to take
advantage of established methods and robust, flexible algorithms. We provide
easy-to-use open source software in the pffr() function for the R-package
refund. Simulations show that the proposed method recovers relevant effects
reliably, handles small sample sizes well and also scales to larger data sets.
Applications with spatially and longitudinally observed functional data
demonstrate the flexibility in modeling and interpretability of results of our
approach.
"
"  Random Effects analysis has been introduced into fMRI research in order to
generalize findings from the study group to the whole population. Generalizing
findings is obviously harder than detecting activation in the study group since
in order to be significant, an activation has to be larger than the
inter-subject variability. Indeed, detected regions are smaller when using
random effect analysis versus fixed effects. The statistical assumptions behind
the classic random effects model are that the effect in each location is
normally distributed over subjects, and ""activation"" refers to a non-null mean
effect. We argue this model is unrealistic compared to the true population
variability, where, due to functional plasticity and registration anomalies, at
each brain location some of the subjects are active and some are not. We
propose a finite-Gaussian--mixture--random-effect. A model that amortizes
between-subject spatial disagreement and quantifies it using the ""prevalence""
of activation at each location. This measure has several desirable properties:
(a) It is more informative than the typical active/inactive paradigm. (b) In
contrast to the hypothesis testing approach (thus t-maps) which are trivially
rejected for large sample sizes, the larger the sample size, the more
informative the prevalence statistic becomes.
  In this work we present a formal definition and an estimation procedure of
this prevalence. The end result of the proposed analysis is a map of the
prevalence at locations with significant activation, highlighting activations
regions that are common over many brains.
"
"  In this letter we revisit the problem of optimal design of quantum
tomographic experiments. In contrast to previous approaches where an optimal
set of measurements is decided in advance of the experiment, we allow for
measurements to be adaptively and efficiently re-optimised depending on data
collected so far. We develop an adaptive statistical framework based on
Bayesian inference and Shannon's information, and demonstrate a ten-fold
reduction in the total number of measurements required as compared to
non-adaptive methods, including mutually unbiased bases.
"
"  Structural equation models and Bayesian networks have been widely used to
study causal relationships between continuous variables. Recently, a
non-Gaussian method called LiNGAM was proposed to discover such causal models
and has been extended in various directions. An important problem with LiNGAM
is that the results are affected by the random sampling of the data as with any
statistical method. Thus, some analysis of the statistical reliability or
confidence level should be conducted. A common method to evaluate a confidence
level is a bootstrap method. However, a confidence level computed by ordinary
bootstrap method is known to be biased as a probability-value ($p$-value) of
hypothesis testing. In this paper, we propose a new procedure to apply an
advanced bootstrap method called multiscale bootstrap to compute confidence
levels, i.e., p-values, of LiNGAM outputs. The multiscale bootstrap method
gives unbiased $p$-values with asymptotic much higher accuracy. Experiments on
artificial data demonstrate the utility of our approach.
"
"  This paper considers the possibility that the daily average Particulate
Matter (PM$_{10}$) concentration is a seasonal fractionally integrated process
with time-dependent variance (volatility). In this context, one convenient
extension is to consider the SARFIMA model (Reisen, et al, 2006a,b) with GARCH
type innovations. The model is theoretically justified and its usefulness is
corroborated with the application to PM$_{10}$ concentration in the city of
Cariacica-ES (Brazil). The model adjusted was able to capture the dynamics in
the series. The out-of-sample forecast intervals were improved by considering
heteroscedastic errors and they were able to identify the periods of more
volatility.
"
"  Variable selection and dimension reduction are two commonly adopted
approaches for high-dimensional data analysis, but have traditionally been
treated separately. Here we propose an integrated approach, called sparse
gradient learning (SGL), for variable selection and dimension reduction via
learning the gradients of the prediction function directly from samples. By
imposing a sparsity constraint on the gradients, variable selection is achieved
by selecting variables corresponding to non-zero partial derivatives, and
effective dimensions are extracted based on the eigenvectors of the derived
sparse empirical gradient covariance matrix. An error analysis is given for the
convergence of the estimated gradients to the true ones in both the Euclidean
and the manifold setting. We also develop an efficient forward-backward
splitting algorithm to solve the SGL problem, making the framework practically
scalable for medium or large datasets. The utility of SGL for variable
selection and feature extraction is explicitly given and illustrated on
artificial data as well as real-world examples. The main advantages of our
method include variable selection for both linear and nonlinear predictions,
effective dimension reduction with sparse loadings, and an efficient algorithm
for large p, small n problems.
"
"  This paper proposes an original approach to cluster multi-component data
sets, including an estimation of the number of clusters. From the construction
of a minimal spanning tree with Prim's algorithm, and the assumption that the
vertices are approximately distributed according to a Poisson distribution, the
number of clusters is estimated by thresholding the Prim's trajectory. The
corresponding cluster centroids are then computed in order to initialize the
generalized Lloyd's algorithm, also known as $K$-means, which allows to
circumvent initialization problems. Some results are derived for evaluating the
false positive rate of our cluster detection algorithm, with the help of
approximations relevant in Euclidean spaces. Metrics used for measuring
similarity between multi-dimensional data points are based on symmetrical
divergences. The use of these informational divergences together with the
proposed method leads to better results, compared to other clustering methods
for the problem of astrophysical data processing. Some applications of this
method in the multi/hyper-spectral imagery domain to a satellite view of Paris
and to an image of the Mars planet are also presented. In order to demonstrate
the usefulness of divergences in our problem, the method with informational
divergence as similarity measure is compared with the same method using
classical metrics. In the astrophysics application, we also compare the method
with the spectral clustering algorithms.
"
"  Regression, unlike classification, has lacked a comprehensive and effective
approach to deal with cost-sensitive problems by the reuse (and not a
re-training) of general regression models. In this paper, a wide variety of
cost-sensitive problems in regression (such as bids, asymmetric losses and
rejection rules) can be solved effectively by a lightweight but powerful
approach, consisting of: (1) the conversion of any traditional one-parameter
crisp regression model into a two-parameter soft regression model, seen as a
normal conditional density estimator, by the use of newly-introduced enrichment
methods; and (2) the reframing of an enriched soft regression model to new
contexts by an instance-dependent optimisation of the expected loss derived
from the conditional normal distribution.
"
"  We study the problem of learning high dimensional regression models
regularized by a structured-sparsity-inducing penalty that encodes prior
structural information on either input or output sides. We consider two widely
adopted types of such penalties as our motivating examples: 1) overlapping
group lasso penalty, based on the l1/l2 mixed-norm penalty, and 2) graph-guided
fusion penalty. For both types of penalties, due to their non-separability,
developing an efficient optimization method has remained a challenging problem.
In this paper, we propose a general optimization approach, called smoothing
proximal gradient method, which can solve the structured sparse regression
problems with a smooth convex loss and a wide spectrum of
structured-sparsity-inducing penalties. Our approach is based on a general
smoothing technique of Nesterov. It achieves a convergence rate faster than the
standard first-order method, subgradient method, and is much more scalable than
the most widely used interior-point method. Numerical results are reported to
demonstrate the efficiency and scalability of the proposed method.
"
"  We consider the problem of object recognition with a large number of classes.
In order to overcome the low amount of labeled examples available in this
setting, we introduce a new feature learning and extraction procedure based on
a factor model we call spike-and-slab sparse coding (S3C). Prior work on S3C
has not prioritized the ability to exploit parallel architectures and scale S3C
to the enormous problem sizes needed for object recognition. We present a novel
inference procedure for appropriate for use with GPUs which allows us to
dramatically increase both the training set size and the amount of latent
factors that S3C may be trained with. We demonstrate that this approach
improves upon the supervised learning capabilities of both sparse coding and
the spike-and-slab Restricted Boltzmann Machine (ssRBM) on the CIFAR-10
dataset. We use the CIFAR-100 dataset to demonstrate that our method scales to
large numbers of classes better than previous methods. Finally, we use our
method to win the NIPS 2011 Workshop on Challenges In Learning Hierarchical
Models? Transfer Learning Challenge.
"
"  In this paper we de ne conditional random elds in reproducing kernel Hilbert
spaces and show connections to Gaussian Process classi cation. More speci
cally, we prove decomposition results for undirected graphical models and we
give constructions for kernels. Finally we present e cient means of solving the
optimization problem using reduced rank decompositions and we show how
stationarity can be exploited e ciently in the optimization process.
"
"  Researchers in functional neuroimaging mostly use activation coordinates to
formulate their hypotheses. Instead, we propose to use the full statistical
images to define regions of interest (ROIs). This paper presents two machine
learning approaches, transfer learning and selection transfer, that are
compared upon their ability to identify the common patterns between brain
activation maps related to two functional tasks. We provide some preliminary
quantification of these similarities, and show that selection transfer makes it
possible to set a spatial scale yielding ROIs that are more specific to the
context of interest than with transfer learning. In particular, selection
transfer outlines well known regions such as the Visual Word Form Area when
discriminating between different visual tasks.
"
"  Conditional autoregressive (CAR) models are commonly used to capture spatial
correlation in areal unit data, and are typically specified as a prior
distribution for a set of random effects, as part of a hierarchical Bayesian
model. The spatial correlation structure induced by these models is determined
by geographical adjacency, so that two areas have correlated random effects if
they share a common border. However, this correlation structure is too
simplistic for real data, which are instead likely to include sub-regions of
strong correlation as well as locations at which the response exhibits a
step-change. Therefore this paper proposes an extension to CAR priors, which
can capture such localised spatial correlation. The proposed approach takes the
form of an iterative algorithm, which sequentially updates the spatial
correlation structure in the data as well as estimating the remaining model
parameters. The efficacy of the approach is assessed by simulation, and its
utility is illustrated in a disease mapping context, using data on respiratory
disease risk in Greater Glasgow, Scotland.
"
"  Discussion of ""Latent variable graphical model selection via convex
optimization"" by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky
[arXiv:1008.1290].
"
"  In this paper we describe the main featuress of the Bergm package for the
open-source R software which provides a comprehensive framework for Bayesian
analysis for exponential random graph models: tools for parameter estimation,
model selection and goodness-of-fit diagnostics. We illustrate the capabilities
of this package describing the algorithms through a tutorial analysis of two
well-known network datasets.
"
"  In this paper we bring to bear some new tools from statistical learning on
the analysis of roll call data. We present a new data-driven model for roll
call voting that is geometric in nature. We construct the model by adapting the
""Partition Decoupling Method,"" an unsupervised learning technique originally
developed for the analysis of families of time series, to produce a multiscale
geometric description of a weighted network associated to a set of roll call
votes. Central to this approach is the quantitative notion of a ""motivation,"" a
cluster-based and learned basis element that serves as a building block in the
representation of roll call data. Motivations enable the formulation of a
quantitative description of ideology and their data-dependent nature makes
possible a quantitative analysis of the evolution of ideological factors. This
approach is generally applicable to roll call data and we apply it in
particular to the historical roll call voting of the U.S. House and Senate.
This methodology provides a mechanism for estimating the dimension of the
underlying action space. We determine that the dominant factors form a low-
(one- or two-) dimensional representation with secondary factors adding
higher-dimensional features. In this way our work supports and extends the
findings of both Poole-Rosenthal and Heckman-Snyder concerning the
dimensionality of the action space. We give a detailed analysis of several
individual Senates and use the AdaBoost technique from statistical learning to
determine those votes with the most powerful discriminatory value. When used as
a predictive model, this geometric view significantly outperforms spatial
models such as the Poole-Rosenthal DW-NOMINATE model and the Heckman-Snyder
6-factor model, both in raw accuracy as well as Aggregate Proportional Reduced
Error (APRE).
"
"  Power-law distributions occur in many situations of scientific interest and
have significant consequences for our understanding of natural and man-made
phenomena. Unfortunately, the detection and characterization of power laws is
complicated by the large fluctuations that occur in the tail of the
distribution -- the part of the distribution representing large but rare events
-- and by the difficulty of identifying the range over which power-law behavior
holds. Commonly used methods for analyzing power-law data, such as
least-squares fitting, can produce substantially inaccurate estimates of
parameters for power-law distributions, and even in cases where such methods
return accurate answers they are still unsatisfactory because they give no
indication of whether the data obey a power law at all. Here we present a
principled statistical framework for discerning and quantifying power-law
behavior in empirical data. Our approach combines maximum-likelihood fitting
methods with goodness-of-fit tests based on the Kolmogorov-Smirnov statistic
and likelihood ratios. We evaluate the effectiveness of the approach with tests
on synthetic data and give critical comparisons to previous approaches. We also
apply the proposed methods to twenty-four real-world data sets from a range of
different disciplines, each of which has been conjectured to follow a power-law
distribution. In some cases we find these conjectures to be consistent with the
data while in others the power law is ruled out.
"
"  Independent component analysis (ICA) aims at decomposing an observed random
vector into statistically independent variables. Deflation-based
implementations, such as the popular one-unit FastICA algorithm and its
variants, extract the independent components one after another. A novel method
for deflationary ICA, referred to as RobustICA, is put forward in this paper.
This simple technique consists of performing exact line search optimization of
the kurtosis contrast function. The step size leading to the global maximum of
the contrast along the search direction is found among the roots of a
fourth-degree polynomial. This polynomial rooting can be performed
algebraically, and thus at low cost, at each iteration. Among other practical
benefits, RobustICA can avoid prewhitening and deals with real- and
complex-valued mixtures of possibly noncircular sources alike. The absence of
prewhitening improves asymptotic performance. The algorithm is robust to local
extrema and shows a very high convergence speed in terms of the computational
cost required to reach a given source extraction quality, particularly for
short data records. These features are demonstrated by a comparative numerical
analysis on synthetic data. RobustICA's capabilities in processing real-world
data involving noncircular complex strongly super-Gaussian sources are
illustrated by the biomedical problem of atrial activity (AA) extraction in
atrial fibrillation (AF) electrocardiograms (ECGs), where it outperforms an
alternative ICA-based technique.
"
"  This paper addresses the problem of inferring sparse causal networks modeled
by multivariate auto-regressive (MAR) processes. Conditions are derived under
which the Group Lasso (gLasso) procedure consistently estimates sparse network
structure. The key condition involves a ""false connection score."" In
particular, we show that consistent recovery is possible even when the number
of observations of the network is far less than the number of parameters
describing the network, provided that the false connection score is less than
one. The false connection score is also demonstrated to be a useful metric of
recovery in non-asymptotic regimes. The conditions suggest a modified gLasso
procedure which tends to improve the false connection score and reduce the
chances of reversing the direction of causal influence. Computational
experiments and a real network based electrocorticogram (ECoG) simulation study
demonstrate the effectiveness of the approach.
"
"  While five-factor models of personality are widespread, there is still not
universal agreement on this as a structural framework. Part of the reason for
the lingering debate is its dependence on factor analysis. In particular,
derivation or refutation of the model via other statistical means is a
worthwhile project. In this paper we use the methodology of spectral clustering
to articulate the structure in the dataset of responses of 20,993 subjects on a
300-item item version of the IPIP NEO personality questionnaire, and we compare
our results to those obtained from a factor analytic solution. We found support
for five- and six-cluster solutions. The five-cluster solution was similar to a
conventional five-factor solution, but the six-cluster and six-factor solutions
differed significantly, and only the six-cluster solution was readily
interpretable: it gave a model similar to the HEXACO model. We suggest that
spectral clustering provides a robust alternative view of personality data.
"
"  We present a probabilistic viewpoint to multiple kernel learning unifying
well-known regularised risk approaches and recent advances in approximate
Bayesian inference relaxations. The framework proposes a general objective
function suitable for regression, robust regression and classification that is
lower bound of the marginal likelihood and contains many regularised risk
approaches as special cases. Furthermore, we derive an efficient and provably
convergent optimisation algorithm.
"
"  In recent years, ideas from statistics and scientific computing have begun to
interact in increasingly sophisticated and fruitful ways with ideas from
computer science and the theory of algorithms to aid in the development of
improved worst-case algorithms that are useful for large-scale scientific and
Internet data analysis problems. In this chapter, I will describe two recent
examples---one having to do with selecting good columns or features from a (DNA
Single Nucleotide Polymorphism) data matrix, and the other having to do with
selecting good clusters or communities from a data graph (representing a social
or information network)---that drew on ideas from both areas and that may serve
as a model for exploiting complementary algorithmic and statistical
perspectives in order to solve applied large-scale data analysis problems.
"
"  We address the problems of multi-domain and single-domain regression based on
distinct and unpaired labeled training sets for each of the domains and a large
unlabeled training set from all domains. We formulate these problems as a
Bayesian estimation with partial knowledge of statistical relations. We propose
a worst-case design strategy and study the resulting estimators. Our analysis
explicitly accounts for the cardinality of the labeled sets and includes the
special cases in which one of the labeled sets is very large or, in the other
extreme, completely missing. We demonstrate our estimators in the context of
removing expressions from facial images and in the context of audio-visual word
recognition, and provide comparisons to several recently proposed multi-modal
learning algorithms.
"
"  Denoising by frame thresholding is one of the most basic and efficient
methods for recovering a discrete signal or image from data that are corrupted
by additive Gaussian white noise. The basic idea is to select a frame of
analyzing elements that separates the data in few large coefficients due to the
signal and many small coefficients mainly due to the noise \epsilon_n. Removing
all data coefficients being in magnitude below a certain threshold yields a
reconstruction of the original signal. In order to properly balance the amount
of noise to be removed and the relevant signal features to be kept, a precise
understanding of the statistical properties of thresholding is important. For
that purpose we derive the asymptotic distribution of max_{\omega \in \Omega_n}
|<\phi_\omega^n,\epsilon_n>| for a wide class of redundant frames
(\phi_\omega^n: \omega \in \Omega_n}. Based on our theoretical results we give
a rationale for universal extreme value thresholding techniques yielding
asymptotically sharp confidence regions and smoothness estimates corresponding
to prescribed significance levels. The results cover many frames used in
imaging and signal recovery applications, such as redundant wavelet systems,
curvelet frames, or unions of bases. We show that `generically' a standard
Gumbel law results as it is known from the case of orthonormal wavelet bases.
However, for specific highly redundant frames other limiting laws may occur. We
indeed verify that the translation invariant wavelet transform shows a
different asymptotic behaviour.
"
"  We describe the first sub-quadratic sampling algorithm for the Multiplicative
Attribute Graph Model (MAGM) of Kim and Leskovec (2010). We exploit the close
connection between MAGM and the Kronecker Product Graph Model (KPGM) of
Leskovec et al. (2010), and show that to sample a graph from a MAGM it suffices
to sample small number of KPGM graphs and \emph{quilt} them together. Under a
restricted set of technical conditions our algorithm runs in $O((\log_2(n))^3
|E|)$ time, where $n$ is the number of nodes and $|E|$ is the number of edges
in the sampled graph. We demonstrate the scalability of our algorithm via
extensive empirical evaluation; we can sample a MAGM graph with 8 million nodes
and 20 billion edges in under 6 hours.
"
"  Quantification of historical sociological processes have recently gained
attention among theoreticians in the effort of providing a solid theoretical
understanding of the behaviors and regularities present in sociopolitical
dynamics. Here we present a reliability theory of polity processes with
emphases on individual political dynamics of African countries. We found that
the structural properties of polity failure rates successfully capture the risk
of political vulnerability and instabilities in which 87.50%, 75%, 71.43%, and
0% of the countries with monotonically increasing, unimodal, U-shaped and
monotonically decreasing polity failure rates, respectively, have high level of
state fragility indices. The quasi-U-shape relationship between average polity
duration and regime types corroborates historical precedents and explains the
stability of the autocracies and democracies.
"
"  Causal inference uses observations to infer the causal structure of the data
generating system. We study a class of functional models that we call Time
Series Models with Independent Noise (TiMINo). These models require independent
residual time series, whereas traditional methods like Granger causality
exploit the variance of residuals. There are two main contributions: (1)
Theoretical: By restricting the model class (e.g. to additive noise) we can
provide a more general identifiability result than existing ones. This result
incorporates lagged and instantaneous effects that can be nonlinear and do not
need to be faithful, and non-instantaneous feedbacks between the time series.
(2) Practical: If there are no feedback loops between time series, we propose
an algorithm based on non-linear independence tests of time series. When the
data are causally insufficient, or the data generating process does not satisfy
the model assumptions, this algorithm may still give partial results, but
mostly avoids incorrect answers. An extension to (non-instantaneous) feedbacks
is possible, but not discussed. It outperforms existing methods on artificial
and real data. Code can be provided upon request.
"
"  Multivariate interaction between two or more classes (or species) has
important consequences in many fields and causes multivariate clustering
patterns such as segregation or association. The spatial segregation occurs
when members of a class tend to be found near members of the same class (i.e.,
near conspecifics) while spatial association occurs when members of a class
tend to be found near members of the other class or classes. These patterns can
be studied using a nearest neighbor contingency table (NNCT). The null
hypothesis is randomness in the nearest neighbor (NN) structure, which may
result from -- among other patterns -- random labeling (RL) or complete spatial
randomness (CSR) of points from two or more classes (which is called the CSR
independence, henceforth). In this article, we introduce new versions of
overall and cell-specific tests based on NNCTs (i.e., NNCT-tests) and compare
them with Dixon's overall and cell-specific tests. These NNCT-tests provide
information on the spatial interaction between the classes at small scales
(i.e., around the average NN distances between the points). Overall tests are
used to detect any deviation from the null case, while the cell-specific tests
are post hoc pairwise spatial interaction tests that are applied when the
overall test yields a significant result. We analyze the distributional
properties of these tests; assess the finite sample performance of the tests by
an extensive Monte Carlo simulation study. Furthermore, we show that the new
NNCT-tests have better performance in terms of Type I error and power. We also
illustrate these NNCT-tests on two real life data sets.
"
"  In this paper, we provide an explicit probability distribution for
classification purposes. It is derived from the Bayesian nonparametric mixture
of Dirichlet process model, but with suitable modifications which remove
unsuitable aspects of the classification based on this model. The resulting
approach then more closely resembles a classical hierarchical grouping rule in
that it depends on sums of squares of neighboring values. The proposed
probability model for classification relies on a simulation algorithm which
will be based on a reversible MCMC algorithm for determining the probabilities,
and we provide numerical illustrations comparing with alternative ideas for
classification.
"
"  Mathematical Morphology proposes to extract shapes from images as connected
components of level sets. These methods prove very suitable for shape
recognition and analysis. We present a method to select the perceptually
significant (i.e., contrasted) level lines (boundaries of level sets), using
the Helmholtz principle as first proposed by Desolneux et al. Contrarily to the
classical formulation by Desolneux et al. where level lines must be entirely
salient, the proposed method allows to detect partially salient level lines,
thus resulting in more robust and more stable detections. We then tackle the
problem of combining two gestalts as a measure of saliency and propose a method
that reinforces detections. Results in natural images show the good performance
of the proposed methods.
"
"  COMET is a single-pass MapReduce algorithm for learning on large-scale data.
It builds multiple random forest ensembles on distributed blocks of data and
merges them into a mega-ensemble. This approach is appropriate when learning
from massive-scale data that is too large to fit on a single machine. To get
the best accuracy, IVoting should be used instead of bagging to generate the
training subset for each decision tree in the random forest. Experiments with
two large datasets (5GB and 50GB compressed) show that COMET compares favorably
(in both accuracy and training time) to learning on a subsample of data using a
serial algorithm. Finally, we propose a new Gaussian approach for lazy ensemble
evaluation which dynamically decides how many ensemble members to evaluate per
data point; this can reduce evaluation cost by 100X or more.
"
"  In a clinical trial of a treatment for alcoholism, a common response variable
of interest is the number of alcoholic drinks consumed by each subject each
day, or an ordinal version of this response, with levels corresponding to
abstinence, light drinking and heavy drinking. In these trials, within-subject
drinking patterns are often characterized by alternating periods of heavy
drinking and abstinence. For this reason, many statistical models for time
series that assume steady behavior over time and white noise errors do not fit
alcohol data well. In this paper we propose to describe subjects' drinking
behavior using Markov models and hidden Markov models (HMMs), which are better
suited to describe processes that make sudden, rather than gradual, changes
over time. We incorporate random effects into these models using a hierarchical
Bayes structure to account for correlated responses within subjects over time,
and we estimate the effects of covariates, including a randomized treatment, on
the outcome in a novel way. We illustrate the models by fitting them to a large
data set from a clinical trial of the drug Naltrexone. The HMM, in particular,
fits this data well and also contains unique features that allow for useful
clinical interpretations of alcohol consumption behavior.
"
"  Active Learning (AL) is increasingly important in a broad range of
applications. Two main AL principles to obtain accurate classification with few
labeled data are refinement of the current decision boundary and exploration of
poorly sampled regions. In this paper we derive a novel AL scheme that balances
these two principles in a natural way. In contrast to many AL strategies, which
are based on an estimated class conditional probability ^p(y|x), a key
component of our approach is to view this quantity as a random variable, hence
explicitly considering the uncertainty in its estimated value. Our main
contribution is a novel mathematical framework for uncertainty-based AL, and a
corresponding AL scheme, where the uncertainty in ^p(y|x) is modeled by a
second-order distribution. On the practical side, we show how to approximate
such second-order distributions for kernel density classification. Finally, we
find that over a large number of UCI, USPS and Caltech4 datasets, our AL scheme
achieves significantly better learning curves than popular AL methods such as
uncertainty sampling and error reduction sampling, when all use the same kernel
density classifier.
"
"  It is now known that an extended Gaussian process model equipped with
rescaling can adapt to different smoothness levels of a function valued
parameter in many nonparametric Bayesian analyses, offering a posterior
convergence rate that is optimal (up to logarithmic factors) for the smoothness
class the true function belongs to. This optimal rate also depends on the
dimension of the function's domain and one could potentially obtain a faster
rate of convergence by casting the analysis in a lower dimensional subspace
that does not amount to any loss of information about the true function. In
general such a subspace is not known a priori but can be explored by equipping
the model with variable selection or linear projection. We demonstrate that for
nonparametric regression, classification, density estimation and density
regression, a rescaled Gaussian process model equipped with variable selection
or linear projection offers a posterior convergence rate that is optimal (up to
logarithmic factors) for the lowest dimension in which the analysis could be
cast without any loss of information about the true function. Theoretical
exploration of such dimension reduction features appears novel for Bayesian
nonparametric models with or without Gaussian processes.
"
"  Some problems of testology are discussed.
"
"  The maximum a posteriori (MAP) configuration of binary variable models with
submodular graph-structured energy functions can be found efficiently and
exactly by graph cuts. Max-product belief propagation (MP) has been shown to be
suboptimal on this class of energy functions by a canonical counterexample
where MP converges to a suboptimal fixed point (Kulesza & Pereira, 2008).
  In this work, we show that under a particular scheduling and damping scheme,
MP is equivalent to graph cuts, and thus optimal. We explain the apparent
contradiction by showing that with proper scheduling and damping, MP always
converges to an optimal fixed point. Thus, the canonical counterexample only
shows the suboptimality of MP with a particular suboptimal choice of schedule
and damping. With proper choices, MP is optimal.
"
"  We consider a framework for structured prediction based on search in the
space of complete structured outputs. Given a structured input, an output is
produced by running a time-bounded search procedure guided by a learned cost
function, and then returning the least cost output uncovered during the search.
This framework can be instantiated for a wide range of search spaces and search
procedures, and easily incorporates arbitrary structured-prediction loss
functions. In this paper, we make two main technical contributions. First, we
define the limited-discrepancy search space over structured outputs, which is
able to leverage powerful classification learning algorithms to improve the
search space quality. Second, we give a generic cost function learning
approach, where the key idea is to learn a cost function that attempts to mimic
the behavior of conducting searches guided by the true loss function. Our
experiments on six benchmark domains demonstrate that using our framework with
only a small amount of search is sufficient for significantly improving on
state-of-the-art structured-prediction performance.
"
"  This article develops a general detection theory for speech analysis based on
time-varying autoregressive models, which themselves generalize the classical
linear predictive speech analysis framework. This theory leads to a
computationally efficient decision-theoretic procedure that may be applied to
detect the presence of vocal tract variation in speech waveform data. A
corresponding generalized likelihood ratio test is derived and studied both
empirically for short data records, using formant-like synthetic examples, and
asymptotically, leading to constant false alarm rate hypothesis tests for
changes in vocal tract configuration. Two in-depth case studies then serve to
illustrate the practical efficacy of this procedure across different time
scales of speech dynamics: first, the detection of formant changes on the scale
of tens of milliseconds of data, and second, the identification of glottal
opening and closing instants on time scales below ten milliseconds.
"
"  We study losses for binary classification and class probability estimation
and extend the understanding of them from margin losses to general composite
losses which are the composition of a proper loss with a link function. We
characterise when margin losses can be proper composite losses, explicitly show
how to determine a symmetric loss in full from half of one of its partial
losses, introduce an intrinsic parametrisation of composite binary losses and
give a complete characterisation of the relationship between proper losses and
``classification calibrated'' losses. We also consider the question of the
``best'' surrogate binary loss. We introduce a precise notion of ``best'' and
show there exist situations where two convex surrogate losses are
incommensurable. We provide a complete explicit characterisation of the
convexity of composite binary losses in terms of the link function and the
weight function associated with the proper loss which make up the composite
loss. This characterisation suggests new ways of ``surrogate tuning''. Finally,
in an appendix we present some new algorithm-independent results on the
relationship between properness, convexity and robustness to misclassification
noise for binary losses and show that all convex proper losses are non-robust
to misclassification noise.
"
"  This paper studies questionnaire design as a formal decision problem,
focusing on one element of the design process: skip sequencing. We propose that
a survey planner use an explicit loss function to quantify the trade-off
between cost and informativeness of the survey and aim to make a design choice
that minimizes loss. We pose a choice between three options: ask all
respondents about an item of interest, use skip sequencing, thereby asking the
item only of respondents who give a certain answer to an opening question, or
do not ask the item at all. The first option is most informative but also most
costly. The use of skip sequencing reduces respondent burden and the cost of
interviewing, but may spread data quality problems across survey items, thereby
reducing informativeness. The last option has no cost but is completely
uninformative about the item of interest. We show how the planner may choose
among these three options in the presence of two inferential problems, item
nonresponse and response error.
"
"  Imbalanced data sets containing much more background than signal instances
are very common in particle physics, and will also be characteristic for the
upcoming analyses of LHC data. Following up the work presented at ACAT 2008, we
use the multivariate technique presented there (a rule growing algorithm with
the meta-methods bagging and instance weighting) on much more imbalanced data
sets, especially a selection of D0 decays without the use of particle
identification. It turns out that the quality of the result strongly depends on
the number of background instances used for training. We discuss methods to
exploit this in order to improve the results significantly, and how to handle
and reduce the size of large training sets without loss of result quality in
general. We will also comment on how to take into account statistical
fluctuation in receiver operation characteristic curves (ROC) for comparing
classifier methods.
"
"  The American Community Survey (ACS) provides one-year (1y), three-year (3y)
and five-year (5y) multi-year estimates (MYEs) of various demographic and
economic variables for each ""community"", although the 1y and 3y may not be
available for communities with a small population. These survey estimates are
not truly measuring the same quantities, since they each cover different time
spans. Using some simplistic models, we demonstrate that comparing different
period-length MYEs results in spurious conclusions about trend movements. A
simple method utilizing weighted averages is presented that reduces the bias
inherent in comparing trends of different MYEs. These weighted averages are
nonparametric, require only a short span of data, and are designed to preserve
polynomial characteristics of the time series that are relevant for trends. The
basic method, which only requires polynomial algebra, is outlined and applied
to ACS data. In some cases there is an improvement to comparability, although a
final verdict must await additional ACS data. We draw the conclusion that MYE
data is not comparable across different periods.
"
"  In many physical, statistical, biological and other investigations it is
desirable to approximate a system of points by objects of lower dimension
and/or complexity. For this purpose, Karl Pearson invented principal component
analysis in 1901 and found 'lines and planes of closest fit to system of
points'. The famous k-means algorithm solves the approximation problem too, but
by finite sets instead of lines and planes. This chapter gives a brief
practical introduction into the methods of construction of general principal
objects, i.e. objects embedded in the 'middle' of the multidimensional data
set. As a basis, the unifying framework of mean squared distance approximation
of finite datasets is selected. Principal graphs and manifolds are constructed
as generalisations of principal components and k-means principal points. For
this purpose, the family of expectation/maximisation algorithms with nearest
generalisations is presented. Construction of principal graphs with controlled
complexity is based on the graph grammar approach.
"
"  An important task in data analysis is the discovery of causal relationships
between observed variables. For continuous-valued data, linear acyclic causal
models are commonly used to model the data-generating process, and the
inference of such models is a well-studied problem. However, existing methods
have significant limitations. Methods based on conditional independencies
(Spirtes et al. 1993; Pearl 2000) cannot distinguish between
independence-equivalent models, whereas approaches purely based on Independent
Component Analysis (Shimizu et al. 2006) are inapplicable to data which is
partially Gaussian. In this paper, we generalize and combine the two
approaches, to yield a method able to learn the model structure in many cases
for which the previous methods provide answers that are either incorrect or are
not as informative as possible. We give exact graphical conditions for when two
distinct models represent the same family of distributions, and empirically
demonstrate the power of our method through thorough simulations.
"
"  Fetal ECG (FECG) telemonitoring is an important branch in telemedicine. The
design of a telemonitoring system via a wireless body-area network with low
energy consumption for ambulatory use is highly desirable. As an emerging
technique, compressed sensing (CS) shows great promise in
compressing/reconstructing data with low energy consumption. However, due to
some specific characteristics of raw FECG recordings such as non-sparsity and
strong noise contamination, current CS algorithms generally fail in this
application.
  This work proposes to use the block sparse Bayesian learning (BSBL) framework
to compress/reconstruct non-sparse raw FECG recordings. Experimental results
show that the framework can reconstruct the raw recordings with high quality.
Especially, the reconstruction does not destroy the interdependence relation
among the multichannel recordings. This ensures that the independent component
analysis decomposition of the reconstructed recordings has high fidelity.
Furthermore, the framework allows the use of a sparse binary sensing matrix
with much fewer nonzero entries to compress recordings. Particularly, each
column of the matrix can contain only two nonzero entries. This shows the
framework, compared to other algorithms such as current CS algorithms and
wavelet algorithms, can greatly reduce code execution in CPU in the data
compression stage.
"
"  I present a critique of the methods used in a typical paper. This leads to
three broad conclusions about the conventional use of statistical methods.
First, results are often reported in an unnecessarily obscure manner. Second,
the null hypothesis testing paradigm is deeply flawed: estimating the size of
effects and citing confidence intervals or levels is usually better. Third,
there are several issues, independent of the particular statistical concepts
employed, which limit the value of any statistical approach: e.g. difficulties
of generalizing to different contexts, and the weakness of some research in
terms of the size of the effects found. The first two of these are easily
remedied: I illustrate some of the possibilities by re-analyzing the data from
the case study article. The third means that in some contexts a statistical
approach may not be worthwhile. My case study is a management paper, but
similar problems arise in other social sciences. Keywords: Confidence,
Hypothesis testing, Null hypothesis significance tests, Philosophy of
statistics, Statistical methods, User-friendliness.
"
"  This paper describes a framework for flexible multiple hypothesis testing of
autoregressive time series. The modeling approach is Bayesian, though a blend
of frequentist and Bayesian reasoning is used to evaluate procedures.
Nonparametric characterizations of both the null and alternative hypotheses
will be shown to be the key robustification step necessary to ensure reasonable
Type-I error performance. The methodology is applied to part of a large
database containing up to 50 years of corporate performance statistics on
24,157 publicly traded American companies, where the primary goal of the
analysis is to flag companies whose historical performance is significantly
different from that expected due to chance.
"
"  We consider testing independence in group-wise selections with some
restrictions on combinations of choices. We present models for frequency data
of selections for which it is easy to perform conditional tests by Markov chain
Monte Carlo (MCMC) methods. When the restrictions on the combinations can be
described in terms of a Segre-Veronese configuration, an explicit form of a
Gr\""obner basis consisting of moves of degree two is readily available for
performing a Markov chain. We illustrate our setting with the National Center
Test for university entrance examinations in Japan. We also apply our method to
testing independence hypotheses involving genotypes at more than one locus or
haplotypes of alleles on the same chromosome.
"
"  We have constructed a comprehensive statistical model for Type Ia supernova
(SN Ia) light curves spanning optical through near infrared (NIR) data. A
hierarchical framework coherently models multiple random and uncertain effects,
including intrinsic supernova light curve covariances, dust extinction and
reddening, and distances. An improved BayeSN MCMC code computes probabilistic
inferences for the hierarchical model by sampling the global probability
density of parameters describing individual supernovae and the population. We
have applied this hierarchical model to optical and NIR data of 127 SN Ia from
PAIRITEL, CfA3, CSP, and the literature. We find an apparent population
correlation between the host galaxy extinction A_V and the the ratio of
total-to-selective dust absorption R_V. For SN with low dust extinction, A_V <
0.4, we find R_V = 2.5 - 2.9, while at high extinctions, A_V > 1, low values of
R_V < 2 are favored. The NIR luminosities are excellent standard candles and
are less sensitive to dust extinction. They exhibit low correlation with
optical peak luminosities, and thus provide independent information on
distances. The combination of NIR and optical data constrains the dust
extinction and improves the predictive precision of individual SN Ia distances
by about 60%. Using cross-validation, we estimate an rms distance modulus
prediction error of 0.11 mag for SN with optical and NIR data versus 0.15 mag
for SN with optical data alone. Continued study of SN Ia in the NIR is
important for improving their utility as precise and accurate cosmological
distance indicators.
"
"  Occasionally during the course of the human learning experience we are faced
with an anomaly. An aberration of sorts, which try as we might, defies
appropriate classification. The recent paper by Spiegelman et al.--Chemical and
forensic analysis of JFK assassination bullet lots: Is a second shooter
possible?--is one such aberration. It is riddled with both misconceptions and
errors of fact. Purporting to cast doubt on the NAA (neutron activation
analysis) work conducted by Dr. Vincent Guinn in the investigation of the
assassination of President John F. Kennedy, it fails miserably. The paper
offers two central conclusions, one which is demonstrably false, and the other
which is specious. The authors opine; ``If the assassination fragments are
derived from three or more separate bullets, then a second assassin is likely,
as the additional bullet would not be attributable to the main suspect, Mr.
Oswald.'' This statement relating to the likelihood of a second assassin based
on the premise of three or more separate bullets is demonstrably false. The
available evidence indicates that Oswald fired three shots, one of which is
believed to have missed. However, on the off chance that all three shots hit
(even though there is absolutely no other supporting forensic evidence for such
a notion) those three shots alone in no way would indicate then that ``a second
assassin is likely.'' The authors' erroneous conclusion was achieved because
they have either been misled (which I personally believe is the case) or they
simply aren't familiar with the evidence.
"
"  Over the past decades, statisticians and machine-learning researchers have
developed literally thousands of new tools for the reduction of
high-dimensional data in order to identify the variables most responsible for a
particular trait. These tools have applications in a plethora of settings,
including data analysis in the fields of business, education, forensics, and
biology (such as microarray, proteomics, brain imaging), to name a few.
  In the present work, we focus our investigation on the limitations and
potential misuses of certain tools in the analysis of the benchmark colon
cancer data (2,000 variables; Alon et al., 1999) and the prostate cancer data
(6,033 variables; Efron, 2010, 2008). Our analysis demonstrates that models
that produce 100% accuracy measures often select different sets of genes and
cannot stand the scrutiny of parameter estimates and model stability.
  Furthermore, we created a host of simulation datasets and ""artificial
diseases"" to evaluate the reliability of commonly used statistical and data
mining tools. We found that certain widely used models can classify the data
with 100% accuracy without using any of the variables responsible for the
disease. With moderate sample size and suitable pre-screening, stochastic
gradient boosting will be shown to be a superior model for gene selection and
variable screening from high-dimensional datasets.
"
"  We propose a solution to the problem of estimating a Riemannian metric
associated with a given differentiable manifold. The metric learning problem is
based on minimizing the relative volume of a given set of points. We derive the
details for a family of metrics on the multinomial simplex. The resulting
metric has applications in text classification and bears some similarity to
TFIDF representation of text documents.
"
"  Testing the global earthquake catalogue for indications of non-Poissonian
attributes has been an area of intense research, especially since the 2011
Tohoku earthquake. The usual approach is to test statistically for the
hypothesis that the global earthquake catalogue is well explained by a
Poissonian process. In this paper we analyse one aspect of this problem which
has been disregarded by the literature: the power of such tests to detect
non-Poissonian features if they existed; that is, the probability of type II
statistical errors. We argue that the low frequency of large events and the
brevity of our earthquake catalogues reduces the power of the statistical tests
so that an unequivocal answer for this question is not granted. We do this by
providing a counter example of a stochastic process that is clustered by
construction and by analysing the resulting distribution of p-values given by
the current tests.
"
"  Optical systems which measure independent random projections of a scene
according to compressed sensing (CS) theory face a myriad of practical
challenges related to the size of the physical platform, photon efficiency, the
need for high temporal resolution, and fast reconstruction in video settings.
This paper describes a coded aperture and keyed exposure approach to
compressive measurement in optical systems. The proposed projections satisfy
the Restricted Isometry Property for sufficiently sparse scenes, and hence are
compatible with theoretical guarantees on the video reconstruction quality.
These concepts can be implemented in both space and time via either amplitude
modulation or phase shifting, and this paper describes the relative merits of
the two approaches in terms of theoretical performance, noise and hardware
considerations, and experimental results. Fast numerical algorithms which
account for the nonnegativity of the projections and temporal correlations in a
video sequence are developed and applied to microscopy and short-wave infrared
data.
"
"  In many application domains, such as computational biology, the goal of
graphical model structure learning is to uncover discrete relationships between
entities. For example, in our problem of interest concerning HIV vaccine
design, we want to infer which HIV peptides interact with which immune system
molecules (HLA molecules). For problems of this nature, we are interested in
determining the number of nonspurious arcs in a learned graphical model. We
describe both a Bayesian and frequentist approach to this problem. In the
Bayesian approach, we use the posterior distribution over model structures to
compute the expected number of true arcs in a learned model. In the frequentist
approach, we develop a method based on the concept of the False Discovery Rate.
On synthetic data sets generated from models similar to the ones learned, we
find that both the Bayesian and frequentist approaches yield accurate estimates
of the number of non-spurious arcs. In addition, we speculate that the
frequentist approach, which is non-parametric, may outperform the parametric
Bayesian approach in situations where the models learned are less
representative of the data. Finally, we apply the frequentist approach to our
problem of HIV vaccine design.
"
"  I report the results of the test, where the takers had to tell the prose of
Charles Dickens from that of Edward Bulwer-Lytton, who is considered by many to
be the worst writer in history of letters. The average score is about 50%,
which is on the level of random guessing. This suggests that the quality of
Dickens' prose is the same as of that of Bulwer-Lytton.
"
"  Recent work in metric learning has significantly improved the
state-of-the-art in k-nearest neighbor classification. Support vector machines
(SVM), particularly with RBF kernels, are amongst the most popular
classification algorithms that uses distance metrics to compare examples. This
paper provides an empirical analysis of the efficacy of three of the most
popular Mahalanobis metric learning algorithms as pre-processing for SVM
training. We show that none of these algorithms generate metrics that lead to
particularly satisfying improvements for SVM-RBF classification. As a remedy we
introduce support vector metric learning (SVML), a novel algorithm that
seamlessly combines the learning of a Mahalanobis metric with the training of
the RBF-SVM parameters. We demonstrate the capabilities of SVML on nine
benchmark data sets of varying sizes and difficulties. In our study, SVML
outperforms all alternative state-of-the-art metric learning algorithms in
terms of accuracy and establishes itself as a serious alternative to the
standard Euclidean metric with model selection by cross validation.
"
"  We consider the problem of covariance matrix estimation in the presence of
latent variables. Under suitable conditions, it is possible to learn the
marginal covariance matrix of the observed variables via a tractable convex
program, where the concentration matrix of the observed variables is decomposed
into a sparse matrix (representing the graphical structure of the observed
variables) and a low rank matrix (representing the marginalization effect of
latent variables). We present an efficient first-order method based on split
Bregman to solve the convex problem. The algorithm is guaranteed to converge
under mild conditions. We show that our algorithm is significantly faster than
the state-of-the-art algorithm on both artificial and real-world data. Applying
the algorithm to a gene expression data involving thousands of genes, we show
that most of the correlation between observed variables can be explained by
only a few dozen latent factors.
"
"  In this article we describe a method for carrying out Bayesian estimation for
the double Pareto lognormal (dPlN) distribution which has been proposed as a
model for heavy-tailed phenomena. We apply our approach to estimate the
$\mathit{dPlN}/M/1$ and $M/\mathit{dPlN}/1$ queueing systems. These systems
cannot be analyzed using standard techniques due to the fact that the dPlN
distribution does not possess a Laplace transform in closed form. This
difficulty is overcome using some recent approximations for the Laplace
transform of the interarrival distribution for the $\mathit{Pareto}/M/1$
system. Our procedure is illustrated with applications in internet traffic
analysis and risk theory.
"
"  The CSA-ES is an Evolution Strategy with Cumulative Step size Adaptation,
where the step size is adapted measuring the length of a so-called cumulative
path. The cumulative path is a combination of the previous steps realized by
the algorithm, where the importance of each step decreases with time. This
article studies the CSA-ES on composites of strictly increasing functions with
affine linear functions through the investigation of its underlying Markov
chains. Rigorous results on the change and the variation of the step size are
derived with and without cumulation. The step-size diverges geometrically fast
in most cases. Furthermore, the influence of the cumulation parameter is
studied.
"
"  We discuss the allocation of finite resources in the presence of a
logarithmic diminishing return law, in analogy to some results from Information
Theory. To exemplify the problem we assume that the proposed logarithmic law is
applied to the problem of how to spend our time.
"
"  We consider mark-recapture-recovery (MRR) data of animals where the model
parameters are a function of individual time-varying continuous covariates. For
such covariates, the covariate value is unobserved if the corresponding
individual is unobserved, in which case the survival probability cannot be
evaluated. For continuous-valued covariates, the corresponding likelihood can
only be expressed in the form of an integral that is analytically intractable
and, to date, no maximum likelihood approach that uses all the information in
the data has been developed. Assuming a first-order Markov process for the
covariate values, we accomplish this task by formulating the MRR setting in a
state-space framework and considering an approximate likelihood approach which
essentially discretizes the range of covariate values, reducing the integral to
a summation. The likelihood can then be efficiently calculated and maximized
using standard techniques for hidden Markov models. We initially assess the
approach using simulated data before applying to real data relating to Soay
sheep, specifying the survival probability as a function of body mass. Models
that have previously been suggested for the corresponding covariate process are
typically of the form of diffusive random walks. We consider an alternative
nondiffusive AR(1)-type model which appears to provide a significantly better
fit to the Soay sheep data.
"
"  Microarrays have been developed that tile the entire nonrepetitive genomes of
many different organisms, allowing for the unbiased mapping of active
transcription regions or protein binding sites across the entire genome. These
tiling array experiments produce massive correlated data sets that have many
experimental artifacts, presenting many challenges to researchers that require
innovative analysis methods and efficient computational algorithms. This paper
presents a doubly stochastic latent variable analysis method for transcript
discovery and protein binding region localization using tiling array data. This
model is unique in that it considers actual genomic distance between probes.
Additionally, the model is designed to be robust to cross-hybridized and
nonresponsive probes, which can often lead to false-positive results in
microarray experiments. We apply our model to a transcript finding data set to
illustrate the consistency of our method. Additionally, we apply our method to
a spike-in experiment that can be used as a benchmark data set for researchers
interested in developing and comparing future tiling array methods. The results
indicate that our method is very powerful, accurate and can be used on a single
sample and without control experiments, thus defraying some of the overhead
cost of conducting experiments on tiling arrays.
"
"  Group model selection is the problem of determining a small subset of groups
of predictors (e.g., the expression data of genes) that are responsible for
majority of the variation in a response variable (e.g., the malignancy of a
tumor). This paper focuses on group model selection in high-dimensional linear
models, in which the number of predictors far exceeds the number of samples of
the response variable. Existing works on high-dimensional group model selection
either require the number of samples of the response variable to be
significantly larger than the total number of predictors contributing to the
response or impose restrictive statistical priors on the predictors and/or
nonzero regression coefficients. This paper provides comprehensive
understanding of a low-complexity approach to group model selection that avoids
some of these limitations. The proposed approach, termed Group Thresholding
(GroTh), is based on thresholding of marginal correlations of groups of
predictors with the response variable and is reminiscent of existing
thresholding-based approaches in the literature. The most important
contribution of the paper in this regard is relating the performance of GroTh
to a polynomial-time verifiable property of the predictors for the general case
of arbitrary (random or deterministic) predictors and arbitrary nonzero
regression coefficients.
"
"  In a dynamic social or biological environment, the interactions between the
actors can undergo large and systematic changes. In this paper we propose a
model-based approach to analyze what we will refer to as the dynamic tomography
of such time-evolving networks. Our approach offers an intuitive but powerful
tool to infer the semantic underpinnings of each actor, such as its social
roles or biological functions, underlying the observed network topologies. Our
model builds on earlier work on a mixed membership stochastic blockmodel for
static networks, and the state-space model for tracking object trajectory. It
overcomes a major limitation of many current network inference techniques,
which assume that each actor plays a unique and invariant role that accounts
for all its interactions with other actors; instead, our method models the role
of each actor as a time-evolving mixed membership vector that allows actors to
behave differently over time and carry out different roles/functions when
interacting with different peers, which is closer to reality. We present an
efficient algorithm for approximate inference and learning using our model; and
we applied our model to analyze a social network between monks (i.e., the
Sampson's network), a dynamic email communication network between the Enron
employees, and a rewiring gene interaction network of fruit fly collected
during its full life cycle. In all cases, our model reveals interesting
patterns of the dynamic roles of the actors.
"
"  We consider the decentralized binary hypothesis testing problem in networks
with feedback, where some or all of the sensors have access to compressed
summaries of other sensors' observations. We study certain two-message feedback
architectures, in which every sensor sends two messages to a fusion center,
with the second message based on full or partial knowledge of the first
messages of the other sensors. We also study one-message feedback
architectures, in which each sensor sends one message to a fusion center, with
a group of sensors having full or partial knowledge of the messages from the
sensors not in that group. Under either a Neyman-Pearson or a Bayesian
formulation, we show that the asymptotically optimal (in the limit of a large
number of sensors) detection performance (as quantified by error exponents)
does not benefit from the feedback messages, if the fusion center remembers all
sensor messages. However, feedback can improve the Bayesian detection
performance in the one-message feedback architecture if the fusion center has
limited memory; for that case, we determine the corresponding optimal error
exponents.
"
"  In this paper I present an extended implementation of the Random ferns
algorithm contained in the R package rFerns. It differs from the original by
the ability of consuming categorical and numerical attributes instead of only
binary ones. Also, instead of using simple attribute subspace ensemble it
employs bagging and thus produce error approximation and variable importance
measure modelled after Random forest algorithm. I also present benchmarks'
results which show that although Random ferns' accuracy is mostly smaller than
achieved by Random forest, its speed and good quality of importance measure it
provides make rFerns a reasonable choice for a specific applications.
"
"  We present a novel method in the family of particle MCMC methods that we
refer to as particle Gibbs with ancestor sampling (PG-AS). Similarly to the
existing PG with backward simulation (PG-BS) procedure, we use backward
sampling to (considerably) improve the mixing of the PG kernel. Instead of
using separate forward and backward sweeps as in PG-BS, however, we achieve the
same effect in a single forward sweep. We apply the PG-AS framework to the
challenging class of non-Markovian state-space models. We develop a truncation
strategy of these models that is applicable in principle to any
backward-simulation-based method, but which is particularly well suited to the
PG-AS framework. In particular, as we show in a simulation study, PG-AS can
yield an order-of-magnitude improved accuracy relative to PG-BS due to its
robustness to the truncation error. Several application examples are discussed,
including Rao-Blackwellized particle smoothing and inference in degenerate
state-space models.
"
"  We use recurrent-events survival analysis techniques and methods to analyze
the duration of Olympic records. The Kaplan-Meier estimator is used to perform
preliminary tests and recurrent event survivor function estimators proposed by
Wang & Chang (1999) and Pena et al. (2001) are used to estimate survival
curves. Extensions of the Cox Proportional Hazards model are employed as well
as a discrete-time logistic model for repeated events to estimate models and
quantify parameter significance. The logistic model was the best fit to the
data according to the Akaike Information Criterion (AIC). We discuss, in
detail, covariate significance for this model and make predictions of how many
records will be set at the 2012 Olympic Games in London.
  Keywords: survival analysis, recurrent events, Kaplan-Meier estimator, Cox
proportional hazards model, Olympics.
"
"  The bootstrap provides a simple and powerful means of assessing the quality
of estimators. However, in settings involving large datasets---which are
increasingly prevalent---the computation of bootstrap-based quantities can be
prohibitively demanding computationally. While variants such as subsampling and
the $m$ out of $n$ bootstrap can be used in principle to reduce the cost of
bootstrap computations, we find that these methods are generally not robust to
specification of hyperparameters (such as the number of subsampled data
points), and they often require use of more prior information (such as rates of
convergence of estimators) than the bootstrap. As an alternative, we introduce
the Bag of Little Bootstraps (BLB), a new procedure which incorporates features
of both the bootstrap and subsampling to yield a robust, computationally
efficient means of assessing the quality of estimators. BLB is well suited to
modern parallel and distributed computing architectures and furthermore retains
the generic applicability and statistical efficiency of the bootstrap. We
demonstrate BLB's favorable statistical performance via a theoretical analysis
elucidating the procedure's properties, as well as a simulation study comparing
BLB to the bootstrap, the $m$ out of $n$ bootstrap, and subsampling. In
addition, we present results from a large-scale distributed implementation of
BLB demonstrating its computational superiority on massive data, a method for
adaptively selecting BLB's hyperparameters, an empirical study applying BLB to
several real datasets, and an extension of BLB to time series data.
"
"  In a regression setup with deterministic design, we study the pure
aggregation problem and introduce a natural extension from the Gaussian
distribution to distributions in the exponential family. While this extension
bears strong connections with generalized linear models, it does not require
identifiability of the parameter or even that the model on the systematic
component is true. It is shown that this problem can be solved by constrained
and/or penalized likelihood maximization and we derive sharp oracle
inequalities that hold both in expectation and with high probability. Finally
all the bounds are proved to be optimal in a minimax sense.
"
"  In this article, we analyze the morphometry of hippocampus in subjects with
very mild dementia of Alzheimer's type (DAT) and nondemented controls and how
it changes over a two-year period. Morphometric differences with respect to a
template hippocampus were measured by the metric distance obtained from the
Large Deformation Diffeomorphic Metric Mapping (LDDMM) algorithm which was
previously used to calculate dense one-to-one correspondence vector fields
between the shapes. LDDMM assigns metric distances on the space of anatomical
images thereby allowing for the direct comparison and quantization of
morphometric changes. We use various statistical methods to compare the metric
distances in a cross-sectional and longitudinal manner. At baseline, the metric
distances for demented subjects are found not to be significantly different
from those for nondemented subjects. At follow-up, the metric distances for
demented subjects were significantly larger compared to nondemented subjects.
The metric distances for demented subjects increased significantly from
baseline to follow-up but not for nondemented subjects. We also use the metric
distances in logistic regression for diagnostic discrimination of subjects. We
compare metric distances with the volumes and obtain similar results. In
classification, the model that uses volume, metric distance, and volume loss
over time together performs better in detecting DAT. Thus, metric distances
with respect to a template computed via LDDMM can be a powerful tool in
detecting differences in shape in cross-sectional as well as longitudinal
studies.
"
"  Many modern data mining applications are concerned with the analysis of
datasets in which the observations are described by paired high-dimensional
vectorial representations or ""views"". Some typical examples can be found in web
mining and genomics applications. In this article we present an algorithm for
data clustering with multiple views, Multi-View Predictive Partitioning (MVPP),
which relies on a novel criterion of predictive similarity between data points.
We assume that, within each cluster, the dependence between multivariate views
can be modelled by using a two-block partial least squares (TB-PLS) regression
model, which performs dimensionality reduction and is particularly suitable for
high-dimensional settings. The proposed MVPP algorithm partitions the data such
that the within-cluster predictive ability between views is maximised. The
proposed objective function depends on a measure of predictive influence of
points under the TB-PLS model which has been derived as an extension of the
PRESS statistic commonly used in ordinary least squares regression. Using
simulated data, we compare the performance of MVPP to that of competing
multi-view clustering methods which rely upon geometric structures of points,
but ignore the predictive relationship between the two views. State-of-art
results are obtained on benchmark web mining datasets.
"
"  The property of perfectness plays an important role in the theory of Bayesian
networks. First, the existence of perfect distributions for arbitrary sets of
variables and directed acyclic graphs implies that various methods for reading
independence from the structure of the graph (e.g., Pearl, 1988; Lauritzen,
Dawid, Larsen & Leimer, 1990) are complete. Second, the asymptotic reliability
of various search methods is guaranteed under the assumption that the
generating distribution is perfect (e.g., Spirtes, Glymour & Scheines, 2000;
Chickering & Meek, 2002). We provide a lower-bound on the probability of
sampling a non-perfect distribution when using a fixed number of bits to
represent the parameters of the Bayesian network. This bound approaches zero
exponentially fast as one increases the number of bits used to represent the
parameters. This result implies that perfect distributions with fixed-length
representations exist. We also provide a lower-bound on the number of bits
needed to guarantee that a distribution sampled from a uniform Dirichlet
distribution is perfect with probability greater than 1/2. This result is
useful for constructing randomized reductions for hardness proofs.
"
"  One of the key challenges in sensor networks is the extraction of information
by fusing data from a multitude of distinct, but possibly unreliable sensors.
Recovering information from the maximum number of dependable sensors while
specifying the unreliable ones is critical for robust sensing. This sensing
task is formulated here as that of finding the maximum number of feasible
subsystems of linear equations, and proved to be NP-hard. Useful links are
established with compressive sampling, which aims at recovering vectors that
are sparse. In contrast, the signals here are not sparse, but give rise to
sparse residuals. Capitalizing on this form of sparsity, four sensing schemes
with complementary strengths are developed. The first scheme is a convex
relaxation of the original problem expressed as a second-order cone program
(SOCP). It is shown that when the involved sensing matrices are Gaussian and
the reliable measurements are sufficiently many, the SOCP can recover the
optimal solution with overwhelming probability. The second scheme is obtained
by replacing the initial objective function with a concave one. The third and
fourth schemes are tailored for noisy sensor data. The noisy case is cast as a
combinatorial problem that is subsequently surrogated by a (weighted) SOCP.
Interestingly, the derived cost functions fall into the framework of robust
multivariate linear regression, while an efficient block-coordinate descent
algorithm is developed for their minimization. The robust sensing capabilities
of all schemes are verified by simulated tests.
"
"  We show that the multi-class support vector machine (MSVM) proposed by Lee
et. al. (2004), can be viewed as a MAP estimation procedure under an
appropriate probabilistic interpretation of the classifier. We also show that
this interpretation can be extended to a hierarchical Bayesian architecture and
to a fully-Bayesian inference procedure for multi-class classification based on
data augmentation. We present empirical results that show that the advantages
of the Bayesian formalism are obtained without a loss in classification
accuracy.
"
"  We consider the problem of designing a sparse Gaussian process classifier
(SGPC) that generalizes well. Viewing SGPC design as constructing an additive
model like in boosting, we present an efficient and effective SGPC design
method to perform a stage-wise optimization of a predictive loss function. We
introduce new methods for two key components viz., site parameter estimation
and basis vector selection in any SGPC design. The proposed adaptive sampling
based basis vector selection method aids in achieving improved generalization
performance at a reduced computational cost. This method can also be used in
conjunction with any other site parameter estimation methods. It has similar
computational and storage complexities as the well-known information vector
machine and is suitable for large datasets. The hyperparameters can be
determined by optimizing a predictive loss function. The experimental results
show better generalization performance of the proposed basis vector selection
method on several benchmark datasets, particularly for relatively smaller basis
vector set sizes or on difficult datasets.
"
"  We present a generative model for representing and reasoning about the
relationships among events in continuous time. We apply the model to the domain
of networked and distributed computing environments where we fit the parameters
of the model from timestamp observations, and then use hypothesis testing to
discover dependencies between the events and changes in behavior for monitoring
and diagnosis. After introducing the model, we present an EM algorithm for
fitting the parameters and then present the hypothesis testing approach for
both dependence discovery and change-point detection. We validate the approach
for both tasks using real data from a trace of network events at Microsoft
Research Cambridge. Finally, we formalize the relationship between the proposed
model and the noisy-or gate for cases when time can be discretized.
"
"  This paper is devoted to adaptive long autoregressive spectral analysis when
(i) very few data are available, (ii) information does exist beforehand
concerning the spectral smoothness and time continuity of the analyzed signals.
The contribution is founded on two papers by Kitagawa and Gersch. The first one
deals with spectral smoothness, in the regularization framework, while the
second one is devoted to time continuity, in the Kalman formalism. The present
paper proposes an original synthesis of the two contributions: a new
regularized criterion is introduced that takes both information into account.
The criterion is efficiently optimized by a Kalman smoother. One of the major
features of the method is that it is entirely unsupervised: the problem of
automatically adjusting the hyperparameters that balance data-based versus
prior-based information is solved by maximum likelihood. The improvement is
quantified in the field of meteorological radar.
"
"  In this paper we propose to numerically assess the performance of standard
Gaussian approximations to probe the posterior distribution that arises from
Bayesian data assimilation in petroleum reservoirs. In particular we assess the
performance of (i) the linearization around the maximum a posterior estimate,
(ii) the randomized maximum likelihood and (iii) standard ensemble Kalman
filter-type methods. In order to fully resolve the posterior distribution we
implement a state-of-the art MCMC method that scales well with respect to the
dimension of the parameter space. Our implementation of the MCMC method
provides the gold standard against which to assess the aforementioned Gaussian
approximations. We present numerical synthetic experiments where we quantify
the capability of each of the {\em ad hoc} Gaussian approximation in
reproducing the mean and the variance of the posterior distribution
(characterized via MCMC) associated to a data assimilation problem. The main
objective of our controlled experiments is to exhibit the substantial
discrepancies of the approximation properties of standard {\em ad hoc} Gaussian
approximations. Numerical investigations of the type we present here will lead
to greater understanding of the cost-efficient, but {\em ad hoc}, Bayesian
techniques used for data assimilation in petroleum reservoirs, and hence
ultimately to improved techniques with more accurate uncertainty
quantification.
"
"  Differential privacy is a cryptographically-motivated definition of privacy
which has gained significant attention over the past few years. Differentially
private solutions enforce privacy by adding random noise to a function computed
over the data, and the challenge in designing such algorithms is to control the
added noise in order to optimize the privacy-accuracy-sample size tradeoff.
  This work studies differentially-private statistical estimation, and shows
upper and lower bounds on the convergence rates of differentially private
approximations to statistical estimators. Our results reveal a formal
connection between differential privacy and the notion of Gross Error
Sensitivity (GES) in robust statistics, by showing that the convergence rate of
any differentially private approximation to an estimator that is accurate over
a large class of distributions has to grow with the GES of the estimator. We
then provide an upper bound on the convergence rate of a differentially private
approximation to an estimator with bounded range and bounded GES. We show that
the bounded range condition is necessary if we wish to ensure a strict form of
differential privacy.
"
"  We show how to control the generalization error of time series models wherein
past values of the outcome are used to predict future values. The results are
based on a generalization of standard i.i.d. concentration inequalities to
dependent data without the mixing assumptions common in the time series
setting. Our proof and the result are simpler than previous analyses with
dependent data or stochastic adversaries which use sequential Rademacher
complexities rather than the expected Rademacher complexity for i.i.d.
processes. We also derive empirical Rademacher results without mixing
assumptions resulting in fully calculable upper bounds.
"
"  Motivated by fluorescence lifetime measurements this paper considers the
problem of nonparametric density estimation in the pile-up model. Adaptive
nonparametric estimators are proposed for the pile-up model in its simple form
as well as in the case of additional measurement errors. Furthermore, oracle
type risk bounds for the mean integrated squared error (MISE) are provided.
Finally, the estimation methods are assessed by a simulation study and the
application to real fluorescence lifetime data.
"
"  We consider cDNA microarray experiments when the cell populations have a
factorial structure, and investigate the problem of their optimal designing
under a baseline parametrization where the objects of interest differ from
those under the more common orthogonal parametrization. First, analytical
results are given for the $2\times 2$ factorial. Since practical applications
often involve a more complex factorial structure, we next explore general
factorials and obtain a collection of optimal designs in the saturated, that
is, most economic, case. This, in turn, is seen to yield an approach for
finding optimal or efficient designs in the practically more important nearly
saturated cases. Thereafter, the findings are extended to the more intricate
situation where the underlying model incorporates dye-coloring effects, and the
role of dye-swapping is critically examined.
"
"  We study the computational capacity of a model neuron, the Tempotron, which
classifies sequences of spikes by linear-threshold operations. We use
statistical mechanics and extreme value theory to derive the capacity of the
system in random classification tasks. In contrast to its static analog, the
Perceptron, the Tempotron's solutions space consists of a large number of small
clusters of weight vectors. The capacity of the system per synapse is finite in
the large size limit and weakly diverges with the stimulus duration relative to
the membrane and synaptic time constants.
"
"  This paper studies sparse density estimation via $\ell_1$ penalization
(SPADES). We focus on estimation in high-dimensional mixture models and
nonparametric adaptive density estimation. We show, respectively, that SPADES
can recover, with high probability, the unknown components of a mixture of
probability densities and that it yields minimax adaptive density estimates.
These results are based on a general sparsity oracle inequality that the SPADES
estimates satisfy. We offer a data driven method for the choice of the tuning
parameter used in the construction of SPADES. The method uses the generalized
bisection method first introduced in \citebb09. The suggested procedure
bypasses the need for a grid search and offers substantial computational
savings. We complement our theoretical results with a simulation study that
employs this method for approximations of one and two-dimensional densities
with mixtures. The numerical results strongly support our theoretical findings.
"
"  We propose a variable decomposition algorithm -greedy block coordinate
descent (GBCD)- in order to make dense Gaussian process regression practical
for large scale problems. GBCD breaks a large scale optimization into a series
of small sub-problems. The challenge in variable decomposition algorithms is
the identification of a subproblem (the active set of variables) that yields
the largest improvement. We analyze the limitations of existing methods and
cast the active set selection into a zero-norm constrained optimization problem
that we solve using greedy methods. By directly estimating the decrease in the
objective function, we obtain not only efficient approximate solutions for
GBCD, but we are also able to demonstrate that the method is globally
convergent. Empirical comparisons against competing dense methods like
Conjugate Gradient or SMO show that GBCD is an order of magnitude faster.
Comparisons against sparse GP methods show that GBCD is both accurate and
capable of handling datasets of 100,000 samples or more.
"
"  Slow feature analysis (SFA) is a method for extracting slowly varying driving
forces from quickly varying nonstationary time series. We show here that it is
possible for SFA to detect a component which is even slower than the driving
force itself (e.g. the envelope of a modulated sine wave). It is shown that it
depends on circumstances like the embedding dimension, the time series
predictability, or the base frequency, whether the driving force itself or a
slower subcomponent is detected. We observe a phase transition from one regime
to the other and it is the purpose of this work to quantify the influence of
various parameters on this phase transition. We conclude that what is percieved
as slow by SFA varies and that a more or less fast switching from one regime to
the other occurs, perhaps showing some similarity to human perception.
"
"  In many applications, such as economics, operations research and
reinforcement learning, one often needs to estimate a multivariate regression
function f subject to a convexity constraint. For example, in sequential
decision processes the value of a state under optimal subsequent decisions may
be known to be convex or concave. We propose a new Bayesian nonparametric
multivariate approach based on characterizing the unknown regression function
as the max of a random collection of unknown hyperplanes. This specification
induces a prior with large support in a Kullback-Leibler sense on the space of
convex functions, while also leading to strong posterior consistency. Although
we assume that f is defined over R^p, we show that this model has a convergence
rate of log(n)^{-1} n^{-1/(d+2)} under the empirical L2 norm when f actually
maps a d dimensional linear subspace to R. We design an efficient reversible
jump MCMC algorithm for posterior computation and demonstrate the methods
through application to value function approximation.
"
"  This paper introduces a statistical method to decide whether two blocks in a
pair of of images match reliably. The method ensures that the selected block
matches are unlikely to have occurred ""just by chance."" The new approach is
based on the definition of a simple but faithful statistical ""background model""
for image blocks learned from the image itself. A theorem guarantees that under
this model not more than a fixed number of wrong matches occurs (on average)
for the whole image. This fixed number (the number of false alarms) is the only
method parameter. Furthermore, the number of false alarms associated with each
match measures its reliability. This ""a contrario"" block-matching method,
however, cannot rule out false matches due to the presence of periodic objects
in the images. But it is successfully complemented by a parameterless
""self-similarity threshold."" Experimental evidence shows that the proposed
method also detects occlusions and incoherent motions due to vehicles and
pedestrians in non simultaneous stereo.
"
"  This paper explores the following question: what kind of statistical
guarantees can be given when doing variable selection in high-dimensional
models? In particular, we look at the error rates and power of some multi-stage
regression methods. In the first stage we fit a set of candidate models. In the
second stage we select one model by cross-validation. In the third stage we use
hypothesis testing to eliminate some variables. We refer to the first two
stages as ""screening"" and the last stage as ""cleaning."" We consider three
screening methods: the lasso, marginal regression, and forward stepwise
regression. Our method gives consistent variable selection under certain
conditions.
"
"  We propose an extension of the concept of Expected Improvement criterion
commonly used in Kriging based optimization. We extend it for more complex
Kriging models, e.g. models using derivatives. The target field of application
are CFD problems, where objective function are extremely expensive to evaluate,
but the theory can be also used in other fields.
"
"  We consider constraint-based methods for causal structure learning, such as
the PC-, FCI-, RFCI- and CCD- algorithms (Spirtes et al. (2000, 1993),
Richardson (1996), Colombo et al. (2012), Claassen et al. (2013)). The first
step of all these algorithms consists of the PC-algorithm. This algorithm is
known to be order-dependent, in the sense that the output can depend on the
order in which the variables are given. This order-dependence is a minor issue
in low-dimensional settings. We show, however, that it can be very pronounced
in high-dimensional settings, where it can lead to highly variable results. We
propose several modifications of the PC-algorithm (and hence also of the other
algorithms) that remove part or all of this order-dependence. All proposed
modifications are consistent in high-dimensional settings under the same
conditions as their original counterparts. We compare the PC-, FCI-, and
RFCI-algorithms and their modifications in simulation studies and on a yeast
gene expression data set. We show that our modifications yield similar
performance in low-dimensional settings and improved performance in
high-dimensional settings. All software is implemented in the R-package pcalg.
"
"  We present several applications of the bias-variance decomposition, beginning
with straightforward Monte Carlo estimation of integrals, but progressing to
the more complex problem of Monte Carlo Optimization (MCO), which involves
finding a set of parameters that optimize a parameterized integral. We present
the similarity of this application to that of Parametric Learning (PL).
Algorithms in this field use a particular interpretation of the bias-variance
trade to improve performance. This interpretation also applies to MCO, and
should therefore improve performance. We verify that this is indeed the case
for a particular MCO problem related to adaptive importance sampling.
"
"  A Hilbert space embedding for probability measures has recently been
proposed, with applications including dimensionality reduction, homogeneity
testing, and independence testing. This embedding represents any probability
measure as a mean element in a reproducing kernel Hilbert space (RKHS). A
pseudometric on the space of probability measures can be defined as the
distance between distribution embeddings: we denote this as $\gamma_k$, indexed
by the kernel function $k$ that defines the inner product in the RKHS.
  We present three theoretical properties of $\gamma_k$. First, we consider the
question of determining the conditions on the kernel $k$ for which $\gamma_k$
is a metric: such $k$ are denoted {\em characteristic kernels}. Unlike
pseudometrics, a metric is zero only when two distributions coincide, thus
ensuring the RKHS embedding maps all distributions uniquely (i.e., the
embedding is injective). While previously published conditions may apply only
in restricted circumstances (e.g. on compact domains), and are difficult to
check, our conditions are straightforward and intuitive: bounded continuous
strictly positive definite kernels are characteristic. Alternatively, if a
bounded continuous kernel is translation-invariant on $\bb{R}^d$, then it is
characteristic if and only if the support of its Fourier transform is the
entire $\bb{R}^d$. Second, we show that there exist distinct distributions that
are arbitrarily close in $\gamma_k$. Third, to understand the nature of the
topology induced by $\gamma_k$, we relate $\gamma_k$ to other popular metrics
on probability measures, and present conditions on the kernel $k$ under which
$\gamma_k$ metrizes the weak topology.
"
"  This paper introduces a new approach to analyzing spatial point data
clustered along or around a system of curves or ""fibres."" Such data arise in
catalogues of galaxy locations, recorded locations of earthquakes, aerial
images of minefields and pore patterns on fingerprints. Finding the underlying
curvilinear structure of these point-pattern data sets may not only facilitate
a better understanding of how they arise but also aid reconstruction of missing
data. We base the space of fibres on the set of integral lines of an
orientation field. Using an empirical Bayes approach, we estimate the field of
orientations from anisotropic features of the data. We then sample from the
posterior distribution of fibres, exploring models with different numbers of
clusters, fitting fibres to the clusters as we proceed. The Bayesian approach
permits inference on various properties of the clusters and associated fibres,
and the results perform well on a number of very different curvilinear
structures.
"
"  The expectation-maximization (EM) algorithm is a powerful computational
technique for finding the maximum likelihood estimates for parametric models
when the data are not fully observed. The EM is best suited for situations
where the expectation in each E-step and the maximization in each M-step are
straightforward. A difficulty with the implementation of the EM algorithm is
that each E-step requires the integration of the log-likelihood function in
closed form. The explicit integration can be avoided by using what is known as
the Monte Carlo EM (MCEM) algorithm. The MCEM uses a random sample to estimate
the integral at each E-step. However, the problem with the MCEM is that it
often converges to the integral quite slowly and the convergence behavior can
also be unstable, which causes a computational burden. In this paper, we
propose what we refer to as the quantile variant of the EM (QEM) algorithm. We
prove that the proposed QEM method has an accuracy of $O(1/K^2)$ while the MCEM
method has an accuracy of $O_p(1/\sqrt{K})$. Thus, the proposed QEM method
possesses faster and more stable convergence properties when compared with the
MCEM algorithm. The improved performance is illustrated through the numerical
studies. Several practical examples illustrating its use in interval-censored
data problems are also provided.
"
"  We address the online linear optimization problem with bandit feedback. Our
contribution is twofold. First, we provide an algorithm (based on exponential
weights) with a regret of order $\sqrt{d n \log N}$ for any finite action set
with $N$ actions, under the assumption that the instantaneous loss is bounded
by 1. This shaves off an extraneous $\sqrt{d}$ factor compared to previous
works, and gives a regret bound of order $d \sqrt{n \log n}$ for any compact
set of actions. Without further assumptions on the action set, this last bound
is minimax optimal up to a logarithmic factor. Interestingly, our result also
shows that the minimax regret for bandit linear optimization with expert advice
in $d$ dimension is the same as for the basic $d$-armed bandit with expert
advice. Our second contribution is to show how to use the Mirror Descent
algorithm to obtain computationally efficient strategies with minimax optimal
regret bounds in specific examples. More precisely we study two canonical
action sets: the hypercube and the Euclidean ball. In the former case, we
obtain the first computationally efficient algorithm with a $d \sqrt{n}$
regret, thus improving by a factor $\sqrt{d \log n}$ over the best known result
for a computationally efficient algorithm. In the latter case, our approach
gives the first algorithm with a $\sqrt{d n \log n}$ regret, again shaving off
an extraneous $\sqrt{d}$ compared to previous works.
"
"  Multiple-channel detection is considered in the context of a sensor network
where raw data are shared only by nodes that have a common edge in the network
graph. Established multiple-channel detectors, such as those based on
generalized coherence or multiple coherence, use pairwise measurements from
every pair of sensors in the network and are thus directly applicable only to
networks whose graphs are completely connected. An approach introduced here
uses a maximum-entropy technique to formulate surrogate values for missing
measurements corresponding to pairs of nodes that do not share an edge in the
network graph. The broader potential merit of maximum-entropy baselines in
quantifying the value of information in sensor network applications is also
noted.
"
"  Sensor networks aim at monitoring their surroundings for event detection and
object tracking. But due to failure or death of sensors, false signal can be
transmitted. In this paper, we consider the problem of fault detection in
wireless sensor network (WSN), in particular, addressing both the noise-related
measurement error and sensor fault simultaneously in fault detection. We assume
that the sensors are placed at the center of a square (or hexagonal) cell in
region of interest (ROI) and, if the event occurs, it occurs at a particular
cell of the ROI. We propose fault detection schemes that take into account
error probabilities into the optimal event detection process. We develop the
schemes under the consideration of Neyman-Pearson test and Bayes test.
"
"  Restricted Boltzmann Machines (RBM) have attracted a lot of attention of
late, as one the principle building blocks of deep networks. Training RBMs
remains problematic however, because of the intractibility of their partition
function. The maximum likelihood gradient requires a very robust sampler which
can accurately sample from the model despite the loss of ergodicity often
incurred during learning. While using Parallel Tempering in the negative phase
of Stochastic Maximum Likelihood (SML-PT) helps address the issue, it imposes a
trade-off between computational complexity and high ergodicity, and requires
careful hand-tuning of the temperatures. In this paper, we show that this
trade-off is unnecessary. The choice of optimal temperatures can be automated
by minimizing average return time (a concept first proposed by [Katzgraber et
al., 2006]) while chains can be spawned dynamically, as needed, thus minimizing
the computational overhead. We show on a synthetic dataset, that this results
in better likelihood scores.
"
"  Driven by the multi-level structure of human intracranial
electroencephalogram (iEEG) recordings of epileptic seizures, we introduce a
new variant of a hierarchical Dirichlet Process---the multi-level clustering
hierarchical Dirichlet Process (MLC-HDP)---that simultaneously clusters
datasets on multiple levels. Our seizure dataset contains brain activity
recorded in typically more than a hundred individual channels for each seizure
of each patient. The MLC-HDP model clusters over channels-types, seizure-types,
and patient-types simultaneously. We describe this model and its implementation
in detail. We also present the results of a simulation study comparing the
MLC-HDP to a similar model, the Nested Dirichlet Process and finally
demonstrate the MLC-HDP's use in modeling seizures across multiple patients. We
find the MLC-HDP's clustering to be comparable to independent human physician
clusterings. To our knowledge, the MLC-HDP model is the first in the epilepsy
literature capable of clustering seizures within and between patients.
"
"  Support vector machines (SVMs) naturally embody sparseness due to their use
of hinge loss functions. However, SVMs can not directly estimate conditional
class probabilities. In this paper we propose and study a family of coherence
functions, which are convex and differentiable, as surrogates of the hinge
function. The coherence function is derived by using the maximum-entropy
principle and is characterized by a temperature parameter. It bridges the hinge
function and the logit function in logistic regression. The limit of the
coherence function at zero temperature corresponds to the hinge function, and
the limit of the minimizer of its expected error is the minimizer of the
expected error of the hinge loss. We refer to the use of the coherence function
in large-margin classification as C-learning, and we present efficient
coordinate descent algorithms for the training of regularized ${\cal
C}$-learning models.
"
"  We investigate the high-dimensional regression problem using adjacency
matrices of unbalanced expander graphs. In this frame, we prove that the
$\ell_{2}$-prediction error and the $\ell_{1}$-risk of the lasso and the
Dantzig selector are optimal up to an explicit multiplicative constant. Thus we
can estimate a high-dimensional target vector with an error term similar to the
one obtained in a situation where one knows the support of the largest
coordinates in advance.
  Moreover, we show that these design matrices have an explicit restricted
eigenvalue. Precisely, they satisfy the restricted eigenvalue assumption and
the compatibility condition with an explicit constant.
  Eventually, we capitalize on the recent construction of unbalanced expander
graphs due to Guruswami, Umans, and Vadhan, to provide a deterministic
polynomial time construction of these design matrices.
"
"  A Bayesian procedure is developed for multivariate stochastic volatility,
using state space models. An autoregressive model for the log-returns is
employed. We generalize the inverted Wishart distribution to allow for
different correlation structure between the observation and state innovation
vectors and we extend the convolution between the Wishart and the multivariate
singular beta distribution. A multiplicative model based on the generalized
inverted Wishart and multivariate singular beta distributions is proposed for
the evolution of the volatility and a flexible sequential volatility updating
is employed. The proposed algorithm for the volatility is fast and
computationally cheap and it can be used for on-line forecasting. The methods
are illustrated with an example consisting of foreign exchange rates data of 8
currencies. The empirical results suggest that time-varying correlations can be
estimated efficiently, even in situations of high dimensional data.
"
"  In this paper it is demonstrated that the scoring at each PGA Tour stroke
play event can be reasonably modeled as a Gaussian random variable. All 46
stroke play events in the 2007 season are analyzed. The distributions of scores
are favorably compared with a Gaussian distribution using the
Kolmogorov-Smirnov test. This observation suggests performance tracking on the
PGA tour should be done in terms of the z-score, calculated by subtracting the
mean from the raw score and dividing by the standard deviation. This
methodology measures performance relative to the field of competitors,
independent of the venue, and in terms of a statistic that has quantitative
meaning. Several examples of the use of this scoring methodology are provided,
including a calculation of the probability that Tiger Woods will break Byron
Nelson's record of eleven consecutive PGA Tour victories.
"
"  Discussion of ""A statistical analysis of multiple temperature proxies: Are
reconstructions of surface temperatures over the last 1000 years reliable?"" by
B.B. McShane and A.J. Wyner [arXiv:1104.4002]
"
"  Percentiles have been established in bibliometrics as an important
alternative to mean-based indicators for obtaining a normalized citation impact
of publications. Percentiles have a number of advantages over standard
bibliometric indicators used frequently: for example, their calculation is not
based on the arithmetic mean which should not be used for skewed bibliometric
data. This study describes the opportunities and limits and the advantages and
disadvantages of using percentiles in bibliometrics. We also address problems
in the calculation of percentiles and percentile rank classes for which there
is not (yet) a satisfactory solution. It will be hard to compare the results of
different percentile-based studies with each other unless it is clear that the
studies were done with the same choices for percentile calculation and rank
assignment.
"
"  We use a discrete-time proportional hazards model of time to involuntary
employment termination. This model enables us to examine both the continuous
effect of the age of an employee and whether that effect has varied over time,
generalizing earlier work [Kadane and Woodworth J. Bus. Econom. Statist. 22
(2004) 182--193]. We model the log hazard surface (over age and time) as a
thin-plate spline, a Bayesian smoothness-prior implementation of penalized
likelihood methods of surface-fitting [Wahba (1990) Spline Models for
Observational Data. SIAM]. The nonlinear component of the surface has only two
parameters, smoothness and anisotropy. The first, a scale parameter, governs
the overall smoothness of the surface, and the second, anisotropy, controls the
relative smoothness over time and over age. For any fixed value of the
anisotropy parameter, the prior is equivalent to a Gaussian process with linear
drift over the time--age plane with easily computed eigenvectors and
eigenvalues that depend only on the configuration of data in the time--age
plane and the anisotropy parameter. This model has application to legal cases
in which a company is charged with disproportionately disadvantaging older
workers when deciding whom to terminate. We illustrate the application of the
modeling approach using data from an actual discrimination case.
"
"  Inverse inference, or ""brain reading"", is a recent paradigm for analyzing
functional magnetic resonance imaging (fMRI) data, based on pattern recognition
and statistical learning. By predicting some cognitive variables related to
brain activation maps, this approach aims at decoding brain activity. Inverse
inference takes into account the multivariate information between voxels and is
currently the only way to assess how precisely some cognitive information is
encoded by the activity of neural populations within the whole brain. However,
it relies on a prediction function that is plagued by the curse of
dimensionality, since there are far more features than samples, i.e., more
voxels than fMRI volumes. To address this problem, different methods have been
proposed, such as, among others, univariate feature selection, feature
agglomeration and regularization techniques. In this paper, we consider a
sparse hierarchical structured regularization. Specifically, the penalization
we use is constructed from a tree that is obtained by spatially-constrained
agglomerative clustering. This approach encodes the spatial structure of the
data at different scales into the regularization, which makes the overall
prediction procedure more robust to inter-subject variability. The
regularization used induces the selection of spatially coherent predictive
brain regions simultaneously at different scales. We test our algorithm on real
data acquired to study the mental representation of objects, and we show that
the proposed algorithm not only delineates meaningful brain regions but yields
as well better prediction accuracy than reference methods.
"
"  Can one parallelize complex exploration exploitation tradeoffs? As an
example, consider the problem of optimal high-throughput experimental design,
where we wish to sequentially design batches of experiments in order to
simultaneously learn a surrogate function mapping stimulus to response and
identify the maximum of the function. We formalize the task as a multi-armed
bandit problem, where the unknown payoff function is sampled from a Gaussian
process (GP), and instead of a single arm, in each round we pull a batch of
several arms in parallel. We develop GP-BUCB, a principled algorithm for
choosing batches, based on the GP-UCB algorithm for sequential GP optimization.
We prove a surprising result; as compared to the sequential approach, the
cumulative regret of the parallel algorithm only increases by a constant factor
independent of the batch size B. Our results provide rigorous theoretical
support for exploiting parallelism in Bayesian global optimization. We
demonstrate the effectiveness of our approach on two real-world applications.
"
"  Voters from m disjoint constituencies (regions, federal states, etc.) are
represented in an assembly which contains one delegate from each constituency
and applies a weighted voting rule. All agents are assumed to have
single-peaked preferences over an interval; each delegate's preferences match
his constituency's median voter; and the collective decision equals the
assembly's Condorcet winner. We characterize the asymptotic behavior of the
probability of a given delegate determining the outcome (i.e., being the
weighted median of medians) in order to address a contentious practical
question: which voting weights w_1, ..., w_m ought to be selected if
constituency sizes differ and all voters are to have a priori equal influence
on collective decisions? It is shown that if ideal point distributions have
identical median M and are suitably continuous, the probability for a given
delegate i's ideal point \lambda_i being the Condorcet winner becomes
asymptotically proportional to i's voting weight w_i times \lambda_i's density
at M as $m\to \infty$. Indirect representation of citizens is approximately
egalitarian for weights proportional to the square root of constituency sizes
if all individual ideal points are i.i.d. In contrast, weights that are linear
in-- or, better, induce a Shapley value linear in-- size are egalitarian when
preferences are sufficiently strongly affiliated within constituencies.
"
"  While it is an important problem to identify the existence of causal
associations between two components of a multivariate time series, a topic
addressed in Runge et al. (2012), it is even more important to assess the
strength of their association in a meaningful way. In the present article we
focus on the problem of defining a meaningful coupling strength using
information theoretic measures and demonstrate the short-comings of the
well-known mutual information and transfer entropy. Instead, we propose a
certain time-delayed conditional mutual information, the momentary information
transfer (MIT), as a measure of association that is general, causal and
lag-specific, reflects a well interpretable notion of coupling strength and is
practically computable. MIT is based on the fundamental concept of source
entropy, which we utilize to yield a notion of coupling strength that is,
compared to mutual information and transfer entropy, well interpretable, in
that for many cases it solely depends on the interaction of the two components
at a certain lag. In particular, MIT is thus in many cases able to exclude the
misleading influence of autodependency within a process in an
information-theoretic way. We formalize and prove this idea analytically and
numerically for a general class of nonlinear stochastic processes and
illustrate the potential of MIT on climatological data.
"
"  A significant problem with most functional data analyses is that of
misaligned curves. Without adjustment, even an analysis as simple as estimation
of the mean will fail. One common method to synchronize a set of curves
involves equating ``landmarks'' such as peaks or troughs. The landmarks method
can work well but will fail if marker events can not be identified or are
missing from some curves. An alternative approach, the ``continuous monotone
registration'' method, works by transforming the curves so that they are as
close as possible to a target function. This method can also perform well but
is highly dependent on identifying an accurate target function. We develop an
alignment method based on equating the ``moments'' of a given set of curves.
These moments are intended to capture the locations of important features which
may represent local behavior, such as maximums and minimums, or more global
characteristics, such as the slope of the curve averaged over time. Our method
works by equating the moments of the curves while also shrinking toward a
common shape. This allows us to capture the advantages of both the landmark and
continuous monotone registration approaches. The method is illustrated on
several data sets and a simulation study is performed.
"
"  We propose a framework for the derivation and evaluation of distributed
iterative algorithms for receiver cooperation in interference-limited wireless
systems. Our approach views the processing within and collaboration between
receivers as the solution to an inference problem in the probabilistic model of
the whole system. The probabilistic model is formulated to explicitly
incorporate the receivers' ability to share information of a predefined type.
We employ a recently proposed unified message-passing tool to infer the
variables of interest in the factor graph representation of the probabilistic
model. The exchange of information between receivers arises in the form of
passing messages along some specific edges of the factor graph; the rate of
updating and passing these messages determines the communication overhead
associated with cooperation. Simulation results illustrate the high performance
of the proposed algorithm even with a low number of message exchanges between
receivers.
"
"  Objective: Modelling the associations from high-throughput experimental
molecular data has provided unprecedented insights into biological pathways and
signalling mechanisms. Graphical models and networks have especially proven to
be useful abstractions in this regard. Ad-hoc thresholds are often used in
conjunction with structure learning algorithms to determine significant
associations. The present study overcomes this limitation by proposing a
statistically-motivated approach for identifying significant associations in a
network.
  Methods and Materials: A new method that identifies significant associations
in graphical models by estimating the threshold minimising the $L_{\mathrm{1}}$
norm between the cumulative distribution function (CDF) of the observed edge
confidences and those of its asymptotic counterpart is proposed. The
effectiveness of the proposed method is demonstrated on popular synthetic data
sets as well as publicly available experimental molecular data corresponding to
gene and protein expression profiles.
  Results: The improved performance of the proposed approach is demonstrated
across the synthetic data sets using sensitivity, specificity and accuracy as
performance metrics. The results are also demonstrated across varying sample
sizes and three different structure learning algorithms with widely varying
assumptions. In all cases, the proposed approach has specificity and accuracy
close to 1, while sensitivity increases linearly in the logarithm of the sample
size. The estimated threshold systematically outperforms common ad-hoc ones in
terms of sensitivity while maintaining comparable levels of specificity and
accuracy. Networks from experimental data sets are reconstructed accurately
with respect to the results from the original papers.
"
"  Gene innovation is a key mechanism on the evolution and phenotypic diversity
of life forms. There is a need for tools able to study gene innovation across
an increasingly large number of genomic sequences to maximally capitalise our
understanding of biological systems. Here we present
Comparative-Phylostratigraphy, an open-source software suite that enables to
time the emergence of new genes across evolutionary time and to correlate
patterns of gene emergence with species traits simultaneously across whole
genomes from multiple species. Such a comparative strategy is a new powerful
tool for starting to dissect the relationship between gene innovation and
phenotypic diversity. We describe and showcase our method by analysing recently
published ant genomes. This new methodology identified significant bouts of new
gene evolution in ant clades, that are associated with shifts in life-history
traits. Our method allows easy integration of new genomic data as it becomes
available, and thus will be a valuable analytical tool for evolutionary
biologists interested in explaining the evolution of diversity of life at the
level of the genes.
"
"  When a scientist performs an experiment they normally acquire a set of
measurements and are expected to demonstrate that their results are
""statistically significant"" thus confirming whatever hypothesis they are
testing. The main method for establishing statistical significance involves
demonstrating that there is a low probability that the observed experimental
results were the product of random chance. This is typically defined as p <
0.05, which indicates there is less than a 5% chance that the observed results
occurred randomly. This research study visually demonstrates that the commonly
used definition for ""statistical significance"" can erroneously imply a
significant finding. This is demonstrated by generating random Gaussian noise
data and analyzing that data using statistical testing based on the established
two-sample t-test. This study demonstrates that insignificant yet
""statistically significant"" findings are possible at moderately large sample
sizes which are very common in many fields of modern science.
"
"  We study learning in a noisy bisection model: specifically, Bayesian
algorithms to learn a target value V given access only to noisy realizations of
whether V is less than or greater than a threshold theta. At step t = 0, 1, 2,
..., the learner sets threshold theta t and observes a noisy realization of
sign(V - theta t). After T steps, the goal is to output an estimate V^ which is
within an eta-tolerance of V . This problem has been studied, predominantly in
environments with a fixed error probability q < 1/2 for the noisy realization
of sign(V - theta t). In practice, it is often the case that q can approach
1/2, especially as theta -> V, and there is little known when this happens. We
give a pseudo-Bayesian algorithm which provably converges to V. When the true
prior matches our algorithm's Gaussian prior, we show near-optimal expected
performance. Our methods extend to the general multiple-threshold setting where
the observation noisily indicates which of k >= 2 regions V belongs to.
"
"  Generalizations of the Monty Hall problem are studied according to George
Boole's (1853) ""An Investigation of the Laws of Thought, on Which Are Founded
the Mathematical Theories of Logic and Probabilities""
"
"  Next-generation sequencing techniques have facilitated a large scale analysis
of human genetic variation. Despite the advances in sequencing speeds, the
computational discovery of structural variants is not yet standard. It is
likely that many variants have remained undiscovered in most sequenced
individuals. Here we present a novel internal segment size based approach,
which organizes all, including also concordant reads into a read alignment
graph where max-cliques represent maximal contradiction-free groups of
alignments. A specifically engineered algorithm then enumerates all max-cliques
and statistically evaluates them for their potential to reflect insertions or
deletions (indels). For the first time in the literature, we compare a large
range of state-of-the-art approaches using simulated Illumina reads from a
fully annotated genome and present various relevant performance statistics. We
achieve superior performance rates in particular on indels of sizes 20--100,
which have been exposed as a current major challenge in the SV discovery
literature and where prior insert size based approaches have limitations. In
that size range, we outperform even split read aligners. We achieve good
results also on real data where we make a substantial amount of correct
predictions as the only tool, which complement the predictions of split-read
aligners. CLEVER is open source (GPL) and available from
http://clever-sv.googlecode.com.
"
"  This paper explores seasonal and long-memory time series properties by using
the seasonal fractional ARIMA model when the seasonal data has one and two
seasonal periods and short-memory counterparts. The stationarity and
invertibility parameter conditions are established for the model studied. To
estimate the memory parameters, the method given in Reisen, Rodrigues and Palma
(2006 a,b) is generalized here to deal with a time series with two seasonal
fractional long-memory parameters. The asymptotic properties are established
and the accuracy of the method is investigated through Monte Carlo experiments.
The good performance of the estimator indicates that it can be an alternative
competitive procedure to estimate seasonal long-memory time series data.
Artificial and PM10 series were considered as examples of applications of the
proposed estimation method.
"
"  Consider observation data, comprised of n observation vectors with values on
a set of attributes. This gives us n points in attribute space. Having data
structured as a tree, implied by having our observations embedded in an
ultrametric topology, offers great advantage for proximity searching. If we
have preprocessed data through such an embedding, then an observation's nearest
neighbor is found in constant computational time, i.e. O(1) time. A further
powerful approach is discussed in this work: the inducing of a hierarchy, and
hence a tree, in linear computational time, i.e. O(n) time for n observations.
It is with such a basis for proximity search and best match that we can address
the burgeoning problems of processing very large, and possibly also very high
dimensional, data sets.
"
"  Propensity score matching is a tool for causal inference in non-randomized
studies that allows for conditioning on large sets of covariates. The use of
propensity scores in the social sciences is currently experiencing a tremendous
increase; however it is far from a commonly used tool. One impediment towards a
more wide-spread use of propensity score methods is the reliance on specialized
software, because many social scientists still use SPSS as their main analysis
tool. The current paper presents an implementation of various propensity score
matching methods in SPSS. Specifically the presented SPSS custom dialog allows
researchers to specify propensity score methods using the familiar
point-and-click interface. The software allows estimation of the propensity
score using logistic regression and specifying nearest-neighbor matching with
many options, e.g., calipers, region of common support, matching with and
without replacement, and matching one to many units. Detailed balance
statistics and graphs are produced by the program.
"
"  Calibrated strategies can be obtained by performing strategies that have no
internal regret in some auxiliary game. Such strategies can be constructed
explicitly with the use of Blackwell's approachability theorem, in an other
auxiliary game. We establish the converse: a strategy that approaches a convex
$B$-set can be derived from the construction of a calibrated strategy. We
develop these tools in the framework of a game with partial monitoring, where
players do not observe the actions of their opponents but receive random
signals, to define a notion of internal regret and construct strategies that
have no such regret.
"
"  Motivation : Molecular signatures for diagnosis or prognosis estimated from
large-scale gene expression data often lack robustness and stability, rendering
their biological interpretation challenging. Increasing the signature's
interpretability and stability across perturbations of a given dataset and, if
possible, across datasets, is urgently needed to ease the discovery of
important biological processes and, eventually, new drug targets. Results : We
propose a new method to construct signatures with increased stability and
easier interpretability. The method uses a gene network as side interpretation
and enforces a large connectivity among the genes in the signature, leading to
signatures typically made of genes clustered in a few subnetworks. It combines
the recently proposed graph Lasso procedure with a stability selection
procedure. We evaluate its relevance for the estimation of a prognostic
signature in breast cancer, and highlight in particular the increase in
interpretability and stability of the signature.
"
"  Maximum likelihood estimation (MLE) and heuristic predictive estimation (HPE)
are two widely used approaches in industrial uncertainty analysis. We review
them from the point of view of decision theory, using Bayesian inference as a
gold standard for comparison. The main drawback of MLE is that it may fail to
properly account for the uncertainty on the physical process generating the
data, especially when only a small amount of data are available. HPE offers an
improvement in that it takes this uncertainty into account. However, we show
that this approach is actually equivalent to Bayes estimation for a particular
cost function that is not explicitly chosen by the decision maker. This may
produce results that are suboptimal from a decisional perspective. These
results plead for a systematic use of Bayes estimators based on carefully
defined cost functions.
"
"  In this paper, we review the problem of matrix completion and expose its
intimate relations with algebraic geometry, combinatorics and graph theory. We
present the first necessary and sufficient combinatorial conditions for
matrices of arbitrary rank to be identifiable from a set of matrix entries,
yielding theoretical constraints and new algorithms for the problem of matrix
completion. We conclude by algorithmically evaluating the tightness of the
given conditions and algorithms for practically relevant matrix sizes, showing
that the algebraic-combinatoric approach can lead to improvements over
state-of-the-art matrix completion methods.
"
"  Although anger is an important emotion that underlies much overt aggression
at great social cost, little is known about how to quantify anger or to specify
the relationship between anger and the overt behaviors that express it. This
paper proposes a novel statistical model which provides both a metric for the
intensity of anger and an approach to determining the quantitative relationship
between anger intensity and the specific behaviors that it controls. From
observed angry behaviors, we reconstruct the time course of the latent anger
intensity and the linkage between anger intensity and the probability of each
angry behavior. The data on which this analysis is based consist of observed
tantrums had by 296 children in the Madison WI area during the period
1994--1996. For each tantrum, eight angry behaviors were recorded as occurring
or not within each consecutive 30-second unit. So, the data can be
characterized as a multivariate, binary, longitudinal (MBL) dataset with a
latent variable (anger intensity) involved. Data such as these are common in
biomedical, psychological and other areas of the medical and social sciences.
Thus, the proposed modeling approach has broad applications.
"
"  The rich set of interactions between individuals in the society results in
complex community structure, capturing highly connected circles of friends,
families, or professional cliques in a social network. Thanks to frequent
changes in the activity and communication patterns of individuals, the
associated social and communication network is subject to constant evolution.
Our knowledge of the mechanisms governing the underlying community dynamics is
limited, but is essential for a deeper understanding of the development and
self-optimisation of the society as a whole. We have developed a new algorithm
based on clique percolation, that allows, for the first time, to investigate
the time dependence of overlapping communities on a large scale and as such, to
uncover basic relationships characterising community evolution. Our focus is on
networks capturing the collaboration between scientists and the calls between
mobile phone users. We find that large groups persist longer if they are
capable of dynamically altering their membership, suggesting that an ability to
change the composition results in better adaptability. The behaviour of small
groups displays the opposite tendency, the condition for stability being that
their composition remains unchanged. We also show that the knowledge of the
time commitment of the members to a given community can be used for estimating
the community's lifetime. These findings offer a new view on the fundamental
differences between the dynamics of small groups and large institutions.
"
"  We demonstrate the use of a multidimensional extension of the latent Markov
model to analyse data from studies with correlated binary responses in
developmental psychology. In particular, we consider an experiment based on a
battery of tests which was administered to pre-school children, at three time
periods, in order to measure their inhibitory control and attentional
flexibility abilities. Our model represents these abilities by two latent
traits which are associated to each state of a latent Markov chain. The
conditional distribution of the tests outcomes given the latent process depends
on these abilities through a multidimensional two-parameter logistic
parameterisation. We outline an EM algorithm to conduct likelihood inference on
the model parameters; we also focus on likelihood ratio testing of hypotheses
on the dimensionality of the model and on the transition matrices of the latent
process. Through the approach based on the proposed model, we find evidence
that supports that inhibitory control and attentional flexibility can be
conceptualised as distinct constructs. Furthermore, we outline developmental
aspects of participants' performance on these abilities based on inspection of
the estimated transition matrices.
"
"  Rate variation among the sites of a molecular sequence is commonly found in
applications of phylogenetic inference. Several approaches exist to account for
this feature but they do not usually enable the investigator to pinpoint the
sites that evolve under one or another rate of evolution in a straightforward
manner. The focus is on Bayesian phylogenetic mixture models, augmented with
allocation variables, as tools for site classification and quantification of
classification uncertainty. The method does not rely on prior knowledge of site
membership to classes or even the number of classes. Furthermore, it does not
require correlated sites to be next to one another in the sequence alignment,
unlike some phylogenetic hidden Markov or change-point models. In the approach
presented, model selection on the number and type of mixture components is
conducted ahead of both model estimation and site classification; the
steppingstone sampler (SS) is used to select amongst competing mixture models.
Example applications of simulated data and mitochondrial DNA of primates
illustrate site classification via 'augmented' Bayesian phylogenetic mixtures.
In both examples, all mixtures outperform commonly-used models of among-site
rate variation and models that do not account for rate heterogeneity. The
examples further demonstrate how site classification is readily available from
the analysis output. The method is directly relevant to the choice of
partitions in Bayesian phylogenetics, and its application may lead to the
discovery of structure not otherwise recognised in a molecular sequence
alignment. Computational aspects of Bayesian phylogenetic model estimation are
discussed, including the use of simple Markov chain Monte Carlo (MCMC) moves
that mix efficiently without tempering the chains.
"
"  Compressed Counting (CC) [22] was recently proposed for estimating the ath
frequency moments of data streams, where 0 < a <= 2. CC can be used for
estimating Shannon entropy, which can be approximated by certain functions of
the ath frequency moments as a -> 1. Monitoring Shannon entropy for anomaly
detection (e.g., DDoS attacks) in large networks is an important task. This
paper presents a new algorithm for improving CC. The improvement is most
substantial when a -> 1--. For example, when a = 0:99, the new algorithm
reduces the estimation variance roughly by 100-fold. This new algorithm would
make CC considerably more practical for estimating Shannon entropy.
Furthermore, the new algorithm is statistically optimal when a = 0.5.
"
"  A goodness-of-fit test for the fitting of a parametric model to data obtained
from a detector with finite resolution and limited acceptance is proposed. The
parameters of the model are found by minimization of a statistic that is used
for comparing experimental data and simulated reconstructed data. Numerical
examples are presented to illustrate and validate the fitting procedure.
"
"  We introduce a modular framework for market making. It combines cost-function
based automated market makers with bandit algorithms. We obtain worst-case
profits guarantee's relative to the best in hindsight within a class of natural
""overround"" cost functions . This combination allow us to have
distribution-free guarantees on the regret of profits while preserving the
bounded worst-case losses and computational tractability over combinatorial
spaces of the cost function based approach. We present simulation results to
better understand the practical behaviour of market makers from the framework.
"
"  Linear inverse problems are very common in signal and image processing. Many
algorithms that aim at solving such problems include unknown parameters that
need tuning. In this work we focus on optimally selecting such parameters in
iterative shrinkage methods for image deblurring and image zooming. Our work
uses the projected Generalized Stein Unbiased Risk Estimator (GSURE) for
determining the threshold value lambda and the iterations number K in these
algorithms. The proposed parameter selection is shown to handle any degradation
operator, including ill-posed and even rectangular ones. This is achieved by
using GSURE on the projected expected error. We further propose an efficient
greedy parameter setting scheme, that tunes the parameter while iterating
without impairing the resulting deblurring performance. Finally, we provide
extensive comparisons to conventional methods for parameter selection, showing
the superiority of the use of the projected GSURE.
"
"  We study a simple modification to the conventional time of flight mass
spectrometry (TOFMS) where a \emph{variable} and (pseudo)-\emph{random} pulsing
rate is used which allows for traces from different pulses to overlap. This
modification requires little alteration to the currently employed hardware.
However, it requires a reconstruction method to recover the spectrum from
highly aliased traces. We propose and demonstrate an efficient algorithm that
can process massive TOFMS data using computational resources that can be
considered modest with today's standards. This approach can be used to improve
duty cycle, speed, and mass resolving power of TOFMS at the same time. We
expect this to extend the applicability of TOFMS to new domains.
"
"  The estimation of parameters in the frequency spectrum of a seasonally
persistent stationary stochastic process is addressed. For seasonal persistence
associated with a pole in the spectrum located away from frequency zero, a new
Whittle-type likelihood is developed that explicitly acknowledges the location
of the pole. This Whittle likelihood is a large sample approximation to the
distribution of the periodogram over a chosen grid of frequencies, and
constitutes an approximation to the time-domain likelihood of the data, via the
linear transformation of an inverse discrete Fourier transform combined with a
demodulation. The new likelihood is straightforward to compute, and as will be
demonstrated has good, yet non-standard, properties. The asymptotic behaviour
of the proposed likelihood estimators is studied; in particular,
$N$-consistency of the estimator of the spectral pole location is established.
Large finite sample and asymptotic distributions of the score and observed
Fisher information are given, and the corresponding distributions of the
maximum likelihood estimators are deduced. A study of the small sample
properties of the likelihood approximation is provided, and its superior
performance to previously suggested methods is shown, as well as agreement with
the developed distributional approximations.
"
"  The problem of finding a reduced dimensionality representation of categorical
variables while preserving their most relevant characteristics is fundamental
for the analysis of complex data. Specifically, given a co-occurrence matrix of
two variables, one often seeks a compact representation of one variable which
preserves information about the other variable. We have recently introduced
``Sufficient Dimensionality Reduction' [GT-2003], a method that extracts
continuous reduced dimensional features whose measurements (i.e., expectation
values) capture maximal mutual information among the variables. However, such
measurements often capture information that is irrelevant for a given task.
Widely known examples are illumination conditions, which are irrelevant as
features for face recognition, writing style which is irrelevant as a feature
for content classification, and intonation which is irrelevant as a feature for
speech recognition. Such irrelevance cannot be deduced apriori, since it
depends on the details of the task, and is thus inherently ill defined in the
purely unsupervised case. Separating relevant from irrelevant features can be
achieved using additional side data that contains such irrelevant structures.
This approach was taken in [CT-2002], extending the information bottleneck
method, which uses clustering to compress the data. Here we use this
side-information framework to identify features whose measurements are
maximally informative for the original data set, but carry as little
information as possible on a side data set. In statistical terms this can be
understood as extracting statistics which are maximally sufficient for the
original dataset, while simultaneously maximally ancillary for the side
dataset. We formulate this tradeoff as a constrained optimization problem and
characterize its solutions. We then derive a gradient descent algorithm for
this problem, which is based on the Generalized Iterative Scaling method for
finding maximum entropy distributions. The method is demonstrated on synthetic
data, as well as on real face recognition datasets, and is shown to outperform
standard methods such as oriented PCA.
"
"  Inference problems in graphical models can be represented as a constrained
optimization of a free energy function. It is known that when the Bethe free
energy is used, the fixedpoints of the belief propagation (BP) algorithm
correspond to the local minima of the free energy. However BP fails to converge
in many cases of interest. Moreover, the Bethe free energy is non-convex for
graphical models with cycles thus introducing great difficulty in deriving
efficient algorithms for finding local minima of the free energy for general
graphs. In this paper we introduce two efficient BP-like algorithms, one
sequential and the other parallel, that are guaranteed to converge to the
global minimum, for any graph, over the class of energies known as ""convex free
energies"". In addition, we propose an efficient heuristic for setting the
parameters of the convex free energy based on the structure of the graph.
"
"  The constraints arising from DAG models with latent variables can be
naturally represented by means of acyclic directed mixed graphs (ADMGs). Such
graphs contain directed and bidirected arrows, and contain no directed cycles.
DAGs with latent variables imply independence constraints in the distribution
resulting from a 'fixing' operation, in which a joint distribution is divided
by a conditional. This operation generalizes marginalizing and conditioning.
Some of these constraints correspond to identifiable 'dormant' independence
constraints, with the well known 'Verma constraint' as one example. Recently,
models defined by a set of the constraints arising after fixing from a DAG with
latents, were characterized via a recursive factorization and a nested Markov
property. In addition, a parameterization was given in the discrete case. In
this paper we use this parameterization to describe a parameter fitting
algorithm, and a search and score structure learning algorithm for these nested
Markov models. We apply our algorithms to a variety of datasets.
"
"  In response to the challenges of data mining, discriminant analysis continues
to evolve as a vital branch of statistics. Our recently introduced method of
vertex discriminant analysis (VDA) is ideally suited to handle multiple
categories and an excess of predictors over training cases. The current paper
explores an elaboration of VDA that conducts classification and variable
selection simultaneously. Adding lasso ($\ell_1$-norm) and Euclidean penalties
to the VDA loss function eliminates unnecessary predictors. Lasso penalties
apply to each predictor coefficient separately; Euclidean penalties group the
collective coefficients of a single predictor. With these penalties in place,
cyclic coordinate descent accelerates estimation of all coefficients. Our tests
on simulated and benchmark real data demonstrate the virtues of penalized VDA
in model building and prediction in high-dimensional settings.
"
"  When applying the support vector machine (SVM) to high-dimensional
classification problems, we often impose a sparse structure in the SVM to
eliminate the influences of the irrelevant predictors. The lasso and other
variable selection techniques have been successfully used in the SVM to perform
automatic variable selection. In some problems, there is a natural hierarchical
structure among the variables. Thus, in order to have an interpretable SVM
classifier, it is important to respect the heredity principle when enforcing
the sparsity in the SVM. Many variable selection methods, however, do not
respect the heredity principle. In this paper we enforce both sparsity and the
heredity principle in the SVM by using the so-called structured variable
selection (SVS) framework originally proposed in Yuan, Joseph and Zou (2007).
We minimize the empirical hinge loss under a set of linear inequality
constraints and a lasso-type penalty. The solution always obeys the desired
heredity principle and enjoys sparsity. The new SVM classifier can be
efficiently fitted, because the optimization problem is a linear program.
Another contribution of this work is to present a nonparametric extension of
the SVS framework, and we propose nonparametric heredity SVMs. Simulated and
real data are used to illustrate the merits of the proposed method.
"
"  Linear models have found widespread use in statistical investigations. For
every linear model there exists a matrix representation for which the ReML
(Restricted Maximum Likelihood) can be constructed from the elements of the
corresponding matrix. This method works in the standard manner when the
covariance structure is non-singular. It can also be used in the case where the
covariance structure is singular, because the method identifies particular
non-stochastic linear combinations of the observations which must be
constrained to zero. In order to use this method, the Cholesky decomposition
has to be generalized to symmetric and indefinite matrices using complex
arithmetic methods. This method is applied to the problem of determining the
spatial size (vertex) for the Higgs Boson decay in the Higgs -> 4 lepton
channel. A comparison based on the Chi-Square variable from the vertex fit for
Higgs signal and t-tbar background is presented and shows that the background
can be greatly suppressed using the Chi-Square variable. One of the major
advantages of this novel method over the currently adopted technique of
b-tagging is that it is not affected by multiple interactions (pile up).
"
"  Class prediction is an important application of microarray gene expression
data analysis. The high-dimensionality of microarray data, where number of
genes (variables) is very large compared to the number of samples (obser-
vations), makes the application of many prediction techniques (e.g., logistic
regression, discriminant analysis) difficult. An efficient way to solve this
prob- lem is by using dimension reduction statistical techniques. Increasingly
used in psychology-related applications, Rasch model (RM) provides an appealing
framework for handling high-dimensional microarray data. In this paper, we
study the potential of RM-based modeling in dimensionality reduction with
binarized microarray gene expression data and investigate its prediction ac-
curacy in the context of class prediction using linear discriminant analysis.
Two different publicly available microarray data sets are used to illustrate a
general framework of the approach. Performance of the proposed method is
assessed by re-randomization scheme using principal component analysis (PCA) as
a benchmark method. Our results show that RM-based dimension reduction is as
effective as PCA-based dimension reduction. The method is general and can be
applied to the other high-dimensional data problems.
"
"  We generalize the well-known mixtures of Gaussians approach to density
estimation and the accompanying Expectation--Maximization technique for finding
the maximum likelihood parameters of the mixture to the case where each data
point carries an individual $d$-dimensional uncertainty covariance and has
unique missing data properties. This algorithm reconstructs the
error-deconvolved or ""underlying"" distribution function common to all samples,
even when the individual data points are samples from different distributions,
obtained by convolving the underlying distribution with the heteroskedastic
uncertainty distribution of the data point and projecting out the missing data
directions. We show how this basic algorithm can be extended with conjugate
priors on all of the model parameters and a ""split-and-merge"" procedure
designed to avoid local maxima of the likelihood. We demonstrate the full
method by applying it to the problem of inferring the three-dimensional
velocity distribution of stars near the Sun from noisy two-dimensional,
transverse velocity measurements from the Hipparcos satellite.
"
"  Although information extraction and coreference resolution appear together in
many applications, most current systems perform them as ndependent steps. This
paper describes an approach to integrated inference for extraction and
coreference based on conditionally-trained undirected graphical models. We
discuss the advantages of conditional probability training, and of a
coreference model structure based on graph partitioning. On a data set of
research paper citations, we show significant reduction in error by using
extraction uncertainty to improve coreference citation matching accuracy, and
using coreference to improve the accuracy of the extracted fields.
"
"  A very important topic in systems biology is developing statistical methods
that automatically find causal relations in gene regulatory networks with no
prior knowledge of causal connectivity. Many methods have been developed for
time series data. However, discovery methods based on steady-state data are
often necessary and preferable since obtaining time series data can be more
expensive and/or infeasible for many biological systems. A conventional
approach is causal Bayesian networks. However, estimation of Bayesian networks
is ill-posed. In many cases it cannot uniquely identify the underlying causal
network and only gives a large class of equivalent causal networks that cannot
be distinguished between based on the data distribution. We propose a new
discovery algorithm for uniquely identifying the underlying causal network of
genes. To the best of our knowledge, the proposed method is the first algorithm
for learning gene networks based on a fully identifiable causal model called
LiNGAM. We here compare our algorithm with competing algorithms using
artificially-generated data, although it is definitely better to test it based
on real microarray gene expression data.
"
"  This paper deals with the binary classification task when the target class
has the lower probability of occurrence. In such situation, it is not possible
to build a powerful classifier by using standard methods such as logistic
regression, classification tree, discriminant analysis, etc. To overcome this
short-coming of these methods which yield classifiers with low sensibility, we
tackled the classification problem here through an approach based on the
association rules learning. This approach has the advantage of allowing the
identification of the patterns that are well correlated with the target class.
Association rules learning is a well known method in the area of data-mining.
It is used when dealing with large database for unsupervised discovery of local
patterns that expresses hidden relationships between input variables. In
considering association rules from a supervised learning point of view, a
relevant set of weak classifiers is obtained from which one derives a
classifier that performs well.
"
"  Global sensitivity analysis is used to quantify the influence of uncertain
input parameters on the response variability of a numerical model. The common
quantitative methods are applicable to computer codes with scalar input
variables. This paper aims to illustrate different variance-based sensitivity
analysis techniques, based on the so-called Sobol indices, when some input
variables are functional, such as stochastic processes or random spatial
fields. In this work, we focus on large cpu time computer codes which need a
preliminary meta-modeling step before performing the sensitivity analysis. We
propose the use of the joint modeling approach, i.e., modeling simultaneously
the mean and the dispersion of the code outputs using two interlinked
Generalized Linear Models (GLM) or Generalized Additive Models (GAM). The
``mean'' model allows to estimate the sensitivity indices of each scalar input
variables, while the ``dispersion'' model allows to derive the total
sensitivity index of the functional input variables. The proposed approach is
compared to some classical SA methodologies on an analytical function. Lastly,
the proposed methodology is applied to a concrete industrial computer code that
simulates the nuclear fuel irradiation.
"
"  Active learning is a type of sequential design for supervised machine
learning, in which the learning algorithm sequentially requests the labels of
selected instances from a large pool of unlabeled data points. The objective is
to produce a classifier of relatively low risk, as measured under the 0-1 loss,
ideally using fewer label requests than the number of random labeled data
points sufficient to achieve the same. This work investigates the potential
uses of surrogate loss functions in the context of active learning.
Specifically, it presents an active learning algorithm based on an arbitrary
classification-calibrated surrogate loss function, along with an analysis of
the number of label requests sufficient for the classifier returned by the
algorithm to achieve a given risk under the 0-1 loss. Interestingly, these
results cannot be obtained by simply optimizing the surrogate risk via active
learning to an extent sufficient to provide a guarantee on the 0-1 loss, as is
common practice in the analysis of surrogate losses for passive learning. Some
of the results have additional implications for the use of surrogate losses in
passive learning.
"
"  Network dynamics may be viewed as a process of change in the edge structure
of a network, in the vertex set on which edges are defined, or in both
simultaneously. Though early studies of such processes were primarily
descriptive, recent work on this topic has increasingly turned to formal
statistical models. While showing great promise, many of these modern dynamic
models are computationally intensive and scale very poorly in the size of the
network under study and/or the number of time points considered. Likewise,
currently employed models focus on edge dynamics, with little support for
endogenously changing vertex sets. Here, we show how an existing approach based
on logistic network regression can be extended to serve as highly scalable
framework for modeling large networks with dynamic vertex sets. We place this
approach within a general dynamic exponential family (ERGM) context, clarifying
the assumptions underlying the framework (and providing a clear path for
extensions), and show how model assessment methods for cross-sectional networks
can be extended to the dynamic case. Finally, we illustrate this approach on a
classic data set involving interactions among windsurfers on a California
beach.
"
"  Sensory stimuli are usually composed of different features (the what)
appearing at irregular times (the when). Neural responses often use spike
patterns to represent sensory information. The what is hypothesized to be
encoded in the identity of the elicited patterns (the pattern categories), and
the when, in the time positions of patterns (the pattern timing). However, this
standard view is oversimplified. In the real world, the what and the when might
not be separable concepts, for instance, if they are correlated in the
stimulus. In addition, neuronal dynamics can condition the pattern timing to be
correlated with the pattern categories. Hence, timing and categories of
patterns may not constitute independent channels of information. In this paper,
we assess the role of spike patterns in the neural code, irrespective of the
nature of the patterns. We first define information-theoretical quantities that
allow us to quantify the information encoded by different aspects of the neural
response. We also introduce the notion of synergy/redundancy between time
positions and categories of patterns. We subsequently establish the relation
between the what and the when in the stimulus with the timing and the
categories of patterns. To that aim, we quantify the mutual information between
different aspects of the stimulus and different aspects of the response. This
formal framework allows us to determine the precise conditions under which the
standard view holds, as well as the departures from this simple case. Finally,
we study the capability of different response aspects to represent the what and
the when in the neural response.
"
"  Image data are increasingly encountered and are of growing importance in many
areas of science. Much of these data are quantitative image data, which are
characterized by intensities that represent some measurement of interest in the
scanned images. The data typically consist of multiple images on the same
domain and the goal of the research is to combine the quantitative information
across images to make inference about populations or interventions. In this
paper we present a unified analysis framework for the analysis of quantitative
image data using a Bayesian functional mixed model approach. This framework is
flexible enough to handle complex, irregular images with many local features,
and can model the simultaneous effects of multiple factors on the image
intensities and account for the correlation between images induced by the
design. We introduce a general isomorphic modeling approach to fitting the
functional mixed model, of which the wavelet-based functional mixed model is
one special case. With suitable modeling choices, this approach leads to
efficient calculations and can result in flexible modeling and adaptive
smoothing of the salient features in the data. The proposed method has the
following advantages: it can be run automatically, it produces inferential
plots indicating which regions of the image are associated with each factor, it
simultaneously considers the practical and statistical significance of
findings, and it controls the false discovery rate.
"
"  The problem of Hybrid Linear Modeling (HLM) is to model and segment data
using a mixture of affine subspaces. Different strategies have been proposed to
solve this problem, however, rigorous analysis justifying their performance is
missing. This paper suggests the Theoretical Spectral Curvature Clustering
(TSCC) algorithm for solving the HLM problem, and provides careful analysis to
justify it. The TSCC algorithm is practically a combination of Govindu's
multi-way spectral clustering framework (CVPR 2005) and Ng et al.'s spectral
clustering algorithm (NIPS 2001). The main result of this paper states that if
the given data is sampled from a mixture of distributions concentrated around
affine subspaces, then with high sampling probability the TSCC algorithm
segments well the different underlying clusters. The goodness of clustering
depends on the within-cluster errors, the between-clusters interaction, and a
tuning parameter applied by TSCC. The proof also provides new insights for the
analysis of Ng et al. (NIPS 2001).
"
"  We find the minimax rate of convergence in Hausdorff distance for estimating
a manifold M of dimension d embedded in R^D given a noisy sample from the
manifold. We assume that the manifold satisfies a smoothness condition and that
the noise distribution has compact support. We show that the optimal rate of
convergence is n^{-2/(2+d)}. Thus, the minimax rate depends only on the
dimension of the manifold, not on the dimension of the space in which M is
embedded.
"
"  We describe theoretical bounds and a practical algorithm for teaching a model
by demonstration in a sequential decision making environment. Unlike previous
efforts that have optimized learners that watch a teacher demonstrate a static
policy, we focus on the teacher as a decision maker who can dynamically choose
different policies to teach different parts of the environment. We develop
several teaching frameworks based on previously defined supervised protocols,
such as Teaching Dimension, extending them to handle noise and sequences of
inputs encountered in an MDP.We provide theoretical bounds on the learnability
of several important model classes in this setting and suggest a practical
algorithm for dynamic teaching.
"
"  We consider the problem of function estimation in the case where the data
distribution may shift between training and test time, and additional
information about it may be available at test time. This relates to popular
scenarios such as covariate shift, concept drift, transfer learning and
semi-supervised learning. This working paper discusses how these tasks could be
tackled depending on the kind of changes of the distributions. It argues that
knowledge of an underlying causal direction can facilitate several of these
tasks.
"
"  Classical learning assumes the learner is given a labeled data sample, from
which it learns a model. The field of Active Learning deals with the situation
where the learner begins not with a training sample, but instead with resources
that it can use to obtain information to help identify the optimal model. To
better understand this task, this paper presents and analyses the simplified
""(budgeted) active model selection"" version, which captures the pure
exploration aspect of many active learning problems in a clean and simple
problem formulation. Here the learner can use a fixed budget of ""model probes""
(where each probe evaluates the specified model on a random indistinguishable
instance) to identify which of a given set of possible models has the highest
expected accuracy. Our goal is a policy that sequentially determines which
model to probe next, based on the information observed so far. We present a
formal description of this task, and show that it is NPhard in general. We then
investigate a number of algorithms for this task, including several existing
ones (eg, ""Round-Robin"", ""Interval Estimation"", ""Gittins"") as well as some
novel ones (e.g., ""Biased-Robin""), describing first their approximation
properties and then their empirical performance on various problem instances.
We observe empirically that the simple biased-robin algorithm significantly
outperforms the other algorithms in the case of identical costs and priors.
"
"  We investigate delay effects on dominant transition pathways (DTP) between
metastable states of stochastic systems. A modified version of the Maier-Stein
model with linear delayed feedback is considered as an example. By a stability
analysis of the {""on-axis""} DTP in trajectory space, we find that a bifurcation
of DTPs will be induced when time delay $\tau$ is large enough. This finding is
soon verified by numerically derived DTPs which are calculated by employing a
recently developed minimum action method extended to delayed stochastic
systems. Further simulation shows that, the delay-induced bifurcation of DTPs
also results in a nontrivial dependence of the transition rate constant on the
delay time. Finally, the bifurcation diagram is given on the $\tau-\beta$
plane, where $\beta$ measures the non-conservation of the original Maier-Stein
model.
"
"  We present both offline and online maximum likelihood estimation (MLE)
techniques for inferring the static parameters of a multiple target tracking
(MTT) model with linear Gaussian dynamics. We present the batch and online
versions of the expectation-maximisation (EM) algorithm for short and long data
sets respectively, and we show how Monte Carlo approximations of these methods
can be implemented. Performance is assessed in numerical examples using
simulated data for various scenarios and a comparison with a Bayesian
estimation procedure is also provided.
"
"  Topic models have proven to be a useful tool for discovering latent
structures in document collections. However, most document collections often
come as temporal streams and thus several aspects of the latent structure such
as the number of topics, the topics' distribution and popularity are
time-evolving. Several models exist that model the evolution of some but not
all of the above aspects. In this paper we introduce infinite dynamic topic
models, iDTM, that can accommodate the evolution of all the aforementioned
aspects. Our model assumes that documents are organized into epochs, where the
documents within each epoch are exchangeable but the order between the
documents is maintained across epochs. iDTM allows for unbounded number of
topics: topics can die or be born at any epoch, and the representation of each
topic can evolve according to a Markovian dynamics. We use iDTM to analyze the
birth and evolution of topics in the NIPS community and evaluated the efficacy
of our model on both simulated and real datasets with favorable outcome.
"
"  In this article, we derive a new generalization of Chebyshev inequality for
random vectors. We demonstrate that the new generalization is much less
conservative than the classical generalization.
"
"  In many data mining applications collection of sufficiently large datasets is
the most time consuming and expensive. On the other hand, industrial methods of
data collection create huge databases, and make difficult direct applications
of the advanced machine learning algorithms. To address the above problems, we
consider active learning (AL), which may be very efficient either for the
experimental design or for the data filtering. In this paper we demonstrate
using the online evaluation opportunity provided by the AL Challenge that quite
competitive results may be produced using a small percentage of the available
data. Also, we present several alternative criteria, which may be useful for
the evaluation of the active learning processes. The author of this paper
attended special presentation in Barcelona, where results of the WCCI 2010 AL
Challenge were discussed.
"
"  What makes a problem suitable for statistical analysis? Are historical and
religious questions addressable using statistical calculations? Such issues
have long been debated in the statistical community and statisticians and
others have used historical information and texts to analyze such questions as
the economics of slavery, the authorship of the Federalist Papers and the
question of the existence of God. But what about historical and religious
attributions associated with information gathered from archeological finds? In
1980, a construction crew working in the Jerusalem neighborhood of East Talpiot
stumbled upon a crypt. Archaeologists from the Israel Antiquities Authority
came to the scene and found 10 limestone burial boxes, known as ossuaries, in
the crypt. Six of these had inscriptions. The remains found in the ossuaries
were reburied, as required by Jewish religious tradition, and the ossuaries
were catalogued and stored in a warehouse. The inscriptions on the ossuaries
were catalogued and published by Rahmani (1994) and by Kloner (1996) but there
reports did not receive widespread public attention. Fast forward to March
2007, when a television ``docudrama'' aired on The Discovery Channel entitled
``The Lost Tomb of Jesus'' touched off a public and religious controversy--one
only need think about the title to see why there might be a controversy! The
program, and a simultaneously published book [Jacobovici and Pellegrino
(2007)], described the ``rediscovery'' of the East Talpiot archeological find
and they presented interpretations of the ossuary inscriptions from a number of
perspectives. Among these was a statistical calculation attributed to the
statistician Andrey Feuerverger: ``that the odds that all six names would
appear together in one tomb are 1 in 600, calculated conservatively--or
possibly even as much as one in one million.''
"
"  Multi-task learning models using Gaussian processes (GP) have been developed
and successfully applied in various applications. The main difficulty with this
approach is the computational cost of inference using the union of examples
from all tasks. Therefore sparse solutions, that avoid using the entire data
directly and instead use a set of informative ""representatives"" are desirable.
The paper investigates this problem for the grouped mixed-effect GP model where
each individual response is given by a fixed-effect, taken from one of a set of
unknown groups, plus a random individual effect function that captures
variations among individuals. Such models have been widely used in previous
work but no sparse solutions have been developed. The paper presents the first
sparse solution for such problems, showing how the sparse approximation can be
obtained by maximizing a variational lower bound on the marginal likelihood,
generalizing ideas from single-task Gaussian processes to handle the
mixed-effect model as well as grouping. Experiments using artificial and real
data validate the approach showing that it can recover the performance of
inference with the full sample, that it outperforms baseline methods, and that
it outperforms state of the art sparse solutions for other multi-task GP
formulations.
"
"  Modern, powerful techniques for the residual analysis of spatial-temporal
point process models are reviewed and compared. These methods are applied to
California earthquake forecast models used in the Collaboratory for the Study
of Earthquake Predictability (CSEP). Assessments of these earthquake
forecasting models have previously been performed using simple, low-power means
such as the L-test and N-test. We instead propose residual methods based on
rescaling, thinning, superposition, weighted K-functions and deviance
residuals. Rescaled residuals can be useful for assessing the overall fit of a
model, but as with thinning and superposition, rescaling is generally
impractical when the conditional intensity $\lambda$ is volatile. While
residual thinning and superposition may be useful for identifying spatial
locations where a model fits poorly, these methods have limited power when the
modeled conditional intensity assumes extremely low or high values somewhere in
the observation region, and this is commonly the case for earthquake
forecasting models. A recently proposed hybrid method of thinning and
superposition, called super-thinning, is a more powerful alternative.
"
"  Statistical learning theory provides the theoretical basis for many of
today's machine learning algorithms. In this article we attempt to give a
gentle, non-technical overview over the key ideas and insights of statistical
learning theory. We target at a broad audience, not necessarily machine
learning researchers. This paper can serve as a starting point for people who
want to get an overview on the field before diving into technical details.
"
"  Chandrasekaran, Parrilo and Willsky (2010) proposed a convex optimization
problem to characterize graphical model selection in the presence of unobserved
variables. This convex optimization problem aims to estimate an inverse
covariance matrix that can be decomposed into a sparse matrix minus a low-rank
matrix from sample data. Solving this convex optimization problem is very
challenging, especially for large problems. In this paper, we propose two
alternating direction methods for solving this problem. The first method is to
apply the classical alternating direction method of multipliers to solve the
problem as a consensus problem. The second method is a proximal gradient based
alternating direction method of multipliers. Our methods exploit and take
advantage of the special structure of the problem and thus can solve large
problems very efficiently. Global convergence result is established for the
proposed methods. Numerical results on both synthetic data and gene expression
data show that our methods usually solve problems with one million variables in
one to two minutes, and are usually five to thirty five times faster than a
state-of-the-art Newton-CG proximal point algorithm.
"
"  We test against two different sets of data an apparently new approach to the
analysis of the variance of a numerical variable which depends on qualitative
characters. We suggest that this approach be used to complement other existing
techniques to study the interdependence of the variables involved. According to
our method the variance is expressed as a sum of orthogonal components,
obtained as differences of conditional means, with respect to the qualitative
characters. The resulting expression for the variance depends on the ordering
in which the characters are considered. We suggest an algorithm which leads to
an ordering which is deemed natural. The first set of data concerns the score
achieved by a population of students, on an entrance examination, based on a
multiple choice test with 30 questions. In this case the qualitative characters
are dyadic and correspond to correct or incorrect answer to each question. The
second set of data concerns the delay in obtaining the degree for a population
of graduates of Italian universities. The variance in this case is analyzed
with respect to a set of seven specific qualitative characters of the
population studied (gender, previous education, working condition, parent's
educational level, field of study, etc.)
"
"  We consider the problem of sensor selection for event detection in wireless
sensor networks (WSNs). We want to choose a subset of p out of n sensors that
yields the best detection performance. As the sensor selection optimality
criteria, we propose the Kullback-Leibler and Chernoff distances between the
distributions of the selected measurements under the two hypothesis. We
formulate the maxmin robust sensor selection problem to cope with the
uncertainties in distribution means. We prove that the sensor selection problem
is NP hard, for both Kullback-Leibler and Chernoff criteria. To (sub)optimally
solve the sensor selection problem, we propose an algorithm of affordable
complexity. Extensive numerical simulations on moderate size problem instances
(when the optimum by exhaustive search is feasible to compute) demonstrate the
algorithm's near optimality in a very large portion of problem instances. For
larger problems, extensive simulations demonstrate that our algorithm
outperforms random searches, once an upper bound on computational time is set.
We corroborate numerically the validity of the Kullback-Leibler and Chernoff
sensor selection criteria, by showing that they lead to sensor selections
nearly optimal both in the Neyman-Pearson and Bayes sense.
"
"  Covariance matrix estimates are an essential part of many signal processing
algorithms, and are often used to determine a low-dimensional principal
subspace via their spectral decomposition. However, exact eigenanalysis is
computationally intractable for sufficiently high-dimensional matrices, and in
the case of small sample sizes, sample eigenvalues and eigenvectors are known
to be poor estimators of their population counterparts. To address these
issues, we propose a covariance estimator that is computationally efficient
while also performing shrinkage on the sample eigenvalues. Our approach is
based on the Nystr\""{o}m method, which uses a data-dependent orthogonal
projection to obtain a fast low-rank approximation of a large positive
semidefinite matrix. We provide a theoretical analysis of the error properties
of our estimator as well as empirical results, including examples of its
application to adaptive beamforming and image denoising.
"
"  In this paper, a similarity-driven cluster merging method is proposed for
unsuper-vised fuzzy clustering. The cluster merging method is used to resolve
the problem of cluster validation. Starting with an overspecified number of
clusters in the data, pairs of similar clusters are merged based on the
proposed similarity-driven cluster merging criterion. The similarity between
clusters is calculated by a fuzzy cluster similarity matrix, while an adaptive
threshold is used for merging. In addition, a modified generalized ob- jective
function is used for prototype-based fuzzy clustering. The function includes
the p-norm distance measure as well as principal components of the clusters.
The number of the principal components is determined automatically from the
data being clustered. The properties of this unsupervised fuzzy clustering
algorithm are illustrated by several experiments.
"
"  In data sets with many more features than observations, independent screening
based on all univariate regression models leads to a computationally convenient
variable selection method. Recent efforts have shown that in the case of
generalized linear models, independent screening may suffice to capture all
relevant features with high probability, even in ultra-high dimension. It is
unclear whether this formal sure screening property is attainable when the
response is a right-censored survival time. We propose a computationally very
efficient independent screening method for survival data which can be viewed as
the natural survival equivalent of correlation screening. We state conditions
under which the method admits the sure screening property within a general
class of single-index hazard rate models with ultra-high dimensional features.
An iterative variant is also described which combines screening with penalized
regression in order to handle more complex feature covariance structures. The
methods are evaluated through simulation studies and through application to a
real gene expression dataset.
"
"  Consider a linear regression model where the design matrix X has n rows and p
columns. We assume (a) p is much large than n, (b) the coefficient vector beta
is sparse in the sense that only a small fraction of its coordinates is
nonzero, and (c) the Gram matrix G = X'X is sparse in the sense that each row
has relatively few large coordinates (diagonals of G are normalized to 1).
  The sparsity in G naturally induces the sparsity of the so-called graph of
strong dependence (GOSD). We find an interesting interplay between the signal
sparsity and the graph sparsity, which ensures that in a broad context, the set
of true signals decompose into many different small-size components of GOSD,
where different components are disconnected.
  We propose Graphlet Screening (GS) as a new approach to variable selection,
which is a two-stage Screen and Clean method. The key methodological innovation
of GS is to use GOSD to guide both the screening and cleaning. Compared to
m-variate brute-forth screening that has a computational cost of p^m, the GS
only has a computational cost of p (up to some multi-log(p) factors) in
screening.
  We measure the performance of any variable selection procedure by the minimax
Hamming distance. We show that in a very broad class of situations, GS achieves
the optimal rate of convergence in terms of the Hamming distance. Somewhat
surprisingly, the well-known procedures subset selection and the lasso are rate
non-optimal, even in very simple settings and even when their tuning parameters
are ideally set.
"
"  Predictions of the uncertainty associated with extreme events are a vital
component of any prediction system for such events. Consequently, the
prediction system ought to be probabilistic in nature, with the predictions
taking the form of probability distributions. This paper concerns probabilistic
prediction systems where the data is assumed to follow either a generalized
extreme value distribution (GEV) or a generalized Pareto distribution (GPD). In
this setting, the properties of proper scoring rules which facilitate the
assessment of the prediction uncertainty are investigated and closed-from
expressions for the continuous ranked probability score (CRPS) are provided. In
an application to peak wind prediction, the predictive performance of a GEV
model under maximum likelihood estimation, optimum score estimation with the
CRPS, and a Bayesian framework are compared. The Bayesian inference yields the
highest overall prediction skill and is shown to be a valuable tool for
covariate selection, while the predictions obtained under optimum CRPS
estimation are the sharpest and give the best performance for high thresholds
and quantiles.
"
"  The greatest root distribution occurs everywhere in classical multivariate
analysis, but even under the null hypothesis the exact distribution has
required extensive tables or special purpose software. We describe a simple
approximation, based on the Tracy--Widom distribution, that in many cases can
be used instead of tables or software, at least for initial screening. The
quality of approximation is studied, and its use illustrated in a variety of
setttings.
"
"  We introduce 'mixed LICORS', an algorithm for learning nonlinear,
high-dimensional dynamics from spatio-temporal data, suitable for both
prediction and simulation. Mixed LICORS extends the recent LICORS algorithm
(Goerg and Shalizi, 2012) from hard clustering of predictive distributions to a
non-parametric, EM-like soft clustering. This retains the asymptotic predictive
optimality of LICORS, but, as we show in simulations, greatly improves
out-of-sample forecasts with limited data. The new method is implemented in the
publicly-available R package ""LICORS""
(http://cran.r-project.org/web/packages/LICORS/).
"
"  Probabilistic models for infectious disease dynamics are useful for
understanding the mechanism underlying the spread of infection. When the
likelihood function for these models is expensive to evaluate, traditional
likelihood-based inference may be computationally intractable. Furthermore,
traditional inference may lead to poor parameter estimates and the fitted model
may not capture important biological characteristics of the observed data. We
propose a novel approach for resolving these issues that is inspired by recent
work in emulation and calibration for complex computer models. Our motivating
example is the gravity time series susceptible-infected-recovered (TSIR) model.
Our approach focuses on the characteristics of the process that are of
scientific interest. We find a Gaussian process approximation to the gravity
model using key summary statistics obtained from model simulations. We
demonstrate via simulated examples that the new approach is computationally
expedient, provides accurate parameter inference, and results in a good model
fit. We apply our method to analyze measles outbreaks in England and Wales in
two periods, the pre-vaccination period from 1944-1965 and the vaccination
period from 1966-1994. Based on our results, we are able to obtain important
scientific insights about the transmission of measles. In general, our method
is applicable to problems where traditional likelihood-based inference is
computationally intractable or produces a poor model fit. It is also an
alternative to approximate Bayesian computation (ABC) when simulations from the
model are expensive.
"
"  We develop and demonstrate a probabilistic method for classifying rare
objects in surveys with the particular goal of building very pure samples. It
works by modifying the output probabilities from a classifier so as to
accommodate our expectation (priors) concerning the relative frequencies of
different classes of objects. We demonstrate our method using the Discrete
Source Classifier, a supervised classifier currently based on Support Vector
Machines, which we are developing in preparation for the Gaia data analysis.
DSC classifies objects using their very low resolution optical spectra. We look
in detail at the problem of quasar classification, because identification of a
pure quasar sample is necessary to define the Gaia astrometric reference frame.
By varying a posterior probability threshold in DSC we can trade off sample
completeness and contamination. We show, using our simulated data, that it is
possible to achieve a pure sample of quasars (upper limit on contamination of 1
in 40,000) with a completeness of 65% at magnitudes of G=18.5, and 50% at
G=20.0, even when quasars have a frequency of only 1 in every 2000 objects. The
star sample completeness is simultaneously 99% with a contamination of 0.7%.
Including parallax and proper motion in the classifier barely changes the
results. We further show that not accounting for class priors in the target
population leads to serious misclassifications and poor predictions for sample
completeness and contamination. (Truncated)
"
"  We address the problem of learning in an online setting where the learner
repeatedly observes features, selects among a set of actions, and receives
reward for the action taken. We provide the first efficient algorithm with an
optimal regret. Our algorithm uses a cost sensitive classification learner as
an oracle and has a running time $\mathrm{polylog}(N)$, where $N$ is the number
of classification rules among which the oracle might choose. This is
exponentially faster than all previous algorithms that achieve optimal regret
in this setting. Our formulation also enables us to create an algorithm with
regret that is additive rather than multiplicative in feedback delay as in all
previous work.
"
"  This paper considers inference for conditional moment inequality models using
a multiscale statistic. We derive the asymptotic distribution of this test
statistic and use the result to propose feasible critical values that have a
simple analytic formula, and to prove the asymptotic validity of a modified
bootstrap procedure. The asymptotic distribution is extreme value, and the
proof uses new techniques to overcome several technical obstacles. The test
detects local alternatives that approach the identified set at the best rate
among available tests in a broad class of models, and is adaptive to the
smoothness properties of the data generating process. Our results also have
implications for the use of moment selection procedures in this setting. We
provide a monte carlo study and an empirical illustration to inference in a
regression model with endogenously censored and missing data.
"
"  Recently, Mahoney and Orecchia demonstrated that popular diffusion-based
procedures to compute a quick \emph{approximation} to the first nontrivial
eigenvector of a data graph Laplacian \emph{exactly} solve certain regularized
Semi-Definite Programs (SDPs). In this paper, we extend that result by
providing a statistical interpretation of their approximation procedure. Our
interpretation will be analogous to the manner in which $\ell_2$-regularized or
$\ell_1$-regularized $\ell_2$-regression (often called Ridge regression and
Lasso regression, respectively) can be interpreted in terms of a Gaussian prior
or a Laplace prior, respectively, on the coefficient vector of the regression
problem. Our framework will imply that the solutions to the Mahoney-Orecchia
regularized SDP can be interpreted as regularized estimates of the
pseudoinverse of the graph Laplacian. Conversely, it will imply that the
solution to this regularized estimation problem can be computed very quickly by
running, e.g., the fast diffusion-based PageRank procedure for computing an
approximation to the first nontrivial eigenvector of the graph Laplacian.
Empirical results are also provided to illustrate the manner in which
approximate eigenvector computation \emph{implicitly} performs statistical
regularization, relative to running the corresponding exact algorithm.
"
"  We formulate weighted graph clustering as a prediction problem: given a
subset of edge weights we analyze the ability of graph clustering to predict
the remaining edge weights. This formulation enables practical and theoretical
comparison of different approaches to graph clustering as well as comparison of
graph clustering with other possible ways to model the graph. We adapt the
PAC-Bayesian analysis of co-clustering (Seldin and Tishby, 2008; Seldin, 2009)
to derive a PAC-Bayesian generalization bound for graph clustering. The bound
shows that graph clustering should optimize a trade-off between empirical data
fit and the mutual information that clusters preserve on the graph nodes. A
similar trade-off derived from information-theoretic considerations was already
shown to produce state-of-the-art results in practice (Slonim et al., 2005;
Yom-Tov and Slonim, 2009). This paper supports the empirical evidence by
providing a better theoretical foundation, suggesting formal generalization
guarantees, and offering a more accurate way to deal with finite sample issues.
We derive a bound minimization algorithm and show that it provides good results
in real-life problems and that the derived PAC-Bayesian bound is reasonably
tight.
"
"  Monotonicity is a key qualitative prediction of a wide array of economic
models derived via robust comparative statics. It is therefore important to
design effective and practical econometric methods for testing this prediction
in empirical analysis. This paper develops a general nonparametric framework
for testing monotonicity of a regression function. Using this framework, a
broad class of new tests is introduced, which gives an empirical researcher a
lot of flexibility to incorporate ex ante information she might have. The paper
also develops new methods for simulating critical values, which are based on
the combination of a bootstrap procedure and new selection algorithms. These
methods yield tests that have correct asymptotic size and are asymptotically
nonconservative. It is also shown how to obtain an adaptive rate optimal test
that has the best attainable rate of uniform consistency against models whose
regression function has Lipschitz-continuous first-order derivatives and that
automatically adapts to the unknown smoothness of the regression function.
Simulations show that the power of the new tests in many cases significantly
exceeds that of some prior tests, e.g. that of Ghosal, Sen, and Van der Vaart
(2000). An application of the developed procedures to the dataset of Ellison
and Ellison (2011) shows that there is some evidence of strategic entry
deterrence in pharmaceutical industry where incumbents may use strategic
investment to prevent generic entries when their patents expire.
"
"  Inference problems in graphical models are often approximated by casting them
as constrained optimization problems. Message passing algorithms, such as
belief propagation, have previously been suggested as methods for solving these
optimization problems. However, there are few convergence guarantees for such
algorithms, and the algorithms are therefore not guaranteed to solve the
corresponding optimization problem. Here we present an oriented tree
decomposition algorithm that is guaranteed to converge to the global optimum of
the Tree-Reweighted (TRW) variational problem. Our algorithm performs local
updates in the convex dual of the TRW problem - an unconstrained generalized
geometric program. Primal updates, also local, correspond to oriented
reparametrization operations that leave the distribution intact.
"
"  Inference and learning of graphical models are both well-studied problems in
statistics and machine learning that have found many applications in science
and engineering. However, exact inference is intractable in general graphical
models, which suggests the problem of seeking the best approximation to a
collection of random variables within some tractable family of graphical
models. In this paper, we focus our attention on the class of planar Ising
models, for which inference is tractable using techniques of statistical
physics [Kac and Ward; Kasteleyn]. Based on these techniques and recent methods
for planarity testing and planar embedding [Chrobak and Payne], we propose a
simple greedy algorithm for learning the best planar Ising model to approximate
an arbitrary collection of binary random variables (possibly from sample data).
Given the set of all pairwise correlations among variables, we select a planar
graph and optimal planar Ising model defined on this graph to best approximate
that set of correlations. We demonstrate our method in some simulations and for
the application of modeling senate voting records.
"
"  We propose a penalized orthogonal-components regression (POCRE) for large p
small n data. Orthogonal components are sequentially constructed to maximize,
upon standardization, their correlation to the response residuals. A new
penalization framework, implemented via empirical Bayes thresholding, is
presented to effectively identify sparse predictors of each component. POCRE is
computationally efficient owing to its sequential construction of leading
sparse principal components. In addition, such construction offers other
properties such as grouping highly correlated predictors and allowing for
collinear or nearly collinear predictors. With multivariate responses, POCRE
can construct common components and thus build up latent-variable models for
large p small n data.
"
"  There has been a lot of work fitting Ising models to multivariate binary data
in order to understand the conditional dependency relationships between the
variables. However, additional covariates are frequently recorded together with
the binary data, and may influence the dependence relationships. Motivated by
such a dataset on genomic instability collected from tumor samples of several
types, we propose a sparse covariate dependent Ising model to study both the
conditional dependency within the binary data and its relationship with the
additional covariates. This results in subject-specific Ising models, where the
subject's covariates influence the strength of association between the genes.
As in all exploratory data analysis, interpretability of results is important,
and we use L1 penalties to induce sparsity in the fitted graphs and in the
number of selected covariates. Two algorithms to fit the model are proposed and
compared on a set of simulated data, and asymptotic results are established.
The results on the tumor dataset and their biological significance are
discussed in detail.
"
"  For binary classification we establish learning rates up to the order of
$n^{-1}$ for support vector machines (SVMs) with hinge loss and Gaussian RBF
kernels. These rates are in terms of two assumptions on the considered
distributions: Tsybakov's noise assumption to establish a small estimation
error, and a new geometric noise condition which is used to bound the
approximation error. Unlike previously proposed concepts for bounding the
approximation error, the geometric noise assumption does not employ any
smoothness assumption.
"
"  Gaussian graphical models are of great interest in statistical learning.
Because the conditional independencies between different nodes correspond to
zero entries in the inverse covariance matrix of the Gaussian distribution, one
can learn the structure of the graph by estimating a sparse inverse covariance
matrix from sample data, by solving a convex maximum likelihood problem with an
$\ell_1$-regularization term. In this paper, we propose a first-order method
based on an alternating linearization technique that exploits the problem's
special structure; in particular, the subproblems solved in each iteration have
closed-form solutions. Moreover, our algorithm obtains an $\epsilon$-optimal
solution in $O(1/\epsilon)$ iterations. Numerical experiments on both synthetic
and real data from gene association networks show that a practical version of
this algorithm outperforms other competitive algorithms.
"
"  We are working to develop automated intelligent agents, which can act and
react as learning machines with minimal human intervention. To accomplish this,
an intelligent agent is viewed as a question-asking machine, which is designed
by coupling the processes of inference and inquiry to form a model-based
learning unit. In order to select maximally-informative queries, the
intelligent agent needs to be able to compute the relevance of a question. This
is accomplished by employing the inquiry calculus, which is dual to the
probability calculus, and extends information theory by explicitly requiring
context. Here, we consider the interaction between two question-asking
intelligent agents, and note that there is a potential information redundancy
with respect to the two questions that the agents may choose to pose. We show
that the information redundancy is minimized by maximizing the joint entropy of
the questions, which simultaneously maximizes the relevance of each question
while minimizing the mutual information between them. Maximum joint entropy is
therefore an important principle of information-based collaboration, which
enables intelligent agents to efficiently learn together.
"
"  We study sparse approximate solutions to convex optimization problems. It is
known that in many engineering applications researchers are interested in an
approximate solution of an optimization problem as a linear combination of
elements from a given system of elements. There is an increasing interest in
building such sparse approximate solutions using different greedy-type
algorithms. The problem of approximation of a given element of a Banach space
by linear combinations of elements from a given system (dictionary) is well
studied in nonlinear approximation theory. At a first glance the settings of
approximation and optimization problems are very different. In the
approximation problem an element is given and our task is to find a sparse
approximation of it. In optimization theory an energy function is given and we
should find an approximate sparse solution to the minimization problem. It
turns out that the same technique can be used for solving both problems. We
show how the technique developed in nonlinear approximation theory, in
particular, the greedy approximation technique can be adjusted for finding a
sparse solution of an optimization problem.
"
"  A conditional independence graph is a concise representation of pairwise
conditional independence among many variables. Graphical Random Forests (GRaFo)
are a novel method for estimating pairwise conditional independence
relationships among mixed-type, i.e. continuous and discrete, variables. The
number of edges is a tuning parameter in any graphical model estimator and
there is no obvious number that constitutes a good choice. Stability Selection
helps choosing this parameter with respect to a bound on the expected number of
false positives (error control).
  The performance of GRaFo is evaluated and compared with various other methods
for p = 50, 100, and 200 possibly mixed-type variables while sample size is n =
100 (n = 500 for maximum likelihood). Furthermore, GRaFo is applied to data
from the Swiss Health Survey in order to evaluate how well it can reproduce the
interconnection of functional health components, personal, and environmental
factors, as hypothesized by the World Health Organization's International
Classification of Functioning, Disability and Health (ICF). Finally, GRaFo is
used to identify risk factors which may be associated with adverse
neurodevelopment of children who suffer from trisomy 21 and experienced
open-heart surgery.
  GRaFo performs well with mixed data and thanks to Stability Selection it
provides an error control mechanism for false positive selection.
"
"  A martingale framework for concept change detection based on testing data
exchangeability was recently proposed (Ho, 2005). In this paper, we describe
the proposed change-detection test based on the Doob's Maximal Inequality and
show that it is an approximation of the sequential probability ratio test
(SPRT). The relationship between the threshold value used in the proposed test
and its size and power is deduced from the approximation. The mean delay time
before a change is detected is estimated using the average sample number of a
SPRT. The performance of the test using various threshold values is examined on
five different data stream scenarios simulated using two synthetic data sets.
Finally, experimental results show that the test is effective in detecting
changes in time-varying data streams simulated using three benchmark data sets.
"
"  Researchers have studied the first passage time of financial time series and
observed that the smallest time interval needed for a stock index to move a
given distance is typically shorter for negative than for positive price
movements. The same is not observed for the index constituents, the individual
stocks. We use the discrete wavelet transform to illustrate that this is a long
rather than short time scale phenomenon -- if enough low frequency content of
the price process is removed, the asymmetry disappears. We also propose a new
model, which explain the asymmetry by prolonged, correlated down movements of
individual stocks.
"
"  Quantitative Magnetic Resonance Imaging (qMRI) provides researchers insight
into pathological and physiological alterations of living tissue, with the help
of which researchers hope to predict (local) therapeutic efficacy early and
determine optimal treatment schedule. However, the analysis of qMRI has been
limited to ad-hoc heuristic methods. Our research provides a powerful
statistical framework for image analysis and sheds light on future localized
adaptive treatment regimes tailored to the individual's response. We assume in
an imperfect world we only observe a blurred and noisy version of the
underlying pathological/physiological changes via qMRI, due to measurement
errors or unpredictable influences. We use a hidden Markov random field to
model the spatial dependence in the data and develop a maximum likelihood
approach via the Expectation--Maximization algorithm with stochastic variation.
An important improvement over previous work is the assessment of variability in
parameter estimation, which is the valid basis for statistical inference. More
importantly, we focus on the expected changes rather than image segmentation.
Our research has shown that the approach is powerful in both simulation studies
and on a real dataset, while quite robust in the presence of some model
assumption violations.
"
"  In this paper, we propose a novel approach to modeling nonstationary spatial
fields. The proposed method works by expanding the geographic plane over which
these processes evolve into higher dimensional spaces, transforming and
clarifying complex patterns in the physical plane. By combining aspects of
multi-dimensional scaling, group lasso, and latent variables models, a
dimensionally sparse projection is found in which the originally nonstationary
field exhibits stationarity. Following a comparison with existing methods in a
simulated environment, dimension expansion is studied on a classic test-bed
data set historically used to study nonstationary models. Following this, we
explore the use of dimension expansion in modeling air pollution in the United
Kingdom, a process known to be strongly influenced by rural/urban effects,
amongst others, which gives rise to a nonstationary field.
"
"  Missing and incomplete information in surveys or databases can be imputed
using different statistical and soft-computing techniques. This paper
comprehensively compares auto-associative neural networks (NN), neuro-fuzzy
(NF) systems and the hybrid combinations the above methods with hot-deck
imputation. The tests are conducted on an eight category antenatal survey and
also under principal component analysis (PCA) conditions. The neural network
outperforms the neuro-fuzzy system for all tests by an average of 5.8%, while
the hybrid method is on average 15.9% more accurate yet 50% less
computationally efficient than the NN or NF systems acting alone. The global
impact assessment of the imputed data is performed by several statistical
tests. It is found that although the imputed accuracy is high, the global
effect of the imputed data causes the PCA inter-relationships between the
dataset to become altered. The standard deviation of the imputed dataset is on
average 36.7% lower than the actual dataset which may cause an incorrect
interpretation of the results.
"
"  Contemporary scientific studies often rely on the understanding of complex
quantum systems via computer simulation. This paper initiates the statistical
study of quantum simulation and proposes a Monte Carlo method for estimating
analytically intractable quantities. We derive the bias and variance for the
proposed Monte Carlo quantum simulation estimator and establish the asymptotic
theory for the estimator. The theory is used to design a computational scheme
for minimizing the mean square error of the estimator.
"
"  The problem of testing instantaneous causality between variables with
time-varying unconditional variance is investigated. It is shown that the
classical tests based on the assumption of stationary processes must be avoided
in our non standard framework. More precisely we underline that the standard
test does not control the type I errors, while the tests with White (1980) and
Heteroscedastic Autocorrelation Consistent (HAC) corrections can suffer from a
severe loss of power when the variance is not constant. Consequently a modified
test based on a bootstrap procedure is proposed. The relevance of the modified
test is underlined through a simulation study. The tests considered in this
paper are also compared by investigating the instantaneous causality relations
between US macroeconomic variables.
"
"  Hierarchical clustering based on pairwise similarities is a common tool used
in a broad range of scientific applications. However, in many problems it may
be expensive to obtain or compute similarities between the items to be
clustered. This paper investigates the hierarchical clustering of N items based
on a small subset of pairwise similarities, significantly less than the
complete set of N(N-1)/2 similarities. First, we show that if the intracluster
similarities exceed intercluster similarities, then it is possible to correctly
determine the hierarchical clustering from as few as 3N log N similarities. We
demonstrate this order of magnitude savings in the number of pairwise
similarities necessitates sequentially selecting which similarities to obtain
in an adaptive fashion, rather than picking them at random. We then propose an
active clustering method that is robust to a limited fraction of anomalous
similarities, and show how even in the presence of these noisy similarity
values we can resolve the hierarchical clustering using only O(N log^2 N)
pairwise similarities.
"
"  Probabilistic graphical models (PGMs) have become a popular tool for
computational analysis of biological data in a variety of domains. But, what
exactly are they and how do they work? How can we use PGMs to discover patterns
that are biologically relevant? And to what extent can PGMs help us formulate
new hypotheses that are testable at the bench? This note sketches out some
answers and illustrates the main ideas behind the statistical approach to
biological pattern discovery.
"
"  Despite more than a decade of reform efforts, students continue to experience
difficulty understanding and applying statistical concepts. The predominant
focus of reform has been on content, pedagogy, technology and assessment, with
little attention to instructor characteristics. However, there is strong
theoretical and empirical evidence that instructors' attitudes impact the
quality of teaching and learning. The objective of this study was to develop
and initially validate a scale to measure instructors' attitudes toward
reform-oriented (or concept-based) teaching of introductory statistics in the
health and behavioral sciences, at the tertiary level. This scale will be
referred to as FATS (Faculty Attitudes Toward Statistics). Data were obtained
from 227 instructors (USA and international), and analyzed using factor
analysis, multidimensional scaling and hierarchical cluster analysis. The
overall scale consists of five sub-scales with a total of 25 items, and an
overall alpha of 0.89. Construct validity was established. Specifically, the
overall scale, and subscales (except perceived difficulty) plausibly
differentiated between low-reform and high-reform practice instructors.
Statistically significant differences in attitude were observed with respect to
age, but not gender, employment status, membership status in professional
organizations, ethnicity, highest academic qualification, and degree
concentration. This scale can be considered a reliable and valid measure of
instructors' attitudes toward reform-oriented (concept-based or constructivist)
teaching of introductory statistics in the health and behavioral sciences at
the tertiary level. These five dimensions influence instructors' attitudes.
Additional studies are required to confirm these structural and psychometric
properties.
"
"  Contextual bandit learning is an increasingly popular approach to optimizing
recommender systems via user feedback, but can be slow to converge in practice
due to the need for exploring a large feature space. In this paper, we propose
a coarse-to-fine hierarchical approach for encoding prior knowledge that
drastically reduces the amount of exploration required. Intuitively, user
preferences can be reasonably embedded in a coarse low-dimensional feature
space that can be explored efficiently, requiring exploration in the
high-dimensional space only as necessary. We introduce a bandit algorithm that
explores within this coarse-to-fine spectrum, and prove performance guarantees
that depend on how well the coarse space captures the user's preferences. We
demonstrate substantial improvement over conventional bandit algorithms through
extensive simulation as well as a live user study in the setting of
personalized news recommendation.
"
"  Multi-task sparse feature learning aims to improve the generalization
performance by exploiting the shared features among tasks. It has been
successfully applied to many applications including computer vision and
biomedical informatics. Most of the existing multi-task sparse feature learning
algorithms are formulated as a convex sparse regularization problem, which is
usually suboptimal, due to its looseness for approximating an $\ell_0$-type
regularizer. In this paper, we propose a non-convex formulation for multi-task
sparse feature learning based on a novel non-convex regularizer. To solve the
non-convex optimization problem, we propose a Multi-Stage Multi-Task Feature
Learning (MSMTFL) algorithm; we also provide intuitive interpretations,
detailed convergence and reproducibility analysis for the proposed algorithm.
Moreover, we present a detailed theoretical analysis showing that MSMTFL
achieves a better parameter estimation error bound than the convex formulation.
Empirical studies on both synthetic and real-world data sets demonstrate the
effectiveness of MSMTFL in comparison with the state of the art multi-task
sparse feature learning algorithms.
"
"  We analyze the Agatston score of coronary artery calcium (CAC) from the
Multi-Ethnic Study of Atherosclerosis (MESA) using the semiparametric
zero-inflated modeling approach, where the observed CAC scores from this cohort
consist of high frequency of zeroes and continuously distributed positive
values. Both partially constrained and unconstrained models are considered to
investigate the underlying biological processes of CAC development from zero to
positive, and from small amount to large amount. Different from existing
studies, a model selection procedure based on likelihood cross-validation is
adopted to identify the optimal model, which is justified by comparative Monte
Carlo studies. A shrinkaged version of cubic regression spline is used for
model estimation and variable selection simultaneously. When applying the
proposed methods to the MESA data analysis, we show that the two biological
mechanisms influencing the initiation of CAC and the magnitude of CAC when it
is positive are better characterized by an unconstrained zero-inflated normal
model. Our results are significantly different from those in published studies,
and may provide further insights into the biological mechanisms underlying CAC
development in humans. This highly flexible statistical framework can be
applied to zero-inflated data analyses in other areas.
"
"  This work is motivated by the problem of image mis-registration in remote
sensing and we are interested in determining the resulting loss in the accuracy
of pattern classification. A statistical formulation is given where we propose
to use data contamination to model and understand the phenomenon of image
mis-registration. This model is widely applicable to many other types of errors
as well, for example, measurement errors and gross errors etc. The impact of
data contamination on classification is studied under a statistical learning
theoretical framework. A closed-form asymptotic bound is established for the
resulting loss in classification accuracy, which is less than
$\epsilon/(1-\epsilon)$ for data contamination of an amount of $\epsilon$. Our
bound is sharper than similar bounds in the domain adaptation literature and,
unlike such bounds, it applies to classifiers with an infinite
Vapnik-Chervonekis (VC) dimension. Extensive simulations have been conducted on
both synthetic and real datasets under various types of data contamination,
including label flipping, feature swapping and the replacement of feature
values with data generated from a random source such as a Gaussian or Cauchy
distribution. Our simulation results show that the bound we derive is fairly
tight.
"
"  In this work we study the problem of constructing stochastic processes with a
predetermined covariance decay by parameterizing its marginals and a given
family of copulas. We show that the proposed methodology is compatibility-free
and present several examples to illustrate the theory, including the important
Gaussian and Euclidean families of copulas. We associate the theory to common
applied time series models.
"
"  The key idea of this model is that firms are the result of an evolutionary
process. Based on demand and supply considerations the evolutionary model
presented here derives explicitly Gibrat's law of proportionate effects as the
result of the competition between products. Applying a preferential attachment
mechanism for firms the theory allows to establish the size distribution of
products and firms. Also established are the growth rate and price distribution
of consumer goods. Taking into account the characteristic property of human
activities to occur in bursts, the model allows also an explanation of the
size-variance relationship of the growth rate distribution of products and
firms. Further the product life cycle, the learning (experience) curve and the
market size in terms of the mean number of firms that can survive in a market
are derived. The model also suggests the existence of an invariant of a market
as the ratio of total profit to total revenue. The relationship between a
neo-classic and an evolutionary view of a market is discussed. The comparison
with empirical investigations suggests that the theory is able to describe the
main stylized facts concerning the size and growth of firms.
"
"  Consider a weighted or unweighted k-nearest neighbor graph that has been
built on n data points drawn randomly according to some density p on R^d. We
study the convergence of the shortest path distance in such graphs as the
sample size tends to infinity. We prove that for unweighted kNN graphs, this
distance converges to an unpleasant distance function on the underlying space
whose properties are detrimental to machine learning. We also study the
behavior of the shortest path distance in weighted kNN graphs.
"
"  The Bourewa beach site on the Rove Peninsula of Viti Levu is the earliest
known human settlement in the Fiji Islands. How did the settlement at Bourewa
develop in space and time? We have radiocarbon dates on sixty specimens, found
in association with evidence for human presence, taken from pits across the
site. Owing to the lack of diagnostic stratigraphy, there is no direct
archaeological evidence for distinct phases of occupation through the period of
interest. We give a spatio-temporal analysis of settlement at Bourewa in which
the deposition rate for dated specimens plays an important role.
Spatio-temporal mapping of radiocarbon date intensity is confounded by uneven
post-depositional thinning. We assume that the confounding processes act in
such a way that the absence of dates remains informative of zero rate for the
original deposition process. We model and fit the onset-field, that is, we
estimate for each location across the site the time at which deposition of
datable specimens began. The temporal process generating our spatial
onset-field is a model of the original settlement dynamics.
"
"  Many forensic genetics problems can be handled using structured systems of
discrete variables, for which Bayesian networks offer an appealing practical
modeling framework, and allow inferences to be computed by probability
propagation methods. However, when standard assumptions are violated--for
example, when allele frequencies are unknown, there is identity by descent or
the population is heterogeneous--dependence is generated among founding genes,
that makes exact calculation of conditional probabilities by propagation
methods less straightforward. Here we illustrate different methodologies for
assessing sensitivity to assumptions about founders in forensic genetics
problems. These include constrained steepest descent, linear fractional
programming and representing dependence by structure. We illustrate these
methods on several forensic genetics examples involving criminal
identification, simple and complex disputed paternity and DNA mixtures.
"
"  We introduce a novel data-driven order reduction method for nonlinear control
systems, drawing on recent progress in machine learning and statistical
dimensionality reduction. The method rests on the assumption that the nonlinear
system behaves linearly when lifted into a high (or infinite) dimensional
feature space where balanced truncation may be carried out implicitly. This
leads to a nonlinear reduction map which can be combined with a representation
of the system belonging to a reproducing kernel Hilbert space to give a closed,
reduced order dynamical system which captures the essential input-output
characteristics of the original model. Empirical simulations illustrating the
approach are also provided.
"
"  We present an approach for polarimetric Synthetic Aperture Radar (SAR) image
region boundary detection based on the use of B-Spline active contours and a
new model for polarimetric SAR data: the GHP distribution. In order to detect
the boundary of a region, initial B-Spline curves are specified, either
automatically or manually, and the proposed algorithm uses a deformable
contours technique to find the boundary. In doing this, the parameters of the
polarimetric GHP model for the data are estimated, in order to find the
transition points between the region being segmented and the surrounding area.
This is a local algorithm since it works only on the region to be segmented.
Results of its performance are presented.
"
"  The situation frequently arises where working with the likelihood function is
problematic. This can happen for several reasons---perhaps the likelihood is
prohibitively computationally expensive, perhaps it lacks some robustness
property, or perhaps it is simply not known for the model under consideration.
In these cases, it is often possible to specify alternative functions of the
parameters and the data that can be maximized to obtain asymptotically normal
estimates. However, these scenarios present obvious problems if one is
interested in applying Bayesian techniques. Here we describe open-faced
sandwich adjustment, a way to incorporate a wide class of non-likelihood
objective functions within Bayesian-like models to obtain asymptotically valid
parameter estimates and inference via MCMC. Two simulation examples show that
the method provides accurate frequentist uncertainty estimates. The open-faced
sandwich adjustment is applied to a Poisson spatio-temporal model to analyze an
ornithology dataset from the citizen science initiative eBird.
"
"  The generalizations of instantaneous frequency and instantaneous bandwidth to
a bivariate signal are derived. These are uniquely defined whether the signal
is represented as a pair of real-valued signals, or as one analytic and one
anti-analytic signal. A nonstationary but oscillatory bivariate signal has a
natural representation as an ellipse whose properties evolve in time, and this
representation provides a simple geometric interpretation for the bivariate
instantaneous moments. The bivariate bandwidth is shown to consist of three
terms measuring the degree of instability of the time-varying ellipse:
amplitude modulation with fixed eccentricity, eccentricity modulation, and
orientation modulation or precession. An application to the analysis of data
from a free-drifting oceanographic float is presented and discussed.
"
"  Objectives: Electronic health records (EHRs) are only a first step in
capturing and utilizing health-related data - the challenge is turning that
data into useful information. Furthermore, EHRs are increasingly likely to
include data relating to patient outcomes, functionality such as clinical
decision support, and genetic information as well, and, as such, can be seen as
repositories of increasingly valuable information about patients' health
conditions and responses to treatment over time. Methods: We describe a case
study of 423 patients treated by Centerstone within Tennessee and Indiana in
which we utilized electronic health record data to generate predictive
algorithms of individual patient treatment response. Multiple models were
constructed using predictor variables derived from clinical, financial and
geographic data. Results: For the 423 patients, 101 deteriorated, 223 improved
and in 99 there was no change in clinical condition. Based on modeling of
various clinical indicators at baseline, the highest accuracy in predicting
individual patient response ranged from 70-72% within the models tested. In
terms of individual predictors, the Centerstone Assessment of Recovery Level -
Adult (CARLA) baseline score was most significant in predicting outcome over
time (odds ratio 4.1 + 2.27). Other variables with consistently significant
impact on outcome included payer, diagnostic category, location and provision
of case management services. Conclusions: This approach represents a promising
avenue toward reducing the current gap between research and practice across
healthcare, developing data-driven clinical decision support based on
real-world populations, and serving as a component of embedded clinical
artificial intelligences that ""learn"" over time.
"
"  Hierarchical modeling of abundance in space or time using closed-population
mark-recapture under heterogeneity (model M$_{h}$) presents two challenges: (i)
finding a flexible likelihood in which abundance appears as an explicit
parameter and (ii) fitting the hierarchical model for abundance. The first
challenge arises because abundance not only indexes the population size, it
also determines the dimension of the capture probabilities in heterogeneity
models. A common approach is to use data augmentation to include these capture
probabilities directly into the likelihood and fit the model using Bayesian
inference via Markov chain Monte Carlo (MCMC). Two such examples of this
approach are (i) explicit trans-dimensional MCMC, and (ii) superpopulation data
augmentation. The superpopulation approach has the advantage of simple
specification that is easily implemented in BUGS and related software. However,
it reparameterizes the model so that abundance is no longer included, except as
a derived quantity. This is a drawback when hierarchical models for abundance,
or related parameters, are desired. Here, we analytically compare the two
approaches and show that they are more closely related than might appear
superficially. We exploit this relationship to specify the model in a way that
allows us to include abundance as a parameter and that facilitates hierarchical
modeling using readily available software such as BUGS. We use this approach to
model trends in grizzly bear abundance in Yellowstone National Park from
1986-1998.
"
"  In this paper, we present a novel framework incorporating a combination of
sparse models in different domains. We posit the observed data as generated
from a linear combination of a sparse Gaussian Markov model (with a sparse
precision matrix) and a sparse Gaussian independence model (with a sparse
covariance matrix). We provide efficient methods for decomposition of the data
into two domains, \viz Markov and independence domains. We characterize a set
of sufficient conditions for identifiability and model consistency. Our
decomposition method is based on a simple modification of the popular
$\ell_1$-penalized maximum-likelihood estimator ($\ell_1$-MLE). We establish
that our estimator is consistent in both the domains, i.e., it successfully
recovers the supports of both Markov and independence models, when the number
of samples $n$ scales as $n = \Omega(d^2 \log p)$, where $p$ is the number of
variables and $d$ is the maximum node degree in the Markov model. Our
conditions for recovery are comparable to those of $\ell_1$-MLE for consistent
estimation of a sparse Markov model, and thus, we guarantee successful
high-dimensional estimation of a richer class of models under comparable
conditions. Our experiments validate these results and also demonstrate that
our models have better inference accuracy under simple algorithms such as loopy
belief propagation.
"
"  In domains like bioinformatics, information retrieval and social network
analysis, one can find learning tasks where the goal consists of inferring a
ranking of objects, conditioned on a particular target object. We present a
general kernel framework for learning conditional rankings from various types
of relational data, where rankings can be conditioned on unseen data objects.
We propose efficient algorithms for conditional ranking by optimizing squared
regression and ranking loss functions. We show theoretically, that learning
with the ranking loss is likely to generalize better than with the regression
loss. Further, we prove that symmetry or reciprocity properties of relations
can be efficiently enforced in the learned models. Experiments on synthetic and
real-world data illustrate that the proposed methods deliver state-of-the-art
performance in terms of predictive power and computational efficiency.
Moreover, we also show empirically that incorporating symmetry or reciprocity
properties can improve the generalization performance.
"
"  This issue includes six articles that develop and apply statistical methods
for the analysis of gene sequencing data of different types. The methods are
tailored to the different data types and, in each case, lead to biological
insights not readily identified without the use of statistical methods. A
common feature in all articles is the development of methods for analyzing
simultaneously data of different types (e.g., genotype, phenotype, pedigree,
etc.); that is, using data of one type to inform the analysis of data from
another type.
"
"  We develop a new estimation technique for recovering depth-of-field from
multiple stereo images. Depth-of-field is estimated by determining the shift in
image location resulting from different camera viewpoints. When this shift is
not divisible by pixel width, the multiple stereo images can be combined to
form a super-resolution image. By modeling this super-resolution image as a
realization of a random field, one can view the recovery of depth as a
likelihood estimation problem. We apply these modeling techniques to the
recovery of cloud height from multiple viewing angles provided by the MISR
instrument on the Terra Satellite. Our efforts are focused on a two layer cloud
ensemble where both layers are relatively planar, the bottom layer is optically
thick and textured, and the top layer is optically thin. Our results
demonstrate that with relative ease, we get comparable estimates to the M2
stereo matcher which is the same algorithm used in the current MISR standard
product (details can be found in [IEEE Transactions on Geoscience and Remote
Sensing 40 (2002) 1547--1559]). Moreover, our techniques provide the
possibility of modeling all of the MISR data in a unified way for cloud height
estimation. Research is underway to extend this framework for fast, quality
global estimates of cloud height.
"
"  This paper introduces a novel approach for learning to rank (LETOR) based on
the notion of monotone retargeting. It involves minimizing a divergence between
all monotonic increasing transformations of the training scores and a
parameterized prediction function. The minimization is both over the
transformations as well as over the parameters. It is applied to Bregman
divergences, a large class of ""distance like"" functions that were recently
shown to be the unique class that is statistically consistent with the
normalized discounted gain (NDCG) criterion [19]. The algorithm uses
alternating projection style updates, in which one set of simultaneous
projections can be computed independent of the Bregman divergence and the other
reduces to parameter estimation of a generalized linear model. This results in
easily implemented, efficiently parallelizable algorithm for the LETOR task
that enjoys global optimum guarantees under mild conditions. We present
empirical results on benchmark datasets showing that this approach can
outperform the state of the art NDCG consistent techniques.
"
"  The multivariate extremal index function relates the asymptotic distribution
of the vector of pointwise maxima of a multivariate stationary sequence to that
of the independent sequence from the same stationary distribution. It also
measures the degree of clustering of extremes in the multivariate process. In
this paper, we construct nonparametric estimators of this function and prove
their asymptotic normality under long-range dependence and moment conditions.
The results are illustrated by means of a simulation study.
"
"  The spatial correlations in transmitter node locations introduced by common
multiple access protocols makes the analysis of interference, outage, and other
related metrics in a wireless network extremely difficult. Most works therefore
assume that nodes are distributed either as a Poisson point process (PPP) or a
grid, and utilize the independence properties of the PPP (or the regular
structure of the grid) to analyze interference, outage and other metrics.
But,the independence of node locations makes the PPP a dubious model for
nontrivial MACs which intentionally introduce correlations, e.g. spatial
separation, while the grid is too idealized to model real networks. In this
paper, we introduce a new technique based on the factorial moment expansion of
functionals of point processes to analyze functions of interference, in
particular outage probability. We provide a Taylor-series type expansion of
functions of interference, wherein increasing the number of terms in the series
provides a better approximation at the cost of increased complexity of
computation. Various examples illustrate how this new approach can be used to
find outage probability in both Poisson and non-Poisson wireless networks.
"
"  Let F be a family of Borel measurable functions on a complete separable
metric space. The gap (or fat-shattering) dimension of F is a combinatorial
quantity that measures the extent to which functions f in F can separate finite
sets of points at a predefined resolution gamma > 0. We establish a connection
between the gap dimension of F and the uniform convergence of its sample
averages under ergodic sampling. In particular, we show that if the gap
dimension of F at resolution gamma > 0 is finite, then for every ergodic
process the sample averages of functions in F are eventually within 10 gamma of
their limiting expectations uniformly over the class F. If the gap dimension of
F is finite for every resolution gamma > 0 then the sample averages of
functions in F converge uniformly to their limiting expectations. We assume
only that F is uniformly bounded and countable (or countably approximable). No
smoothness conditions are placed on F, and no assumptions beyond ergodicity are
placed on the sampling processes. Our results extend existing work for i.i.d.
processes.
"
"  Applications of cyber technologies improve the quality of monitoring and
decision making in smart grid. These cyber technologies are vulnerable to
malicious attacks, and compromising them can have serious technical and
economical problems. This paper specifies the effect of compromising each
measurement on the price of electricity, so that the attacker is able to change
the prices in the desired direction (increasing or decreasing). Attacking and
defending all measurements are impossible for the attacker and defender,
respectively. This situation is modeled as a zero sum game between the attacker
and defender. The game defines the proportion of times that the attacker and
defender like to attack and defend different measurements, respectively. From
the simulation results based on the PJM 5 Bus test system, we can show the
effectiveness and properties of the studied game.
"
"  We propose a novel interpretation of the collapsed variational Bayes
inference with a zero-order Taylor expansion approximation, called CVB0
inference, for latent Dirichlet allocation (LDA). We clarify the properties of
the CVB0 inference by using the alpha-divergence. We show that the CVB0
inference is composed of two different divergence projections: alpha=1 and -1.
This interpretation will help shed light on CVB0 works.
"
"  Nearest neighbor (k-NN) graphs are widely used in machine learning and data
mining applications, and our aim is to better understand what they reveal about
the cluster structure of the unknown underlying distribution of points.
Moreover, is it possible to identify spurious structures that might arise due
to sampling variability?
  Our first contribution is a statistical analysis that reveals how certain
subgraphs of a k-NN graph form a consistent estimator of the cluster tree of
the underlying distribution of points. Our second and perhaps most important
contribution is the following finite sample guarantee. We carefully work out
the tradeoff between aggressive and conservative pruning and are able to
guarantee the removal of all spurious cluster structures at all levels of the
tree while at the same time guaranteeing the recovery of salient clusters. This
is the first such finite sample result in the context of clustering.
"
"  In this paper we propose a novel framework for the construction of
sparsity-inducing priors. In particular, we define such priors as a mixture of
exponential power distributions with a generalized inverse Gaussian density
(EP-GIG). EP-GIG is a variant of generalized hyperbolic distributions, and the
special cases include Gaussian scale mixtures and Laplace scale mixtures.
Furthermore, Laplace scale mixtures can subserve a Bayesian framework for
sparse learning with nonconvex penalization. The densities of EP-GIG can be
explicitly expressed. Moreover, the corresponding posterior distribution also
follows a generalized inverse Gaussian distribution. These properties lead us
to EM algorithms for Bayesian sparse learning. We show that these algorithms
bear an interesting resemblance to iteratively re-weighted $\ell_2$ or $\ell_1$
methods. In addition, we present two extensions for grouped variable selection
and logistic regression.
"
"  Functional MRI (fMRI) has become the most common method for investigating the
human brain. However, fMRI data present some complications for statistical
analysis and modeling. One recently developed approach to these data focuses on
estimation of computational encoding models that describe how stimuli are
transformed into brain activity measured in individual voxels. Here we aim at
building encoding models for fMRI signals recorded in the primary visual cortex
of the human brain. We use residual analyses to reveal systematic nonlinearity
across voxels not taken into account by previous models. We then show how a
sparse nonparametric method [J. Roy. Statist. Soc. Ser. B 71 (2009b) 1009-1030]
can be used together with correlation screening to estimate nonlinear encoding
models effectively. Our approach produces encoding models that predict about
25% more accurately than models estimated using other methods [Nature 452
(2008a) 352--355]. The estimated nonlinearity impacts the inferred properties
of individual voxels, and it has a plausible biological interpretation. One
benefit of quantitative encoding models is that estimated models can be used to
decode brain activity, in order to identify which specific image was seen by an
observer. Encoding models estimated by our approach also improve such image
identification by about 12% when the correct image is one of 11,500 possible
images.
"
"  In record linkage (RL), or exact file matching, the goal is to identify the
links between entities with information on two or more files. RL is an
important activity in areas including counting the population, enhancing survey
frames and data, and conducting epidemiological and follow-up studies. RL is
challenging when files are very large, no accurate personal identification (ID)
number is present on all files for all units, and some information is recorded
with error. Without an unique ID number one must rely on comparisons of names,
addresses, dates, and other information to find the links. Latent class models
can be used to automatically score the value of information for determining
match status. Data for fitting models come from comparisons made within groups
of units that pass initial file blocking requirements. Data distributions can
vary across blocks. This article examines the use of prior information and
hierarchical latent class models in the context of RL.
"
"  Predictability of undesired events is a question of great interest in many
scientific disciplines including seismology, economy, and epidemiology. Here,
we focus on the predictability of invasion of a broad class of epidemics caused
by diseases that lead to permanent immunity of infected hosts after recovery or
death. We approach the problem from the perspective of the science of
complexity by proposing and testing several strategies for the estimation of
important characteristics of epidemics, such as the probability of invasion.
Our results suggest that parsimonious approximate methodologies may lead to the
most reliable and robust predictions. The proposed methodologies are first
applied to analysis of experimentally observed epidemics: invasion of the
fungal plant pathogen \emph{Rhizoctonia solani} in replicated host microcosms.
We then consider numerical experiments of the SIR
(susceptible-infected-removed) model to investigate the performance of the
proposed methods in further detail. The suggested framework can be used as a
valuable tool for quick assessment of epidemic threat at the stage when
epidemics only start developing. Moreover, our work amplifies the significance
of the small-scale and finite-time microcosm realizations of epidemics
revealing their predictive power.
"
"  We consider adaptive system identification problems with convex constraints
and propose a family of regularized Least-Mean-Square (LMS) algorithms. We show
that with a properly selected regularization parameter the regularized LMS
provably dominates its conventional counterpart in terms of mean square
deviations. We establish simple and closed-form expressions for choosing this
regularization parameter. For identifying an unknown sparse system we propose
sparse and group-sparse LMS algorithms, which are special examples of the
regularized LMS family. Simulation results demonstrate the advantages of the
proposed filters in both convergence rate and steady-state error under sparsity
assumptions on the true coefficient vector.
"
"  Whole and targeted sequencing of human genomes is a promising, increasingly
feasible tool for discovering genetic contributions to risk of complex
diseases. A key step is calling an individual's genotype from the multiple
aligned short read sequences of his DNA, each of which is subject to nucleotide
read error. Current methods are designed to call genotypes separately at each
locus from the sequence data of unrelated individuals. Here we propose
likelihood-based methods that improve calling accuracy by exploiting two
features of sequence data. The first is the linkage disequilibrium (LD) between
nearby SNPs. The second is the Mendelian pedigree information available when
related individuals are sequenced. In both cases the likelihood involves the
probabilities of read variant counts given genotypes, summed over the
unobserved genotypes. Parameters governing the prior genotype distribution and
the read error rates can be estimated either from the sequence data itself or
from external reference data. We use simulations and synthetic read data based
on the 1000 Genomes Project to evaluate the performance of the proposed
methods. An R-program to apply the methods to small families is freely
available at http://med.stanford.edu/epidemiology/PHGC/.
"
"  We introduce a new version of forward stepwise regression. Our modification
finds solutions to regression problems where the selected predictors appear in
a structured pattern, with respect to a predefined distance measure over the
candidate predictors. Our method is motivated by the problem of predicting
HIV-1 drug resistance from protein sequences. We find that our method improves
the interpretability of drug resistance while producing comparable predictive
accuracy to standard methods. We also demonstrate our method in a simulation
study and present some theoretical results and connections.
"
"  The ability to represent scientific data and concepts visually is becoming
increasingly important due to the unprecedented exponential growth of
computational power during the present digital age. The data sets and
simulations scientists in all fields can now create are literally thousands of
times as large as those created just 20 years ago. Historically successful
methods for data visualization can, and should, be applied to today's huge data
sets, but new approaches, also enabled by technology, are needed as well.
Increasingly, ""modular craftsmanship"" will be applied, as relevant
functionality from the graphically and technically best tools for a job are
combined as-needed, without low-level programming.
"
"  The accuracy of machine learning systems is a widely studied research topic.
Established techniques such as cross-validation predict the accuracy on unseen
data of the classifier produced by applying a given learning method to a given
training data set. However, they do not predict whether incurring the cost of
obtaining more data and undergoing further training will lead to higher
accuracy. In this paper we investigate techniques for making such early
predictions. We note that when a machine learning algorithm is presented with a
training set the classifier produced, and hence its error, will depend on the
characteristics of the algorithm, on training set's size, and also on its
specific composition. In particular we hypothesise that if a number of
classifiers are produced, and their observed error is decomposed into bias and
variance terms, then although these components may behave differently, their
behaviour may be predictable.
  We test our hypothesis by building models that, given a measurement taken
from the classifier created from a limited number of samples, predict the
values that would be measured from the classifier produced when the full data
set is presented. We create separate models for bias, variance and total error.
Our models are built from the results of applying ten different machine
learning algorithms to a range of data sets, and tested with ""unseen""
algorithms and datasets. We analyse the results for various numbers of initial
training samples, and total dataset sizes. Results show that our predictions
are very highly correlated with the values observed after undertaking the extra
training. Finally we consider the more complex case where an ensemble of
heterogeneous classifiers is trained, and show how we can accurately estimate
an upper bound on the accuracy achievable after further training.
"
"  We are often interested in explaining data through a set of hidden factors or
features. When the number of hidden features is unknown, the Indian Buffet
Process (IBP) is a nonparametric latent feature model that does not bound the
number of active features in dataset. However, the IBP assumes that all latent
features are uncorrelated, making it inadequate for many realworld problems. We
introduce a framework for correlated nonparametric feature models, generalising
the IBP. We use this framework to generate several specific models and
demonstrate applications on realworld datasets.
"
"  Fisher-consistent loss functions play a fundamental role in the construction
of successful binary margin-based classifiers. In this paper we establish the
Fisher-consistency condition for multicategory classification problems. Our
approach uses the margin vector concept which can be regarded as a
multicategory generalization of the binary margin. We characterize a wide class
of smooth convex loss functions that are Fisher-consistent for multicategory
classification. We then consider using the margin-vector-based loss functions
to derive multicategory boosting algorithms. In particular, we derive two new
multicategory boosting algorithms by using the exponential and logistic
regression losses.
"
"  We consider the problem of modeling discrete-valued vector time series data
using extensions of Chow-Liu tree models to capture both dependencies across
time and dependencies across variables. Conditional Chow-Liu tree models are
introduced, as an extension to standard Chow-Liu trees, for modeling
conditional rather than joint densities. We describe learning algorithms for
such models and show how they can be used to learn parsimonious representations
for the output distributions in hidden Markov models. These models are applied
to the important problem of simulating and forecasting daily precipitation
occurrence for networks of rain stations. To demonstrate the effectiveness of
the models, we compare their performance versus a number of alternatives using
historical precipitation data from Southwestern Australia and the Western
United States. We illustrate how the structure and parameters of the models can
be used to provide an improved meteorological interpretation of such data.
"
"  In this paper we solve support vector machines in reproducing kernel Banach
spaces with reproducing kernels defined on nonsymmetric domains instead of the
traditional methods in reproducing kernel Hilbert spaces. Using the
orthogonality of semi-inner-products, we can obtain the explicit
representations of the dual (normalized-duality-mapping) elements of support
vector machine solutions. In addition, we can introduce the reproduction
property in a generalized native space by Fourier transform techniques such
that it becomes a reproducing kernel Banach space, which can be even embedded
into Sobolev spaces, and its reproducing kernel is set up by the related
positive definite function. The representations of the optimal solutions of
support vector machines (regularized empirical risks) in these reproducing
kernel Banach spaces are formulated explicitly in terms of positive definite
functions, and their finite numbers of coefficients can be computed by fixed
point iteration. We also give some typical examples of reproducing kernel
Banach spaces induced by Mat\'ern functions (Sobolev splines) so that their
support vector machine solutions are well computable as the classical
algorithms. Moreover, each of their reproducing bases includes information from
multiple training data points. The concept of reproducing kernel Banach spaces
offers us a new numerical tool for solving support vector machines.
"
"  In multivariate time series, the estimation of the covariance matrix of the
observation innovations plays an important role in forecasting as it enables
the computation of the standardized forecast error vectors as well as it
enables the computation of confidence bounds of the forecasts. We develop an
on-line, non-iterative Bayesian algorithm for estimation and forecasting. It is
empirically found that, for a range of simulated time series, the proposed
covariance estimator has good performance converging to the true values of the
unknown observation covariance matrix. Over a simulated time series, the new
method approximates the correct estimates, produced by a non-sequential Monte
Carlo simulation procedure, which is used here as the gold standard. The
special, but important, vector autoregressive (VAR) and time-varying VAR models
are illustrated by considering London metal exchange data consisting of spot
prices of aluminium, copper, lead and zinc.
"
"  In this paper, we consider capture-recapture experiments with heterogenous
catchability. In the setting we consider, the widespread Huggins-Alho estimator
is not very suitable and we introduce and study a new generalized
Horvitz-Thompson estimator. Our motivation is Respondent Driven Sampling (RDS),
a prime example for such a setting where the capture probability is dependent
on both the unknown population size as well as on an observable covariate, the
network degree of an individual, due to peer recruitment. After discussing the
theoretical properties of the new estimator, with full details given in the
appendix, we evaluate it on various empirical and simulated data-sets, focusing
on an RDS survey in a population in rural Uganda in which the population size
is known a priori. The results thus obtained demonstrate that the adjusted
estimator is less biased than the naive Lincoln-Petersen estimator.
"
"  In this paper, we give a new sharp generalization bound of lp-MKL which is a
generalized framework of multiple kernel learning (MKL) and imposes
lp-mixed-norm regularization instead of l1-mixed-norm regularization. We
utilize localization techniques to obtain the sharp learning rate. The bound is
characterized by the decay rate of the eigenvalues of the associated kernels. A
larger decay rate gives a faster convergence rate. Furthermore, we give the
minimax learning rate on the ball characterized by lp-mixed-norm in the product
space. Then we show that our derived learning rate of lp-MKL achieves the
minimax optimal rate on the lp-mixed-norm ball.
"
"  Motivation: Time course data obtained from biological samples subject to
specific treatments can be very useful for revealing complex and novel
biological phenomena. Although an increasing number of time course microarray
datasets becomes available, most of them contain few biological replicates and
time points. So far there are few computational methods that can effectively
reveal differentially expressed genes and their patterns in such data.
  Results: We have proposed a new two-step nonparametric statistical procedure,
LRSA, to reveal differentially expressed genes and their expression trends in
temporal microarray data. We have also employed external controls as a
surrogate to estimate false discovery rates and thus to guide the discovery of
differentially expressed genes. Our results showed that LRSA reveals
substantially more differentially expressed genes and have much lower than two
other methods, STEM and ANOVA, in both real data and the simulated data. Our
computational results are confirmed using real-time PCRs.
  Contact: wuw2@upmc.edu
"
"  Modern statisticians are often presented with hundreds or thousands of
hypothesis testing problems to evaluate at the same time, generated from new
scientific technologies such as microarrays, medical and satellite imaging
devices, or flow cytometry counters. The relevant statistical literature tends
to begin with the tacit assumption that a single combined analysis, for
instance, a False Discovery Rate assessment, should be applied to the entire
set of problems at hand. This can be a dangerous assumption, as the examples in
the paper show, leading to overly conservative or overly liberal conclusions
within any particular subclass of the cases. A simple Bayesian theory yields a
succinct description of the effects of separation or combination on false
discovery rate analyses. The theory allows efficient testing within small
subclasses, and has applications to ``enrichment,'' the detection of multi-case
effects.
"
"  Optimal B-robust estimate is constructed for multidimensional parameter in
drift coefficient of diffusion type process with small noise. Optimal
mean-variance robust (optimal V -robust) trading strategy is find to hedge in
mean-variance sense the contingent claim in incomplete financial market with
arbitrary information structure and misspecified volatility of asset price,
which is modelled by multidimensional continuous semimartingale. Obtained
results are applied to stochastic volatility model, where the model of latent
volatility process contains unknown multidimensional parameter in drift
coefficient and small parameter in diffusion term.
"
"  In this paper, we study the nonnegative matrix factorization problem under
the separability assumption (that is, there exists a cone spanned by a small
subset of the columns of the input nonnegative data matrix containing all
columns), which is equivalent to the hyperspectral unmixing problem under the
linear mixing model and the pure-pixel assumption. We present a family of fast
recursive algorithms, and prove they are robust under any small perturbations
of the input data matrix. This family generalizes several existing
hyperspectral unmixing algorithms and hence provides for the first time a
theoretical justification of their better practical performance.
"
"  Set-functions appear in many areas of computer science and applied
mathematics, such as machine learning, computer vision, operations research or
electrical networks. Among these set-functions, submodular functions play an
important role, similar to convex functions on vector spaces. In this tutorial,
the theory of submodular functions is presented, in a self-contained way, with
all results shown from first principles. A good knowledge of convex analysis is
assumed.
"
"  We assume data sampled from a mixture of d-dimensional linear subspaces with
spherically symmetric distributions within each subspace and an additional
outlier component with spherically symmetric distribution within the ambient
space (for simplicity we may assume that all distributions are uniform on their
corresponding unit spheres). We also assume mixture weights for the different
components. We say that one of the underlying subspaces of the model is most
significant if its mixture weight is higher than the sum of the mixture weights
of all other subspaces. We study the recovery of the most significant subspace
by minimizing the lp-averaged distances of data points from d-dimensional
subspaces, where p>0. Unlike other lp minimization problems, this minimization
is non-convex for all p>0 and thus requires different methods for its analysis.
We show that if 0<p<=1, then for any fraction of outliers the most significant
subspace can be recovered by lp minimization with overwhelming probability
(which depends on the generating distribution and its parameters). We show that
when adding small noise around the underlying subspaces the most significant
subspace can be nearly recovered by lp minimization for any 0<p<=1 with an
error proportional to the noise level. On the other hand, if p>1 and there is
more than one underlying subspace, then with overwhelming probability the most
significant subspace cannot be recovered or nearly recovered. This last result
does not require spherically symmetric outliers.
"
"  Distance metric learning is an important component for many tasks, such as
statistical classification and content-based image retrieval. Existing
approaches for learning distance metrics from pairwise constraints typically
suffer from two major problems. First, most algorithms only offer point
estimation of the distance metric and can therefore be unreliable when the
number of training examples is small. Second, since these algorithms generally
select their training examples at random, they can be inefficient if labeling
effort is limited. This paper presents a Bayesian framework for distance metric
learning that estimates a posterior distribution for the distance metric from
labeled pairwise constraints. We describe an efficient algorithm based on the
variational method for the proposed Bayesian approach. Furthermore, we apply
the proposed Bayesian framework to active distance metric learning by selecting
those unlabeled example pairs with the greatest uncertainty in relative
distance. Experiments in classification demonstrate that the proposed framework
achieves higher classification accuracy and identifies more informative
training examples than the non-Bayesian approach and state-of-the-art distance
metric learning algorithms.
"
"  We consider model-based reinforcement learning in finite Markov De- cision
Processes (MDPs), focussing on so-called optimistic strategies. In MDPs,
optimism can be implemented by carrying out extended value it- erations under a
constraint of consistency with the estimated model tran- sition probabilities.
The UCRL2 algorithm by Auer, Jaksch and Ortner (2009), which follows this
strategy, has recently been shown to guarantee near-optimal regret bounds. In
this paper, we strongly argue in favor of using the Kullback-Leibler (KL)
divergence for this purpose. By studying the linear maximization problem under
KL constraints, we provide an ef- ficient algorithm, termed KL-UCRL, for
solving KL-optimistic extended value iteration. Using recent deviation bounds
on the KL divergence, we prove that KL-UCRL provides the same guarantees as
UCRL2 in terms of regret. However, numerical experiments on classical
benchmarks show a significantly improved behavior, particularly when the MDP
has reduced connectivity. To support this observation, we provide elements of
com- parison between the two algorithms based on geometric considerations.
"
"  Maximizing high-dimensional, non-convex functions through noisy observations
is a notoriously hard problem, but one that arises in many applications. In
this paper, we tackle this challenge by modeling the unknown function as a
sample from a high-dimensional Gaussian process (GP) distribution. Assuming
that the unknown function only depends on few relevant variables, we show that
it is possible to perform joint variable selection and GP optimization. We
provide strong performance guarantees for our algorithm, bounding the sample
complexity of variable selection, and as well as providing cumulative regret
bounds. We further provide empirical evidence on the effectiveness of our
algorithm on several benchmark optimization problems.
"
"  A wealth of epidemiological data suggests an association between
mortality/morbidity from pulmonary and cardiovascular adverse events and air
pollution, but uncertainty remains as to the extent implied by those
associations although the abundance of the data. In this paper we describe an
SSA (Singular Spectrum Analysis) based approach in order to decompose the
time-series of particulate matter concentration into a set of exposure
variables, each one representing a different timescale. We implement our
methodology to investigate both acute and long-term effects of $PM_{10}$
exposure on morbidity from respiratory causes within the urban area of Bari,
Italy.
"
"  Short-range forecasts of precipitation fields are needed in a wealth of
agricultural, hydrological, ecological and other applications. Forecasts from
numerical weather prediction models are often biased and do not provide
uncertainty information. Here we present a postprocessing technique for such
numerical forecasts that produces correlated probabilistic forecasts of
precipitation accumulation at multiple sites simultaneously. The statistical
model is a spatial version of a two-stage model that represents the
distribution of precipitation by a mixture of a point mass at zero and a Gamma
density for the continuous distribution of precipitation accumulation. Spatial
correlation is captured by assuming that two Gaussian processes drive
precipitation occurrence and precipitation amount, respectively. The first
process is latent and drives precipitation occurrence via a threshold. The
second process explains the spatial correlation in precipitation accumulation.
It is related to precipitation via a site-specific transformation function, so
as to retain the marginal right-skewed distribution of precipitation while
modeling spatial dependence. Both processes take into account the information
contained in the numerical weather forecast and are modeled as stationary
isotropic spatial processes with an exponential correlation function. The
two-stage spatial model was applied to 48-hour-ahead forecasts of daily
precipitation accumulation over the Pacific Northwest in 2004. The predictive
distributions from the two-stage spatial model were calibrated and sharp, and
outperformed reference forecasts for spatially composite and areally averaged
quantities.
"
"  Monte Carlo approaches have recently been proposed to quantify connectivity
in neuronal networks. The key problem is to sample from the conditional
distribution of a single neuronal spike train, given the activity of the other
neurons in the network. Dependencies between neurons are usually relatively
weak; however, temporal dependencies within the spike train of a single neuron
are typically strong. In this paper we develop several specialized
Metropolis--Hastings samplers which take advantage of this dependency
structure. These samplers are based on two ideas: (1) an adaptation of fast
forward--backward algorithms from the theory of hidden Markov models to take
advantage of the local dependencies inherent in spike trains, and (2) a
first-order expansion of the conditional likelihood which allows for efficient
exact sampling in the limit of weak coupling between neurons. We also
demonstrate that these samplers can effectively incorporate side information,
in particular, noisy fluorescence observations in the context of
calcium-sensitive imaging experiments. We quantify the efficiency of these
samplers in a variety of simulated experiments in which the network parameters
are closely matched to data measured in real cortical networks, and also
demonstrate the sampler applied to real calcium imaging data.
"
"  The dose regimen of Warfarin is separated into two phases. Firstly a loading
dose is given, which is designed to bring the International Normalisation Ratio
(INR) to within therapeutic range. Then a stable maintenance dose is given to
maintain the INR within therapeutic range. In the United Kingdom (UK) the
loading dose is usually given as three individual daily doses, the standard
loading dose being 10mg on days one and two and 5mgs on day three, which can be
varied at the discretion of the clinician. However, due to the large
inter-individual variation in the response to Warfarin therapy, it is difficult
to identify which patients will reach the narrow therapeutic window for target
INR, and which will be above or below the therapeutic window. The aim of this
research was to develop a methodology using a neural networks classification
algorithm and data mining techniques to predict for a given loading dose and
patient characteristics if the patient is more likely to achieve target INR or
more likely to be above or below therapeutic range.
  Multilayer perceptron (MLP) and 10-fold stratified cross validation
algorithms were used to determine an artificial neural network to classify
patients' response to their initial Warfarin loading dose. The resulting neural
network model correctly classifies an individual's response to their Warfarin
loading dose over 80% of the time. As well as taking into account the initial
loading dose, the final model also includes demographic, genetic and a number
of other potential confounding factors. With this model clinicians can
predetermine whether a given loading regimen, along with specific patient
characteristics will achieve a therapeutic response for a particular patient.
Thus tailoring the loading dose regimen to meet the individual needs of the
patient and reducing the risk of adverse drug reactions associated with
Warfarin.
"
"  User preferences for items can be inferred from either explicit feedback,
such as item ratings, or implicit feedback, such as rental histories. Research
in collaborative filtering has concentrated on explicit feedback, resulting in
the development of accurate and scalable models. However, since explicit
feedback is often difficult to collect it is important to develop effective
models that take advantage of the more widely available implicit feedback. We
introduce a probabilistic approach to collaborative filtering with implicit
feedback based on modelling the user's item selection process. In the interests
of scalability, we restrict our attention to tree-structured distributions over
items and develop a principled and efficient algorithm for learning item trees
from data. We also identify a problem with a widely used protocol for
evaluating implicit feedback models and propose a way of addressing it using a
small quantity of explicit feedback data.
"
"  The mathematical theory of compressed sensing (CS) asserts that one can
acquire signals from measurements whose rate is much lower than the total
bandwidth. Whereas the CS theory is now well developed, challenges concerning
hardware implementations of CS-based acquisition devices---especially in
optics---have only started being addressed. This paper presents an
implementation of compressive sensing in fluorescence microscopy and its
applications to biomedical imaging. Our CS microscope combines a dynamic
structured wide-field illumination and a fast and sensitive single-point
fluorescence detection to enable reconstructions of images of fluorescent
beads, cells and tissues with undersampling ratios (between the number of
pixels and number of measurements) up to 32. We further demonstrate a
hyperspectral mode and record images with 128 spectral channels and
undersampling ratios up to 64, illustrating the potential benefits of CS
acquisition for higher dimensional signals which typically exhibits extreme
redundancy. Altogether, our results emphasize the interest of CS schemes for
acquisition at a significantly reduced rate and point out to some remaining
challenges for CS fluorescence microscopy.
"
"  We combine Bayesian networks (BNs) and structural reliability methods (SRMs)
to create a new computational framework, termed enhanced Bayesian network
(eBN), for reliability and risk analysis of engineering structures and
infrastructure. BNs are efficient in representing and evaluating complex
probabilistic dependence structures, as present in infrastructure and
structural systems, and they facilitate Bayesian updating of the model when new
information becomes available. On the other hand, SRMs enable accurate
assessment of probabilities of rare events represented by computationally
demanding, physically-based models. By combining the two methods, the eBN
framework provides a unified and powerful tool for efficiently computing
probabilities of rare events in complex structural and infrastructure systems
in which information evolves in time. Strategies for modeling and efficiently
analyzing the eBN are described by way of several conceptual examples. The
companion paper applies the eBN methodology to example structural and
infrastructure systems.
"
"  We develop methodology for the estimation of the functional mean and the
functional principal components when the functions form a spatial process. The
data consist of curves $X(\mathbf{s}_k;t),t\in[0,T],$ observed at spatial
locations $\mathbf{s}_1,\mathbf{s}_2,...,\mathbf{s}_N$. We propose several
methods, and evaluate them by means of a simulation study. Next, we develop a
significance test for the correlation of two such functional spatial fields.
After validating the finite sample performance of this test by means of a
simulation study, we apply it to determine if there is correlation between
long-term trends in the so-called critical ionospheric frequency and decadal
changes in the direction of the internal magnetic field of the Earth. The test
provides conclusive evidence for correlation, thus solving a long-standing
space physics conjecture. This conclusion is not apparent if the spatial
dependence of the curves is neglected.
"
"  Age-period-cohort analysis is mathematically intractable because of
fundamental nonidentifiability of linear trends. However, some understanding
can be gained in the context of individual problems.
"
"  Discussion of ""A statistical analysis of multiple temperature proxies: Are
reconstructions of surface temperatures over the last 1000 years reliable?"" by
B.B. McShane and A.J. Wyner [arXiv:1104.4002]
"
"  Epidemics are often modelled using non-linear dynamical systems observed
through partial and noisy data. In this paper, we consider stochastic
extensions in order to capture unknown influences (changing behaviors, public
interventions, seasonal effects etc). These models assign diffusion processes
to the time-varying parameters, and our inferential procedure is based on a
suitably adjusted adaptive particle MCMC algorithm. The performance of the
proposed computational methods is validated on simulated data and the adopted
model is applied to the 2009 H1N1 pandemic in England. In addition to
estimating the effective contact rate trajectories, the methodology is applied
in real time to provide evidence in related public health decisions. Diffusion
driven SEIR-type models with age structure are also introduced.
"
"  The paper deals with the probabilistic estimates of extreme maximum rainfall
(Annual basis) in the Ranchi, Jharkhand (India). Extreme Value Distribution
family models are tried to capture the uncertainty of data and finally
Generalized Extreme Value (GEV) distribution model is found as the best fitted
distribution model. The GEV model satisfied the selection criteria
[Anderson-Darling test (A-D test or Goodness of fit test) and Normality test
(Q-Q plot)], which are adopted under the present study. The return levels are
estimated for 5, 10, 50, 100 and 200 years which are consistently increasing
for long run in future.
"
"  It is difficult to accurately estimate the rates of rape and domestic
violence due to the sensitive nature of these crimes. There is evidence that
bias in estimating the crime rates from survey data may arise because some
women respondents are ""gagged"" in reporting some types of crimes by the use of
a telephone rather than a personal interview, and by the presence of a spouse
during the interview. On the other hand, as data on these crimes are collected
every year, it would be more efficient in data analysis if we could identify
and make use of information from previous data. In this paper we propose a
model to adjust the estimates of the rates of rape and domestic violence to
account for the response bias due to the ""gag"" factors. To estimate parameters
in the model, we identify the information that is not sensitive to time and
incorporate this into prior distributions. The strength of Bayesian estimators
is their ability to combine information from long observational records in a
sensible way. Within a Bayesian framework, we develop an
Expectation-Maximization-Bayesian (EMB) algorithm for computation in analyzing
contingency table and we apply the jackknife to estimate the accuracy of the
estimates. Our approach is illustrated using the yearly crime data from the
National Crime Victimization Survey. The illustration shows that compared with
the classical method, our model leads to more efficient estimation but does not
require more complicated computation.
"
"  This paper proposes an information theory approach to estimate the number of
changepoints and their locations in a climatic time series. A model is
introduced that has an unknown number of changepoints and allows for series
autocorrelations, periodic dynamics, and a mean shift at each changepoint time.
An objective function gauging the number of changepoints and their locations,
based on a minimum description length (MDL) information criterion, is derived.
A genetic algorithm is then developed to optimize the objective function. The
methods are applied in the analysis of a century of monthly temperatures from
Tuscaloosa, Alabama.
"
"  The importance of nodes in a network constantly fluctuates based on changes
in the network structure as well as changes in external interest. We propose an
evolving teleportation adaptation of the PageRank method to capture how changes
in external interest influence the importance of a node. This framework
seamlessly generalizes PageRank because the importance of a node will converge
to the PageRank values if the external influence stops changing. We demonstrate
the effectiveness of the evolving teleportation on the Wikipedia graph and the
Twitter social network. The external interest is given by the number of hourly
visitors to each page and the number of monthly tweets for each user.
"
"  Using the uniform most powerful unbiased test, we observed the sales
distribution of consumer electronics in Japan on a daily basis and report that
it follows both a lognormal distribution and a power-law distribution and
depends on the state of the market. We show that these switches occur quite
often. The underlying sales dynamics found between both periods nicely matched
a multiplicative process. However, even though the multiplicative term in the
process displays a size-dependent relationship when a steady lognormal
distribution holds, it shows a size-independent relationship when the power-law
distribution holds. This difference in the underlying dynamics is responsible
for the difference in the two observed distributions.
"
"  Despite the fact that the Euler allocation principle has been adopted by many
financial institutions for their internal capital allocation process, a
comprehensive description of Euler allocation seems still to be missing. We try
to fill this gap by presenting the theoretical background as well as practical
aspects. In particular, we discuss how Euler risk contributions can be
estimated for some important risk measures. We furthermore investigate the
analysis of CDO tranche expected losses by means of Euler's theorem and suggest
an approach to measure the impact of risk factors on non-linear portfolios.
"
"  Many estimation problems in astrophysics are highly complex, with
high-dimensional, non-standard data objects (e.g., images, spectra, entire
distributions, etc.) that are not amenable to formal statistical analysis. To
utilize such data and make accurate inferences, it is crucial to transform the
data into a simpler, reduced form. Spectral kernel methods are non-linear data
transformation methods that efficiently reveal the underlying geometry of
observable data. Here we focus on one particular technique: diffusion maps or
more generally spectral connectivity analysis (SCA). We give examples of
applications in astronomy; e.g., photometric redshift estimation, prototype
selection for estimation of star formation history, and supernova light curve
classification. We outline some computational and statistical challenges that
remain, and we discuss some promising future directions for astronomy and data
mining.
"
"  The influence of speed limits on roadway safety is an extremely important
social issue and is subject to an extensive debate in the State of Indiana and
nationwide. With around 800-900 fatalities and thousands of injuries annually
in Indiana, traffic accidents place an incredible social and economic burden on
the state. Still, speed limits posted on highways and other roads are routinely
exceeded as individual drivers try to balance safety and mobility (speed). This
research explores the relationship between speed limits and roadway safety.
Namely, the research focuses on the influence of the posted speed limit on the
causation and severity of accidents. Data on individual accidents from the
Indiana Electronic Vehicle Crash Record System is used in the research, and
appropriate statistical models are estimated for causation and severity of
different types of accidents on all road classes. The results of the modeling
show that speed limits do not have a statistically significant adverse effect
on unsafe-speed-related causation of accidents on all roads, but generally
increase the severity of accidents on the majority of roads other than highways
(the accident severity on highways is unaffected by speed limits). Our findings
can perhaps save both lives and travel time by helping the Indiana Department
of Transportation determine optimal speed limit policies in the state.
"
"  Nonnegative Matrix Factorization (NMF) is a widely used technique in many
applications such as face recognition, motion segmentation, etc. It
approximates the nonnegative data in an original high dimensional space with a
linear representation in a low dimensional space by using the product of two
nonnegative matrices. In many applications data are often partially corrupted
with large additive noise. When the positions of noise are known, some existing
variants of NMF can be applied by treating these corrupted entries as missing
values. However, the positions are often unknown in many real world
applications, which prevents the usage of traditional NMF or other existing
variants of NMF. This paper proposes a Robust Nonnegative Matrix Factorization
(RobustNMF) algorithm that explicitly models the partial corruption as large
additive noise without requiring the information of positions of noise. In
practice, large additive noise can be used to model outliers. In particular,
the proposed method jointly approximates the clean data matrix with the product
of two nonnegative matrices and estimates the positions and values of
outliers/noise. An efficient iterative optimization algorithm with a solid
theoretical justification has been proposed to learn the desired matrix
factorization. Experimental results demonstrate the advantages of the proposed
algorithm.
"
"  Fiscal year-end balances of the Individual Indian Money System (a part of the
Indian Trust) were constructed from data related to money collected in the
system and disbursed by the system from 1887 to 2007. The data set of fiscal
year accounting information had a high proportion of missing values, and much
of the available data did not satisfy basic accounting relationships. Instead
of just calculating a single estimate and arguing to the Court that the
assumptions needed for the computation were reasonable, a distribution of
calculated balances was developed using multiple imputation and time series
models. These provided information to assess the uncertainty of the estimate
due to missing and questionable data.
"
"  Automatic image annotation (AIA) raises tremendous challenges to machine
learning as it requires modeling of data that are both ambiguous in input and
output, e.g., images containing multiple objects and labeled with multiple
semantic tags. Even more challenging is that the number of candidate tags is
usually huge (as large as the vocabulary size) yet each image is only related
to a few of them. This paper presents a hybrid generative-discriminative
classifier to simultaneously address the extreme data-ambiguity and
overfitting-vulnerability issues in tasks such as AIA. Particularly: (1) an
Exponential-Multinomial Mixture (EMM) model is established to capture both the
input and output ambiguity and in the meanwhile to encourage prediction
sparsity; and (2) the prediction ability of the EMM model is explicitly
maximized through discriminative learning that integrates variational inference
of graphical models and the pairwise formulation of ordinal regression.
Experiments show that our approach achieves both superior annotation
performance and better tag scalability.
"
"  During primary HIV infection, the kinetics of plasma virus concentrations and
CD4+ cell counts is very complex. Parametric and nonparametric models have been
suggested for fitting repeated measurements of these markers. Alternatively,
mechanistic approaches based on ordinary differential equations have also been
proposed. These latter models are constructed according to biological knowledge
and take into account the complex nonlinear interactions between viruses and
cells. However, estimating the parameters of these models is difficult. A main
difficulty in the context of primary HIV infection is that the date of
infection is generally unknown. For some patients, the date of last negative
HIV test is available in addition to the date of first positive HIV test
(seroconverters). In this paper we propose a likelihood-based method for
estimating the parameters of dynamical models using a population approach and
taking into account the uncertainty of the infection date. We applied this
method to a sample of 761 HIV-infected patients from the Concerted Action on
SeroConversion to AIDS and Death in Europe (CASCADE).
"
"  Presence-only data, point locations where a species has been recorded as
being present, are often used in modeling the distribution of a species as a
function of a set of explanatory variables---whether to map species occurrence,
to understand its association with the environment, or to predict its response
to environmental change. Currently, ecologists most commonly analyze
presence-only data by adding randomly chosen ""pseudo-absences"" to the data such
that it can be analyzed using logistic regression, an approach which has
weaknesses in model specification, in interpretation, and in implementation. To
address these issues, we propose Poisson point process modeling of the
intensity of presences. We also derive a link between the proposed approach and
logistic regression---specifically, we show that as the number of
pseudo-absences increases (in a regular or uniform random arrangement),
logistic regression slope parameters and their standard errors converge to
those of the corresponding Poisson point process model. We discuss the
practical implications of these results. In particular, point process modeling
offers a framework for choice of the number and location of pseudo-absences,
both of which are currently chosen by ad hoc and sometimes ineffective methods
in ecology, a point which we illustrate by example.
"
"  Collaborative filtering (CF) allows the preferences of multiple users to be
pooled to make recommendations regarding unseen products. We consider in this
paper the problem of online and interactive CF: given the current ratings
associated with a user, what queries (new ratings) would most improve the
quality of the recommendations made? We cast this terms of expected value of
information (EVOI); but the online computational cost of computing optimal
queries is prohibitive. We show how offline prototyping and computation of
bounds on EVOI can be used to dramatically reduce the required online
computation. The framework we develop is general, but we focus on derivations
and empirical study in the specific case of the multiple-cause vector
quantization model.
"
"  We consider the problem of reconstructing a low rank matrix from noisy
observations of a subset of its entries. This task has applications in
statistical learning, computer vision, and signal processing. In these
contexts, ""noise"" generically refers to any contribution to the data that is
not captured by the low-rank model. In most applications, the noise level is
large compared to the underlying signal and it is important to avoid
overfitting. In order to tackle this problem, we define a regularized cost
function well suited for spectral reconstruction methods. Within a random noise
model, and in the large system limit, we prove that the resulting accuracy
undergoes a phase transition depending on the noise level and on the fraction
of observed entries. The cost function can be minimized using OPTSPACE (a
manifold gradient descent algorithm). Numerical simulations show that this
approach is competitive with state-of-the-art alternatives.
"
"  Although the standard formulations of prediction problems involve
fully-observed and noiseless data drawn in an i.i.d. manner, many applications
involve noisy and/or missing data, possibly involving dependence, as well. We
study these issues in the context of high-dimensional sparse linear regression,
and propose novel estimators for the cases of noisy, missing and/or dependent
data. Many standard approaches to noisy or missing data, such as those using
the EM algorithm, lead to optimization problems that are inherently nonconvex,
and it is difficult to establish theoretical guarantees on practical
algorithms. While our approach also involves optimizing nonconvex programs, we
are able to both analyze the statistical error associated with any global
optimum, and more surprisingly, to prove that a simple algorithm based on
projected gradient descent will converge in polynomial time to a small
neighborhood of the set of all global minimizers. On the statistical side, we
provide nonasymptotic bounds that hold with high probability for the cases of
noisy, missing and/or dependent data. On the computational side, we prove that
under the same types of conditions required for statistical consistency, the
projected gradient descent algorithm is guaranteed to converge at a geometric
rate to a near-global minimizer. We illustrate these theoretical predictions
with simulations, showing close agreement with the predicted scalings.
"
"  We present a class of models that, via a simple construction, enables exact,
incremental, non-parametric, polynomial-time, Bayesian inference of conditional
measures. The approach relies upon creating a sequence of covers on the
conditioning variable and maintaining a different model for each set within a
cover. Inference remains tractable by specifying the probabilistic model in
terms of a random walk within the sequence of covers. We demonstrate the
approach on problems of conditional density estimation, which, to our knowledge
is the first closed-form, non-parametric Bayesian approach to this problem.
"
"  Confidence measures for the generalization error are crucial when small
training samples are used to construct classifiers. A common approach is to
estimate the generalization error by resampling and then assume the resampled
estimator follows a known distribution to form a confidence set [Kohavi 1995,
Martin 1996,Yang 2006]. Alternatively, one might bootstrap the resampled
estimator of the generalization error to form a confidence set. Unfortunately,
these methods do not reliably provide sets of the desired confidence. The poor
performance appears to be due to the lack of smoothness of the generalization
error as a function of the learned classifier. This results in a non-normal
distribution of the estimated generalization error. We construct a confidence
set for the generalization error by use of a smooth upper bound on the
deviation between the resampled estimate and generalization error. The
confidence set is formed by bootstrapping this upper bound. In cases in which
the approximation class for the classifier can be represented as a parametric
additive model, we provide a computationally efficient algorithm. This method
exhibits superior performance across a series of test and simulated data sets.
"
"  We propose a novel method to embed a functional magnetic resonance imaging
(fMRI) dataset in a low-dimensional space. The embedding optimally preserves
the local functional coupling between fMRI time series and provides a
low-dimensional coordinate system for detecting activated voxels. To compute
the embedding, we build a graph of functionally connected voxels. We use the
commute time, instead of the geodesic distance, to measure functional distances
on the graph. Because the commute time can be computed directly from the
eigenvectors of (a symmetric version) the graph probability transition matrix,
we use these eigenvectors to embed the dataset in low dimensions. After
clustering the datasets in low dimensions, coherent structures emerge that can
be easily interpreted. We performed an extensive evaluation of our method
comparing it to linear and nonlinear techniques using synthetic datasets and in
vivo datasets. We analyzed datasets from the EBC competition obtained with
subjects interacting in an urban virtual reality environment. Our exploratory
approach is able to detect independently visual areas (V1/V2, V5/MT), auditory
areas, and language areas. Our method can be used to analyze fMRI collected
during ``natural stimuli''.
"
"  We address online linear optimization problems when the possible actions of
the decision maker are represented by binary vectors. The regret of the
decision maker is the difference between her realized loss and the best loss
she would have achieved by picking, in hindsight, the best possible action. Our
goal is to understand the magnitude of the best possible (minimax) regret. We
study the problem under three different assumptions for the feedback the
decision maker receives: full information, and the partial information models
of the so-called ""semi-bandit"" and ""bandit"" problems. Combining the Mirror
Descent algorithm and the INF (Implicitely Normalized Forecaster) strategy, we
are able to prove optimal bounds for the semi-bandit case. We also recover the
optimal bounds for the full information setting. In the bandit case we discuss
existing results in light of a new lower bound, and suggest a conjecture on the
optimal regret in that case. Finally we also prove that the standard
exponentially weighted average forecaster is provably suboptimal in the setting
of online combinatorial optimization.
"
"  We consider two parametrized random digraph families, namely,
proportional-edge and central similarity proximity catch digraphs (PCDs) and
compare the performance of these two PCD families in testing spatial point
patterns. These PCD families are based on relative positions of data points
from two classes and the relative density of the PCDs is used as a statistic
for testing segregation and association against complete spatial randomness.
When scaled properly, the relative density of a PCD is a U-statistic. We extend
the distribution of the relative density of central similarity PCDs for
expansion parameter being larger than one. We compare the asymptotic
distribution of the statistic for the two PCD families, using the standard
central limit theory of U-statistics. We compare finite sample performance of
the tests by Monte Carlo simulations and prove the consistency of the tests
under the alternatives. The asymptotic performance of the tests under the
alternatives is assessed by Pitman's asymptotic efficiency. We find the optimal
expansion parameters of the PCDs for testing each of the segregation and
association alternatives in finite samples and in the limit. We demonstrate
that in terms of empirical power (i.e., for finite samples) relative density of
central similarity PCD has better performance (which occurs for expansion
parameter values larger than one) under segregation alternative, while relative
density of proportional-edge PCD has better performance under association
alternative. The methods are illustrated in a real-life example from plant
ecology.
"
"  Individual-level health data are often not publicly available due to
confidentiality; masked data are released instead. Therefore, it is important
to evaluate the utility of using the masked data in statistical analyses such
as regression. In this paper we propose a data masking method which is based on
spatial smoothing techniques. The proposed method allows for selecting both the
form and the degree of masking, thus resulting in a large degree of
flexibility. We investigate the utility of the masked data sets in terms of the
mean square error (MSE) of regression parameter estimates when fitting a
Generalized Linear Model (GLM) to the masked data. We also show that
incorporating prior knowledge on the spatial pattern of the exposure into the
data masking may reduce the bias and MSE of the parameter estimates. By
evaluating both utility and disclosure risk as functions of the form and the
degree of masking, our method produces a risk-utility profile which can
facilitate the selection of masking parameters. We apply the method to a study
of racial disparities in mortality rates using data on more than 4 million
Medicare enrollees residing in 2095 zip codes in the Northeast region of the
United States.
"
"  In this paper, we present a simple non-parametric method for learning the
structure of undirected graphs from data that drawn from an underlying unknown
distribution. We propose to use Brownian distance covariance to estimate the
conditional independences between the random variables and encodes pairwise
Markov graph. This framework can be applied in high-dimensional setting, where
the number of parameters much be larger than the sample size.
"
"  The vast amount of biological knowledge accumulated over the years has
allowed researchers to identify various biochemical interactions and define
different families of pathways. There is an increased interest in identifying
pathways and pathway elements involved in particular biological processes. Drug
discovery efforts, for example, are focused on identifying biomarkers as well
as pathways related to a disease. We propose a Bayesian model that addresses
this question by incorporating information on pathways and gene networks in the
analysis of DNA microarray data. Such information is used to define pathway
summaries, specify prior distributions, and structure the MCMC moves to fit the
model. We illustrate the method with an application to gene expression data
with censored survival outcomes. In addition to identifying markers that would
have been missed otherwise and improving prediction accuracy, the integration
of existing biological knowledge into the analysis provides a better
understanding of underlying molecular processes.
"
"  The $\ell$-1 norm based optimization is widely used in signal processing,
especially in recent compressed sensing theory. This paper studies the solution
path of the $\ell$-1 norm penalized least-square problem, whose constrained
form is known as Least Absolute Shrinkage and Selection Operator (LASSO). A
solution path is the set of all the optimizers with respect to the evolution of
the hyperparameter (Lagrange multiplier). The study of the solution path is of
great significance in viewing and understanding the profile of the tradeoff
between the approximation and regularization terms. If the solution path of a
given problem is known, it can help us to find the optimal hyperparameter under
a given criterion such as the Akaike Information Criterion. In this paper we
present a sufficient condition on $\ell$-1 norm penalized least-square problem.
Under this sufficient condition, the number of nonzero entries in the optimizer
or solution vector increases monotonically when the hyperparameter decreases.
We also generalize the result to the often used total variation case, where the
$\ell$-1 norm is taken over the first order derivative of the solution vector.
We prove that the proposed condition has intrinsic connections with the
condition given by Donoho, et al \cite{Donoho08} and the positive cone
condition by Efron {\it el al} \cite{Efron04}. However, the proposed condition
does not need to assume the sparsity level of the signal as required by Donoho
et al's condition, and is easier to verify than Efron, et al's positive cone
condition when being used for practical applications.
"
"  In nonlinear latent variable models or dynamic models, if we consider the
latent variables as confounders (common causes), the noise dependencies imply
further relations between the observed variables. Such models are then closely
related to causal discovery in the presence of nonlinear confounders, which is
a challenging problem. However, generally in such models the observation noise
is assumed to be independent across data dimensions, and consequently the noise
dependencies are ignored. In this paper we focus on the Gaussian process latent
variable model (GPLVM), from which we develop an extended model called
invariant GPLVM (IGPLVM), which can adapt to arbitrary noise covariances. With
the Gaussian process prior put on a particular transformation of the latent
nonlinear functions, instead of the original ones, the algorithm for IGPLVM
involves almost the same computational loads as that for the original GPLVM.
Besides its potential application in causal discovery, IGPLVM has the advantage
that its estimated latent nonlinear manifold is invariant to any nonsingular
linear transformation of the data. Experimental results on both synthetic and
realworld data show its encouraging performance in nonlinear manifold learning
and causal discovery.
"
"  The Baire metric induces an ultrametric on a dataset and is of linear
computational complexity, contrasted with the standard quadratic time
agglomerative hierarchical clustering algorithm. In this work we evaluate
empirically this new approach to hierarchical clustering. We compare
hierarchical clustering based on the Baire metric with (i) agglomerative
hierarchical clustering, in terms of algorithm properties; (ii) generalized
ultrametrics, in terms of definition; and (iii) fast clustering through k-means
partititioning, in terms of quality of results. For the latter, we carry out an
in depth astronomical study. We apply the Baire distance to spectrometric and
photometric redshifts from the Sloan Digital Sky Survey using, in this work,
about half a million astronomical objects. We want to know how well the (more
costly to determine) spectrometric redshifts can predict the (more easily
obtained) photometric redshifts, i.e. we seek to regress the spectrometric on
the photometric redshifts, and we use clusterwise regression for this.
"
"  Learning algorithms normally assume that there is at most one annotation or
label per data point. However, in some scenarios, such as medical diagnosis and
on-line collaboration,multiple annotations may be available. In either case,
obtaining labels for data points can be expensive and time-consuming (in some
circumstances ground-truth may not exist). Semi-supervised learning approaches
have shown that utilizing the unlabeled data is often beneficial in these
cases. This paper presents a probabilistic semi-supervised model and algorithm
that allows for learning from both unlabeled and labeled data in the presence
of multiple annotators. We assume that it is known what annotator labeled which
data points. The proposed approach produces annotator models that allow us to
provide (1) estimates of the true label and (2) annotator variable expertise
for both labeled and unlabeled data. We provide numerical comparisons under
various scenarios and with respect to standard semi-supervised learning.
Experiments showed that the presented approach provides clear advantages over
multi-annotator methods that do not use the unlabeled data and over methods
that do not use multi-labeler information.
"
"  Technology has had an unquestionable impact on the way people watch sports.
Along with this technological evolution has come a higher standard to ensure a
good viewing experience for the casual sports fan. It can be argued that the
pervasion of statistical analysis in sports serves to satiate the fan's desire
for detailed sports statistics. The goal of statistical analysis in sports is a
simple one: to eliminate subjective analysis. In this paper, we review previous
work that attempts to analyze various aspects in sports by using ideas from
Markov Chains, Bayesian Inference and Markov Chain Monte Carlo (MCMC) methods.
The unifying goal of these works is to achieve an accurate representation of
the player's ability, the sport, or the environmental effects on the player's
performance. With the prevalence of cheap computation, it is possible that
using techniques in Artificial Intelligence could improve the result of
statistical analysis in sport. This is best illustrated when evaluating
football using Neuro Dynamic Programming, a Control Theory paradigm heavily
based on theory in Stochastic processes. The results from this method suggest
that statistical analysis in sports may benefit from using ideas from the area
of Control Theory or Machine Learning
"
"  We consider a standard binary classification problem. The performance of any
binary classifier based on the training data is characterized by the excess
risk. We study Bahadur's type exponential bounds on the minimax accuracy
confidence function based on the excess risk. We study how this quantity
depends on the complexity of the class of distributions characterized by
exponents of entropies of the class of regression functions or of the class of
Bayes classifiers corresponding to the distributions from the class. We also
study its dependence on margin parameters of the classification problem.
"
"  Economic modeling in the presence of endogeneity is subject to model
uncertainty at both the instrument and covariate level. We propose a Two-Stage
Bayesian Model Averaging (2SBMA) methodology that extends the Two-Stage Least
Squares (2SLS) estimator. By constructing a Two-Stage Unit Information Prior in
the endogenous variable model, we are able to efficiently combine established
methods for addressing model uncertainty in regression models with the classic
technique of 2SLS. To assess the validity of instruments in the 2SBMA context,
we develop Bayesian tests of the identification restriction that are based on
model averaged posterior predictive p-values. A simulation study showed that
2SBMA has the ability to recover structure in both the instrument and covariate
set, and substantially improves the sharpness of resulting coefficient
estimates in comparison to 2SLS using the full specification in an automatic
fashion. Due to the increased parsimony of the 2SBMA estimate, the Bayesian
Sargan test had a power of 50 percent in detecting a violation of the
exogeneity assumption, while the method based on 2SLS using the full
specification had negligible power. We apply our approach to the problem of
development accounting, and find support not only for institutions, but also
for geography and integration as development determinants, once both model
uncertainty and endogeneity have been jointly addressed.
"
"  The performance of the Lasso is well understood under the assumptions of the
standard linear model with homoscedastic noise. However, in several
applications, the standard model does not describe the important features of
the data. This paper examines how the Lasso performs on a non-standard model
that is motivated by medical imaging applications. In these applications, the
variance of the noise scales linearly with the expectation of the observation.
Like all heteroscedastic models, the noise terms in this Poisson-like model are
\textit{not} independent of the design matrix.
  More specifically, this paper studies the sign consistency of the Lasso under
a sparse Poisson-like model. In addition to studying sufficient conditions for
the sign consistency of the Lasso estimate, this paper also gives necessary
conditions for sign consistency. Both sets of conditions are comparable to
results for the homoscedastic model, showing that when a measure of the signal
to noise ratio is large, the Lasso performs well on both Poisson-like data and
homoscedastic data.
  Simulations reveal that the Lasso performs equally well in terms of model
selection performance on both Poisson-like data and homoscedastic data (with
properly scaled noise variance), across a range of parameterizations. Taken as
a whole, these results suggest that the Lasso is robust to the Poisson-like
heteroscedastic noise.
"
"  This thesis responds to the challenges of using a large number, such as
thousands, of features in regression and classification problems.
  There are two situations where such high dimensional features arise. One is
when high dimensional measurements are available, for example, gene expression
data produced by microarray techniques. For computational or other reasons,
people may select only a small subset of features when modelling such data, by
looking at how relevant the features are to predicting the response, based on
some measure such as correlation with the response in the training data.
Although it is used very commonly, this procedure will make the response appear
more predictable than it actually is. In Chapter 2, we propose a Bayesian
method to avoid this selection bias, with application to naive Bayes models and
mixture models.
  High dimensional features also arise when we consider high-order
interactions. The number of parameters will increase exponentially with the
order considered. In Chapter 3, we propose a method for compressing a group of
parameters into a single one, by exploiting the fact that many predictor
variables derived from high-order interactions have the same values for all the
training cases. The number of compressed parameters may have converged before
considering the highest possible order. We apply this compression method to
logistic sequence prediction models and logistic classification models.
  We use both simulated data and real data to test our methods in both
chapters.
"
"  This paper summarizes a presentation for a panel discussion on ""The Future of
Astrostatistics"" held at the Statistical Challenges in Modern Astronomy V
conference at Pennsylvania State University in June 2011. I argue that the
emerging needs of astrostatistics may both motivate and benefit from
fundamental developments in statistics. I highlight some recent work within
statistics on fundamental topics relevant to astrostatistical practice,
including the Bayesian/frequentist debate (and ideas for a synthesis),
multilevel models, and multiple testing. As an important direction for future
work in statistics, I emphasize that astronomers need a statistical framework
that explicitly supports unfolding chains of discovery, with acquisition,
cataloging, and modeling of data not seen as isolated tasks, but rather as
parts of an ongoing, integrated sequence of analyses, with information and
uncertainty propagating forward and backward through the chain. A prototypical
example is surveying of astronomical populations, where source detection,
demographic modeling, and the design of survey instruments and strategies all
interact.
"
"  Motivated by widely observed examples in nature, society and software, where
groups of already related nodes arrive together and attach to an existing
network, we consider network growth via sequential attachment of linked node
groups, or graphlets. We analyze the simplest case, attachment of the three
node V-graphlet, where, with probability alpha, we attach a peripheral node of
the graphlet, and with probability (1-alpha), we attach the central node. Our
analytical results and simulations show that tuning alpha produces a wide range
in degree distribution and degree assortativity, achieving assortativity values
that capture a diverse set of many real-world systems. We introduce a
fifteen-dimensional attribute vector derived from seven well-known network
properties, which enables comprehensive comparison between any two networks.
Principal Component Analysis (PCA) of this attribute vector space shows a
significantly larger coverage potential of real-world network properties by a
simple extension of the above model when compared against a classic model of
network growth.
"
"  We model messaging activities as a hierarchical doubly stochastic point
process with three main levels, and develop an iterative algorithm for
inferring actors' relative latent positions from a stream of messaging activity
data. Each of the message-exchanging actors is modeled as a process in a latent
space. The actors' latent positions are assumed to be influenced by the
distribution of a much larger population over the latent space. Each actor's
movement in the latent space is modeled as being governed by two parameters
that we call confidence and visibility, in addition to dependence on the
population distribution. The messaging frequency between a pair of actors is
assumed to be inversely proportional to the distance between their latent
positions. Our inference algorithm is based on a projection approach to an
online filtering problem. The algorithm associates each actor with a
probability density-valued process, and each probability density is assumed to
be a mixture of basis functions. For efficient numerical experiments, we
further develop our algorithm for the case where the basis functions are
obtained by translating and scaling a standard Gaussian density.
"
"  As inductive inference and machine learning methods in computer science see
continued success, researchers are aiming to describe ever more complex
probabilistic models and inference algorithms. It is natural to ask whether
there is a universal computational procedure for probabilistic inference. We
investigate the computability of conditional probability, a fundamental notion
in probability theory and a cornerstone of Bayesian statistics. We show that
there are computable joint distributions with noncomputable conditional
distributions, ruling out the prospect of general inference algorithms, even
inefficient ones. Specifically, we construct a pair of computable random
variables in the unit interval such that the conditional distribution of the
first variable given the second encodes the halting problem. Nevertheless,
probabilistic inference is possible in many common modeling settings, and we
prove several results giving broadly applicable conditions under which
conditional distributions are computable. In particular, conditional
distributions become computable when measurements are corrupted by independent
computable noise with a sufficiently smooth bounded density.
"
"  Due to space limitations, our submission ""Source Separation and Clustering of
Phase-Locked Subspaces"", accepted for publication on the IEEE Transactions on
Neural Networks in 2011, presented some results without proof. Those proofs are
provided in this paper.
"
"  We develop a new principal components analysis (PCA) type dimension reduction
method for binary data. Different from the standard PCA which is defined on the
observed data, the proposed PCA is defined on the logit transform of the
success probabilities of the binary observations. Sparsity is introduced to the
principal component (PC) loading vectors for enhanced interpretability and more
stable extraction of the principal components. Our sparse PCA is formulated as
solving an optimization problem with a criterion function motivated from a
penalized Bernoulli likelihood. A Majorization--Minimization algorithm is
developed to efficiently solve the optimization problem. The effectiveness of
the proposed sparse logistic PCA method is illustrated by application to a
single nucleotide polymorphism data set and a simulation study.
"
"  We develop the Latent Multi-group Membership Graph (LMMG) model, a model of
networks with rich node feature structure. In the LMMG model, each node belongs
to multiple groups and each latent group models the occurrence of links as well
as the node feature structure. The LMMG can be used to summarize the network
structure, to predict links between the nodes, and to predict missing features
of a node. We derive efficient inference and learning algorithms and evaluate
the predictive performance of the LMMG on several social and document network
datasets.
"
"  Modelling the real world complexity of music is a challenge for machine
learning. We address the task of modeling melodic sequences from the same music
genre. We perform a comparative analysis of two probabilistic models; a
Dirichlet Variable Length Markov Model (Dirichlet-VMM) and a Time Convolutional
Restricted Boltzmann Machine (TC-RBM). We show that the TC-RBM learns
descriptive music features, such as underlying chords and typical melody
transitions and dynamics. We assess the models for future prediction and
compare their performance to a VMM, which is the current state of the art in
melody generation. We show that both models perform significantly better than
the VMM, with the Dirichlet-VMM marginally outperforming the TC-RBM. Finally,
we evaluate the short order statistics of the models, using the
Kullback-Leibler divergence between test sequences and model samples, and show
that our proposed methods match the statistics of the music genre significantly
better than the VMM.
"
"  If I know of a few persons of interest, how can a combination of human
language technology and graph theory help me find other people similarly
interesting? If I know of a few people committing a crime, how can I determine
their co-conspirators? Given a set of actors deemed interesting, we seek other
actors who are similarly interesting. We use a collection of communications
encoded as an attributed graph, where vertices represents actors and edges
connect pairs of actors that communicate. Attached to each edge is the set of
documents wherein that pair of actors communicate, providing content in context
- the communication topic in the context of who communicates with whom. In
these documents, our identified interesting actors communicate amongst each
other and with other actors whose interestingness is unknown. Our objective is
to nominate the most likely interesting vertex from all vertices with unknown
interestingness. As an illustrative example, the Enron email corpus consists of
communications between actors, some of which are allegedly committing fraud.
Some of their fraudulent activity is captured in emails, along with many
innocuous emails (both between the fraudsters and between the other employees
of Enron); we are given the identities of a few fraudster vertices and asked to
nominate other vertices in the graph as likely representing other actors
committing fraud. Foundational theory and initial experimental results indicate
that approaching this task with a joint model of content and context improves
the performance (as measured by standard information retrieval measures) over
either content or context alone.
"
"  Genomic imprinting has been thought to play an important role in seed
development in flowering plants. Seed in a flowering plant normally contains
diploid embryo and triploid endosperm. Empirical studies have shown that some
economically important endosperm traits are genetically controlled by imprinted
genes. However, the exact number and location of the imprinted genes are
largely unknown due to the lack of efficient statistical mapping methods. Here
we propose a general statistical variance components framework by utilizing the
natural information of sex-specific allelic sharing among sibpairs in line
crosses, to map imprinted quantitative trait loci (iQTL) underlying endosperm
traits. We propose a new variance components partition method considering the
unique characteristic of the triploid endosperm genome, and develop a
restricted maximum likelihood estimation method in an interval scan for
estimating and testing genome-wide iQTL effects. Cytoplasmic maternal effect
which is thought to have primary influences on yield and grain quality is also
considered when testing for genomic imprinting. Extension to multiple iQTL
analysis is proposed. Asymptotic distribution of the likelihood ratio test for
testing the variance components under irregular conditions are studied. Both
simulation study and real data analysis indicate good performance and
powerfulness of the developed approach.
"
"  We prove that the density function of the gradient of a sufficiently smooth
function $S : \Omega \subset \mathbb{R}^d \rightarrow \mathbb{R}$, obtained via
a random variable transformation of a uniformly distributed random variable, is
increasingly closely approximated by the normalized power spectrum of
$\phi=\exp\left(\frac{iS}{\tau}\right)$ as the free parameter $\tau \rightarrow
0$. The result is shown using the stationary phase approximation and standard
integration techniques and requires proper ordering of limits. We highlight a
relationship with the well-known characteristic function approach to density
estimation, and detail why our result is distinct from this approach.
"
"  Relationship-aware sequential pattern mining is the problem of mining
frequent patterns in sequences in which the events of a sequence are mutually
related by one or more concepts from some respective hierarchical taxonomies,
based on the type of the events. Additionally events themselves are also
described with a certain number of taxonomical concepts. We present RaSP an
algorithm that is able to mine relationship-aware patterns over such sequences;
RaSP follows a two stage approach. In the first stage it mines for frequent
type patterns and {\em all} their occurrences within the different sequences.
In the second stage it performs hierarchical mining where for each frequent
type pattern and its occurrences it mines for more specific frequent patterns
in the lower levels of the taxonomies. We test RaSP on a real world medical
application, that provided the inspiration for its development, in which we
mine for frequent patterns of medical behavior in the antibiotic treatment of
microbes and show that it has a very good computational performance given the
complexity of the relationship-aware sequential pattern mining problem.
"
"  To assess the classification accuracy of a continuous diagnostic result, the
receiver operating characteristic (ROC) curve is commonly used in applications.
The partial area under the ROC curve (pAUC) is one of widely accepted summary
measures due to its generality and ease of probability interpretation. In the
field of life science, a direct extension of the pAUC into the time-to-event
setting can be used to measure the usefulness of a biomarker for disease
detection over time. Without using a trapezoidal rule, we propose nonparametric
estimators, which are easily computed and have closed-form expressions, for the
time-dependent pAUC. The asymptotic Gaussian processes of the estimators are
established and the estimated variance-covariance functions are provided, which
are essential in the construction of confidence intervals. The finite sample
performance of the proposed inference procedures are investigated through a
series of simulations. Our method is further applied to evaluate the
classification ability of CD4 cell counts on patient's survival time in the
AIDS Clinical Trials Group (ACTG) 175 study. In addition, the inferences can be
generalized to compare the time-dependent pAUCs between patients received the
prior antiretroviral therapy and those without it.
"
"  This paper introduces the novel class of modulated cyclostationary processes,
a class of non-stationary processes exhibiting frequency coupling, and proposes
a method of their estimation from repeated trials. Cyclostationary processes
also exhibit frequency correlation but have Loeve spectra whose support lies
only on parallel lines in the dual-frequency plane. Such extremely sparse
structure does not adequately represent many biological processes. Thus, we
propose a model that, in the time domain, modulates the covariance of
cyclostationary processes and consequently broadens their frequency support in
the dual-frequency plane. The spectra and the cross-coherence of the proposed
modulated cyclostationary process are first estimated using multitaper methods.
A shrinkage procedure is then applied to each trial-specific estimate to reduce
the estimation risk.
  Multiple trials of each series are observed. When combining information
across trials, we carefully take into account the bias that may be introduced
by phase misalignment and the fact that the Loeve spectra and cross-coherence
across replicates may only be ""similar"" - but not necessarily identical -
across replicates. The application of the inference methods developed for the
modulated cyclostationary model to EEG data also demonstrates that the proposed
model captures statistically significant cross-frequency interactions, that
ought to be further examined by neuroscientists.
"
"  Public policy-makers use cost-effectiveness analyses (CEA) to decide which
health and social care interventions to provide. Appropriate methods have not
been developed for handling missing data in complex settings, such as for CEA
that use data from cluster randomised trials (CRTs). We present a multilevel
multiple imputation (MI) approach that recognises when missing data have a
hierarchical structure, and is compatible with the bivariate multilevel models
used to report cost-effectiveness. We contrast the multilevel MI approach with
single-level MI and complete case analysis in a CEA alongside a CRT. The paper
highlights the importance of adopting a principled approach to handling missing
values in settings with complex data structures.
"
"  This paper proposes a new method for estimating sparse precision matrices in
the high dimensional setting. It has been popular to study fast computation and
adaptive procedures for this problem. We propose a novel approach, called
Sparse Column-wise Inverse Operator, to address these two issues. We analyze an
adaptive procedure based on cross validation, and establish its convergence
rate under the Frobenius norm. The convergence rates under other matrix norms
are also established. This method also enjoys the advantage of fast computation
for large-scale problems, via a coordinate descent algorithm. Numerical merits
are illustrated using both simulated and real datasets. In particular, it
performs favorably on an HIV brain tissue dataset and an ADHD resting-state
fMRI dataset.
"
"  The decentralized particle filter (DPF) was proposed recently to increase the
level of parallelism of particle filtering. Given a decomposition of the state
space into two nested sets of variables, the DPF uses a particle filter to
sample the first set and then conditions on this sample to generate a set of
samples for the second set of variables. The DPF can be understood as a variant
of the popular Rao-Blackwellized particle filter (RBPF), where the second step
is carried out using Monte Carlo approximations instead of analytical
inference. As a result, the range of applications of the DPF is broader than
the one for the RBPF. In this paper, we improve the DPF in two ways. First, we
derive a Monte Carlo approximation of the optimal proposal distribution and,
consequently, design and implement a more efficient look-ahead DPF. Although
the decentralized filters were initially designed to capitalize on parallel
implementation, we show that the look-ahead DPF can outperform the standard
particle filter even on a single machine. Second, we propose the use of bandit
algorithms to automatically configure the state space decomposition of the DPF.
"
"  A voting bloc is defined to be a group of voters who have similar voting
preferences. The cleavage of the Irish electorate into voting blocs is of
interest. Irish elections employ a ``single transferable vote'' electoral
system; under this system voters rank some or all of the electoral candidates
in order of preference. These rank votes provide a rich source of preference
information from which inferences about the composition of the electorate may
be drawn. Additionally, the influence of social factors or covariates on the
electorate composition is of interest. A mixture of experts model is a mixture
model in which the model parameters are functions of covariates. A mixture of
experts model for rank data is developed to provide a model-based method to
cluster Irish voters into voting blocs, to examine the influence of social
factors on this clustering and to examine the characteristic preferences of the
voting blocs. The Benter model for rank data is employed as the family of
component densities within the mixture of experts model; generalized linear
model theory is employed to model the influence of covariates on the mixing
proportions. Model fitting is achieved via a hybrid of the EM and MM
algorithms. An example of the methodology is illustrated by examining an Irish
presidential election. The existence of voting blocs in the electorate is
established and it is determined that age and government satisfaction levels
are important factors in influencing voting in this election.
"
"  The auroras on Jupiter and Saturn can be studied with a high sensitivity and
resolution by the Hubble Space Telescope (HST) ultraviolet (UV) and
far-ultraviolet (FUV) Space Telescope spectrograph (STIS) and Advanced Camera
for Surveys (ACS) instruments. We present results of automatic detection and
segmentation of Jupiter's auroral emissions as observed by HST ACS instrument
with VOronoi Image SEgmentation (VOISE). VOISE is a dynamic algorithm for
partitioning the underlying pixel grid of an image into regions according to a
prescribed homogeneity criterion. The algorithm consists of an iterative
procedure that dynamically constructs a tessellation of the image plane based
on a Voronoi Diagram, until the intensity of the underlying image within each
region is classified as homogeneous. The computed tessellations allow the
extraction of quantitative information about the auroral features such as mean
intensity, latitudinal and longitudinal extents and length scales. These
outputs thus represent a more automated and objective method of characterising
auroral emissions than manual inspection.
"
"  In the context of the usual calibration model, we consider the case in which
the independent variable is unobservable, but a pre-fixed value on its
surrogate is available. Thus, considering controlled variables and assuming
that the measurement errors have equal variances we propose a new calibration
model. Likelihood based methodology is used to estimate the model parameters
and the Fisher information matrix is used to construct a confidence interval
for the unknown value of the regressor variable. A simulation study is carried
out to asses the effect of the measurement error on the estimation of the
parameter of interest. This new approach is illustrated with an example.
"
"  In spiking neural networks, the information is conveyed by the spike times,
that depend on the intrinsic dynamics of each neuron, the input they receive
and on the connections between neurons. In this article we study the Markovian
nature of the sequence of spike times in stochastic neural networks, and in
particular the ability to deduce from a spike train the next spike time, and
therefore produce a description of the network activity only based on the spike
times regardless of the membrane potential process.
  To study this question in a rigorous manner, we introduce and study an
event-based description of networks of noisy integrate-and-fire neurons, i.e.
that is based on the computation of the spike times. We show that the firing
times of the neurons in the networks constitute a Markov chain, whose
transition probability is related to the probability distribution of the
interspike interval of the neurons in the network. In the cases where the
Markovian model can be developed, the transition probability is explicitly
derived in such classical cases of neural networks as the linear
integrate-and-fire neuron models with excitatory and inhibitory interactions,
for different types of synapses, possibly featuring noisy synaptic integration,
transmission delays and absolute and relative refractory period. This covers
most of the cases that have been investigated in the event-based description of
spiking deterministic neural networks.
"
"  Accurate density estimation methodologies play an integral role in a variety
of scientific disciplines, with applications including simulation models,
decision support tools, and exploratory data analysis. In the past, histograms
and kernel density estimators have been the predominant tools of choice,
primarily due to their ease of use and mathematical simplicity. More recently,
the use of wavelets for density estimation has gained in popularity due to
their ability to approximate a large class of functions, including those with
localized, abrupt variations. However, a well-known attribute of wavelet bases
is that they can not be simultaneously symmetric, orthogonal, and compactly
supported. Multiwavelets-a more general, vector-valued, construction of
wavelets-overcome this disadvantage, making them natural choices for estimating
density functions, many of which exhibit local symmetries around features such
as a mode. We extend the methodology of wavelet density estimation to use
multiwavelet bases and illustrate several empirical results where multiwavelet
estimators outperform their wavelet counterparts at coarser resolution levels.
"
"  Margin maximization in the hard-margin sense, proposed as feature elimination
criterion by the MFE-LO method, is combined here with data radius utilization
to further aim to lower generalization error, as several published bounds and
bound-related formulations pertaining to lowering misclassification risk (or
error) pertain to radius e.g. product of squared radius and weight vector
squared norm. Additionally, we propose additional novel feature elimination
criteria that, while instead being in the soft-margin sense, too can utilize
data radius, utilizing previously published bound-related formulations for
approaching radius for the soft-margin sense, whereby e.g. a focus was on the
principle stated therein as ""finding a bound whose minima are in a region with
small leave-one-out values may be more important than its tightness"". These
additional criteria we propose combine radius utilization with a novel and
computationally low-cost soft-margin light classifier retraining approach we
devise named QP1; QP1 is the soft-margin alternative to the hard-margin LO. We
correct an error in the MFE-LO description, find MFE-LO achieves the highest
generalization accuracy among the previously published margin-based feature
elimination (MFE) methods, discuss some limitations of MFE-LO, and find our
novel methods herein outperform MFE-LO, attain lower test set classification
error rate. On several datasets that each both have a large number of features
and fall into the `large features few samples' dataset category, and on
datasets with lower (low-to-intermediate) number of features, our novel methods
give promising results. Especially, among our methods the tunable ones, that do
not employ (the non-tunable) LO approach, can be tuned more aggressively in the
future than herein, to aim to demonstrate for them even higher performance than
herein.
"
"  We propose a new problem formulation which is similar to, but more
informative than, the binary multiple-instance learning problem. In this
setting, we are given groups of instances (described by feature vectors) along
with estimates of the fraction of positively-labeled instances per group. The
task is to learn an instance level classifier from this information. That is,
we are trying to estimate the unknown binary labels of individuals from
knowledge of group statistics. We propose a principled probabilistic model to
solve this problem that accounts for uncertainty in the parameters and in the
unknown individual labels. This model is trained with an efficient MCMC
algorithm. Its performance is demonstrated on both synthetic and real-world
data arising in general object recognition.
"
"  We present a set of high-probability inequalities that control the
concentration of weighted averages of multiple (possibly uncountably many)
simultaneously evolving and interdependent martingales. Our results extend the
PAC-Bayesian analysis in learning theory from the i.i.d. setting to martingales
opening the way for its application to importance weighted sampling,
reinforcement learning, and other interactive learning domains, as well as many
other domains in probability theory and statistics, where martingales are
encountered.
  We also present a comparison inequality that bounds the expectation of a
convex function of a martingale difference sequence shifted to the [0,1]
interval by the expectation of the same function of independent Bernoulli
variables. This inequality is applied to derive a tighter analog of
Hoeffding-Azuma's inequality.
"
"  I propose a new procedure to estimate the False Alarm Probability, the
measure of significance for peaks of periodograms. The key element of the new
procedure is the use of generalized extreme-value distributions, the limiting
distribution for maxima of variables from most continuous distributions. This
technique allows reliable extrapolation to the very high probability levels
required by multiple hypothesis testing, and enables the derivation of
confidence intervals of the estimated levels. The estimates are stable against
deviations from distributional assumptions, which are otherwise usually made
either about the observations themselves or about the theoretical univariate
distribution of the periodogram. The quality and the performance of the
procedure is demonstrated on simulations and on two multimode variable stars
from Sloan Digital Sky Survey Stripe 82.
"
"  Reproducibility is essential to reliable scientific discovery in
high-throughput experiments. In this work we propose a unified approach to
measure the reproducibility of findings identified from replicate experiments
and identify putative discoveries using reproducibility. Unlike the usual
scalar measures of reproducibility, our approach creates a curve, which
quantitatively assesses when the findings are no longer consistent across
replicates. Our curve is fitted by a copula mixture model, from which we derive
a quantitative reproducibility score, which we call the ""irreproducible
discovery rate"" (IDR) analogous to the FDR. This score can be computed at each
set of paired replicate ranks and permits the principled setting of thresholds
both for assessing reproducibility and combining replicates. Since our approach
permits an arbitrary scale for each replicate, it provides useful descriptive
measures in a wide variety of situations to be explored. We study the
performance of the algorithm using simulations and give a heuristic analysis of
its theoretical properties. We demonstrate the effectiveness of our method in a
ChIP-seq experiment.
"
"  In this paper, we present an algorithm for minimizing the difference between
two submodular functions using a variational framework which is based on (an
extension of) the concave-convex procedure [17]. Because several commonly used
metrics in machine learning, like mutual information and conditional mutual
information, are submodular, the problem of minimizing the difference of two
submodular problems arises naturally in many machine learning applications. Two
such applications are learning discriminatively structured graphical models and
feature selection under computational complexity constraints. A commonly used
metric for measuring discriminative capacity is the EAR measure which is the
difference between two conditional mutual information terms. Feature selection
taking complexity considerations into account also fall into this framework
because both the information that a set of features provide and the cost of
computing and using the features can be modeled as submodular functions. This
problem is NP-hard, and we give a polynomial time heuristic for it. We also
present results on synthetic data to show that classifiers based on
discriminative graphical models using this algorithm can significantly
outperform classifiers based on generative graphical models.
"
"  Image analysis frequently deals with shape estimation and image
reconstruction. The ob jects of interest in these problems may be thought of as
random sets, and one is interested in finding a representative, or expected,
set. We consider a definition of set expectation using oriented distance
functions and study the properties of the associated empirical set. Conditions
are given such that the empirical average is consistent, and a method to
calculate a confidence region for the expected set is introduced. The proposed
method is applied to both real and simulated data examples.
"
"  We sharply characterize the performance of different penalization schemes for
the problem of selecting the relevant variables in the multi-task setting.
Previous work focuses on the regression problem where conditions on the design
matrix complicate the analysis. A clearer and simpler picture emerges by
studying the Normal means model. This model, often used in the field of
statistics, is a simplified model that provides a laboratory for studying
complex procedures.
"
"  Recognizing group activities is challenging due to the difficulties in
isolating individual entities, finding the respective roles played by the
individuals and representing the complex interactions among the participants.
Individual actions and group activities in videos can be represented in a
common framework as they share the following common feature: both are composed
of a set of low-level features describing motions, e.g., optical flow for each
pixel or a trajectory for each feature point, according to a set of composition
constraints in both temporal and spatial dimensions. In this paper, we present
a unified model to assess the similarity between two given individual or group
activities. Our approach avoids explicit extraction of individual actors,
identifying and representing the inter-person interactions. With the proposed
approach, retrieval from a video database can be performed through
Query-by-Example; and activities can be recognized by querying videos
containing known activities. The suggested video matching process can be
performed in an unsupervised manner. We demonstrate the performance of our
approach by recognizing a set of human actions and football plays.
"
"  Nonnegative matrix factorization (NMF) has become a very popular technique in
machine learning because it automatically extracts meaningful features through
a sparse and part-based representation. However, NMF has the drawback of being
highly ill-posed, that is, there typically exist many different but equivalent
factorizations. In this paper, we introduce a completely new way to obtaining
more well-posed NMF problems whose solutions are sparser. Our technique is
based on the preprocessing of the nonnegative input data matrix, and relies on
the theory of M-matrices and the geometric interpretation of NMF. This approach
provably leads to optimal and sparse solutions under the separability
assumption of Donoho and Stodden (NIPS, 2003), and, for rank-three matrices,
makes the number of exact factorizations finite. We illustrate the
effectiveness of our technique on several image datasets.
"
"  We consider the problem of learning a measure of distance among vectors in a
feature space and propose a hybrid method that simultaneously learns from
similarity ratings assigned to pairs of vectors and class labels assigned to
individual vectors. Our method is based on a generative model in which class
labels can provide information that is not encoded in feature vectors but yet
relates to perceived similarity between objects. Experiments with synthetic
data as well as a real medical image retrieval problem demonstrate that
leveraging class labels through use of our method improves retrieval
performance significantly.
"
"  In this paper we present an algorithm for rapid Bayesian analysis that
combines the benefits of nested sampling and artificial neural networks. The
blind accelerated multimodal Bayesian inference (BAMBI) algorithm implements
the MultiNest package for nested sampling as well as the training of an
artificial neural network (NN) to learn the likelihood function. In the case of
computationally expensive likelihoods, this allows the substitution of a much
more rapid approximation in order to increase significantly the speed of the
analysis. We begin by demonstrating, with a few toy examples, the ability of a
NN to learn complicated likelihood surfaces. BAMBI's ability to decrease
running time for Bayesian inference is then demonstrated in the context of
estimating cosmological parameters from Wilkinson Microwave Anisotropy Probe
and other observations. We show that valuable speed increases are achieved in
addition to obtaining NNs trained on the likelihood functions for the different
model and data combinations. These NNs can then be used for an even faster
follow-up analysis using the same likelihood and different priors. This is a
fully general algorithm that can be applied, without any pre-processing, to
other problems with computationally expensive likelihood functions.
"
"  Identity verification is an increasingly important process in our daily
lives, and biometric recognition is a natural solution to the authentication
problem.
  One of the most important research directions in the field of biometrics is
the characterization of novel biometric traits that can be used in conjunction
with other traits, to limit their shortcomings or to enhance their performance.
  The aim of this work is to introduce the reader to the usage of heart sounds
for biometric recognition, describing the strengths and the weaknesses of this
novel trait and analyzing in detail the methods developed so far by different
research groups and their performance.
"
"  Assessing the statistical power to detect susceptibility variants plays a
critical role in GWA studies both from the prospective and retrospective points
of view. Power is empirically estimated by simulating phenotypes under a
disease model H1. For this purpose, the ""gold"" standard consists in simulating
genotypes given the phenotypes (e.g. Hapgen). We introduce here an alternative
approach for simulating phenotypes under H1 that does not require generating
new genotypes for each simulation. In order to simulate phenotypes with a fixed
total number of cases and under a given disease model, we suggest three
algorithms: i) a simple rejection algorithm; ii) a numerical Markov Chain
Monte-Carlo (MCMC) approach; iii) and an exact and efficient backward sampling
algorithm. In our study, we validated the three algorithms both on a
toy-dataset and by comparing them with Hapgen on a more realistic dataset. As
an application, we then conducted a simulation study on a 1000 Genomes Project
dataset consisting of 629 individuals (314 cases) and 8,048 SNPs from
Chromosome X. We arbitrarily defined an additive disease model with two
susceptibility SNPs and an epistatic effect. The three algorithms are
consistent, but backward sampling is dramatically faster than the other two.
Our approach also gives consistent results with Hapgen. Using our application
data, we showed that our limited design requires a biological a priori to limit
the investigated region. We also proved that epistatic effects can play a
significant role even when simple marker statistics (e.g. trend) are used. We
finally showed that the overall performance of a GWA study strongly depends on
the prevalence of the disease: the larger the prevalence, the better the power.
"
"  The multiple disorder problem seeks to determine a sequence of stopping times
which are as close as possible to the unknown times of disorders at which the
observation process changes its probability characteristics. We derive closed
form solutions in two formulations of the multiple disorder problem for an
observable Brownian motion with switching constant drift rates. The method of
proof is based on the reduction of the initial problems to appropriate optimal
switching problems and the analysis of the associated coupled free-boundary
problems. We also describe the sequential switching multiple disorder detection
procedures resulting from these formulations.
"
"  Brain signal variability in the measurements obtained from different subjects
during different sessions significantly deteriorates the accuracy of most
brain-computer interface (BCI) systems. Moreover these variabilities, also
known as inter-subject or inter-session variabilities, require lengthy
calibration sessions before the BCI system can be used. Furthermore, the
calibration session has to be repeated for each subject independently and
before use of the BCI due to the inter-session variability. In this study, we
present an algorithm in order to minimize the above-mentioned variabilities and
to overcome the time-consuming and usually error-prone calibration time. Our
algorithm is based on linear programming support-vector machines and their
extensions to a multiple kernel learning framework. We tackle the inter-subject
or -session variability in the feature spaces of the classifiers. This is done
by incorporating each subject- or session-specific feature spaces into much
richer feature spaces with a set of optimal decision boundaries. Each decision
boundary represents the subject- or a session specific spatio-temporal
variabilities of neural signals. Consequently, a single classifier with
multiple feature spaces will generalize well to new unseen test patterns even
without the calibration steps. We demonstrate that classifiers maintain good
performances even under the presence of a large degree of BCI variability. The
present study analyzes BCI variability related to oxy-hemoglobin neural signals
measured using a functional near-infrared spectroscopy.
"
"  With the present revival of interest in bistatic radar systems, research in
that area has gained momentum. Given some of the strategic advantages for a
bistatic configuration, and tech- nological advances in the past few years,
large-scale implementation of the bistatic systems is a scope for the near
future. If the bistatic systems are to replace the monostatic systems (at least
par- tially), then all the existing usages of a monostatic system should be
manageable in a bistatic system. A detailed investigation of the possibilities
of an automatic target recognition (ATR) facil- ity in a bistatic radar system
is presented. Because of the lack of data, experiments were carried out on
simulated data. Still, the results are positive and make a positive case for
the introduction of the bistatic configuration. First, it was found that,
contrary to the popular expectation that the bistatic ATR performance might be
substantially worse than the monostatic ATR performance, the bistatic ATR
performed fairly well (though not better than the monostatic ATR). Second, the
ATR per- formance does not deteriorate substantially with increasing bistatic
angle. Last, the polarimetric data from bistatic scattering were found to have
distinct information, contrary to expert opinions. Along with these results,
suggestions were also made about how to stabilise the bistatic-ATR per-
formance with changing bistatic angle. Finally, a new fast and robust ATR
algorithm (developed in the present work) has been presented.
"
"  We consider the problem of binary classification where one can, for a
particular cost, choose not to classify an observation. We present a simple
proof for the oracle inequality for the excess risk of structural risk
minimizers using a lasso type penalty.
"
"  Freedman [Adv. in Appl. Math. 40 (2008) 180-193; Ann. Appl. Stat. 2 (2008)
176-196] critiqued ordinary least squares regression adjustment of estimated
treatment effects in randomized experiments, using Neyman's model for
randomization inference. Contrary to conventional wisdom, he argued that
adjustment can lead to worsened asymptotic precision, invalid measures of
precision, and small-sample bias. This paper shows that in sufficiently large
samples, those problems are either minor or easily fixed. OLS adjustment cannot
hurt asymptotic precision when a full set of treatment-covariate interactions
is included. Asymptotically valid confidence intervals can be constructed with
the Huber-White sandwich standard error estimator. Checks on the asymptotic
approximations are illustrated with data from Angrist, Lang, and Oreopoulos's
[Am. Econ. J.: Appl. Econ. 1:1 (2009) 136--163] evaluation of strategies to
improve college students' achievement. The strongest reasons to support
Freedman's preference for unadjusted estimates are transparency and the dangers
of specification search.
"
"  The standard in the high energy physics community for claiming discovery of
new physics is a $5\sigma$ excess in the observed signal over the estimated
background. While a $3\sigma$ excess is not enough to claim discovery, it is
certainly enough to pique the interest of both experimentalists and theorists.
However, with a large number of searches performed by both the ATLAS and CMS
collaborations at the LHC, one expects a nonzero number of multi-$\sigma$
results simply due to statistical fluctuations in the no-signal scenario. Our
analysis examines the distribution of p-values for CMS and ATLAS supersymmetry
(SUSY) searches using the full 2011 data set to determine if the collaborations
are being overly conservative in their analyses. We find that there is a
statistically significant excess of `medium' $\sigma$ values at the level of
$p=0.005$, indicating over-conservativism in the estimation of uncertainties.
"
"  We consider the problem of large-scale inference on the row or column
variables of data in the form of a matrix. Often this data is transposable,
meaning that both the row variables and column variables are of potential
interest. An example of this scenario is detecting significant genes in
microarrays when the samples or arrays may be dependent due to underlying
relationships. We study the effect of both row and column correlations on
commonly used test-statistics, null distributions, and multiple testing
procedures, by explicitly modeling the covariances with the matrix-variate
normal distribution. Using this model, we give both theoretical and simulation
results revealing the problems associated with using standard statistical
methodology on transposable data. We solve these problems by estimating the row
and column covariances simultaneously, with transposable regularized covariance
models, and de-correlating or sphering the data as a pre-processing step. Under
reasonable assumptions, our method gives test statistics that follow the scaled
theoretical null distribution and are approximately independent. Simulations
based on various models with structured and observed covariances from real
microarray data reveal that our method offers substantial improvements in two
areas: 1) increased statistical power and 2) correct estimation of false
discovery rates.
"
"  In model-based clustering and classification, the cluster-weighted model
constitutes a convenient approach when the random vector of interest
constitutes a response variable Y and a set p of explanatory variables X.
However, its applicability may be limited when p is high. To overcome this
problem, this paper assumes a latent factor structure for X in each mixture
component. This leads to the cluster-weighted factor analyzers (CWFA) model. By
imposing constraints on the variance of Y and the covariance matrix of X, a
novel family of sixteen CWFA models is introduced for model-based clustering
and classification. The alternating expectation-conditional maximization
algorithm, for maximum likelihood estimation of the parameters of all the
models in the family, is described; to initialize the algorithm, a 5-step
hierarchical procedure is proposed, which uses the nested structures of the
models within the family and thus guarantees the natural ranking among the
sixteen likelihoods. Artificial and real data show that these models have very
good clustering and classification performance and that the algorithm is able
to recover the parameters very well.
"
"  The use of multicomponent images has become widespread with the improvement
of multisensor systems having increased spatial and spectral resolutions.
However, the observed images are often corrupted by an additive Gaussian noise.
In this paper, we are interested in multichannel image denoising based on a
multiscale representation of the images. A multivariate statistical approach is
adopted to take into account both the spatial and the inter-component
correlations existing between the different wavelet subbands. More precisely,
we propose a new parametric nonlinear estimator which generalizes many reported
denoising methods. The derivation of the optimal parameters is achieved by
applying Stein's principle in the multivariate case. Experiments performed on
multispectral remote sensing images clearly indicate that our method
outperforms conventional wavelet denoising techniques
"
"  We analyze the convergence behaviour of a recently proposed algorithm for
regularized estimation called Dual Augmented Lagrangian (DAL). Our analysis is
based on a new interpretation of DAL as a proximal minimization algorithm. We
theoretically show under some conditions that DAL converges super-linearly in a
non-asymptotic and global sense. Due to a special modelling of sparse
estimation problems in the context of machine learning, the assumptions we make
are milder and more natural than those made in conventional analysis of
augmented Lagrangian algorithms. In addition, the new interpretation enables us
to generalize DAL to wide varieties of sparse estimation problems. We
experimentally confirm our analysis in a large scale $\ell_1$-regularized
logistic regression problem and extensively compare the efficiency of DAL
algorithm to previously proposed algorithms on both synthetic and benchmark
datasets.
"
"  The non-Gaussian quasi maximum likelihood estimator is frequently used in
GARCH models with intension to improve the efficiency of the GARCH parameters.
However, unless the quasi-likelihood happens to be the true one, non-Gaussian
QMLE methods suffers inconsistency even if shape parameters in the
quasi-likelihood are estimated. To correct this bias, we identify an unknown
scale parameter that is critical to the consistent estimation of non-Gaussian
QMLE, and propose a two-step non-Gaussian QMLE (2SNG-QMLE) for estimation of
the scale parameter and GARCH parameters. This novel approach is consistent and
asymptotically normal. Moreover, it has higher efficiency than the Gaussian
QMLE, particularly when the innovation error has heavy tails. Two extensions
are proposed to further improve the efficiency of 2SNG-QMLE. The impact of
relative heaviness of tails of the innovation and quasi-likelihood
distributions on the asymptotic efficiency has been thoroughly investigated.
Monte Carlo simulations and an empirical study confirm the advantages of the
proposed approach.
"
"  Quantitative portfolio allocation requires the accurate and tractable
estimation of covariances between a large number of assets, whose histories can
greatly vary in length. Such data are said to follow a monotone missingness
pattern, under which the likelihood has a convenient factorization. Upon
further assuming that asset returns are multivariate normally distributed, with
histories at least as long as the total asset count, maximum likelihood (ML)
estimates are easily obtained by performing repeated ordinary least squares
(OLS) regressions, one for each asset. Things get more interesting when there
are more assets than historical returns. OLS becomes unstable due to
rank--deficient design matrices, which is called a ""big p small n"" problem. We
explore remedies that involve making a change of basis, as in principal
components or partial least squares regression, or by applying shrinkage
methods like ridge regression or the lasso. This enables the estimation of
covariances between large sets of assets with histories of essentially
arbitrary length, and offers improvements in accuracy and interpretation. We
further extend the method by showing how external factors can be incorporated.
This allows for the adaptive use of factors without the restrictive assumptions
common in factor models. Our methods are demonstrated on randomly generated
data, and then benchmarked by the performance of balanced portfolios using real
historical financial returns. An accompanying R package called monomvn,
containing code implementing the estimators described herein, has been made
freely available on CRAN.
"
"  The problem of distributed learning and channel access is considered in a
cognitive network with multiple secondary users. The availability statistics of
the channels are initially unknown to the secondary users and are estimated
using sensing decisions. There is no explicit information exchange or prior
agreement among the secondary users. We propose policies for distributed
learning and access which achieve order-optimal cognitive system throughput
(number of successful secondary transmissions) under self play, i.e., when
implemented at all the secondary users. Equivalently, our policies minimize the
regret in distributed learning and access. We first consider the scenario when
the number of secondary users is known to the policy, and prove that the total
regret is logarithmic in the number of transmission slots. Our distributed
learning and access policy achieves order-optimal regret by comparing to an
asymptotic lower bound for regret under any uniformly-good learning and access
policy. We then consider the case when the number of secondary users is fixed
but unknown, and is estimated through feedback. We propose a policy in this
scenario whose asymptotic sum regret which grows slightly faster than
logarithmic in the number of transmission slots.
"
"  We describe a method that infers whether statistical dependences between two
observed variables X and Y are due to a ""direct"" causal link or only due to a
connecting causal path that contains an unobserved variable of low complexity,
e.g., a binary variable. This problem is motivated by statistical genetics.
Given a genetic marker that is correlated with a phenotype of interest, we want
to detect whether this marker is causal or it only correlates with a causal
one. Our method is based on the analysis of the location of the conditional
distributions P(Y|x) in the simplex of all distributions of Y. We report
encouraging results on semi-empirical data.
"
"  The main focus of this work is on developing models for the activity profile
of a terrorist group, detecting sudden spurts and downfalls in this profile,
and, in general, tracking it over a period of time. Toward this goal, a
$d$-state hidden Markov model (HMM) that captures the latent states underlying
the dynamics of the group and thus its activity profile is developed. The
simplest setting of $d=2$ corresponds to the case where the dynamics are
coarsely quantized as Active and Inactive, respectively. A state estimation
strategy that exploits the underlying HMM structure is then developed for spurt
detection and tracking. This strategy is shown to track even nonpersistent
changes that last only for a short duration at the cost of learning the
underlying model. Case studies with real terrorism data from open-source
databases are provided to illustrate the performance of the proposed
methodology.
"
"  We consider multivariate skew-t distributions for modeling composition data
of high energy cosmic rays. The model has been validated with simulated data
for different primary nuclei and hadronic models focusing on the depth of
maximum Xmax and number of muons N{\mu} observables. Further, we consider
mixtures of multivariate skew-t distributions for cosmic ray mass composition
determination and event-by-event classification. With respect to other
approaches in the field, it is based on analytical calculations and allows to
incorporate different sets of constraints provided by the present hadronic
models. We present some applications to simulated data sets generated with
different nuclear abundances assumptions. As it does not fully rely on the
hadronic model predictions, the method is particularly suited to the current
experimental scenario in which evidences of discrepancies of the measured data
with respect to the models have been reported for some shower observables, such
as the number of muons at ground level.
"
"  Multi-class classification methods based on both labeled and unlabeled
functional data sets are discussed. We present a semi-supervised logistic model
for classification in the context of functional data analysis. Unknown
parameters in our proposed model are estimated by regularization with the help
of EM algorithm. A crucial point in the modeling procedure is the choice of a
regularization parameter involved in the semi-supervised functional logistic
model. In order to select the adjusted parameter, we introduce model selection
criteria from information-theoretic and Bayesian viewpoints. Monte Carlo
simulations and a real data analysis are given to examine the effectiveness of
our proposed modeling strategy.
"
"  We introduce a new discriminant analysis method (Empirical Discriminant
Analysis or EDA) for binary classification in machine learning. Given a dataset
of feature vectors, this method defines an empirical feature map transforming
the training and test data into new data with components having Gaussian
empirical distributions. This map is an empirical version of the Gaussian
copula used in probability and mathematical finance. The purpose is to form a
feature mapped dataset as close as possible to Gaussian, after which standard
quadratic discriminants can be used for classification. We discuss this method
in general, and apply it to some datasets in computational biology.
"
"  Confounding of three binary-variables counterfactual model is discussed in
this paper. According to the effect between the control variable and the
covariate variable, we investigate three counterfactual models: the control
variable is independent of the covariate variable, the control variable has the
effect on the covariate variable and the covariate variable affects the control
variable. Using the ancillary information based on conditional independence
hypotheses, the sufficient conditions to determine whether the covariate
variable is an irrelevant factor or a confounder in each counterfactual model
are obtained.
"
"  Topic models, such as latent Dirichlet allocation (LDA), can be useful tools
for the statistical analysis of document collections and other discrete data.
The LDA model assumes that the words of each document arise from a mixture of
topics, each of which is a distribution over the vocabulary. A limitation of
LDA is the inability to model topic correlation even though, for example, a
document about genetics is more likely to also be about disease than X-ray
astronomy. This limitation stems from the use of the Dirichlet distribution to
model the variability among the topic proportions. In this paper we develop the
correlated topic model (CTM), where the topic proportions exhibit correlation
via the logistic normal distribution [J. Roy. Statist. Soc. Ser. B 44 (1982)
139--177]. We derive a fast variational inference algorithm for approximate
posterior inference in this model, which is complicated by the fact that the
logistic normal is not conjugate to the multinomial. We apply the CTM to the
articles from Science published from 1990--1999, a data set that comprises 57M
words. The CTM gives a better fit of the data than LDA, and we demonstrate its
use as an exploratory tool of large document collections.
"
"  In this paper we develop a scientific approach to control inter-country
conflict. This system makes use of a neural network and a feedback control
approach. It was found that by controlling the four controllable inputs:
Democracy, Dependency, Allies and Capability simultaneously, all the predicted
dispute outcomes could be avoided. Furthermore, it was observed that
controlling a single input Dependency or Capability also avoids all the
predicted conflicts. When the influence of each input variable on conflict is
assessed, Dependency, Capability, and Democracy emerge as key variables that
influence conflict.
"
"  Predicting the occurrence of links is a fundamental problem in networks. In
the link prediction problem we are given a snapshot of a network and would like
to infer which interactions among existing members are likely to occur in the
near future or which existing interactions are we missing. Although this
problem has been extensively studied, the challenge of how to effectively
combine the information from the network structure with rich node and edge
attribute data remains largely open.
  We develop an algorithm based on Supervised Random Walks that naturally
combines the information from the network structure with node and edge level
attributes. We achieve this by using these attributes to guide a random walk on
the graph. We formulate a supervised learning task where the goal is to learn a
function that assigns strengths to edges in the network such that a random
walker is more likely to visit the nodes to which new links will be created in
the future. We develop an efficient training algorithm to directly learn the
edge strength estimation function.
  Our experiments on the Facebook social graph and large collaboration networks
show that our approach outperforms state-of-the-art unsupervised approaches as
well as approaches that are based on feature extraction.
"
"  We introduce a new method for forecasting emergency call arrival rates that
combines integer-valued time series models with a dynamic latent factor
structure. Covariate information is captured via simple constraints on the
factor loadings. We directly model the count-valued arrivals per hour, rather
than using an artificial assumption of normality. This is crucial for the
emergency medical service context, in which the volume of calls may be very
low. Smoothing splines are used in estimating the factor levels and loadings to
improve long-term forecasts. We impose time series structure at the hourly
level, rather than at the daily level, capturing the fine-scale dependence in
addition to the long-term structure. Our analysis considers all emergency
priority calls received by Toronto EMS between January 2007 and December 2008
for which an ambulance was dispatched. Empirical results demonstrate
significantly reduced error in forecasting call arrival volume. To quantify the
impact of reduced forecast errors, we design a queueing model simulation that
approximates the dynamics of an ambulance system. The results show better
performance as the forecasting method improves. This notion of quantifying the
operational impact of improved statistical procedures may be of independent
interest.
"
"  We propose a computationally intensive method, the random lasso method, for
variable selection in linear models. The method consists of two major steps. In
step 1, the lasso method is applied to many bootstrap samples, each using a set
of randomly selected covariates. A measure of importance is yielded from this
step for each covariate. In step 2, a similar procedure to the first step is
implemented with the exception that for each bootstrap sample, a subset of
covariates is randomly selected with unequal selection probabilities determined
by the covariates' importance. Adaptive lasso may be used in the second step
with weights determined by the importance measures. The final set of covariates
and their coefficients are determined by averaging bootstrap results obtained
from step 2. The proposed method alleviates some of the limitations of lasso,
elastic-net and related methods noted especially in the context of microarray
data analysis: it tends to remove highly correlated variables altogether or
select them all, and maintains maximal flexibility in estimating their
coefficients, particularly with different signs; the number of selected
variables is no longer limited by the sample size; and the resulting prediction
accuracy is competitive or superior compared to the alternatives. We illustrate
the proposed method by extensive simulation studies. The proposed method is
also applied to a Glioblastoma microarray data analysis.
"
"  We propose a hierarchical Bayesian model to estimate the proportional
contribution of source populations to a newly founded colony. Samples are
derived from the first generation offspring in the colony, but mating may occur
preferentially among migrants from the same source population. Genotypes of the
newly founded colony and source populations are used to estimate the mixture
proportions, and the mixture proportions are related to environmental and
demographic factors that might affect the colonizing process. We estimate an
assortative mating coefficient, mixture proportions, and regression
relationships between environmental factors and the mixture proportions in a
single hierarchical model. The first-stage likelihood for genotypes in the
newly founded colony is a mixture multinomial distribution reflecting the
colonizing process. The environmental and demographic data are incorporated
into the model through a hierarchical prior structure. A simulation study is
conducted to investigate the performance of the model by using different levels
of population divergence and number of genetic markers included in the
analysis. We use Markov chain Monte Carlo (MCMC) simulation to conduct
inference for the posterior distributions of model parameters. We apply the
model to a data set derived from grey seals in the Orkney Islands, Scotland. We
compare our model with a similar model previously used to analyze these data.
The results from both the simulation and application to real data indicate that
our model provides better estimates for the covariate effects.
"
"  Inference in popular nonparametric Bayesian models typically relies on
sampling or other approximations. This paper presents a general methodology for
constructing novel tractable nonparametric Bayesian methods by applying the
kernel trick to inference in a parametric Bayesian model. For example, Gaussian
process regression can be derived this way from Bayesian linear regression.
Despite the success of the Gaussian process framework, the kernel trick is
rarely explicitly considered in the Bayesian literature. In this paper, we aim
to fill this gap and demonstrate the potential of applying the kernel trick to
tractable Bayesian parametric models in a wider context than just regression.
As an example, we present an intuitive Bayesian kernel machine for density
estimation that is obtained by applying the kernel trick to a Gaussian
generative model in feature space.
"
"  Non-negative matrix factorization (NMF) approximates a non-negative matrix
$X$ by a product of two non-negative low-rank factor matrices $W$ and $H$. NMF
and its extensions minimize either the Kullback-Leibler divergence or the
Euclidean distance between $X$ and $W^T H$ to model the Poisson noise or the
Gaussian noise. In practice, when the noise distribution is heavy tailed, they
cannot perform well. This paper presents Manhattan NMF (MahNMF) which minimizes
the Manhattan distance between $X$ and $W^T H$ for modeling the heavy tailed
Laplacian noise. Similar to sparse and low-rank matrix decompositions, MahNMF
robustly estimates the low-rank part and the sparse part of a non-negative
matrix and thus performs effectively when data are contaminated by outliers. We
extend MahNMF for various practical applications by developing box-constrained
MahNMF, manifold regularized MahNMF, group sparse MahNMF, elastic net inducing
MahNMF, and symmetric MahNMF. The major contribution of this paper lies in two
fast optimization algorithms for MahNMF and its extensions: the rank-one
residual iteration (RRI) method and Nesterov's smoothing method. In particular,
by approximating the residual matrix by the outer product of one row of W and
one row of $H$ in MahNMF, we develop an RRI method to iteratively update each
variable of $W$ and $H$ in a closed form solution. Although RRI is efficient
for small scale MahNMF and some of its extensions, it is neither scalable to
large scale matrices nor flexible enough to optimize all MahNMF extensions.
Since the objective functions of MahNMF and its extensions are neither convex
nor smooth, we apply Nesterov's smoothing method to recursively optimize one
factor matrix with another matrix fixed. By setting the smoothing parameter
inversely proportional to the iteration number, we improve the approximation
accuracy iteratively for both MahNMF and its extensions.
"
"  In this paper, we present $\ell_{1,p}$ multi-task structure learning for
Gaussian graphical models. We analyze the sufficient number of samples for the
correct recovery of the support union and edge signs. We also analyze the
necessary number of samples for any conceivable method by providing
information-theoretic lower bounds. We compare the statistical efficiency of
multi-task learning versus that of single-task learning. For experiments, we
use a block coordinate descent method that is provably convergent and generates
a sequence of positive definite solutions. We provide experimental validation
on synthetic data as well as on two publicly available real-world data sets,
including functional magnetic resonance imaging and gene expression data.
"
"  Collaborative filtering (CF) and content-based filtering (CBF) have widely
been used in information filtering applications. Both approaches have their
strengths and weaknesses which is why researchers have developed hybrid
systems. This paper proposes a novel approach to unify CF and CBF in a
probabilistic framework, named collaborative ensemble learning. It uses
probabilistic SVMs to model each user's profile (as CBF does).At the prediction
phase, it combines a society OF users profiles, represented by their respective
SVM models, to predict an active users preferences(the CF idea).The combination
scheme is embedded in a probabilistic framework and retains an intuitive
explanation.Moreover, collaborative ensemble learning does not require a global
training stage and thus can incrementally incorporate new data.We report
results based on two data sets. For the Reuters-21578 text data set, we
simulate user ratings under the assumption that each user is interested in only
one category. In the second experiment, we use users' opinions on a set of 642
art images that were collected through a web-based survey. For both data sets,
collaborative ensemble achieved excellent performance in terms of
recommendation accuracy.
"
"  In this paper we discuss a novel framework for multiclass learning, defined
by a suitable coding/decoding strategy, namely the simplex coding, that allows
to generalize to multiple classes a relaxation approach commonly used in binary
classification. In this framework, a relaxation error analysis can be developed
avoiding constraints on the considered hypotheses class. Moreover, we show that
in this setting it is possible to derive the first provably consistent
regularized method with training/tuning complexity which is independent to the
number of classes. Tools from convex analysis are introduced that can be used
beyond the scope of this paper.
"
"  Although nonnegative matrix factorization (NMF) is NP-hard in general, it has
been shown very recently that it is tractable under the assumption that the
input nonnegative data matrix is close to being separable (separability
requires that all columns of the input matrix belongs to the cone spanned by a
small subset of these columns). Since then, several algorithms have been
designed to handle this subclass of NMF problems. In particular, Bittorf,
Recht, R\'e and Tropp (`Factoring nonnegative matrices with linear programs',
NIPS 2012) proposed a linear programming model, referred to as Hottopixx. In
this paper, we provide a new and more general robustness analysis of their
method. In particular, we design a provably more robust variant using a
post-processing strategy which allows us to deal with duplicates and near
duplicates in the dataset.
"
"  We propose a new approach for clustering DNA features using array CGH data
from multiple tumor samples. We distinguish data-collapsing: joining contiguous
DNA clones or probes with extremely similar data into regions, from clustering:
joining contiguous, correlated regions based on a maximum likelihood principle.
The model-based clustering algorithm accounts for the apparent spatial patterns
in the data. We evaluate the randomness of the clustering result by a cluster
stability score in combination with cross-validation. Moreover, we argue that
the clustering really captures spatial genomic dependency by showing that
coincidental clustering of independent regions is very unlikely. Using the
region and cluster information, we combine testing of these for association
with a clinical variable in an hierarchical multiple testing approach. This
allows for interpreting the significance of both regions and clusters while
controlling the Family-Wise Error Rate simultaneously. We prove that in the
context of permutation tests and permutation-invariant clusters it is allowed
to perform clustering and testing on the same data set. Our procedures are
illustrated on two cancer data sets.
"
"  A nonparametric kernel-based method for realizing Bayes' rule is proposed,
based on representations of probabilities in reproducing kernel Hilbert spaces.
Probabilities are uniquely characterized by the mean of the canonical map to
the RKHS. The prior and conditional probabilities are expressed in terms of
RKHS functions of an empirical sample: no explicit parametric model is needed
for these quantities. The posterior is likewise an RKHS mean of a weighted
sample. The estimator for the expectation of a function of the posterior is
derived, and rates of consistency are shown. Some representative applications
of the kernel Bayes' rule are presented, including Baysian computation without
likelihood and filtering with a nonparametric state-space model.
"
"  Discussion of ``Statistical analysis of an archeological find'' by Andrey
Feuerverger [arXiv:0804.0079]
"
"  We show that the sets in a family with finite VC dimension can be uniformly
approximated within a given error by a finite partition. Immediate corollaries
include the fact that VC classes have finite bracketing numbers, satisfy
uniform laws of averages under strong dependence, and exhibit uniform mixing.
Our results are based on recent work concerning uniform laws of averages for VC
classes under ergodic sampling.
"
"  Understanding functional organization of genetic information is a major
challenge in modern biology. Following the initial publication of the human
genome sequence in 2001, advances in high-throughput measurement technologies
and efficient sharing of research material through community databases have
opened up new views to the study of living organisms and the structure of life.
In this thesis, novel computational strategies have been developed to
investigate a key functional layer of genetic information, the human
transcriptome, which regulates the function of living cells through protein
synthesis. The key contributions of the thesis are general exploratory tools
for high-throughput data analysis that have provided new insights to
cell-biological networks, cancer mechanisms and other aspects of genome
function.
  A central challenge in functional genomics is that high-dimensional genomic
observations are associated with high levels of complex and largely unknown
sources of variation. By combining statistical evidence across multiple
measurement sources and the wealth of background information in genomic data
repositories it has been possible to solve some the uncertainties associated
with individual observations and to identify functional mechanisms that could
not be detected based on individual measurement sources. Statistical learning
and probabilistic models provide a natural framework for such modeling tasks.
Open source implementations of the key methodological contributions have been
released to facilitate further adoption of the developed methods by the
research community.
"
"  We describe a method for detecting, locating and quantifying sources of gas
emissions to the atmosphere using remotely obtained gas concentration data; the
method is applicable to gases of environmental concern. We demonstrate its
performance using methane data collected from aircraft. Atmospheric point
concentration measurements are modelled as the sum of a spatially and
temporally smooth atmospheric background concentration, augmented by
concentrations due to local sources. We model source emission rates with a
Gaussian mixture model and use a Markov random field to represent the
atmospheric background concentration component of the measurements. A Gaussian
plume atmospheric eddy dispersion model represents gas dispersion between
sources and measurement locations. Initial point estimates of background
concentrations and source emission rates are obtained using mixed L2-L1
optimisation over a discretised grid of potential source locations. Subsequent
reversible jump Markov chain Monte Carlo inference provides estimated values
and uncertainties for the number, emission rates and locations of sources
unconstrained by a grid. Source area, atmospheric background concentrations and
other model parameters are also estimated. We investigate the performance of
the approach first using a synthetic problem, then apply the method to real
data collected from an aircraft flying over: a 1600 km^2 area containing two
landfills, then a 225 km^2 area containing a gas flare stack.
"
"  Matching one set of objects to another is a ubiquitous task in machine
learning and computer vision that often reduces to some form of the quadratic
assignment problem (QAP). The QAP is known to be notoriously hard, both in
theory and in practice. Here, we investigate if this difficulty can be
mitigated when some additional piece of information is available: (a) that all
QAP instances of interest come from the same application, and (b) the correct
solution for a set of such QAP instances is given. We propose a new approach to
accelerate the solution of QAPs based on learning parameters for a modified
objective function from prior QAP instances. A key feature of our approach is
that it takes advantage of the algebraic structure of permutations, in
conjunction with special methods for optimizing functions over the symmetric
group Sn in Fourier space. Experiments show that in practical domains the new
method can outperform existing approaches.
"
"  We consider the problem of estimating the joint distribution function of the
event time and a continuous mark variable based on censored data. More
specifically, the event time is subject to current status censoring and the
continuous mark is only observed in case inspection takes place after the event
time. The nonparametric maximum likelihood estimator (MLE) in this model is
known to be inconsistent. We propose and study an alternative likelihood based
estimator, maximizing a smoothed log-likelihood, hence called a maximum
smoothed likelihood estimator (MSLE). This estimator is shown to be well
defined and consistent, and a simple algorithm is described that can be used to
compute it. The MSLE is compared with other estimators in a small simulation
study.
"
"  The hierarchical Dirichlet process (HDP) has become an important Bayesian
nonparametric model for grouped data, such as document collections. The HDP is
used to construct a flexible mixed-membership model where the number of
components is determined by the data. As for most Bayesian nonparametric
models, exact posterior inference is intractable---practitioners use Markov
chain Monte Carlo (MCMC) or variational inference. Inspired by the split-merge
MCMC algorithm for the Dirichlet process (DP) mixture model, we describe a
novel split-merge MCMC sampling algorithm for posterior inference in the HDP.
We study its properties on both synthetic data and text corpora. We find that
split-merge MCMC for the HDP can provide significant improvements over
traditional Gibbs sampling, and we give some understanding of the data
properties that give rise to larger improvements.
"
"  Unsupervised two-view learning, or detection of dependencies between two
paired data sets, is typically done by some variant of canonical correlation
analysis (CCA). CCA searches for a linear projection for each view, such that
the correlations between the projections are maximized. The solution is
invariant to any linear transformation of either or both of the views; for
tasks with small sample size such flexibility implies overfitting, which is
even worse for more flexible nonparametric or kernel-based dependency discovery
methods. We develop variants which reduce the degrees of freedom by assuming
constraints on similarity of the projections in the two views. A particular
example is provided by a cancer gene discovery application where chromosomal
distance affects the dependencies between gene copy number and activity levels.
Similarity constraints are shown to improve detection performance of known
cancer genes.
"
"  The Bayesian framework is a well-studied and successful framework for
inductive reasoning, which includes hypothesis testing and confirmation,
parameter estimation, sequence prediction, classification, and regression. But
standard statistical guidelines for choosing the model class and prior are not
always available or fail, in particular in complex situations. Solomonoff
completed the Bayesian framework by providing a rigorous, unique, formal, and
universal choice for the model class and the prior. We discuss in breadth how
and in which sense universal (non-i.i.d.) sequence prediction solves various
(philosophical) problems of traditional Bayesian sequence prediction. We show
that Solomonoff's model possesses many desirable properties: Strong total and
weak instantaneous bounds, and in contrast to most classical continuous prior
densities has no zero p(oste)rior problem, i.e. can confirm universal
hypotheses, is reparametrization and regrouping invariant, and avoids the
old-evidence and updating problem. It even performs well (actually better) in
non-computable environments.
"
"  We introduce a rich class of graphical models for multi-armed bandit problems
that permit both the state or context space and the action space to be very
large, yet succinctly specify the payoffs for any context-action pair. Our main
result is an algorithm for such models whose regret is bounded by the number of
parameters and whose running time depends only on the treewidth of the graph
substructure induced by the action space.
"
"  Obtaining more accurate equity value estimates is the starting point for
stock selection, value-based indexing in a noisy market, and beating benchmark
indices through tactical style rotation. Unfortunately, discounted cash flow,
method of comparables, and fundamental analysis typically yield discrepant
valuation estimates. Moreover, the valuation estimates typically disagree with
market price. Can one form a superior valuation estimate by averaging over the
individual estimates, including market price? This article suggests a Bayesian
framework for combining two or more estimates into a superior valuation
estimate. The framework justifies the common practice of averaging over several
estimates to arrive at a final point estimate.
"
"  Rating prediction is an important application, and a popular research topic
in collaborative filtering. However, both the validity of learning algorithms,
and the validity of standard testing procedures rest on the assumption that
missing ratings are missing at random (MAR). In this paper we present the
results of a user study in which we collect a random sample of ratings from
current users of an online radio service. An analysis of the rating data
collected in the study shows that the sample of random ratings has markedly
different properties than ratings of user-selected songs. When asked to report
on their own rating behaviour, a large number of users indicate they believe
their opinion of a song does affect whether they choose to rate that song, a
violation of the MAR condition. Finally, we present experimental results
showing that incorporating an explicit model of the missing data mechanism can
lead to significant improvements in prediction performance on the random sample
of ratings.
"
"  We present a geometric formulation of the Multiple Kernel Learning (MKL)
problem. To do so, we reinterpret the problem of learning kernel weights as
searching for a kernel that maximizes the minimum (kernel) distance between two
convex polytopes. This interpretation combined with novel structural insights
from our geometric formulation allows us to reduce the MKL problem to a simple
optimization routine that yields provable convergence as well as quality
guarantees. As a result our method scales efficiently to much larger data sets
than most prior methods can handle. Empirical evaluation on eleven datasets
shows that we are significantly faster and even compare favorably with a
uniform unweighted combination of kernels.
"
"  This work addresses the following question: Under what assumptions on the
data generating process can one infer the causal graph from the joint
distribution? The approach taken by conditional independence-based causal
discovery methods is based on two assumptions: the Markov condition and
faithfulness. It has been shown that under these assumptions the causal graph
can be identified up to Markov equivalence (some arrows remain undirected)
using methods like the PC algorithm. In this work we propose an alternative by
defining Identifiable Functional Model Classes (IFMOCs). As our main theorem we
prove that if the data generating process belongs to an IFMOC, one can identify
the complete causal graph. To the best of our knowledge this is the first
identifiability result of this kind that is not limited to linear functional
relationships. We discuss how the IFMOC assumption and the Markov and
faithfulness assumptions relate to each other and explain why we believe that
the IFMOC assumption can be tested more easily on given data. We further
provide a practical algorithm that recovers the causal graph from finitely many
data; experiments on simulated data support the theoretical findings.
"
"  The secular stellar mass-loss causes an amplification of the orbital
separation in fragile, common proper motion, binary systems with separations of
the order of 1000 A.U. In these systems, companions evolve as two independent
coeval stars as they experience negligible mutual tidal interactions or mass
transfer. We present models for how post-main sequence mass-loss statistically
distorts the frequency distribution of separations in fragile binaries. These
models demonstrate the expected increase in orbital seapration resulting from
stellar mass-loss, as well as a perturbation of associated orbital parameters.
Comparisons between our models and observations resulting from the Luyten
survey of wide visual binaries, specifically those containing MS and
white-dwarf pairs, demonstrate a good agreement between the calculated and the
observed angular separation distribution functions.
"
"  This paper examines the effectiveness of a sparse Bayesian algorithm to
estimate multivariate autoregressive coefficients when a large amount of
background interference exists. This paper employs computer experiments to
compare two methods in the source-space causality analysis: the conventional
least-squares method and a sparse Bayesian method. Results of our computer
experiments show that the interference affects the least-squares method in a
very severe manner. It produces large false-positive results, unless the
signal-to-interference ratio is very high. On the other hand, the sparse
Bayesian method is relatively insensitive to the existence of interference.
However, this robustness of the sparse Bayesian method is attained on the
scarifies of the detectability of true causal relationship. Our experiments
also show that the surrogate data bootstrapping method tends to give a
statistical threshold that are too low for the sparse method.
  The permutation-test-based method gives a higher (more conservative)
threshold and it should be used with the sparse Bayesian method whenever the
control period is available.
"
"  We study a generalized framework for structured sparsity. It extends the
well-known methods of Lasso and Group Lasso by incorporating additional
constraints on the variables as part of a convex optimization problem. This
framework provides a straightforward way of favouring prescribed sparsity
patterns, such as orderings, contiguous regions and overlapping groups, among
others. Existing optimization methods are limited to specific constraint sets
and tend to not scale well with sample size and dimensionality. We propose a
novel first order proximal method, which builds upon results on fixed points
and successive approximations. The algorithm can be applied to a general class
of conic and norm constraints sets and relies on a proximity operator
subproblem which can be computed explicitly. Experiments on different
regression problems demonstrate the efficiency of the optimization algorithm
and its scalability with the size of the problem. They also demonstrate state
of the art statistical performance, which improves over Lasso and StructOMP.
"
"  This paper applies machine learning techniques to student modeling. It
presents a method for discovering high-level student behaviors from a very
large set of low-level traces corresponding to problem-solving actions in a
learning environment. Basic actions are encoded into sets of domain-dependent
attribute-value patterns called cases. Then a domain-independent hierarchical
clustering identifies what we call general attitudes, yielding automatic
diagnosis expressed in natural language, addressed in principle to teachers.
The method can be applied to individual students or to entire groups, like a
class. We exhibit examples of this system applied to thousands of students'
actions in the domain of algebraic transformations.
"
"  A common problem in astrophysics is determining how bright a source could be
and still not be detected. Despite the simplicity with which the problem can be
stated, the solution involves complex statistical issues that require careful
analysis. In contrast to the confidence bound, this concept has never been
formally analyzed, leading to a great variety of often ad hoc solutions. Here
we formulate and describe the problem in a self-consistent manner. Detection
significance is usually defined by the acceptable proportion of false positives
(the TypeI error), and we invoke the complementary concept of false negatives
(the TypeII error), based on the statistical power of a test, to compute an
upper limit to the detectable source intensity. To determine the minimum
intensity that a source must have for it to be detected, we first define a
detection threshold, and then compute the probabilities of detecting sources of
various intensities at the given threshold. The intensity that corresponds to
the specified TypeII error probability defines that minimum intensity, and is
identified as the upper limit. Thus, an upper limit is a characteristic of the
detection procedure rather than the strength of any particular source and
should not be confused with confidence intervals or other estimates of source
intensity. This is particularly important given the large number of catalogs
that are being generated from increasingly sensitive surveys. We discuss the
differences between these upper limits and confidence bounds. Both measures are
useful quantities that should be reported in order to extract the most science
from catalogs, though they answer different statistical questions: an upper
bound describes an inference range on the source intensity, while an upper
limit calibrates the detection process. We provide a recipe for computing upper
limits that applies to all detection algorithms.
"
"  In this paper, we address the problem of identifying protein functionality
using the information contained in its aminoacid sequence. We propose a method
to define sequence similarity relationships that can be used as input for
classification and clustering via well known metric based statistical methods.
In our examples, we specifically address two problems of supervised and
unsupervised learning in structural genomics via simple metric based techniques
on the space of trees 1)Unsupervised detection of functionality families via K
means clustering in the space of trees, 2)Classification of new proteins into
known families via k nearest neighbour trees. We found evidence that the
similarity measure induced by our approach concentrates information for
discrimination. Classification has the same high performance than others VLMC
approaches. Clustering is a harder task, though, but our approach for
clustering is alignment free and automatic, and may lead to many interesting
variations by choosing other clustering or classification procedures that are
based on pre-computed similarity information, as the ones that performs
clustering using flow simulation, see (Yona et al 2000, Enright et al, 2003).
"
"  We address the sparse signal recovery problem in the context of multiple
measurement vectors (MMV) when elements in each nonzero row of the solution
matrix are temporally correlated. Existing algorithms do not consider such
temporal correlations and thus their performance degrades significantly with
the correlations. In this work, we propose a block sparse Bayesian learning
framework which models the temporal correlations. In this framework we derive
two sparse Bayesian learning (SBL) algorithms, which have superior recovery
performance compared to existing algorithms, especially in the presence of high
temporal correlations. Furthermore, our algorithms are better at handling
highly underdetermined problems and require less row-sparsity on the solution
matrix. We also provide analysis of the global and local minima of their cost
function, and show that the SBL cost function has the very desirable property
that the global minimum is at the sparsest solution to the MMV problem.
Extensive experiments also provide some interesting results that motivate
future theoretical research on the MMV model.
"
"  Independent component analysis (ICA) has been widely used for blind source
separation in many fields such as brain imaging analysis, signal processing and
telecommunication. Many statistical techniques based on M-estimates have been
proposed for estimating the mixing matrix. Recently, several nonparametric
methods have been developed, but in-depth analysis of asymptotic efficiency has
not been available. We analyze ICA using semiparametric theories and propose a
straightforward estimate based on the efficient score function by using
B-spline approximations. The estimate is asymptotically efficient under
moderate conditions and exhibits better performance than standard ICA methods
in a variety of simulations.
"
"  Discussion of ""Treelets--An adaptive multi-scale basis for sparse unordered
data"" [arXiv:0707.0481]
"
"  Although living organisms are affected by many interrelated and unidentified
variables, this complexity does not automatically impose a fundamental
limitation on statistical inference. Nor need one invoke such complexity as an
explanation of the ""Truth Wears Off"" or ""decline"" effect; similar ""decline""
effects occur with far simpler systems studied in physics. Selective reporting
and publication bias, and scientists' biases in favour of reporting
eye-catching results (in general) or conforming to others' results (in physics)
better explain this feature of the ""Truth Wears Off"" effect than Rabin's
suggested limitation on statistical inference.
"
"  In this paper, we consider the problem of preserving privacy in the online
learning setting. We study the problem in the online convex programming (OCP)
framework---a popular online learning setting with several interesting
theoretical and practical implications---while using differential privacy as
the formal privacy measure. For this problem, we distill two critical
attributes that a private OCP algorithm should have in order to provide
reasonable privacy as well as utility guarantees: 1) linearly decreasing
sensitivity, i.e., as new data points arrive their effect on the learning model
decreases, 2) sub-linear regret bound---regret bound is a popular
goodness/utility measure of an online learning algorithm.
  Given an OCP algorithm that satisfies these two conditions, we provide a
general framework to convert the given algorithm into a privacy preserving OCP
algorithm with good (sub-linear) regret. We then illustrate our approach by
converting two popular online learning algorithms into their differentially
private variants while guaranteeing sub-linear regret ($O(\sqrt{T})$). Next, we
consider the special case of online linear regression problems, a practically
important class of online learning problems, for which we generalize an
approach by Dwork et al. to provide a differentially private algorithm with
just $O(\log^{1.5} T)$ regret. Finally, we show that our online learning
framework can be used to provide differentially private algorithms for offline
learning as well. For the offline learning problem, our approach obtains better
error bounds as well as can handle larger class of problems than the existing
state-of-the-art methods Chaudhuri et al.
"
"  We consider the most common variants of linear regression, including Ridge,
Lasso and Support-vector regression, in a setting where the learner is allowed
to observe only a fixed number of attributes of each example at training time.
We present simple and efficient algorithms for these problems: for Lasso and
Ridge regression they need the same total number of attributes (up to
constants) as do full-information algorithms, for reaching a certain accuracy.
For Support-vector regression, we require exponentially less attributes
compared to the state of the art. By that, we resolve an open problem recently
posed by Cesa-Bianchi et al. (2010). Experiments show the theoretical bounds to
be justified by superior performance compared to the state of the art.
"
"  In this note we illustrate and develop further with mathematics and examples,
the work on successive standardization (or normalization) that is studied
earlier by the same authors in Olshen and Rajaratnam (2010) and Olshen and
Rajaratnam (2011). Thus, we deal with successive iterations applied to
rectangular arrays of numbers, where to avoid technical difficulties an array
has at least three rows and at least three columns. Without loss, an iteration
begins with operations on columns: first subtract the mean of each column; then
divide by its standard deviation. The iteration continues with the same two
operations done successively for rows. These four operations applied in
sequence completes one iteration. One then iterates again, and again, and
again,.... In Olshen and Rajaratnam (2010) it was argued that if arrays are
made up of real numbers, then the set for which convergence of these successive
iterations fails has Lebesgue measure 0. The limiting array has row and column
means 0, row and column standard deviations 1. A basic result on convergence
given in Olshen and Rajaratnam (2010) is true, though the argument in Olshen
and Rajaratnam (2010) is faulty. The result is stated in the form of a theorem
here, and the argument for the theorem is correct. Moreover, many graphics
given in Olshen and Rajaratnam (2010) suggest that but for a set of entries of
any array with Lebesgue measure 0, convergence is very rapid, eventually
exponentially fast in the number of iterations. Because we learned this set of
rules from Bradley Efron, we call it ""Efron's algorithm"". More importantly, the
rapidity of convergence is illustrated by numerical examples.
"
"  In this paper, a novel learning paradigm is presented to automatically
identify groups of informative and correlated features from very high
dimensions. Specifically, we explicitly incorporate correlation measures as
constraints and then propose an efficient embedded feature selection method
using recently developed cutting plane strategy. The benefits of the proposed
algorithm are two-folds. First, it can identify the optimal discriminative and
uncorrelated feature subset to the output labels, denoted here as Support
Features, which brings about significant improvements in prediction performance
over other state of the art feature selection methods considered in the paper.
Second, during the learning process, the underlying group structures of
correlated features associated with each support feature, denoted as Affiliated
Features, can also be discovered without any additional cost. These affiliated
features serve to improve the interpretations on the learning tasks. Extensive
empirical studies on both synthetic and very high dimensional real-world
datasets verify the validity and efficiency of the proposed method.
"
"  We study the theoretical advantages of active learning over passive learning.
Specifically, we prove that, in noise-free classifier learning for VC classes,
any passive learning algorithm can be transformed into an active learning
algorithm with asymptotically strictly superior label complexity for all
nontrivial target functions and distributions. We further provide a general
characterization of the magnitudes of these improvements in terms of a novel
generalization of the disagreement coefficient. We also extend these results to
active learning in the presence of label noise, and find that even under broad
classes of noise distributions, we can typically guarantee strict improvements
over the known results for passive learning.
"
"  Generative models for graphs have been typically committed to strong prior
assumptions concerning the form of the modeled distributions. Moreover, the
vast majority of currently available models are either only suitable for
characterizing some particular network properties (such as degree distribution
or clustering coefficient), or they are aimed at estimating joint probability
distributions, which is often intractable in large-scale networks. In this
paper, we first propose a novel network statistic, based on the Laplacian
spectrum of graphs, which allows to dispense with any parametric assumption
concerning the modeled network properties. Second, we use the defined statistic
to develop the Fiedler random graph model, switching the focus from the
estimation of joint probability distributions to a more tractable conditional
estimation setting. After analyzing the dependence structure characterizing
Fiedler random graphs, we evaluate them experimentally in edge prediction over
several real-world networks, showing that they allow to reach a much higher
prediction accuracy than various alternative statistical models.
"
"  We present a simple and fast geometric method for modeling data by a union of
affine subspaces. The method begins by forming a collection of local best-fit
affine subspaces, i.e., subspaces approximating the data in local
neighborhoods. The correct sizes of the local neighborhoods are determined
automatically by the Jones' $\beta_2$ numbers (we prove under certain geometric
conditions that our method finds the optimal local neighborhoods). The
collection of subspaces is further processed by a greedy selection procedure or
a spectral method to generate the final model. We discuss applications to
tracking-based motion segmentation and clustering of faces under different
illuminating conditions. We give extensive experimental evidence demonstrating
the state of the art accuracy and speed of the suggested algorithms on these
problems and also on synthetic hybrid linear data as well as the MNIST
handwritten digits data; and we demonstrate how to use our algorithms for fast
determination of the number of affine subspaces.
"
"  We consider the problem of simultaneously learning to linearly combine a very
large number of kernels and learn a good predictor based on the learnt kernel.
When the number of kernels $d$ to be combined is very large, multiple kernel
learning methods whose computational cost scales linearly in $d$ are
intractable. We propose a randomized version of the mirror descent algorithm to
overcome this issue, under the objective of minimizing the group $p$-norm
penalized empirical risk. The key to achieve the required exponential speed-up
is the computationally efficient construction of low-variance estimates of the
gradient. We propose importance sampling based estimates, and find that the
ideal distribution samples a coordinate with a probability proportional to the
magnitude of the corresponding gradient. We show the surprising result that in
the case of learning the coefficients of a polynomial kernel, the combinatorial
structure of the base kernels to be combined allows the implementation of
sampling from this distribution to run in $O(\log(d))$ time, making the total
computational cost of the method to achieve an $\epsilon$-optimal solution to
be $O(\log(d)/\epsilon^2)$, thereby allowing our method to operate for very
large values of $d$. Experiments with simulated and real data confirm that the
new algorithm is computationally more efficient than its state-of-the-art
alternatives.
"
"  MaxEnt's variational principle, in conjunction with Shannon's logarithmic
information measure, yields only exponential functional forms in
straightforward fashion. In this communication we show how to overcome this
limitation via the incorporation, into the variational process, of suitable
dynamical information. As a consequence, we are able to formulate a somewhat
generalized Shannonian Maximum Entropy approach which provides a unifying
""thermodynamic-like"" explanation for the scale-invariant phenomena observed in
social contexts, as city-population distributions. We confirm the MaxEnt
predictions by means of numerical experiments with random walkers, and compare
them with some empirical data.
"
"  This paper considers the problem of matrix completion when some number of the
columns are completely and arbitrarily corrupted, potentially by a malicious
adversary. It is well-known that standard algorithms for matrix completion can
return arbitrarily poor results, if even a single column is corrupted. One
direct application comes from robust collaborative filtering. Here, some number
of users are so-called manipulators who try to skew the predictions of the
algorithm by calibrating their inputs to the system. In this paper, we develop
an efficient algorithm for this problem based on a combination of a trimming
procedure and a convex program that minimizes the nuclear norm and the
$\ell_{1,2}$ norm. Our theoretical results show that given a vanishing fraction
of observed entries, it is nevertheless possible to complete the underlying
matrix even when the number of corrupted columns grows. Significantly, our
results hold without any assumptions on the locations or values of the observed
entries of the manipulated columns. Moreover, we show by an
information-theoretic argument that our guarantees are nearly optimal in terms
of the fraction of sampled entries on the authentic columns, the fraction of
corrupted columns, and the rank of the underlying matrix. Our results therefore
sharply characterize the tradeoffs between sample, robustness and rank in
matrix completion.
"
"  Discussion of ``Statistical analysis of an archeological find'' by Andrey
Feuerverger [arXiv:0804.0079]
"
"  Graphical Gaussian models have proven to be useful tools for exploring
network structures based on multivariate data. Applications to studies of gene
expression have generated substantial interest in these models, and resulting
recent progress includes the development of fitting methodology involving
penalization of the likelihood function. In this paper we advocate the use of
multivariate $t$-distributions for more robust inference of graphs. In
particular, we demonstrate that penalized likelihood inference combined with an
application of the EM algorithm provides a computationally efficient approach
to model selection in the $t$-distribution case. We consider two versions of
multivariate $t$-distributions, one of which requires the use of approximation
techniques. For this distribution, we describe a Markov chain Monte Carlo EM
algorithm based on a Gibbs sampler as well as a simple variational
approximation that makes the resulting method feasible in large problems.
"
"  Studying the topology of so-called {\em real networks}, that is networks
obtained from sociological or biological data for instance, has become a major
field of interest in the last decade. One way to deal with it is to consider
that networks are built from small functional units called {\em motifs}, which
can be found by looking for small subgraphs whose numbers of occurrences in the
whole network of interest are surprisingly high. In this paper, we propose to
define motifs through a local over-representation in the network and develop a
statistic which allows us to detect them limiting the number of false positives
and without time-consuming simulations. We apply it to the Yeast gene
interaction data and show that the known biologically relevant motifs are found
again and that our method gives some more information than the existing ones.
"
"  We address the problem of minimizing a convex function over the space of
large matrices with low rank. While this optimization problem is hard in
general, we propose an efficient greedy algorithm and derive its formal
approximation guarantees. Each iteration of the algorithm involves
(approximately) finding the left and right singular vectors corresponding to
the largest singular value of a certain matrix, which can be calculated in
linear time. This leads to an algorithm which can scale to large matrices
arising in several applications such as matrix completion for collaborative
filtering and robust low rank matrix approximation.
"
"  Security issues are crucial in a number of machine learning applications,
especially in scenarios dealing with human activity rather than natural
phenomena (e.g., information ranking, spam detection, malware detection, etc.).
It is to be expected in such cases that learning algorithms will have to deal
with manipulated data aimed at hampering decision making. Although some
previous work addressed the handling of malicious data in the context of
supervised learning, very little is known about the behavior of anomaly
detection methods in such scenarios. In this contribution we analyze the
performance of a particular method -- online centroid anomaly detection -- in
the presence of adversarial noise. Our analysis addresses the following
security-related issues: formalization of learning and attack processes,
derivation of an optimal attack, analysis of its efficiency and constraints. We
derive bounds on the effectiveness of a poisoning attack against centroid
anomaly under different conditions: bounded and unbounded percentage of
traffic, and bounded false positive rate. Our bounds show that whereas a
poisoning attack can be effectively staged in the unconstrained case, it can be
made arbitrarily difficult (a strict upper bound on the attacker's gain) if
external constraints are properly used. Our experimental evaluation carried out
on real HTTP and exploit traces confirms the tightness of our theoretical
bounds and practicality of our protection mechanisms.
"
"  Parameter estimates for associated genetic variants, report ed in the initial
discovery samples, are often grossly inflated compared to the values observed
in the follow-up replication samples. This type of bias is a consequence of the
sequential procedure in which the estimated effect of an associated genetic
marker must first pass a stringent significance threshold. We propose a
hierarchical Bayes method in which a spike-and-slab prior is used to account
for the possibility that the significant test result may be due to chance. We
examine the robustness of the method using different priors corresponding to
different degrees of confidence in the testing results and propose a Bayesian
model averaging procedure to combine estimates produced by different models.
The Bayesian estimators yield smaller variance compared to the conditional
likelihood estimator and outperform the latter in studies with low power. We
investigate the performance of the method with simulations and applications to
four real data examples.
"
"  We describe a novel binary classification technique called Banded SVM
(B-SVM). In the standard C-SVM formulation of Cortes et al. (1995), the
decision rule is encouraged to lie in the interval [1, \infty]. The new B-SVM
objective function contains a penalty term that encourages the decision rule to
lie in a user specified range [\rho_1, \rho_2]. In addition to the standard set
of support vectors (SVs) near the class boundaries, B-SVM results in a second
set of SVs in the interior of each class.
"
"  We consider a class of finite Markov moment problems with arbitrary number of
positive and negative branches. We show criteria for the existence and
uniqueness of solutions, and we characterize in detail the non-unique solution
families. Moreover, we present a constructive algorithm to solve the moment
problems numerically and prove that the algorithm computes the right solution.
"
"  There is widespread emphasis on reform in the teaching of introductory
statistics at the college level. Underpinning this reform is a consensus among
educators and practitioners that traditional curricular materials and
pedagogical strategies have not been effective in promoting statistical
literacy, a competency that is becoming increasingly necessary for effective
decision-making and evidence-based practice. This paper explains the historical
context of, and rationale for reform-oriented teaching of introductory
statistics (at the college level) in the health, social and behavioral sciences
(evidence-based disciplines). A firm understanding and appreciation of the
basis for change in pedagogical approach is important, in order to facilitate
commitment to reform, consensus building on appropriate strategies, and
adoption and maintenance of best practices. In essence, reform-oriented
pedagogy, in this context, is a function of the interaction among content,
pedagogy, technology, and assessment. The challenge is to create an appropriate
balance among these domains.
"
"  Network sampling is integral to the analysis of social, information, and
biological networks. Since many real-world networks are massive in size,
continuously evolving, and/or distributed in nature, the network structure is
often sampled in order to facilitate study. For these reasons, a more thorough
and complete understanding of network sampling is critical to support the field
of network science. In this paper, we outline a framework for the general
problem of network sampling, by highlighting the different objectives,
population and units of interest, and classes of network sampling methods. In
addition, we propose a spectrum of computational models for network sampling
methods, ranging from the traditionally studied model based on the assumption
of a static domain to a more challenging model that is appropriate for
streaming domains. We design a family of sampling methods based on the concept
of graph induction that generalize across the full spectrum of computational
models (from static to streaming) while efficiently preserving many of the
topological properties of the input graphs. Furthermore, we demonstrate how
traditional static sampling algorithms can be modified for graph streams for
each of the three main classes of sampling methods: node, edge, and
topology-based sampling. Our experimental results indicate that our proposed
family of sampling methods more accurately preserves the underlying properties
of the graph for both static and streaming graphs. Finally, we study the impact
of network sampling algorithms on the parameter estimation and performance
evaluation of relational classification algorithms.
"
"  The spatial dependence of total column ozone varies strongly with latitude,
so that homogeneous models (invariant to all rotations) are clearly unsuitable.
However, an assumption of axial symmetry, which means that the process model is
invariant to rotations about the Earth's axis, is much more plausible and
considerably simplifies the modeling. Using TOMS (Total Ozone Mapping
Spectrometer) measurements of total column ozone over a six-day period, this
work investigates the modeling of axially symmetric processes on the sphere
using expansions in spherical harmonics. It turns out that one can capture many
of the large scale features of the spatial covariance structure using a
relatively small number of terms in such an expansion, but the resulting fitted
model provides a horrible fit to the data when evaluated via its likelihood
because of its inability to describe accurately the process's local behavior.
Thus, there remains the challenge of developing computationally tractable
models that capture both the large and small scale structure of these data.
"
"  Scene understanding remains a significant challenge in the computer vision
community. The visual psychophysics literature has demonstrated the importance
of interdependence among parts of the scene. Yet, the majority of methods in
computer vision remain local. Pictorial structures have arisen as a fundamental
parts-based model for some vision problems, such as articulated object
detection. However, the form of classical pictorial structures limits their
applicability for global problems, such as semantic pixel labeling. In this
paper, we propose an extension of the pictorial structures approach, called
pixel-support parts-sparse pictorial structures, or PS3, to overcome this
limitation. Our model extends the classical form in two ways: first, it defines
parts directly based on pixel-support rather than in a parametric form, and
second, it specifies a space of plausible parts-based scene models and permits
one to be used for inference on any given image. PS3 makes strides toward
unifying object-level and pixel-level modeling of scene elements. In this
report, we implement the first half of our model and rely upon external
knowledge to provide an initial graph structure for a given image. Our
experimental results on benchmark datasets demonstrate the capability of this
new parts-based view of scene modeling.
"
"  Due to the growing ubiquity of unlabeled data, learning with unlabeled data
is attracting increasing attention in machine learning. In this paper, we
propose a novel semi-supervised kernel learning method which can seamlessly
combine manifold structure of unlabeled data and Regularized Least-Squares
(RLS) to learn a new kernel. Interestingly, the new kernel matrix can be
obtained analytically with the use of spectral decomposition of graph Laplacian
matrix. Hence, the proposed algorithm does not require any numerical
optimization solvers. Moreover, by maximizing kernel target alignment on
labeled data, we can also learn model parameters automatically with a
closed-form solution. For a given graph Laplacian matrix, our proposed method
does not need to tune any model parameter including the tradeoff parameter in
RLS and the balance parameter for unlabeled data. Extensive experiments on ten
benchmark datasets show that our proposed two-stage parameter-free spectral
kernel learning algorithm can obtain comparable performance with fine-tuned
manifold regularization methods in transductive setting, and outperform
multiple kernel learning in supervised setting.
"
"  We characterize conjugate nonparametric Bayesian models as projective limits
of conjugate, finite-dimensional Bayesian models. In particular, we identify a
large class of nonparametric models representable as infinite-dimensional
analogues of exponential family distributions and their canonical conjugate
priors. This class contains most models studied in the literature, including
Dirichlet processes and Gaussian process regression models. To derive these
results, we introduce a representation of infinite-dimensional Bayesian models
by projective limits of regular conditional probabilities. We show under which
conditions the nonparametric model itself, its sufficient statistics, and -- if
they exist -- conjugate updates of the posterior are projective limits of their
respective finite-dimensional counterparts. We illustrate our results both by
application to existing nonparametric models and by construction of a model on
infinite permutations.
"
"  Here we perform a statistical analysis of the official data from recent
Russian parliamentary and presidential elections (held on December 4th, 2011
and March 4th, 2012, respectively). A number of anomalies are identified that
persistently skew the results in favour of the pro-government party, United
Russia (UR), and its leader Vladimir Putin. The main irregularities are: (i)
remarkably high correlation between turnout and voting results; (ii) a large
number of polling stations where the UR/Putin results are given by a round
number of percent; (iii) constituencies showing improbably low or (iv)
anomalously high dispersion of results across polling stations; (v) substantial
difference between results at paper-based and electronic polling stations.
These anomalies, albeit less prominent in the presidential elections, hardly
conform to the assumptions of fair and free voting. The approaches proposed
here can be readily extended to quantify fingerprints of electoral fraud in any
other problematic elections.
"
"  We consider an important class of signal processing problems where the signal
of interest is known to be sparse, and can be recovered from data given
auxiliary information about how the data was generated. For example, a sparse
Green's function may be recovered from seismic experimental data using sparsity
optimization when the source signature is known. Unfortunately, in practice
this information is often missing, and must be recovered from data along with
the signal using deconvolution techniques.
  In this paper, we present a novel methodology to simultaneously solve for the
sparse signal and auxiliary parameters using a recently proposed variable
projection technique. Our main contribution is to combine variable projection
with sparsity promoting optimization, obtaining an efficient algorithm for
large-scale sparse deconvolution problems. We demonstrate the algorithm on a
seismic imaging example.
"
"  Culturomics was recently introduced as the application of high-throughput
data collection and analysis to the study of human culture. Here we make use of
this data by investigating fluctuations in yearly usage frequencies of specific
words that describe social and natural phenomena, as derived from books that
were published over the course of the past two centuries. We show that the
determination of the Hurst parameter by means of fractal analysis provides
fundamental insights into the nature of long-range correlations contained in
the culturomic trajectories, and by doing so, offers new interpretations as to
what might be the main driving forces behind the examined phenomena. Quite
remarkably, we find that social and natural phenomena are governed by
fundamentally different processes. While natural phenomena have properties that
are typical for processes with persistent long-range correlations, social
phenomena are better described as nonstationary, on-off intermittent, or Levy
walk processes.
"
"  Collaborative filtering (CF) aims to predict users' ratings on items
according to historical user-item preference data. In many real-world
applications, preference data are usually sparse, which would make models
overfit and fail to give accurate predictions. Recently, several research works
show that by transferring knowledge from some manually selected source domains,
the data sparseness problem could be mitigated. However for most cases, parts
of source domain data are not consistent with the observations in the target
domain, which may misguide the target domain model building. In this paper, we
propose a novel criterion based on empirical prediction error and its variance
to better capture the consistency across domains in CF settings. Consequently,
we embed this criterion into a boosting framework to perform selective
knowledge transfer. Comparing to several state-of-the-art methods, we show that
our proposed selective transfer learning framework can significantly improve
the accuracy of rating prediction tasks on several real-world recommendation
tasks.
"
"  Android and Facebook provide third-party applications with access to users'
private data and the ability to perform potentially sensitive operations (e.g.,
post to a user's wall or place phone calls). As a security measure, these
platforms restrict applications' privileges with permission systems: users must
approve the permissions requested by applications before the applications can
make privacy- or security-relevant API calls. However, recent studies have
shown that users often do not understand permission requests and lack a notion
of typicality of requests. As a first step towards simplifying permission
systems, we cluster a corpus of 188,389 Android applications and 27,029
Facebook applications to find patterns in permission requests. Using a method
for Boolean matrix factorization for finding overlapping clusters, we find that
Facebook permission requests follow a clear structure that exhibits high
stability when fitted with only five clusters, whereas Android applications
demonstrate more complex permission requests. We also find that low-reputation
applications often deviate from the permission request patterns that we
identified for high-reputation applications suggesting that permission request
patterns are indicative for user satisfaction or application quality.
"
"  We generalise the problem of inverse reinforcement learning to multiple
tasks, from multiple demonstrations. Each one may represent one expert trying
to solve a different task, or as different experts trying to solve the same
task. Our main contribution is to formalise the problem as statistical
preference elicitation, via a number of structured priors, whose form captures
our biases about the relatedness of different tasks or expert policies. In
doing so, we introduce a prior on policy optimality, which is more natural to
specify. We show that our framework allows us not only to learn to efficiently
from multiple experts but to also effectively differentiate between the goals
of each. Possible applications include analysing the intrinsic motivations of
subjects in behavioural experiments and learning from multiple teachers.
"
"  The problem of detecting terms that can be interesting to the advertiser is
considered. If a company has already bought some advertising terms which
describe certain services, it is reasonable to find out the terms bought by
competing companies. A part of them can be recommended as future advertising
terms to the company. The goal of this work is to propose better interpretable
recommendations based on FCA and association rules.
"
"  We consider learning continuous probabilistic graphical models in the face of
missing data. For non-Gaussian models, learning the parameters and structure of
such models depends on our ability to perform efficient inference, and can be
prohibitive even for relatively modest domains. Recently, we introduced the
Copula Bayesian Network (CBN) density model - a flexible framework that
captures complex high-dimensional dependency structures while offering direct
control over the univariate marginals, leading to improved generalization. In
this work we show that the CBN model also offers significant computational
advantages when training data is partially observed. Concretely, we leverage on
the specialized form of the model to derive a computationally amenable learning
objective that is a lower bound on the log-likelihood function. Importantly,
our energy-like bound circumvents the need for costly inference of an auxiliary
distribution, thus facilitating practical learning of highdimensional
densities. We demonstrate the effectiveness of our approach for learning the
structure and parameters of a CBN model for two reallife continuous domains.
"
"  Epsilon-machines are minimal, unifilar presentations of stationary stochastic
processes. They were originally defined in the history machine sense, as hidden
Markov models whose states are the equivalence classes of infinite pasts with
the same probability distribution over futures. In analyzing synchronization,
though, an alternative generator definition was given: unifilar, edge-emitting
hidden Markov models with probabilistically distinct states. The key difference
is that history epsilon-machines are defined by a process, whereas generator
epsilon-machines define a process. We show here that these two definitions are
equivalent in the finite-state case.
"
"  This paper presents new results for the (partial) maximum a posteriori (MAP)
problem in Bayesian networks, which is the problem of querying the most
probable state configuration of some of the network variables given evidence.
First, it is demonstrated that the problem remains hard even in networks with
very simple topology, such as binary polytrees and simple trees (including the
Naive Bayes structure). Such proofs extend previous complexity results for the
problem. Inapproximability results are also derived in the case of trees if the
number of states per variable is not bounded. Although the problem is shown to
be hard and inapproximable even in very simple scenarios, a new exact algorithm
is described that is empirically fast in networks of bounded treewidth and
bounded number of states per variable. The same algorithm is used as basis of a
Fully Polynomial Time Approximation Scheme for MAP under such assumptions.
Approximation schemes were generally thought to be impossible for this problem,
but we show otherwise for classes of networks that are important in practice.
The algorithms are extensively tested using some well-known networks as well as
random generated cases to show their effectiveness.
"
"  We propose the use of a new false discovery rate (FDR) controlling procedure
as a model selection penalized method, and compare its performance to that of
other penalized methods over a wide range of realistic settings: nonorthogonal
design matrices, moderate and large pool of explanatory variables, and both
sparse and nonsparse models, in the sense that they may include a small and
large fraction of the potential variables (and even all). The comparison is
done by a comprehensive simulation study, using a quantitative framework for
performance comparisons in the form of empirical minimaxity relative to a
""random oracle"": the oracle model selection performance on data dependent
forward selected family of potential models. We show that FDR based procedures
have good performance, and in particular the newly proposed method, emerges as
having empirical minimax performance. Interestingly, using FDR level of 0.05 is
a global best.
"
"  Despite great interest in solving RNA secondary structures due to their
impact on function, it remains an open problem to determine structure from
sequence. Among experimental approaches, a promising candidate is the ""chemical
modification strategy"", which involves application of chemicals to RNA that are
sensitive to structure and that result in modifications that can be assayed via
sequencing technologies. One approach that can reveal paired nucleotides via
chemical modification followed by sequencing is SHAPE, and it has been used in
conjunction with capillary electrophoresis (SHAPE-CE) and high-throughput
sequencing (SHAPE-Seq). The solution of mathematical inverse problems is needed
to relate the sequence data to the modified sites, and a number of approaches
have been previously suggested for SHAPE-CE, and separately for SHAPE-Seq
analysis. Here we introduce a new model for inference of chemical modification
experiments, whose formulation results in closed-form maximum likelihood
estimates that can be easily applied to data. The model can be specialized to
both SHAPE-CE and SHAPE-Seq, and therefore allows for a direct comparison of
the two technologies. We then show that the extra information obtained with
SHAPE-Seq but not with SHAPE-CE is valuable with respect to ML estimation.
"
"  One of the basic tasks for Bayesian networks (BNs) is that of learning a
network structure from data. The BN-learning problem is NP-hard, so the
standard solution is heuristic search. Many approaches have been proposed for
this task, but only a very small number outperform the baseline of greedy
hill-climbing with tabu lists; moreover, many of the proposed algorithms are
quite complex and hard to implement. In this paper, we propose a very simple
and easy-to-implement method for addressing this task. Our approach is based on
the well-known fact that the best network (of bounded in-degree) consistent
with a given node ordering can be found very efficiently. We therefore propose
a search not over the space of structures, but over the space of orderings,
selecting for each ordering the best network consistent with it. This search
space is much smaller, makes more global search steps, has a lower branching
factor, and avoids costly acyclicity checks. We present results for this
algorithm on both synthetic and real data sets, evaluating both the score of
the network found and in the running time. We show that ordering-based search
outperforms the standard baseline, and is competitive with recent algorithms
that are much harder to implement.
"
"  In an increasing number of applications, it is of interest to recover an
approximately low-rank data matrix from noisy observations. This paper develops
an unbiased risk estimate---holding in a Gaussian model---for any spectral
estimator obeying some mild regularity assumptions. In particular, we give an
unbiased risk estimate formula for singular value thresholding (SVT), a popular
estimation strategy which applies a soft-thresholding rule to the singular
values of the noisy observations. Among other things, our formulas offer a
principled and automated way of selecting regularization parameters in a
variety of problems. In particular, we demonstrate the utility of the unbiased
risk estimation for SVT-based denoising of real clinical cardiac MRI series
data. We also give new results concerning the differentiability of certain
matrix-valued functions.
"
"  Predicting historic temperatures based on tree rings, ice cores, and other
natural proxies is a difficult endeavor. The relationship between proxies and
temperature is weak and the number of proxies is far larger than the number of
target data points. Furthermore, the data contain complex spatial and temporal
dependence structures which are not easily captured with simple models. In this
paper, we assess the reliability of such reconstructions and their statistical
significance against various null models. We find that the proxies do not
predict temperature significantly better than random series generated
independently of temperature. Furthermore, various model specifications that
perform similarly at predicting temperature produce extremely different
historical backcasts. Finally, the proxies seem unable to forecast the high
levels of and sharp run-up in temperature in the 1990s either in-sample or from
contiguous holdout blocks, thus casting doubt on their ability to predict such
phenomena if in fact they occurred several hundred years ago. We propose our
own reconstruction of Northern Hemisphere average annual land temperature over
the last millennium, assess its reliability, and compare it to those from the
climate science literature. Our model provides a similar reconstruction but has
much wider standard errors, reflecting the weak signal and large uncertainty
encountered in this setting.
"
"  BDeu marginal likelihood score is a popular model selection criterion for
selecting a Bayesian network structure based on sample data. This
non-informative scoring criterion assigns same score for network structures
that encode same independence statements. However, before applying the BDeu
score, one must determine a single parameter, the equivalent sample size alpha.
Unfortunately no generally accepted rule for determining the alpha parameter
has been suggested. This is disturbing, since in this paper we show through a
series of concrete experiments that the solution of the network structure
optimization problem is highly sensitive to the chosen alpha parameter value.
Based on these results, we are able to give explanations for how and why this
phenomenon happens, and discuss ideas for solving this problem.
"
"  The fundamental building block of social influence is for one person to
elicit a response in another. Researchers measuring a ""response"" in social
media typically depend either on detailed models of human behavior or on
platform-specific cues such as re-tweets, hash tags, URLs, or mentions. Most
content on social networks is difficult to model because the modes and
motivation of human expression are diverse and incompletely understood. We
introduce content transfer, an information-theoretic measure with a predictive
interpretation that directly quantifies the strength of the effect of one
user's content on another's in a model-free way. Estimating this measure is
made possible by combining recent advances in non-parametric entropy estimation
with increasingly sophisticated tools for content representation. We
demonstrate on Twitter data collected for thousands of users that content
transfer is able to capture non-trivial, predictive relationships even for
pairs of users not linked in the follower or mention graph. We suggest that
this measure makes large quantities of previously under-utilized social media
content accessible to rigorous statistical causal analysis.
"
"  We propose a Bayesian expectation-maximization (EM) algorithm for
reconstructing Markov-tree sparse signals via belief propagation. The
measurements follow an underdetermined linear model where the
regression-coefficient vector is the sum of an unknown approximately sparse
signal and a zero-mean white Gaussian noise with an unknown variance. The
signal is composed of large- and small-magnitude components identified by
binary state variables whose probabilistic dependence structure is described by
a Markov tree. Gaussian priors are assigned to the signal coefficients given
their state variables and the Jeffreys' noninformative prior is assigned to the
noise variance. Our signal reconstruction scheme is based on an EM iteration
that aims at maximizing the posterior distribution of the signal and its state
variables given the noise variance. We construct the missing data for the EM
iteration so that the complete-data posterior distribution corresponds to a
hidden Markov tree (HMT) probabilistic graphical model that contains no loops
and implement its maximization (M) step via a max-product algorithm. This EM
algorithm estimates the vector of state variables as well as solves iteratively
a linear system of equations to obtain the corresponding signal estimate. We
select the noise variance so that the corresponding estimated signal and state
variables obtained upon convergence of the EM iteration have the largest
marginal posterior distribution. We compare the proposed and existing
state-of-the-art reconstruction methods via signal and image reconstruction
experiments.
"
"  We introduce {\em vector diffusion maps} (VDM), a new mathematical framework
for organizing and analyzing massive high dimensional data sets, images and
shapes. VDM is a mathematical and algorithmic generalization of diffusion maps
and other non-linear dimensionality reduction methods, such as LLE, ISOMAP and
Laplacian eigenmaps. While existing methods are either directly or indirectly
related to the heat kernel for functions over the data, VDM is based on the
heat kernel for vector fields. VDM provides tools for organizing complex data
sets, embedding them in a low dimensional space, and interpolating and
regressing vector fields over the data. In particular, it equips the data with
a metric, which we refer to as the {\em vector diffusion distance}. In the
manifold learning setup, where the data set is distributed on (or near) a low
dimensional manifold $\MM^d$ embedded in $\RR^{p}$, we prove the relation
between VDM and the connection-Laplacian operator for vector fields over the
manifold.
"
"  Joint alignment of a collection of functions is the process of independently
transforming the functions so that they appear more similar to each other.
Typically, such unsupervised alignment algorithms fail when presented with
complex data sets arising from multiple modalities or make restrictive
assumptions about the form of the functions or transformations, limiting their
generality. We present a transformed Bayesian infinite mixture model that can
simultaneously align and cluster a data set. Our model and associated learning
scheme offer two key advantages: the optimal number of clusters is determined
in a data-driven fashion through the use of a Dirichlet process prior, and it
can accommodate any transformation function parameterized by a continuous
parameter vector. As a result, it is applicable to a wide range of data types,
and transformation functions. We present positive results on synthetic
two-dimensional data, on a set of one-dimensional curves, and on various image
data sets, showing large improvements over previous work. We discuss several
variations of the model and conclude with directions for future work.
"
"  Accumulation of standardized data collections is opening up novel
opportunities for holistic characterization of genome function. The limited
scalability of current preprocessing techniques has, however, formed a
bottleneck for full utilization of contemporary microarray collections. While
short oligonucleotide arrays constitute a major source of genome-wide profiling
data, scalable probe-level preprocessing algorithms have been available only
for few measurement platforms based on pre-calculated model parameters from
restricted reference training sets. To overcome these key limitations, we
introduce a fully scalable online-learning algorithm that provides tools to
process large microarray atlases including tens of thousands of arrays. Unlike
the alternatives, the proposed algorithm scales up in linear time with respect
to sample size and is readily applicable to all short oligonucleotide
platforms. This is the only available preprocessing algorithm that can learn
probe-level parameters based on sequential hyperparameter updates at small,
consecutive batches of data, thus circumventing the extensive memory
requirements of the standard approaches and opening up novel opportunities to
take full advantage of contemporary microarray data collections. Moreover,
using the most comprehensive data collections to estimate probe-level effects
can assist in pinpointing individual probes affected by various biases and
provide new tools to guide array design and quality control. The implementation
is freely available in R/Bioconductor at
http://www.bioconductor.org/packages/devel/bioc/html/RPA.html
"
"  We study the problem of identifying the top $m$ arms in a multi-armed bandit
game. Our proposed solution relies on a new algorithm based on successive
rejects of the seemingly bad arms, and successive accepts of the good ones.
This algorithmic contribution allows to tackle other multiple identifications
settings that were previously out of reach. In particular we show that this
idea of successive accepts and rejects applies to the multi-bandit best arm
identification problem.
"
"  The problem of joint feature selection across a group of related tasks has
applications in many areas including biomedical informatics and computer
vision. We consider the l2,1-norm regularized regression model for joint
feature selection from multiple tasks, which can be derived in the
probabilistic framework by assuming a suitable prior from the exponential
family. One appealing feature of the l2,1-norm regularization is that it
encourages multiple predictors to share similar sparsity patterns. However, the
resulting optimization problem is challenging to solve due to the
non-smoothness of the l2,1-norm regularization. In this paper, we propose to
accelerate the computation by reformulating it as two equivalent smooth convex
optimization problems which are then solved via the Nesterov's method-an
optimal first-order black-box method for smooth convex optimization. A key
building block in solving the reformulations is the Euclidean projection. We
show that the Euclidean projection for the first reformulation can be
analytically computed, while the Euclidean projection for the second one can be
computed in linear time. Empirical evaluations on several data sets verify the
efficiency of the proposed algorithms.
"
"  In the social sciences, there is a longstanding tension between data
collection methods that facilitate quantification and those that are open to
unanticipated information. Advances in technology now enable new, hybrid
methods that combine some of the benefits of both approaches. Drawing
inspiration from online information aggregation systems like Wikipedia and from
traditional survey research, we propose a new class of research instruments
called wiki surveys. Just as Wikipedia evolves over time based on contributions
from participants, we envision an evolving survey driven by contributions from
respondents. We develop three general principles that underlie wiki surveys:
they should be greedy, collaborative, and adaptive. Building on these
principles, we develop methods for data collection and data analysis for one
type of wiki survey, a pairwise wiki survey. Using two proof-of-concept case
studies involving our free and open-source website www.allourideas.org, we show
that pairwise wiki surveys can yield insights that would be difficult to obtain
with other methods.
"
"  Comparisons of DNA sequences between Neandertals and present-day humans have
shown that Neandertals share more genetic variants with non-Africans than with
Africans. This could be due to interbreeding between Neandertals and modern
humans when the two groups met subsequent to the emergence of modern humans
outside Africa. However, it could also be due to population structure that
antedates the origin of Neandertal ancestors in Africa. We measure the extent
of linkage disequilibrium (LD) in the genomes of present-day Europeans and find
that the last gene flow from Neandertals (or their relatives) into Europeans
likely occurred 37,000-86,000 years before the present (BP), and most likely
47,000-65,000 years ago. This supports the recent interbreeding hypothesis, and
suggests that interbreeding may have occurred when modern humans carrying Upper
Paleolithic technologies encountered Neandertals as they expanded out of
Africa.
"
"  It is now widely accepted that forensic DNA profiles are rare, so it was a
surprise to some people that different people represented in offender databases
are being found to have the same profile. In the first place this is just an
illustration of the birthday problem, but a deeper analysis must take into
account dependencies among profiles caused by family or population membership.
"
"  We analyze distributions of rain-event sizes, rain-event durations, and
dry-spell durations for data obtained from a network of 20 rain gauges
scattered in a region of the NW Mediterranean coast. While power-law
distributions model the dry-spell durations with a common exponent 1.50 +-
0.05, density analysis is inconclusive for event sizes and event durations, due
to finite size effects. However, we present alternative evidence of the
existence of scale invariance in these distributions by means of different data
collapses of the distributions. These results are in agreement with the
expectations from the Self-Organized Criticality paradigm, and demonstrate that
scaling properties of rain events and dry spells can also be observed for
medium resolution rain data.
"
"  We investigate relations between best selling artists in last decade on
phonographic market and from perspective of listeners by using the Social
Network Analyzes. Starting network is obtained from the matrix of correlations
between the world's best selling artists by considering the synchronous time
evolution of weekly record sales. This method reveals the structure of
phonographic market, but we claim that it has no impact on people who see
relationship between artists and music genres. We compare 'sale' (based on
correlation of record sales) or 'popularity' (based on data mining of the
record charts) networks with 'similarity' (obtained mainly from survey within
music experts opinion) and find no significant relations. We postulate that
non-laminar phenomena on this specific market introduce turbulence to how
people view relations of artists.
"
"  Inference of hidden classes in stochastic block model is a classical problem
with important applications. Most commonly used methods for this problem
involve na\""{\i}ve mean field approaches or heuristic spectral methods.
Recently, belief propagation was proposed for this problem. In this
contribution we perform a comparative study between the three methods on
synthetically created networks. We show that belief propagation shows much
better performance when compared to na\""{\i}ve mean field and spectral
approaches. This applies to accuracy, computational efficiency and the tendency
to overfit the data.
"
"  Leo Breiman was a unique character. There will not be another like him. I
consider it one of my great fortunes in life to have know and worked with him.
Along with John Tukey, Leo had the greatest influence on shaping my approach to
statistical problems. I did some of my best work collaborating with Leo, but
more importantly, we both had great fun doing it. I look back on those years
when we worked closely together with great fondness and regard them as among
the happiest and most fruitful of my professional career.
"
"  Learning invariant representations is an important problem in machine
learning and pattern recognition. In this paper, we present a novel framework
of transformation-invariant feature learning by incorporating linear
transformations into the feature learning algorithms. For example, we present
the transformation-invariant restricted Boltzmann machine that compactly
represents data by its weights and their transformations, which achieves
invariance of the feature representation via probabilistic max pooling. In
addition, we show that our transformation-invariant feature learning framework
can also be extended to other unsupervised learning methods, such as
autoencoders or sparse coding. We evaluate our method on several image
classification benchmark datasets, such as MNIST variations, CIFAR-10, and
STL-10, and show competitive or superior classification performance when
compared to the state-of-the-art. Furthermore, our method achieves
state-of-the-art performance on phone classification tasks with the TIMIT
dataset, which demonstrates wide applicability of our proposed algorithms to
other domains.
"
"  Sparse coding is a common approach to learning local features for object
recognition. Recently, there has been an increasing interest in learning
features from spatio-temporal, binocular, or other multi-observation data,
where the goal is to encode the relationship between images rather than the
content of a single image. We provide an analysis of multi-view feature
learning, which shows that hidden variables encode transformations by detecting
rotation angles in the eigenspaces shared among multiple image warps. Our
analysis helps explain recent experimental results showing that
transformation-specific features emerge when training complex cell models on
videos. Our analysis also shows that transformation-invariant features can
emerge as a by-product of learning representations of transformations.
"
"  The short-term periodicities of the daily sunspot area fluctuations from
August 1923 to October 1933 are discussed. For these data the correlative
analysis indicates negative correlation for the periodicity of about 155 days,
but the power spectrum analysis indicates a statistically significant peak in
this time interval. A new method of the diagnosis of an echo-effect in spectrum
is proposed and it is stated that the 155-day periodicity is a harmonic of the
periodicities from the interval of [400,500] days.
  The autocorrelation functions for the daily sunspot area fluctuations and for
the fluctuations of the one rotation time interval in the northern hemisphere,
separately for the whole solar cycle 16 and for the maximum activity period of
this cycle do not show differences, especially in the interval of [57, 173]
days. It proves against the thesis of the existence of strong positive
fluctuations of the about 155-day interval in the maximum activity period of
the solar cycle 16 in the northern hemisphere. However, a similar analysis for
data from the southern hemisphere indicates that there is the periodicity of
about 155 days in sunspot area data in the maximum activity period of the cycle
16 only.
"
"  The use of bonus-malus systems in compulsory liability automobile insurance
is a worldwide applied method for premium pricing. If certain assumptions hold,
like the conditional Poisson distribution of the policyholders claim number,
then an interesting task is to evaluate the so called claims frequency of the
individuals. Here we introduce 3 techniques, two is based on the bonus-malus
class, and the third based on claims history. The article is devoted to choose
the method, which fits to the frequency parameters the best for certain input
parameters. For measuring the goodness-of-fit we will use scores, similar to
better known divergence measures. The detailed method is also suitable to
compare bonus-malus systems in the sense that how much information they contain
about drivers.
"
"  Kernel-based online learning has often shown state-of-the-art performance for
many online learning tasks. It, however, suffers from a major shortcoming, that
is, the unbounded number of support vectors, making it non-scalable and
unsuitable for applications with large-scale datasets. In this work, we study
the problem of bounded kernel-based online learning that aims to constrain the
number of support vectors by a predefined budget. Although several algorithms
have been proposed in literature, they are neither computationally efficient
due to their intensive budget maintenance strategy nor effective due to the use
of simple Perceptron algorithm. To overcome these limitations, we propose a
framework for bounded kernel-based online learning based on an online gradient
descent approach. We propose two efficient algorithms of bounded online
gradient descent (BOGD) for scalable kernel-based online learning: (i) BOGD by
maintaining support vectors using uniform sampling, and (ii) BOGD++ by
maintaining support vectors using non-uniform sampling. We present theoretical
analysis of regret bound for both algorithms, and found promising empirical
performance in terms of both efficacy and efficiency by comparing them to
several well-known algorithms for bounded kernel-based online learning on
large-scale datasets.
"
"  Discussion of ``Statistical analysis of an archeological find'' by Andrey
Feuerverger [arXiv:0804.0079]
"
"  This perspective chapter briefly surveys: (1) past growth in the use of
Bayesian methods in astrophysics; (2) current misconceptions about both
frequentist and Bayesian statistical inference that hinder wider adoption of
Bayesian methods by astronomers; and (3) multilevel (hierarchical) Bayesian
modeling as a major future direction for research in Bayesian astrostatistics,
exemplified in part by presentations at the first ISI invited session on
astrostatistics, commemorated in this volume. It closes with an intentionally
provocative recommendation for astronomical survey data reporting, motivated by
the multilevel Bayesian perspective on modeling cosmic populations: that
astronomers cease producing catalogs of estimated fluxes and other source
properties from surveys. Instead, summaries of likelihood functions (or
marginal likelihood functions) for source properties should be reported (not
posterior probability density functions), including nontrivial summaries (not
simply upper limits) for candidate objects that do not pass traditional
detection thresholds.
"
"  We show that the Bregman divergence provides a rich framework to estimate
unnormalized statistical models for continuous or discrete random variables,
that is, models which do not integrate or sum to one, respectively. We prove
that recent estimation methods such as noise-contrastive estimation, ratio
matching, and score matching belong to the proposed framework, and explain
their interconnection based on supervised learning. Further, we discuss the
role of boosting in unsupervised learning.
"
"  We present a joint copula-based model for insurance claims and sizes. It uses
bivariate copulae to accommodate for the dependence between these quantities.
We derive the general distribution of the policy loss without the restrictive
assumption of independence. We illustrate that this distribution tends to be
skewed and multi-modal, and that an independence assumption can lead to
substantial bias in the estimation of the policy loss. Further, we extend our
framework to regression models by combining marginal generalized linear models
with a copula. We show that this approach leads to a flexible class of models,
and that the parameters can be estimated efficiently using maximum-likelihood.
We propose a test procedure for the selection of the optimal copula family. The
usefulness of our approach is illustrated in a simulation study and in an
analysis of car insurance policies.
"
"  We provide fast algorithms for overconstrained $\ell_p$ regression and
related problems: for an $n\times d$ input matrix $A$ and vector
$b\in\mathbb{R}^n$, in $O(nd\log n)$ time we reduce the problem
$\min_{x\in\mathbb{R}^d} \|Ax-b\|_p$ to the same problem with input matrix
$\tilde A$ of dimension $s \times d$ and corresponding $\tilde b$ of dimension
$s\times 1$. Here, $\tilde A$ and $\tilde b$ are a coreset for the problem,
consisting of sampled and rescaled rows of $A$ and $b$; and $s$ is independent
of $n$ and polynomial in $d$. Our results improve on the best previous
algorithms when $n\gg d$, for all $p\in[1,\infty)$ except $p=2$. We also
provide a suite of improved results for finding well-conditioned bases via
ellipsoidal rounding, illustrating tradeoffs between running time and
conditioning quality, including a one-pass conditioning algorithm for general
$\ell_p$ problems.
  We also provide an empirical evaluation of implementations of our algorithms
for $p=1$, comparing them with related algorithms. Our empirical results show
that, in the asymptotic regime, the theory is a very good guide to the
practical performance of these algorithms. Our algorithms use our faster
constructions of well-conditioned bases for $\ell_p$ spaces and, for $p=1$, a
fast subspace embedding of independent interest that we call the Fast Cauchy
Transform: a distribution over matrices $\Pi:\mathbb{R}^n\mapsto
\mathbb{R}^{O(d\log d)}$, found obliviously to $A$, that approximately
preserves the $\ell_1$ norms: that is, with large probability, simultaneously
for all $x$, $\|Ax\|_1 \approx \|\Pi Ax\|_1$, with distortion $O(d^{2+\eta})$,
for an arbitrarily small constant $\eta>0$; and, moreover, $\Pi A$ can be
computed in $O(nd\log d)$ time. The techniques underlying our Fast Cauchy
Transform include fast Johnson-Lindenstrauss transforms, low-coherence
matrices, and rescaling by Cauchy random variables.
"
"  We address the issue of estimating the regression vector $\beta$ in the
generic $s$-sparse linear model $y = X\beta+z$, with $\beta\in\R^{p}$,
$y\in\R^{n}$, $z\sim\mathcal N(0,\sg^2 I)$ and $p> n$ when the variance
$\sg^{2}$ is unknown. We study two LASSO-type methods that jointly estimate
$\beta$ and the variance. These estimators are minimizers of the $\ell_1$
penalized least-squares functional, where the relaxation parameter is tuned
according to two different strategies. In the first strategy, the relaxation
parameter is of the order $\ch{\sigma} \sqrt{\log p}$, where $\ch{\sigma}^2$ is
the empirical variance. %The resulting optimization problem can be solved by
running only a few successive LASSO instances with %recursive updating of the
relaxation parameter. In the second strategy, the relaxation parameter is
chosen so as to enforce a trade-off between the fidelity and the penalty terms
at optimality. For both estimators, our assumptions are similar to the ones
proposed by Cand\`es and Plan in {\it Ann. Stat. (2009)}, for the case where
$\sg^{2}$ is known. We prove that our estimators ensure exact recovery of the
support and sign pattern of $\beta$ with high probability. We present
simulations results showing that the first estimator enjoys nearly the same
performances in practice as the standard LASSO (known variance case) for a wide
range of the signal to noise ratio. Our second estimator is shown to outperform
both in terms of false detection, when the signal to noise ratio is low.
"
"  The exploration-exploitation trade-off is among the central challenges of
reinforcement learning. The optimal Bayesian solution is intractable in
general. This paper studies to what extent analytic statements about optimal
learning are possible if all beliefs are Gaussian processes. A first order
approximation of learning of both loss and dynamics, for nonlinear,
time-varying systems in continuous time and space, subject to a relatively weak
restriction on the dynamics, is described by an infinite-dimensional partial
differential equation. An approximate finite-dimensional projection gives an
impression for how this result may be helpful.
"
"  We propose a new integrated phase I/II trial design to identify the most
efficacious dose combination that also satisfies certain safety requirements
for drug-combination trials. We first take a Bayesian copula-type model for
dose finding in phase I. After identifying a set of admissible doses, we
immediately move the entire set forward to phase II. We propose a novel
adaptive randomization scheme to favor assigning patients to more efficacious
dose-combination arms. Our adaptive randomization scheme takes into account
both the point estimate and variability of efficacy. By using a moving
reference to compare the relative efficacy among treatment arms, our method
achieves a high resolution to distinguish different arms. We also consider
groupwise adaptive randomization when efficacy is late-onset. We conduct
extensive simulation studies to examine the operating characteristics of the
proposed design, and illustrate our method using a phase I/II melanoma clinical
trial.
"
"  In order to interpret and explain the physiological signal behaviors, it can
be interesting to find some constants among the fluctuations of these data
during all the effort or during different stages of the race (which can be
detected using a change points detection method). Several recent papers have
proposed the long-range dependence (Hurst) parameter as such a constant.
However, their results induce two main problems. Firstly, DFA method is usually
applied for estimating this parameter. Clearly, such a method does not provide
the most efficient estimator and moreover it is not at all robust even in the
case of smooth trends. Secondly, this method often gives estimated Hurst
parameters larger than 1, which is the larger possible value for long memory
stationary processes. In this article we propose solutions for both these
problems and we define a new model allowing such estimated parameters.
"
"  Distributions over permutations arise in applications ranging from
multi-object tracking to ranking of instances. The difficulty of dealing with
these distributions is caused by the size of their domain, which is factorial
in the number of considered entities ($n!$). It makes the direct definition of
a multinomial distribution over permutation space impractical for all but a
very small $n$. In this work we propose an embedding of all $n!$ permutations
for a given $n$ in a surface of a hypersphere defined in
$\mathbbm{R}^{(n-1)^2}$. As a result of the embedding, we acquire ability to
define continuous distributions over a hypersphere with all the benefits of
directional statistics. We provide polynomial time projections between the
continuous hypersphere representation and the $n!$-element permutation space.
The framework provides a way to use continuous directional probability
densities and the methods developed thereof for establishing densities over
permutations. As a demonstration of the benefits of the framework we derive an
inference procedure for a state-space model over permutations. We demonstrate
the approach with applications.
"
"  Existing Bayesian models, especially nonparametric Bayesian methods, rely on
specially conceived priors to incorporate domain knowledge for discovering
improved latent representations. While priors can affect posterior
distributions through Bayes' rule, imposing posterior regularization is
arguably more direct and in some cases more natural and general. In this paper,
we present regularized Bayesian inference (RegBayes), a novel computational
framework that performs posterior inference with a regularization term on the
desired post-data posterior distribution under an information theoretical
formulation. RegBayes is more flexible than the procedure that elicits expert
knowledge via priors, and it covers both directed Bayesian networks and
undirected Markov networks whose Bayesian formulation results in hybrid chain
graph models. When the regularization is induced from a linear operator on the
posterior distributions, such as the expectation operator, we present a general
convex-analysis theorem to characterize the solution of RegBayes. Furthermore,
we present two concrete examples of RegBayes, infinite latent support vector
machines (iLSVM) and multi-task infinite latent support vector machines
(MT-iLSVM), which explore the large-margin idea in combination with a
nonparametric Bayesian model for discovering predictive latent features for
classification and multi-task learning, respectively. We present efficient
inference methods and report empirical studies on several benchmark datasets,
which appear to demonstrate the merits inherited from both large-margin
learning and Bayesian nonparametrics. Such results were not available until
now, and contribute to push forward the interface between these two important
subfields, which have been largely treated as isolated in the community.
"
"  In this paper, we derive a novel probabilistic model of boosting as a Product
of Experts. We re-derive the boosting algorithm as a greedy incremental model
selection procedure which ensures that addition of new experts to the ensemble
does not decrease the likelihood of the data. These learning rules lead to a
generic boosting algorithm - POE- Boost which turns out to be similar to the
AdaBoost algorithm under certain assumptions on the expert probabilities. The
paper then extends the POEBoost algorithm to POEBoost.CS which handles
hypothesis that produce probabilistic predictions. This new algorithm is shown
to have better generalization performance compared to other state of the art
algorithms.
"
"  Gaussian graphical models are often used to infer gene networks based on
microarray expression data. Many scientists, however, have begun using
high-throughput sequencing technologies to measure gene expression. As the
resulting high-dimensional count data consists of counts of sequencing reads
for each gene, Gaussian graphical models are not optimal for modeling gene
networks based on this discrete data. We develop a novel method for estimating
high-dimensional Poisson graphical models, the Log-Linear Graphical Model,
allowing us to infer networks based on high-throughput sequencing data. Our
model assumes a pair-wise Markov property: conditional on all other variables,
each variable is Poisson. We estimate our model locally via neighborhood
selection by fitting 1-norm penalized log-linear models. Additionally, we
develop a fast parallel algorithm, an approach we call the Poisson Graphical
Lasso, permitting us to fit our graphical model to high-dimensional genomic
data sets. In simulations, we illustrate the effectiveness of our methods for
recovering network structure from count data. A case study on breast cancer
microRNAs, a novel application of graphical models, finds known regulators of
breast cancer genes and discovers novel microRNA clusters and hubs that are
targets for future research.
"
"  In this paper we study a bootstrap strategy for estimating the variance of a
mean taken over large multifactor crossed random effects data sets. We apply
bootstrap reweighting independently to the levels of each factor, giving each
observation the product of independently sampled factor weights. No exact
bootstrap exists for this problem [McCullagh (2000) Bernoulli 6 285-301]. We
show that the proposed bootstrap is mildly conservative, meaning biased toward
overestimating the variance, under sufficient conditions that allow very
unbalanced and heteroscedastic inputs. Earlier results for a resampling
bootstrap only apply to two factors and use multinomial weights that are poorly
suited to online computation. The proposed reweighting approach can be
implemented in parallel and online settings. The results for this method apply
to any number of factors. The method is illustrated using a 3 factor data set
of comment lengths from Facebook.
"
"  We propose a family of statistical models for social network evolution over
time, which represents an extension of Exponential Random Graph Models (ERGMs).
Many of the methods for ERGMs are readily adapted for these models, including
maximum likelihood estimation algorithms. We discuss models of this type and
their properties, and give examples, as well as a demonstration of their use
for hypothesis testing and classification. We believe our temporal ERG models
represent a useful new framework for modeling time-evolving social networks,
and rewiring networks from other domains such as gene regulation circuitry, and
communication networks.
"
"  Dynamic treatment regimes are of growing interest across the clinical
sciences as these regimes provide one way to operationalize and thus inform
sequential personalized clinical decision making. A dynamic treatment regime is
a sequence of decision rules, with a decision rule per stage of clinical
intervention; each decision rule maps up-to-date patient information to a
recommended treatment. We briefly review a variety of approaches for using data
to construct the decision rules. We then review an interesting challenge, that
of nonregularity that often arises in this area. By nonregularity, we mean the
parameters indexing the optimal dynamic treatment regime are nonsmooth
functionals of the underlying generative distribution.
  A consequence is that no regular or asymptotically unbiased estimator of
these parameters exists. Nonregularity arises in inference for parameters in
the optimal dynamic treatment regime; we illustrate the effect of nonregularity
on asymptotic bias and via sensitivity of asymptotic, limiting, distributions
to local perturbations. We propose and evaluate a locally consistent Adaptive
Confidence Interval (ACI) for the parameters of the optimal dynamic treatment
regime. We use data from the Adaptive Interventions for Children with ADHD
study as an illustrative example. We conclude by highlighting and discussing
emerging theoretical problems in this area.
"
"  Collective classification models attempt to improve classification
performance by taking into account the class labels of related instances.
However, they tend not to learn patterns of interactions between classes and/or
make the assumption that instances of the same class link to each other
(assortativity assumption). Blockmodels provide a solution to these issues,
being capable of modelling assortative and disassortative interactions, and
learning the pattern of interactions in the form of a summary network. The
Supervised Blockmodel provides good classification performance using link
structure alone, whilst simultaneously providing an interpretable summary of
network interactions to allow a better understanding of the data. This work
explores three variants of supervised blockmodels of varying complexity and
tests them on four structurally different real world networks.
"
"  We propose in this work a new family of kernels for variable-length time
series. Our work builds upon the vector autoregressive (VAR) model for
multivariate stochastic processes: given a multivariate time series x, we
consider the likelihood function p_{\theta}(x) of different parameters \theta
in the VAR model as features to describe x. To compare two time series x and
x', we form the product of their features p_{\theta}(x) p_{\theta}(x') which is
integrated out w.r.t \theta using a matrix normal-inverse Wishart prior. Among
other properties, this kernel can be easily computed when the dimension d of
the time series is much larger than the lengths of the considered time series x
and x'. It can also be generalized to time series taking values in arbitrary
state spaces, as long as the state space itself is endowed with a kernel
\kappa. In that case, the kernel between x and x' is a a function of the Gram
matrices produced by \kappa on observations and subsequences of observations
enumerated in x and x'. We describe a computationally efficient implementation
of this generalization that uses low-rank matrix factorization techniques.
These kernels are compared to other known kernels using a set of benchmark
classification tasks carried out with support vector machines.
"
"  We present a comprehensive framework for structured sparse coding and
modeling extending the recent ideas of using learnable fast regressors to
approximate exact sparse codes. For this purpose, we develop a novel
block-coordinate proximal splitting method for the iterative solution of
hierarchical sparse coding problems, and show an efficient feed forward
architecture derived from its iteration. This architecture faithfully
approximates the exact structured sparse codes with a fraction of the
complexity of the standard optimization methods. We also show that by using
different training objective functions, learnable sparse encoders are no longer
restricted to be mere approximants of the exact sparse code for a pre-given
dictionary, as in earlier formulations, but can be rather used as full-featured
sparse encoders or even modelers. A simple implementation shows several orders
of magnitude speedup compared to the state-of-the-art at minimal performance
degradation, making the proposed framework suitable for real time and
large-scale applications.
"
"  In biospectroscopy, suitably annotated and statistically independent samples
(e. g. patients, batches, etc.) for classifier training and testing are scarce
and costly. Learning curves show the model performance as function of the
training sample size and can help to determine the sample size needed to train
good classifiers. However, building a good model is actually not enough: the
performance must also be proven. We discuss learning curves for typical small
sample size situations with 5 - 25 independent samples per class. Although the
classification models achieve acceptable performance, the learning curve can be
completely masked by the random testing uncertainty due to the equally limited
test sample size. In consequence, we determine test sample sizes necessary to
achieve reasonable precision in the validation and find that 75 - 100 samples
will usually be needed to test a good but not perfect classifier. Such a data
set will then allow refined sample size planning on the basis of the achieved
performance. We also demonstrate how to calculate necessary sample sizes in
order to show the superiority of one classifier over another: this often
requires hundreds of statistically independent test samples or is even
theoretically impossible. We demonstrate our findings with a data set of ca.
2550 Raman spectra of single cells (five classes: erythrocytes, leukocytes and
three tumour cell lines BT-20, MCF-7 and OCI-AML3) as well as by an extensive
simulation that allows precise determination of the actual performance of the
models in question.
"
"  We consider an original problem that arises from the issue of security
analysis of a power system and that we name optimal discovery with
probabilistic expert advice. We address it with an algorithm based on the
optimistic paradigm and on the Good-Turing missing mass estimator. We prove two
different regret bounds on the performance of this algorithm under weak
assumptions on the probabilistic experts. Under more restrictive hypotheses, we
also prove a macroscopic optimality result, comparing the algorithm both with
an oracle strategy and with uniform sampling. Finally, we provide numerical
experiments illustrating these theoretical findings.
"
"  Discussion of ``Statistical analysis of an archeological find'' by Andrey
Feuerverger [arXiv:0804.0079]
"
"  We explore the use of generalized t priors on regression coefficients to help
understand the nature of association signal within ""hit regions"" of genome-wide
association studies. The particular generalized t distribution we adopt is a
Student distribution on the absolute value of its argument. For low degrees of
freedom we show that the generalized t exhibits 'sparsity-prior' properties
with some attractive features over other common forms of sparse priors and
includes the well known double-exponential distribution as the degrees of
freedom tends to infinity. We pay particular attention to graphical
representations of posterior statistics obtained from sparsity-path-analysis
(SPA) where we sweep over the setting of the scale (shrinkage / precision)
parameter in the prior to explore the space of posterior models obtained over a
range of complexities, from very sparse models with all coefficient
distributions heavily concentrated around zero, to models with diffuse priors
and coefficients distributed around their maximum likelihood estimates. The SPA
plots are akin to LASSO plots of maximum a posteriori (MAP) estimates but they
characterise the complete marginal posterior distributions of the coefficients
plotted as a function of the precision of the prior. Generating posterior
distributions over a range of prior precisions is computationally challenging
but naturally amenable to sequential Monte Carlo (SMC) algorithms indexed on
the scale parameter. We show how SMC simulation on graphic-processing-units
(GPUs) provides very efficient inference for SPA. We also present a
scale-mixture representation of the generalized t prior that leads to an EM
algorithm to obtain MAP estimates should only these be required.
"
"  Aircraft engines are designed to be used during several tens of years. Their
maintenance is a challenging and costly task, for obvious security reasons. The
goal is to ensure a proper operation of the engines, in all conditions, with a
zero probability of failure, while taking into account aging. The fact that the
same engine is sometimes used on several aircrafts has to be taken into account
too. The maintenance can be improved if an efficient procedure for the
prediction of failures is implemented. The primary source of information on the
health of the engines comes from measurement during flights. Several variables
such as the core speed, the oil pressure and quantity, the fan speed, etc. are
measured, together with environmental variables such as the outside
temperature, altitude, aircraft speed, etc. In this paper, we describe the
design of a procedure aiming at visualizing successive data measured on
aircraft engines. The data are multi-dimensional measurements on the engines,
which are projected on a self-organizing map in order to allow us to follow the
trajectories of these data over time. The trajectories consist in a succession
of points on the map, each of them corresponding to the two-dimensional
projection of the multi-dimensional vector of engine measurements. Analyzing
the trajectories aims at visualizing any deviation from a normal behavior,
making it possible to anticipate an operation failure.
"
"  With the widespread availability of satellite-based instruments, many
geophysical processes are measured on a global scale and they often show strong
nonstationarity in the covariance structure. In this paper we present a
flexible class of parametric covariance models that can capture the
nonstationarity in global data, especially strong dependency of covariance
structure on latitudes. We apply the Discrete Fourier Transform to data on
regular grids, which enables us to calculate the exact likelihood for large
data sets. Our covariance model is applied to global total column ozone level
data on a given day. We discuss how our covariance model compares with some
existing models.
"
"  This paper develops a Bayesian procedure for estimation and forecasting of
the volatility of multivariate time series. The foundation of this work is the
matrix-variate dynamic linear model, for the volatility of which we adopt a
multiplicative stochastic evolution, using Wishart and singular multivariate
beta distributions. A diagonal matrix of discount factors is employed in order
to discount the variances element by element and therefore allowing a flexible
and pragmatic variance modelling approach. Diagnostic tests and sequential
model monitoring are discussed in some detail. The proposed estimation theory
is applied to a four-dimensional time series, comprising spot prices of
aluminium, copper, lead and zinc of the London metal exchange. The empirical
findings suggest that the proposed Bayesian procedure can be effectively
applied to financial data, overcoming many of the disadvantages of existing
volatility models.
"
"  Owners of a web-site are often interested in analysis of groups of users of
their site. Information on these groups can help optimizing the structure and
contents of the site. In this paper we use an approach based on formal concepts
for constructing taxonomies of user groups. For decreasing the huge amount of
concepts that arise in applications, we employ stability index of a concept,
which describes how a group given by a concept extent differs from other such
groups. We analyze resulting taxonomies of user groups for three target
websites.
"
"  K-nearest neighbors (KNN) method is used in many supervised learning
classification problems. Potential Energy (PE) method is also developed for
classification problems based on its physical metaphor. The energy potential
used in the experiments are Yukawa potential and Gaussian Potential. In this
paper, I use both applet and MATLAB program with real life benchmark data to
analyze the performances of KNN and PE method in classification problems. The
results show that in general, KNN and PE methods have similar performance. In
particular, PE with Yukawa potential has worse performance than KNN when the
density of the data is higher in the distribution of the database. When the
Gaussian potential is applied, the results from PE and KNN have similar
behavior. The indicators used are correlation coefficients and information
gain.
"
"  For statistical analysis of functional Magnetic Resonance Imaging (fMRI) data
sets, we propose a data-driven approach based on Independent Component Analysis
(ICA) implemented in a new version of the AnalyzeFMRI R package. For fMRI data
sets, spatial dimension being much greater than temporal dimension, spatial ICA
is the tractable approach generally proposed. However, for some neuroscientific
applications, temporal independence of source signals can be assumed and
temporal ICA becomes then an attracting exploratory technique. In this work, we
use a classical linear algebra result ensuring the tractability of temporal
ICA. We report several experiments on synthetic data and real MRI data sets
that demonstrate the potential interest of our R package.
"
"  We present a system and a set of techniques for learning linear predictors
with convex losses on terascale datasets, with trillions of features, {The
number of features here refers to the number of non-zero entries in the data
matrix.} billions of training examples and millions of parameters in an hour
using a cluster of 1000 machines. Individually none of the component techniques
are new, but the careful synthesis required to obtain an efficient
implementation is. The result is, up to our knowledge, the most scalable and
efficient linear learning system reported in the literature (as of 2011 when
our experiments were conducted). We describe and thoroughly evaluate the
components of the system, showing the importance of the various design choices.
"
"  We consider decentralized detection through distributed sensors that perform
level-triggered sampling and communicate with a fusion center via noisy
channels. Each sensor computes its local log-likelihood ratio (LLR), samples it
using the level-triggered sampling, and upon sampling transmits a single bit to
the FC. Upon receiving a bit from a sensor, the FC updates the global LLR and
performs a sequential probability ratio test (SPRT) step. We derive the fusion
rules under various types of channels. We further provide an asymptotic
analysis on the average detection delay for the proposed channel-aware scheme,
and show that the asymptotic detection delay is characterized by a KL
information number. The delay analysis facilitates the choice of appropriate
signaling schemes under different channel types for sending the 1-bit
information from sensors to the FC.
"
"  A method of `network filtering' has been proposed recently to detect the
effects of certain external perturbations on the interacting members in a
network. However, with large networks, the goal of detection seems a priori
difficult to achieve, especially since the number of observations available
often is much smaller than the number of variables describing the effects of
the underlying network. Under the assumption that the network possesses a
certain sparsity property, we provide a formal characterization of the accuracy
with which the external effects can be detected, using a network filtering
system that combines Lasso regression in a sparse simultaneous equation model
with simple residual analysis. We explore the implications of the technical
conditions underlying our characterization, in the context of various network
topologies, and we illustrate our method using simulated data.
"
"  Population dynamic of getting divorced depends on many global factors,
including social norms, economy, law or demographics as well as individual
factors like the level of interpersonal or problem-solving skills of the
spouses. We sought to find such a relationship incorporating only quantitative
variables and test theoretical model considering phase transition between
coupling (pairs) and free (single) preferential states as a function of social
and economic. The analyzed data has been collected by UN across almost all the
countries since 1948. Our first approach is followed by Bouchaud's model of
social network of opinions, which works well with dynamics of fertility rates
in postwar Europe. Unfortunately, we postulate that this pure sociological and
pure economic approach fail in general. Thus, we did some observation about why
it went wrong and where economy (e. g. Poland) or law (e. g. Portugal) has
bigger impact on getting divorce than social pressure.
"
"  We study the problem of multivariate regression where the data are naturally
grouped, and a regression matrix is to be estimated for each group. We propose
an approach in which a dictionary of low rank parameter matrices is estimated
across groups, and a sparse linear combination of the dictionary elements is
estimated to form a model within each group. We refer to the method as
conditional sparse coding since it is a coding procedure for the response
vectors Y conditioned on the covariate vectors X. This approach captures the
shared information across the groups while adapting to the structure within
each group. It exploits the same intuition behind sparse coding that has been
successfully developed in computer vision and computational neuroscience. We
propose an algorithm for conditional sparse coding, analyze its theoretical
properties in terms of predictive accuracy, and present the results of
simulation and brain imaging experiments that compare the new technique to
reduced rank regression.
"
"  Discussion of ""A statistical analysis of multiple temperature proxies: Are
reconstructions of surface temperatures over the last 1000 years reliable?"" by
B.B. McShane and A.J. Wyner [arXiv:1104.4002]
"
"  Recently, prediction markets have shown considerable promise for developing
flexible mechanisms for machine learning. In this paper, agents with isoelastic
utilities are considered. It is shown that the costs associated with
homogeneous markets of agents with isoelastic utilities produce equilibrium
prices corresponding to alpha-mixtures, with a particular form of mixing
component relating to each agent's wealth. We also demonstrate that wealth
accumulation for logarithmic and other isoelastic agents (through payoffs on
prediction of training targets) can implement both Bayesian model updates and
mixture weight updates by imposing different market payoff structures. An
iterative algorithm is given for market equilibrium computation. We demonstrate
that inhomogeneous markets of agents with isoelastic utilities outperform state
of the art aggregate classifiers such as random forests, as well as single
classifiers (neural networks, decision trees) on a number of machine learning
benchmarks, and show that isoelastic combination methods are generally better
than their logarithmic counterparts.
"
"  We analyze two communication-efficient algorithms for distributed statistical
optimization on large-scale data sets. The first algorithm is a standard
averaging method that distributes the $N$ data samples evenly to $\nummac$
machines, performs separate minimization on each subset, and then averages the
estimates. We provide a sharp analysis of this average mixture algorithm,
showing that under a reasonable set of conditions, the combined parameter
achieves mean-squared error that decays as $\order(N^{-1}+(N/m)^{-2})$.
Whenever $m \le \sqrt{N}$, this guarantee matches the best possible rate
achievable by a centralized algorithm having access to all $\totalnumobs$
samples. The second algorithm is a novel method, based on an appropriate form
of bootstrap subsampling. Requiring only a single round of communication, it
has mean-squared error that decays as $\order(N^{-1} + (N/m)^{-3})$, and so is
more robust to the amount of parallelization. In addition, we show that a
stochastic gradient-based method attains mean-squared error decaying as
$O(N^{-1} + (N/ m)^{-3/2})$, easing computation at the expense of penalties in
the rate of convergence. We also provide experimental evaluation of our
methods, investigating their performance both on simulated data and on a
large-scale regression problem from the internet search domain. In particular,
we show that our methods can be used to efficiently solve an advertisement
prediction problem from the Chinese SoSo Search Engine, which involves logistic
regression with $N \approx 2.4 \times 10^8$ samples and $d \approx 740,000$
covariates.
"
"  Recognition of evolutionary units (species, populations) requires integrating
several kinds of data such as genetic or phenotypic markers or spatial
information, in order to get a comprehensive view concerning the
differentiation of the units. We propose a statistical model with a double
original advantage: (i) it incorporates information about the spatial
distribution of the samples, with the aim to increase inference power and to
relate more explicitly observed patterns to geography; and (ii) it allows one
to analyze genetic and phenotypic data within a unified model and inference
framework, thus opening the way to robust comparisons between markers and
possibly combined analyzes. We show from simulated data as well are real data
from the literature that our method estimates parameters accurately and
improves alternative approaches in many situations. The interest of this method
is exemplified using an intricate case of inter- and intra-species
differentiation based on an original data-set of georeferenced genetic and
morphometric markers obtained on {\em Myodes} voles from Sweden. A computer
program is made available as an extension of the R package Geneland.
"
"  Protein function prediction is the important problem in modern biology. In
this paper, the un-normalized, symmetric normalized, and random walk graph
Laplacian based semi-supervised learning methods will be applied to the
integrated network combined from multiple networks to predict the functions of
all yeast proteins in these multiple networks. These multiple networks are
network created from Pfam domain structure, co-participation in a protein
complex, protein-protein interaction network, genetic interaction network, and
network created from cell cycle gene expression measurements. Multiple networks
are combined with fixed weights instead of using convex optimization to
determine the combination weights due to high time complexity of convex
optimization method. This simple combination method will not affect the
accuracy performance measures of the three semi-supervised learning methods.
Experiment results show that the un-normalized and symmetric normalized graph
Laplacian based methods perform slightly better than random walk graph
Laplacian based method for integrated network. Moreover, the accuracy
performance measures of these three semi-supervised learning methods for
integrated network are much better than the best accuracy performance measures
of these three methods for the individual network.
"
"  Hybrid continuous-discrete models naturally represent many real-world
applications in robotics, finance, and environmental engineering. Inference
with large-scale models is challenging because relational structures
deteriorate rapidly during inference with observations. The main contribution
of this paper is an efficient relational variational inference algorithm that
factors largescale probability models into simpler variational models, composed
of mixtures of iid (Bernoulli) random variables. The algorithm takes
probability relational models of largescale hybrid systems and converts them to
a close-to-optimal variational models. Then, it efficiently calculates marginal
probabilities on the variational models by using a latent (or lifted) variable
elimination or a lifted stochastic sampling. This inference is unique because
it maintains the relational structure upon individual observations and during
inference steps.
"
"  We add a set of convex constraints to the lasso to produce sparse interaction
models that honor the hierarchy restriction that an interaction only be
included in a model if one or both variables are marginally important. We give
a precise characterization of the effect of this hierarchy constraint, prove
that hierarchy holds with probability one and derive an unbiased estimate for
the degrees of freedom of our estimator. A bound on this estimate reveals the
amount of fitting ""saved"" by the hierarchy constraint. We distinguish between
parameter sparsity - the number of nonzero coefficients - and practical
sparsity - the number of raw variables one must measure to make a new
prediction. Hierarchy focuses on the latter, which is more closely tied to
important data collection concerns such as cost, time and effort. We develop an
algorithm, available in the R package hierNet, and perform an empirical study
of our method.
"
"  A simple and computationally efficient scheme for tree-structured vector
quantization is presented. Unlike previous methods, its quantization error
depends only on the intrinsic dimension of the data distribution, rather than
the apparent dimension of the space in which the data happen to lie.
"
"  We introduce a stochastic process with Wishart marginals: the generalised
Wishart process (GWP). It is a collection of positive semi-definite random
matrices indexed by any arbitrary dependent variable. We use it to model
dynamic (e.g. time varying) covariance matrices. Unlike existing models, it can
capture a diverse class of covariance structures, it can easily handle missing
data, the dependent variable can readily include covariates other than time,
and it scales well with dimension; there is no need for free parameters, and
optional parameters are easy to interpret. We describe how to construct the
GWP, introduce general procedures for inference and predictions, and show that
it outperforms its main competitor, multivariate GARCH, even on financial data
that especially suits GARCH. We also show how to predict the mean of a
multivariate process while accounting for dynamic correlations.
"
"  Database theory and database practice are typically the domain of computer
scientists who adopt what may be termed an algorithmic perspective on their
data. This perspective is very different than the more statistical perspective
adopted by statisticians, scientific computers, machine learners, and other who
work on what may be broadly termed statistical data analysis. In this article,
I will address fundamental aspects of this algorithmic-statistical disconnect,
with an eye to bridging the gap between these two very different approaches. A
concept that lies at the heart of this disconnect is that of statistical
regularization, a notion that has to do with how robust is the output of an
algorithm to the noise properties of the input data. Although it is nearly
completely absent from computer science, which historically has taken the input
data as given and modeled algorithms discretely, regularization in one form or
another is central to nearly every application domain that applies algorithms
to noisy data. By using several case studies, I will illustrate, both
theoretically and empirically, the nonobvious fact that approximate
computation, in and of itself, can implicitly lead to statistical
regularization. This and other recent work suggests that, by exploiting in a
more principled way the statistical properties implicit in worst-case
algorithms, one can in many cases satisfy the bicriteria of having algorithms
that are scalable to very large-scale databases and that also have good
inferential or predictive properties.
"
"  We consider the problem of approximately reconstructing a partially-observed,
approximately low-rank matrix. This problem has received much attention lately,
mostly using the trace-norm as a surrogate to the rank. Here we study low-rank
matrix reconstruction using both the trace-norm, as well as the less-studied
max-norm, and present reconstruction guarantees based on existing analysis on
the Rademacher complexity of the unit balls of these norms. We show how these
are superior in several ways to recently published guarantees based on
specialized analysis.
"
"  We describe a new method for visualizing topics, the distributions over terms
that are automatically extracted from large text corpora using latent variable
models. Our method finds significant $n$-grams related to a topic, which are
then used to help understand and interpret the underlying distribution.
Compared with the usual visualization, which simply lists the most probable
topical terms, the multi-word expressions provide a better intuitive impression
for what a topic is ""about."" Our approach is based on a language model of
arbitrary length expressions, for which we develop a new methodology based on
nested permutation tests to find significant phrases. We show that this method
outperforms the more standard use of $\chi^2$ and likelihood ratio tests. We
illustrate the topic presentations on corpora of scientific abstracts and news
articles.
"
"  Introduction to papers on the modeling and analysis of network data
"
"  Wind direction plays an important role in the spread of pollutant levels over
a geographical region. We discuss how to include wind directional information
in the covariance function of spatial models. We follow the spatial convolution
approach initially proposed by Higdon and co-authors, wherein a spatial process
is described by a convolution between a smoothing kernel and a white noise
process. We propose two different ways of accounting for wind direction in the
kernel function. For comparison purposes, we also consider a more flexible
kernel parametrization, that makes use of latent processes which vary smoothly
across the region. Inference procedure follows the Bayesian paradigm, and
uncertainty about parameter estimation is naturally accounted for when
performing spatial interpolation. We analyze ozone levels observed at a
monitoring network in the Northeast of the USA. Sam- ples from the posterior
distribution under our proposed models are obtained much faster when compared
to the kernel based on latent processes. Our models provide better results, in
terms of model fitting and spatial interpolation, when compared to simple
isotropic and geometrical anisotropic models. Despite the small number of
parameters, our proposed models provide fits which are comparable to those
obtained under the kernel based on latent processes.
"
"  Given the superposition of a low-rank matrix plus the product of a known fat
compression matrix times a sparse matrix, the goal of this paper is to
establish deterministic conditions under which exact recovery of the low-rank
and sparse components becomes possible. This fundamental identifiability issue
arises with traffic anomaly detection in backbone networks, and subsumes
compressed sensing as well as the timely low-rank plus sparse matrix recovery
tasks encountered in matrix decomposition problems. Leveraging the ability of
$\ell_1$- and nuclear norms to recover sparse and low-rank matrices, a convex
program is formulated to estimate the unknowns. Analysis and simulations
confirm that the said convex program can recover the unknowns for sufficiently
low-rank and sparse enough components, along with a compression matrix
possessing an isometry property when restricted to operate on sparse vectors.
When the low-rank, sparse, and compression matrices are drawn from certain
random ensembles, it is established that exact recovery is possible with high
probability. First-order algorithms are developed to solve the nonsmooth convex
optimization problem with provable iteration complexity guarantees. Insightful
tests with synthetic and real network data corroborate the effectiveness of the
novel approach in unveiling traffic anomalies across flows and time, and its
ability to outperform existing alternatives.
"
"  We use partial class memberships in soft classification to model uncertain
labelling and mixtures of classes. Partial class memberships are not restricted
to predictions, but may also occur in reference labels (ground truth, gold
standard diagnosis) for training and validation data.
  Classifier performance is usually expressed as fractions of the confusion
matrix, such as sensitivity, specificity, negative and positive predictive
values. We extend this concept to soft classification and discuss the bias and
variance properties of the extended performance measures. Ambiguity in
reference labels translates to differences between best-case, expected and
worst-case performance. We show a second set of measures comparing expected and
ideal performance which is closely related to regression performance, namely
the root mean squared error RMSE and the mean absolute error MAE.
  All calculations apply to classical crisp classification as well as to soft
classification (partial class memberships and/or one-class classifiers). The
proposed performance measures allow to test classifiers with actual borderline
cases. In addition, hardening of e.g. posterior probabilities into class labels
is not necessary, avoiding the corresponding information loss and increase in
variance.
  We implement the proposed performance measures in the R package
""softclassval"", which is available from CRAN and at
http://softclassval.r-forge.r-project.org.
  Our reasoning as well as the importance of partial memberships for
chemometric classification is illustrated by a real-word application:
astrocytoma brain tumor tissue grading (80 patients, 37000 spectra) for finding
surgical excision borders. As borderline cases are the actual target of the
analytical technique, samples which are diagnosed to be borderline cases must
be included in the validation.
"
"  We consider pricing weather derivatives for use as protection against weather
extremes. The method described utilizes results from spatial statistics and
extreme value theory to first model extremes in the weather as a max-stable
process, and then use these models to simulate payments for a general
collection of weather derivatives. These simulations capture the spatial
dependence of payments. Incorporating results from catastrophe ratemaking, we
show how this method can be used to compute risk loads and premiums for weather
derivatives which are renewal-additive.
"
"  The structure of a Bayesian network encodes most of the information about the
probability distribution of the data, which is uniquely identified given some
general distributional assumptions. Therefore it's important to study the
variability of its network structure, which can be used to compare the
performance of different learning algorithms and to measure the strength of any
arbitrary subset of arcs.
  In this paper we will introduce some descriptive statistics and the
corresponding parametric and Monte Carlo tests on the undirected graph
underlying the structure of a Bayesian network, modeled as a multivariate
Bernoulli random variable.
"
"  In a communication network, point-to-point traffic volumes over time are
critical for designing protocols that route information efficiently and for
maintaining security, whether at the scale of an internet service provider or
within a corporation. While technically feasible, the direct measurement of
point-to-point traffic imposes a heavy burden on network performance and is
typically not implemented. Instead, indirect aggregate traffic volumes are
routinely collected. We consider the problem of estimating point-to-point
traffic volumes, x_t, from aggregate traffic volumes, y_t, given information
about the network routing protocol encoded in a matrix A. This estimation task
can be reformulated as finding the solutions to a sequence of ill-posed linear
inverse problems, y_t = A x_t, since the number of origin-destination routes of
interest is higher than the number of aggregate measurements available.
  Here, we introduce a novel multilevel state-space model of aggregate traffic
volumes with realistic features. We implement a naive strategy for estimating
unobserved point-to-point traffic volumes from indirect measurements of
aggregate traffic, based on particle filtering. We then develop a more
efficient two-stage inference strategy that relies on model-based
regularization: a simple model is used to calibrate regularization parameters
that lead to efficient and scalable inference in the multilevel state-space
model. We apply our methods to corporate and academic networks, where we show
that the proposed inference strategy outperforms existing approaches and scales
to larger networks. We also design a simulation study to explore the factors
that influence the performance. Our results suggest that model-based
regularization may be an efficient strategy for inference in other complex
multilevel models.
"
"  We focus on the problem of minimizing a convex function $f$ over a convex set
$S$ given $T$ queries to a stochastic first order oracle. We argue that the
complexity of convex minimization is only determined by the rate of growth of
the function around its minimizer $x^*_{f,S}$, as quantified by a Tsybakov-like
noise condition. Specifically, we prove that if $f$ grows at least as fast as
$\|x-x^*_{f,S}\|^\kappa$ around its minimum, for some $\kappa > 1$, then the
optimal rate of learning $f(x^*_{f,S})$ is
$\Theta(T^{-\frac{\kappa}{2\kappa-2}})$. The classic rate $\Theta(1/\sqrt T)$
for convex functions and $\Theta(1/T)$ for strongly convex functions are
special cases of our result for $\kappa \rightarrow \infty$ and $\kappa=2$, and
even faster rates are attained for $\kappa <2$. We also derive tight bounds for
the complexity of learning $x_{f,S}^*$, where the optimal rate is
$\Theta(T^{-\frac{1}{2\kappa-2}})$. Interestingly, these precise rates for
convex optimization also characterize the complexity of active learning and our
results further strengthen the connections between the two fields, both of
which rely on feedback-driven queries.
"
"  In this paper we focus on comparative diagnostic trials which are frequently
employed to compare two markers with continuous or ordinal results. We derive
explicit expressions for the optimal sampling ratio based on a common variance
structure shared by existing summary statistics of the receiver operating
characteristic (ROC) curve. Estimating the optimal ratio requires either pilot
data or parametric model assumptions; however, pilot data are often unavailable
at the planning stage of diagnostic trials. In the absence of pilot data, some
distributions have to be assumed for carrying out the calculation. An optimal
ratio from an incorrect distributional assumption may lead to an underpowered
study. We propose a two-stage procedure to adaptively estimate the optimal
ratio in comparative diagnostic trials without pilot data or assuming
parametric distributions. We illustrate the properties of the proposed method
through theoretical proofs and extensive simulation studies. We use an example
in cancer diagnostic studies to illustrate the application of our method. We
find that our method increases the power, or reduces the required overall
sample size dramatically.
"
"  We introduce a dynamical system which we call the AdaBoost flow. The flow is
defined by a system of ODEs with control. We show that three algorithms of the
AdaBoost family (i) the AdaBoost algorithm of Schapire and Freund (ii) the
arc-gv algorithm of Breiman (iii) the confidence rated prediction of Schapire
and Singer can be can be embedded in the AdaBoost flow.
  The nontrivial part of the AdaBoost flow equations coincides with the
equations of dynamics of nonperiodic Toda system written in terms of spectral
variables. We provide a novel invariant geometrical description of the AdaBoost
algorithm as a gradient flow on a foliation defined by level sets of the
potential function.
  We propose a new approach for constructing boosting algorithms as a
continuous time gradient flow on measures defined by various metrics and
potential functions. Finally we explain similarity of the AdaBoost algorithm
with the Perelman's construction for the Ricci flow.
"
"  An individual-based model of the infectious disease spread among the urban
population is considered. A system of stochastic equations, which describes
changes in quantities of four population groups, susceptible, exposed, infected
individuals and individuals in the state of remission, is built. The system of
equations of the model is supplemented with correlations, which consider
disease heaviness and duration for every infected individual. An algorithm and
a modelling program based on Monte-Carlo methods which allows to investigate
the group number dynamics is developed.
"
"  Polyploidy is an important speciation mechanism, particularly in land plants.
Allopolyploid species are formed after hybridization between otherwise
intersterile parental species. Recent theoretical progress has led to
successful implementation of species tree models that take population genetic
parameters into account. However, these models have not included allopolyploid
hybridization and the special problems imposed when species trees of
allopolyploids are inferred. Here, two new models for the statistical inference
of the evolutionary history of allopolyploids are evaluated using simulations
and demonstrated on two empirical data sets. It is assumed that there has been
a single hybridization event between two diploid species resulting in a genomic
allotetraploid. The evolutionary history can be represented as a network or as
a multiply labeled tree, in which some pairs of tips are labeled with the same
species. In one of the models (AlloppMUL), the multiply labeled tree is
inferred directly. This is the simplest model and the most widely applicable,
since fewer assumptions are made. The second model (AlloppNET) incorporates the
hybridization event explicitly which means that fewer parameters need to be
estimated. Both models are implemented in the BEAST framework. Simulations show
that both models are useful and that AlloppNET is more accurate if the
assumptions it is based on are valid. The models are demonstrated on previously
analyzed data from the genus Pachycladon (Brassicaceae) and from the genus
Silene (Caryophyllaceae).
"
"  Current tomographic imaging systems need major improvements, especially when
multi-dimensional, multi-scale, multi-temporal and multi-parametric phenomena
are under investigation. Both preclinical and clinical imaging now depend on in
vivo tomography, often requiring separate evaluations by different imaging
modalities to define morphologic details, delineate interval changes due to
disease or interventions, and study physiological functions that have
interconnected aspects. Over the past decade, fusion of multimodality images
has emerged with two different approaches: post-hoc image registration and
combined acquisition on PET-CT, PET-MRI and other hybrid scanners. There are
intrinsic limitations for both the post-hoc image analysis and dual/triple
modality approaches defined by registration errors and physical constraints in
the acquisition chain. We envision that tomography will evolve beyond current
modality fusion and towards grand fusion, a large scale fusion of all or many
imaging modalities, which may be referred to as omni-tomography or
multi-tomography. Unlike modality fusion, grand fusion is here proposed for
truly simultaneous but often localized reconstruction in terms of all or many
relevant imaging mechanisms such as CT, MRI, PET, SPECT, US, optical, and
possibly more. In this paper, the technical basis for omni-tomography is
introduced and illustrated with a top-level design of a next generation
scanner, interior tomographic reconstructions of representative modalities, and
anticipated applications of omni-tomography.
"
"  We propose a new method of learning a sparse nonnegative-definite target
matrix. Our primary example of the target matrix is the inverse of a population
covariance or correlation matrix. The algorithm first estimates each column of
the target matrix by the scaled Lasso and then adjusts the matrix estimator to
be symmetric. The penalty level of the scaled Lasso for each column is
completely determined by data via convex minimization, without using
cross-validation.
  We prove that this scaled Lasso method guarantees the fastest proven rate of
convergence in the spectrum norm under conditions of weaker form than those in
the existing analyses of other $\ell_1$ regularized algorithms, and has faster
guaranteed rate of convergence when the ratio of the $\ell_1$ and spectrum
norms of the target inverse matrix diverges to infinity. A simulation study
demonstrates the computational feasibility and superb performance of the
proposed method.
  Our analysis also provides new performance bounds for the Lasso and scaled
Lasso to guarantee higher concentration of the error at a smaller threshold
level than previous analyses, and to allow the use of the union bound in
column-by-column applications of the scaled Lasso without an adjustment of the
penalty level. In addition, the least squares estimation after the scaled Lasso
selection is considered and proven to guarantee performance bounds similar to
that of the scaled Lasso.
"
"  This note introduces the method of cross-conformal prediction, which is a
hybrid of the methods of inductive conformal prediction and cross-validation,
and studies its validity and predictive efficiency empirically.
"
"  We propose a new probabilistic graphical model that jointly models the
difficulties of questions, the abilities of participants and the correct
answers to questions in aptitude testing and crowdsourcing settings. We devise
an active learning/adaptive testing scheme based on a greedy minimization of
expected model entropy, which allows a more efficient resource allocation by
dynamically choosing the next question to be asked based on the previous
responses. We present experimental results that confirm the ability of our
model to infer the required parameters and demonstrate that the adaptive
testing scheme requires fewer questions to obtain the same accuracy as a static
test scenario.
"
"  Sparse non-Gaussian component analysis (SNGCA) is an unsupervised method of
extracting a linear structure from a high dimensional data based on estimating
a low-dimensional non-Gaussian data component. In this paper we discuss a new
approach to direct estimation of the projector on the target space based on
semidefinite programming which improves the method sensitivity to a broad
variety of deviations from normality. We also discuss the procedures which
allows to recover the structure when its effective dimension is unknown.
"
"  Discussion of ""A statistical analysis of multiple temperature proxies: Are
reconstructions of surface temperatures over the last 1000 years reliable?"" by
B.B. McShane and A.J. Wyner [arXiv:1104.4002]
"
"  This paper investigates the problem of determining a binary-valued function
through a sequence of strategically selected queries. The focus is an algorithm
called Generalized Binary Search (GBS). GBS is a well-known greedy algorithm
for determining a binary-valued function through a sequence of strategically
selected queries. At each step, a query is selected that most evenly splits the
hypotheses under consideration into two disjoint subsets, a natural
generalization of the idea underlying classic binary search. This paper
develops novel incoherence and geometric conditions under which GBS achieves
the information-theoretically optimal query complexity; i.e., given a
collection of N hypotheses, GBS terminates with the correct function after no
more than a constant times log N queries. Furthermore, a noise-tolerant version
of GBS is developed that also achieves the optimal query complexity. These
results are applied to learning halfspaces, a problem arising routinely in
image processing and machine learning.
"
"  A central problem in neuroscience is reconstructing neuronal circuits on the
synapse level. Due to a wide range of scales in brain architecture such
reconstruction requires imaging that is both high-resolution and
high-throughput. Existing electron microscopy (EM) techniques possess required
resolution in the lateral plane and either high-throughput or high depth
resolution but not both. Here, we exploit recent advances in unsupervised
learning and signal processing to obtain high depth-resolution EM images
computationally without sacrificing throughput. First, we show that the brain
tissue can be represented as a sparse linear combination of localized basis
functions that are learned using high-resolution datasets. We then develop
compressive sensing-inspired techniques that can reconstruct the brain tissue
from very few (typically 5) tomographic views of each section. This enables
tracing of neuronal processes and, hence, high throughput reconstruction of
neural circuits on the level of individual synapses.
"
"  With the proliferation of modern high-resolution measuring instruments
mounted on satellites, planes, ground-based vehicles and monitoring stations, a
need has arisen for statistical methods suitable for the analysis of large
spatial datasets observed on large spatial domains. Statistical analyses of
such datasets provide two main challenges: First, traditional
spatial-statistical techniques are often unable to handle large numbers of
observations in a computationally feasible way. Second, for large and
heterogeneous spatial domains, it is often not appropriate to assume that a
process of interest is stationary over the entire domain.
  We address the first challenge by using a model combining a low-rank
component, which allows for flexible modeling of medium-to-long-range
dependence via a set of spatial basis functions, with a tapered remainder
component, which allows for modeling of local dependence using a compactly
supported covariance function. Addressing the second challenge, we propose two
extensions to this model that result in increased flexibility: First, the model
is parameterized based on a nonstationary Matern covariance, where the
parameters vary smoothly across space. Second, in our fully Bayesian model, all
components and parameters are considered random, including the number,
locations, and shapes of the basis functions used in the low-rank component.
  Using simulated data and a real-world dataset of high-resolution soil
measurements, we show that both extensions can result in substantial
improvements over the current state-of-the-art.
"
"  We present a technique to characterize differentially expressed genes in
terms of their position in a high-dimensional co-expression network. The set-up
of Gaussian graphical models is used to construct representations of the
co-expression network in such a way that redundancy and the propagation of
spurious information along the network are avoided. The proposed inference
procedure is based on the minimization of the Bayesian Information Criterion
(BIC) in the class of decomposable graphical models. This class of models can
be used to represent complex relationships and has suitable properties that
allow to make effective inference in problems with high degree of complexity
(e.g. several thousands of genes) and small number of observations (e.g.
10-100) as typically occurs in high throughput gene expression studies. Taking
advantage of the internal structure of decomposable graphical models, we
construct a compact representation of the co-expression network that allows to
identify the regions with high concentration of differentially expressed genes.
It is argued that differentially expressed genes located in highly
interconnected regions of the co-expression network are less informative than
differentially expressed genes located in less interconnected regions. Based on
that idea, a measure of uncertainty that resembles the notion of relative
entropy is proposed. Our methods are illustrated with three publically
available data sets on microarray experiments (the larger involving more than
50,000 genes and 64 patients) and a short simulation study.
"
"  We study algorithms for matching user tracks, consisting of time-ordered
location points, to paths in the road network. Previous work has focused on the
scenario where the location data is linearly ordered and consists of fairly
dense and regular samples. In this work, we consider the \emph{multi-track map
matching}, where the location data comes from different trips on the same
route, each with very sparse samples. This captures the realistic scenario
where users repeatedly travel on regular routes and samples are sparsely
collected, either due to energy consumption constraints or because samples are
only collected when the user actively uses a service. In the multi-track
problem, the total set of combined locations is only partially ordered, rather
than globally ordered as required by previous map-matching algorithms. We
propose two methods, the iterative projection scheme and the graph Laplacian
scheme, to solve the multi-track problem by using a single-track map-matching
subroutine. We also propose a boosting technique which may be applied to either
approach to improve the accuracy of the estimated paths. In addition, in order
to deal with variable sampling rates in single-track map matching, we propose a
method based on a particular regularized cost function that can be adapted for
different sampling rates and measurement errors. We evaluate the effectiveness
of our techniques for reconstructing tracks under several different
configurations of sampling error and sampling rate.
"
"  We address the sequential change-point detection problem for the Gaussian
model where baseline distribution is Gaussian with variance \sigma^2 and mean
\mu such that \sigma^2=a\mu, where a>0 is a known constant; the change is in
\mu from one known value to another. First, we carry out a comparative
performance analysis of four detection procedures: the CUSUM procedure, the
Shiryaev-Roberts (SR) procedure, and two its modifications - the
Shiryaev-Roberts-Pollak and Shiryaev-Roberts-r procedures. The performance is
benchmarked via Pollak's maximal average delay to detection and Shiryaev's
stationary average delay to detection, each subject to a fixed average run
length to false alarm. The analysis shows that in practically interesting cases
the accuracy of asymptotic approximations is ""reasonable"" to ""excellent"". We
also consider an application of change-point detection to cybersecurity - for
rapid anomaly detection in computer networks. Using real network data we show
that statistically traffic's intensity can be well-described by the proposed
Gaussian model with \sigma^2=a\mu instead of the traditional Poisson model,
which requires \sigma^2=\mu. By successively devising the SR and CUSUM
procedures to ""catch"" a low-contrast network anomaly (caused by an ICMP
reflector attack), we then show that the SR rule is quicker. We conclude that
the SR procedure is a better cyber ""watch dog"" than the popular CUSUM
procedure.
"
"  Unfortunately, working scientists sometimes reflexively continue to use ""buzz
phrases"" grounded in once prevalent paradigms that have been subsequently
refuted. This can impede both earthquake research and hazard mitigation.
Well-worn seismological buzz phrases include ""earthquake cycle,"" ""seismic
cycle,"" ""seismic gap,"" and ""characteristic earthquake."" They all assume that
there are sequences of earthquakes that are nearly identical except for the
times of their occurrence. If so, the complex process of earthquake occurrence
could be reduced to a description of one ""characteristic"" earthquake plus the
times of the others in the sequence. A common additional assumption is that
characteristic earthquakes dominate the displacement on fault or plate boundary
""segments."" The ""seismic gap"" (or the effectively equivalent ""seismic cycle"")
model depends entirely on the ""characteristic"" assumption, with the added
assumption that characteristic earthquakes are quasi-periodic. However, since
the 1990s numerous statistical tests have failed to support characteristic
earthquake and seismic gap models, and the 2004 Sumatra earthquake and 2011
Tohoku earthquake both ripped through several supposed segment boundaries.
Earthquake scientists should scrap ideas that have been rejected by objective
testing or are too vague to be testable.
"
"  Here we propose a novel model family with the objective of learning to
disentangle the factors of variation in data. Our approach is based on the
spike-and-slab restricted Boltzmann machine which we generalize to include
higher-order interactions among multiple latent variables. Seen from a
generative perspective, the multiplicative interactions emulates the entangling
of factors of variation. Inference in the model can be seen as disentangling
these generative factors. Unlike previous attempts at disentangling latent
factors, the proposed model is trained using no supervised information
regarding the latent factors. We apply our model to the task of facial
expression classification.
"
"  We consider two statistical regularities that were used to explain Omori's
law of the aftershock rate decay: the Levy and Inverse Gaussian (IGD)
distributions. These distributions are thought to describe stress behavior
influenced by various random factors: post-earthquake stress time history is
described by a Brownian motion. Both distributions decay to zero for time
intervals close to zero. But this feature contradicts the high immediate
aftershock level according to Omori's law. We propose that these statistical
distributions are influenced by the power-law stress distribution near the
earthquake focal zone and we derive new distributions as a mixture of power-law
stress with the exponent psi and Levy as well as IGD distributions. Such new
distributions describe the resulting inter-earthquake time intervals and
closely resemble Omori's law. The new Levy distribution has a pure power-law
form with the exponent -(1+psi/2) and the mixed IGD has two exponents: the same
as Levy for small time intervals and -(1+psi) for longer times. For even longer
time intervals this power-law behavior should be replaced by a uniform
seismicity rate corresponding to the long-term tectonic deformation. We compute
these background rates using our former analysis of earthquake size
distribution and its connection to plate tectonics. We analyze several
earthquake catalogs to confirm and illustrate our theoretical results. Finally,
we discuss how the parameters of random stress dynamics can be determined
through a more detailed statistical analysis of earthquake occurrence or by new
laboratory experiments.
"
"  Traditional analyses of capture-recapture data are based on likelihood
functions that explicitly integrate out all missing data. We use a complete
data likelihood (CDL) to show how a wide range of capture-recapture models can
be easily fitted using readily available software JAGS/BUGS even when there are
individual-specific time-varying covariates. The models we describe extend
those that condition on first capture to include abundance parameters, or
parameters related to abundance, such as population size, birth rates or
lifetime. The use of a CDL means that any missing data, including uncertain
individual covariates, can be included in models without the need for
customized likelihood functions. This approach also facilitates modeling
processes of demographic interest rather than the complexities caused by
non-ignorable missing data. We illustrate using two examples, (i) open
population modeling in the presence of a censored time-varying individual
covariate in a full robust-design, and (ii) full open population multi-state
modeling in the presence of a partially observed categorical variable.
"
"  Markov state models (MSMs) have become a popular approach for investigating
the conformational dynamics of proteins and other biomolecules. MSMs are
typically built from numerous molecular dynamics simulations by dividing the
sampled configurations into a large number of microstates based on geometric
criteria. The resulting microstate model can then be coarse-grained into a more
understandable macro state model by lumping together rapidly mixing microstates
into larger, metastable aggregates. However, finite sampling often results in
the creation of many poorly sampled microstates. During coarse-graining, these
states are mistakenly identified as being kinetically important because
transitions to/from them appear to be slow. In this paper we propose a
formalism based on an algebraic principle for matrix approximation, i.e. the
Nystrom method, to deal with such poorly sampled microstates. Our scheme builds
a hierarchy of microstates from high to low populations and progressively
applies spectral clustering on sets of microstates within each level of the
hierarchy. It helps spectral clustering identify metastable aggregates with
highly populated microstates rather than being distracted by lowly populated
states. We demonstrate the ability of this algorithm to discover the major
metastable states on two model systems, the alanine dipeptide and TrpZip2.
"
"  We introduce a novel latent grouping model for predicting the relevance of a
new document to a user. The model assumes a latent group structure for both
users and documents. We compared the model against a state-of-the-art method,
the User Rating Profile model, where only users have a latent group structure.
We estimate both models by Gibbs sampling. The new method predicts relevance
more accurately for new documents that have few known ratings. The reason is
that generalization over documents then becomes necessary and hence the twoway
grouping is profitable.
"
"  Approximating non-linear kernels using feature maps has gained a lot of
interest in recent years due to applications in reducing training and testing
times of SVM classifiers and other kernel based learning algorithms. We extend
this line of work and present low distortion embeddings for dot product kernels
into linear Euclidean spaces. We base our results on a classical result in
harmonic analysis characterizing all dot product kernels and use it to define
randomized feature maps into explicit low dimensional Euclidean spaces in which
the native dot product provides an approximation to the dot product kernel with
high confidence.
"
"  We present a semi-supervised method for photometric supernova typing. Our
approach is to first use the nonlinear dimension reduction technique diffusion
map to detect structure in a database of supernova light curves and
subsequently employ random forest classification on a spectroscopically
confirmed training set to learn a model that can predict the type of each newly
observed supernova. We demonstrate that this is an effective method for
supernova typing. As supernova numbers increase, our semi-supervised method
efficiently utilizes this information to improve classification, a property not
enjoyed by template based methods. Applied to supernova data simulated by
Kessler et al. (2010b) to mimic those of the Dark Energy Survey, our methods
achieve (cross-validated) 95% Type Ia purity and 87% Type Ia efficiency on the
spectroscopic sample, but only 50% Type Ia purity and 50% efficiency on the
photometric sample due to their spectroscopic follow-up strategy. To improve
the performance on the photometric sample, we search for better spectroscopic
follow-up procedures by studying the sensitivity of our machine learned
supernova classification on the specific strategy used to obtain training sets.
With a fixed amount of spectroscopic follow-up time, we find that deeper
magnitude-limited spectroscopic surveys are better for producing training sets.
For supernova Ia (II-P) typing, we obtain a 44% (1%) increase in purity to 72%
(87%) and 30% (162%) increase in efficiency to 65% (84%) of the sample using a
25th (24.5th) magnitude-limited survey instead of the shallower spectroscopic
sample used in the original simulations. When redshift information is
available, we incorporate it into our analysis using a novel method of altering
the diffusion map representation of the supernovae. Incorporating host
redshifts leads to a 5% improvement in Type Ia purity and 13% improvement in
Type Ia efficiency.
"
"  We describe dimensionally constrained symbolic regression which has been
developed for mass measurement in certain classes of events in high-energy
physics (HEP). With symbolic regression, we can derive equations that are well
known in HEP. However, in problems with large number of variables, we find that
by constraining the terms allowed in the symbolic regression, convergence
behavior is improved. Dimensionally constrained symbolic regression (DCSR)
finds solutions with much better fitness than is normally possible with
symbolic regression. In some cases, novel solutions are found.
"
"  Estimating the size of an elusive target population is of prominent interest
in many areas in the life and social sciences. Our aim is to provide an
efficient and workable method to estimate the unknown population size, given
the frequency distribution of counts of repeated identifications of units of
the population of interest. This counting variable is necessarily
zero-truncated, since units that have never been identified are not in the
sample. We consider several applications: clinical medicine, where interest is
in estimating patients with adenomatous polyps which have been overlooked by
the diagnostic procedure; drug user studies, where interest is in estimating
the number of hidden drug users which are not identified; veterinary
surveillance of scrapie in the UK, where interest is in estimating the hidden
amount of scrapie; and entomology and microbial ecology, where interest is in
estimating the number of unobserved species of organisms. In all these
examples, simple models such as the homogenous Poisson are not appropriate
since they do not account for present and latent heterogeneity. The
Poisson-Gamma (negative binomial) model provides a flexible alternative and
often leads to well-fitting models. It has a long history and was recently used
in the development of the Chao-Bunge estimator. Here we use a different
property of the Poisson-Gamma model: if we consider ratios of neighboring
Poisson-Gamma probabilities, then these are linearly related to the counts of
repeated identifications.
"
"  Effective connectivity analysis provides an understanding of the functional
organization of the brain by studying how activated regions influence one
other. We propose a nonparametric Bayesian approach to model effective
connectivity assuming a dynamic nonstationary neuronal system. Our approach
uses the Dirichlet process to specify an appropriate (most plausible according
to our prior beliefs) dynamic model as the ""expectation"" of a set of plausible
models upon which we assign a probability distribution. This addresses model
uncertainty associated with dynamic effective connectivity. We derive a Gibbs
sampling approach to sample from the joint (and marginal) posterior
distributions of the unknowns. Results on simulation experiments demonstrate
our model to be flexible and a better candidate in many situations. We also
used our approach to analyzing functional Magnetic Resonance Imaging (fMRI)
data on a Stroop task: our analysis provided new insight into the mechanism by
which an individual brain distinguishes and learns about shapes of objects.
"
"  We develop a Bayesian statistical model and estimation methodology based on
Forward Projection Adaptive Markov chain Monte Carlo in order to perform the
calibration of a high-dimensional non-linear system of Ordinary Differential
Equations representing an epidemic model for Human Papillomavirus types 6 and
11 (HPV-6, HPV-11). The model is compartmental and involves stratification by
age, gender and sexual activity-group. Developing this model and a means to
calibrate it efficiently is relevant since HPV is a very multi-typed and common
sexually transmitted infection with more than 100 types currently known. The
two types studied in this paper, types 6 and 11, are causing about 90% of
anogenital warts.
  We extend the development of a sexual mixing matrix for the population, based
on a formulation first suggested by Garnett and Anderson. In particular we
consider a stochastic mixing matrix framework which allows us to jointly
estimate unknown attributes and parameters of the mixing matrix along with the
parameters involved in the calibration of the HPV epidemic model. This matrix
describes the sexual interactions between members of the population under study
and relies on several quantities which are a-priori unknown. The Bayesian model
developed allows one to estimate jointly the HPV-6 and HPV-11 epidemic model
parameters such as the probability of transmission, HPV incubation period,
duration of infection, duration of genital warts treatment, duration of
immunity, the probability of seroconversion, per gender, age-group and sexual
activity-group, as well as unknown sexual mixing matrix parameters related to
assortativity. We conclude with simulation studies on synthetic and actual data
from studies undertaken recently in Australia.
"
"  We introduce a useful tool for analyzing boosting algorithms called the
``smooth margin function,'' a differentiable approximation of the usual margin
for boosting algorithms. We present two boosting algorithms based on this
smooth margin, ``coordinate ascent boosting'' and ``approximate coordinate
ascent boosting,'' which are similar to Freund and Schapire's AdaBoost
algorithm and Breiman's arc-gv algorithm. We give convergence rates to the
maximum margin solution for both of our algorithms and for arc-gv. We then
study AdaBoost's convergence properties using the smooth margin function. We
precisely bound the margin attained by AdaBoost when the edges of the weak
classifiers fall within a specified range. This shows that a previous bound
proved by R\""{a}tsch and Warmuth is exactly tight. Furthermore, we use the
smooth margin to capture explicit properties of AdaBoost in cases where cyclic
behavior occurs.
"
"  In many applications that require matrix solutions of minimal rank, the
underlying cost function is non-convex leading to an intractable, NP-hard
optimization problem. Consequently, the convex nuclear norm is frequently used
as a surrogate penalty term for matrix rank. The problem is that in many
practical scenarios there is no longer any guarantee that we can correctly
estimate generative low-rank matrices of interest, theoretical special cases
notwithstanding. Consequently, this paper proposes an alternative empirical
Bayesian procedure build upon a variational approximation that, unlike the
nuclear norm, retains the same globally minimizing point estimate as the rank
function under many useful constraints. However, locally minimizing solutions
are largely smoothed away via marginalization, allowing the algorithm to
succeed when standard convex relaxations completely fail. While the proposed
methodology is generally applicable to a wide range of low-rank applications,
we focus our attention on the robust principal component analysis problem
(RPCA), which involves estimating an unknown low-rank matrix with unknown
sparse corruptions. Theoretical and empirical evidence are presented to show
that our method is potentially superior to related MAP-based approaches, for
which the convex principle component pursuit (PCP) algorithm (Candes et al.,
2011) can be viewed as a special case.
"
"  Plant traits are a key to understanding and predicting the adaptation of
ecosystems to environmental changes, which motivates the TRY project aiming at
constructing a global database for plant traits and becoming a standard
resource for the ecological community. Despite its unprecedented coverage, a
large percentage of missing data substantially constrains joint trait analysis.
Meanwhile, the trait data is characterized by the hierarchical phylogenetic
structure of the plant kingdom. While factorization based matrix completion
techniques have been widely used to address the missing data problem,
traditional matrix factorization methods are unable to leverage the
phylogenetic structure. We propose hierarchical probabilistic matrix
factorization (HPMF), which effectively uses hierarchical phylogenetic
information for trait prediction. We demonstrate HPMF's high accuracy,
effectiveness of incorporating hierarchical structure and ability to capture
trait correlation through experiments.
"
"  In this chapter we discuss conceptually high dimensional sparse econometric
models as well as estimation of these models using L1-penalization and
post-L1-penalization methods. Focusing on linear and nonparametric regression
frameworks, we discuss various econometric examples, present basic theoretical
results, and illustrate the concepts and methods with Monte Carlo simulations
and an empirical application. In the application, we examine and confirm the
empirical validity of the Solow-Swan model for international economic growth.
"
"  In query learning, the goal is to identify an unknown object while minimizing
the number of ""yes"" or ""no"" questions (queries) posed about that object. A
well-studied algorithm for query learning is known as generalized binary search
(GBS). We show that GBS is a greedy algorithm to optimize the expected number
of queries needed to identify the unknown object. We also generalize GBS in two
ways. First, we consider the case where the cost of querying grows
exponentially in the number of queries and the goal is to minimize the expected
exponential cost. Then, we consider the case where the objects are partitioned
into groups, and the objective is to identify only the group to which the
object belongs. We derive algorithms to address these issues in a common,
information-theoretic framework. In particular, we present an exact formula for
the objective function in each case involving Shannon or Renyi entropy, and
develop a greedy algorithm for minimizing it. Our algorithms are demonstrated
on two applications of query learning, active learning and emergency response.
"
"  Probabilistic principal component analysis (PPCA) seeks a low dimensional
representation of a data set in the presence of independent spherical Gaussian
noise. The maximum likelihood solution for the model is an eigenvalue problem
on the sample covariance matrix. In this paper we consider the situation where
the data variance is already partially explained by other actors, for example
sparse conditional dependencies between the covariates, or temporal
correlations leaving some residual variance. We decompose the residual variance
into its components through a generalised eigenvalue problem, which we call
residual component analysis (RCA). We explore a range of new algorithms that
arise from the framework, including one that factorises the covariance of a
Gaussian density into a low-rank and a sparse-inverse component. We illustrate
the ideas on the recovery of a protein-signaling network, a gene expression
time-series data set and the recovery of the human skeleton from motion capture
3-D cloud data.
"
"  In this paper we consider the estimation of population size from one-source
capture--recapture data, that is, a list in which individuals can potentially
be found repeatedly and where the question is how many individuals are missed
by the list. As a typical example, we provide data from a drug user study in
Bangkok from 2001 where the list consists of drug users who repeatedly contact
treatment institutions. Drug users with 1, 2, 3$,...$ contacts occur, but drug
users with zero contacts are not present, requiring the size of this group to
be estimated. Statistically, these data can be considered as stemming from a
zero-truncated count distribution. We revisit an estimator for the population
size suggested by Zelterman that is known to be robust under potential
unobserved heterogeneity. We demonstrate that the Zelterman estimator can be
viewed as a maximum likelihood estimator for a locally truncated Poisson
likelihood which is equivalent to a binomial likelihood. This result allows the
extension of the Zelterman estimator by means of logistic regression to include
observed heterogeneity in the form of covariates. We also review an estimator
proposed by Chao and explain why we are not able to obtain similar results for
this estimator. The Zelterman estimator is applied in two case studies, the
first a drug user study from Bangkok, the second an illegal immigrant study in
the Netherlands. Our results suggest the new estimator should be used, in
particular, if substantial unobserved heterogeneity is present.
"
"  We propose dynamical systems trees (DSTs) as a flexible class of models for
describing multiple processes that interact via a hierarchy of aggregating
parent chains. DSTs extend Kalman filters, hidden Markov models and nonlinear
dynamical systems to an interactive group scenario. Various individual
processes interact as communities and sub-communities in a tree structure that
is unrolled in time. To accommodate nonlinear temporal activity, each
individual leaf process is modeled as a dynamical system containing discrete
and/or continuous hidden states with discrete and/or Gaussian emissions.
Subsequent higher level parent processes act like hidden Markov models and
mediate the interaction between leaf processes or between other parent
processes in the hierarchy. Aggregator chains are parents of child processes
that they combine and mediate, yielding a compact overall parameterization. We
provide tractable inference and learning algorithms for arbitrary DST
topologies via an efficient structured mean-field algorithm. The diverse
applicability of DSTs is demonstrated by experiments on gene expression data
and by modeling group behavior in the setting of an American football game.
"
"  As part of optimizing the reliability, Thales Optronics now includes systems
that examine the state of its equipment. The aim of this paper is to use hidden
Markov Model to detect as soon as possible a change of state of optronic
equipment in order to propose maintenance before failure. For this, we
carefully observe the dynamic of a variable called ""cool down time"" and noted
Tmf, which reflects the state of the cooling system. Indeed, the Tmf is an
indirect observation of the hidden state of the system. This one is modelled by
a Markov chain and the Tmf is a noisy function of it. Thanks to filtering
equations, we obtain results on the probability that an appliance is in
degraded state at time $t$, knowing the history of the Tmf until this moment.
We have evaluated the numerical behavior of our approach on simulated data.
Then we have applied this methodology on our real data and we have checked that
the results are consistent with the reality. This method can be implemented in
a HUMS (Health and Usage Monitoring System). This simple example of HUMS would
allow the Thales Optronics Company to improve its maintenance system. This
company will be able to recall appliances which are estimated to be in degraded
state and do not control to soon those estimated in stable state.
"
"  In this paper we derive control charts for the variance of a Gaussian process
using the likelihood ratio approach, the generalized likelihood ratio approach,
the sequential probability ratio method and a generalized sequential
probability ratio procedure, the Shiryaev-Roberts procedure and a generalized
Shiryaev-Roberts ap- proach. Recursive presentations for the calculation of the
control statistics are given for autoregressive processes of order 1. In an
extensive simulation study these schemes are compared with existing control
charts for the variance. In order to asses the performance of the schemes both
the average run length and the average delay are used.
"
"  This work presents a technique for particle size generation and placement in
arbitrary closed domains. Its main application is the simulation of granular
media described by disks. Particle size generation is based on the statistical
analysis of granulometric curves which are used as empirical cumulative
distribution functions to sample from mixtures of uniform distributions. The
desired porosity is attained by selecting a certain number of particles, and
their placement is performed by a stochastic point process. We present an
application analyzing different types of sand and clay, where we model the
grain size with the gamma, lognormal, Weibull and hyperbolic distributions. The
parameters from the resulting best fit are used to generate samples from the
theoretical distribution, which are used for filling a finite-size area with
non-overlapping disks deployed by a Simple Sequential Inhibition stochastic
point process. Such filled areas are relevant as plausible inputs for assessing
Discrete Element Method and similar techniques.
"
"  Recent developments in extracting and processing biological and clinical data
are allowing quantitative approaches to studying living systems.
High-throughput sequencing, expression profiles, proteomics, and electronic
health records are some examples of such technologies. Extracting meaningful
information from those technologies requires careful analysis of the large
volumes of data they produce. In this note, we present a set of distributions
that commonly appear in the analysis of such data. These distributions present
some interesting features: they are discontinuous in the rational numbers, but
continuous in the irrational numbers, and possess a certain self-similar
(fractal-like) structure. The first set of examples which we present here are
drawn from a high-throughput sequencing experiment. Here, the self-similar
distributions appear as part of the evaluation of the error rate of the
sequencing technology and the identification of tumorogenic genomic
alterations. The other examples are obtained from risk factor evaluation and
analysis of relative disease prevalence and co-mordbidity as these appear in
electronic clinical data. The distributions are also relevant to identification
of subclonal populations in tumors and the study of the evolution of infectious
diseases, and more precisely the study of quasi-species and intrahost diversity
of viral populations.
"
"  A conceptual framework for cluster analysis from the viewpoint of p-adic
geometry is introduced by describing the space of all dendrograms for n
datapoints and relating it to the moduli space of p-adic Riemannian spheres
with punctures using a method recently applied by Murtagh (2004b). This method
embeds a dendrogram as a subtree into the Bruhat-Tits tree associated to the
p-adic numbers, and goes back to Cornelissen et al. (2001) in p-adic geometry.
After explaining the definitions, the concept of classifiers is discussed in
the context of moduli spaces, and upper bounds for the number of hidden
vertices in dendrograms are given.
"
"  In this paper, we propose a statistical theory on measurement and estimation
of Rayleigh fading channels in wireless communications and provide complete
solutions to the fundamental problems: What is the optimum estimator for the
statistical parameters associated with the Rayleigh fading channel, and how
many measurements are sufficient to estimate these parameters with the
prescribed margin of error and confidence level? Our proposed statistical
theory suggests that two testing signals of different strength be used. The
maximum likelihood (ML) estimator is obtained for estimation of the statistical
parameters of the Rayleigh fading channel that is both sufficient and complete
statistic. Moreover, the ML estimator is the minimum variance (MV) estimator
that in fact achieves the Cramer-Rao lower bound.
"
"  This paper considers the stability of online learning algorithms and its
implications for learnability (bounded regret). We introduce a novel quantity
called {\em forward regret} that intuitively measures how good an online
learning algorithm is if it is allowed a one-step look-ahead into the future.
We show that given stability, bounded forward regret is equivalent to bounded
regret. We also show that the existence of an algorithm with bounded regret
implies the existence of a stable algorithm with bounded regret and bounded
forward regret. The equivalence results apply to general, possibly non-convex
problems. To the best of our knowledge, our analysis provides the first general
connection between stability and regret in the online setting that is not
restricted to a particular class of algorithms. Our stability-regret connection
provides a simple recipe for analyzing regret incurred by any online learning
algorithm. Using our framework, we analyze several existing online learning
algorithms as well as the ""approximate"" versions of algorithms like RDA that
solve an optimization problem at each iteration. Our proofs are simpler than
existing analysis for the respective algorithms, show a clear trade-off between
stability and forward regret, and provide tighter regret bounds in some cases.
Furthermore, using our recipe, we analyze ""approximate"" versions of several
algorithms such as follow-the-regularized-leader (FTRL) that requires solving
an optimization problem at each step.
"
"  In this paper, we give a new generalization error bound of Multiple Kernel
Learning (MKL) for a general class of regularizations, and discuss what kind of
regularization gives a favorable predictive accuracy. Our main target in this
paper is dense type regularizations including \ellp-MKL. According to the
recent numerical experiments, the sparse regularization does not necessarily
show a good performance compared with dense type regularizations. Motivated by
this fact, this paper gives a general theoretical tool to derive fast learning
rates of MKL that is applicable to arbitrary mixed-norm-type regularizations in
a unifying manner. This enables us to compare the generalization performances
of various types of regularizations. As a consequence, we observe that the
homogeneity of the complexities of candidate reproducing kernel Hilbert spaces
(RKHSs) affects which regularization strategy (\ell1 or dense) is preferred. In
fact, in homogeneous complexity settings where the complexities of all RKHSs
are evenly same, \ell1-regularization is optimal among all isotropic norms. On
the other hand, in inhomogeneous complexity settings, dense type
regularizations can show better learning rate than sparse \ell1-regularization.
We also show that our learning rate achieves the minimax lower bound in
homogeneous complexity settings.
"
"  The group lasso is a penalized regression method, used in regression problems
where the covariates are partitioned into groups to promote sparsity at the
group level. Existing methods for finding the group lasso estimator either use
gradient projection methods to update the entire coefficient vector
simultaneously at each step, or update one group of coefficients at a time
using an inexact line search to approximate the optimal value for the group of
coefficients when all other groups' coefficients are fixed. We present a new
method of computation for the group lasso in the linear regression case, the
Single Line Search (SLS) algorithm, which operates by computing the exact
optimal value for each group (when all other coefficients are fixed) with one
univariate line search. We perform simulations demonstrating that the SLS
algorithm is often more efficient than existing computational methods. We also
extend the SLS algorithm to the sparse group lasso problem via the Signed
Single Line Search (SSLS) algorithm, and give theoretical results to support
both algorithms.
"
"  The purpose of cancer genome sequencing studies is to determine the nature
and types of alterations present in a typical cancer and to discover genes
mutated at high frequencies. In this article we discuss statistical methods for
the analysis of somatic mutation frequency data generated in these studies. We
place special emphasis on a two-stage study design introduced by Sj\""{o}blom et
al. [Science 314 (2006) 268--274]. In this context, we describe and compare
statistical methods for constructing scores that can be used to prioritize
candidate genes for further investigation and to assess the statistical
significance of the candidates thus identified. Controversy has surrounded the
reliability of the false discovery rates estimates provided by the
approximations used in early cancer genome studies. To address these, we
develop a semiparametric Bayesian model that provides an accurate fit to the
data. We use this model to generate a large collection of realistic scenarios,
and evaluate alternative approaches on this collection. Our assessment is
impartial in that the model used for generating data is not used by any of the
approaches compared. And is objective, in that the scenarios are generated by a
model that fits data. Our results quantify the conservative control of the
false discovery rate with the Benjamini and Hockberg method compared to the
empirical Bayes approach and the multiple testing method proposed in Storey [J.
R. Stat. Soc. Ser. B Stat. Methodol. 64 (2002) 479--498]. Simulation results
also show a negligible departure from the target false discovery rate for the
methodology used in Sj\""{o}blom et al. [Science 314 (2006) 268--274].
"
"  We consider two active binary-classification problems with atypical
objectives. In the first, active search, our goal is to actively uncover as
many members of a given class as possible. In the second, active surveying, our
goal is to actively query points to ultimately predict the proportion of a
given class. Numerous real-world problems can be framed in these terms, and in
either case typical model-based concerns such as generalization error are only
of secondary importance.
  We approach these problems via Bayesian decision theory; after choosing
natural utility functions, we derive the optimal policies. We provide three
contributions. In addition to introducing the active surveying problem, we
extend previous work on active search in two ways. First, we prove a novel
theoretical result, that less-myopic approximations to the optimal policy can
outperform more-myopic approximations by any arbitrary degree. We then derive
bounds that for certain models allow us to reduce (in practice dramatically)
the exponential search space required by a naive implementation of the optimal
policy, enabling further lookahead while still ensuring that optimal decisions
are always made.
"
"  We describe many vantage points on the Baire metric and its use in clustering
data, or its use in preprocessing and structuring data in order to support
search and retrieval operations. In some cases, we proceed directly to clusters
and do not directly determine the distances. We show how a hierarchical
clustering can be read directly from one pass through the data. We offer
insights also on practical implications of precision of data measurement. As a
mechanism for treating multidimensional data, including very high dimensional
data, we use random projections.
"
"  This paper considers cooperative spectrum sensing in Cognitive Radios. In our
previous work we have developed DualSPRT, a distributed algorithm for
cooperative spectrum sensing using Sequential Probability Ratio Test (SPRT) at
the Cognitive Radios as well as at the fusion center. This algorithm works
well, but is not optimal. In this paper we propose an improved algorithm-
SPRT-CSPRT, which is motivated from Cumulative Sum Procedures (CUSUM). We
analyse it theoretically. We also modify this algorithm to handle uncertainties
in SNR's and fading.
"
"  We describe Information Forests, an approach to classification that
generalizes Random Forests by replacing the splitting criterion of non-leaf
nodes from a discriminative one -- based on the entropy of the label
distribution -- to a generative one -- based on maximizing the information
divergence between the class-conditional distributions in the resulting
partitions. The basic idea consists of deferring classification until a measure
of ""classification confidence"" is sufficiently high, and instead breaking down
the data so as to maximize this measure. In an alternative interpretation,
Information Forests attempt to partition the data into subsets that are ""as
informative as possible"" for the purpose of the task, which is to classify the
data. Classification confidence, or informative content of the subsets, is
quantified by the Information Divergence. Our approach relates to active
learning, semi-supervised learning, mixed generative/discriminative learning.
"
"  The scientific method relies on the iterated processes of inference and
inquiry. The inference phase consists of selecting the most probable models
based on the available data; whereas the inquiry phase consists of using what
is known about the models to select the most relevant experiment. Optimizing
inquiry involves searching the parameterized space of experiments to select the
experiment that promises, on average, to be maximally informative. In the case
where it is important to learn about each of the model parameters, the
relevance of an experiment is quantified by Shannon entropy of the distribution
of experimental outcomes predicted by a probable set of models. If the set of
potential experiments is described by many parameters, we must search this
high-dimensional entropy space. Brute force search methods will be slow and
computationally expensive. We present an entropy-based search algorithm, called
nested entropy sampling, to select the most informative experiment for
efficient experimental design. This algorithm is inspired by Skilling's nested
sampling algorithm used in inference and borrows the concept of a rising
threshold while a set of experiment samples are maintained. We demonstrate that
this algorithm not only selects highly relevant experiments, but also is more
efficient than brute force search. Such entropic search techniques promise to
greatly benefit autonomous experimental design.
"
"  I arrived in Berkeley in 1957, at which time Leo was an Acting Assistant
Professor of Mathematics here. He had recently proven the ""individual ergodic
theorem of information theory""---a triumph---and since this was becoming
central to my own interests, it would have been natural for us to work
together. However, Leo's interests shifted to more applied work, specifically
statistics, and he soon moved to UCLA. So we never became collaborators, but we
did became good friends, especially after 1980 when he returned to Berkeley as
a Professor of Statistics.
"
"  We consider an adversarial online learning setting where a decision maker can
choose an action in every stage of the game. In addition to observing the
reward of the chosen action, the decision maker gets side observations on the
reward he would have obtained had he chosen some of the other actions. The
observation structure is encoded as a graph, where node i is linked to node j
if sampling i provides information on the reward of j. This setting naturally
interpolates between the well-known ""experts"" setting, where the decision maker
can view all rewards, and the multi-armed bandits setting, where the decision
maker can only view the reward of the chosen action. We develop practical
algorithms with provable regret guarantees, which depend on non-trivial
graph-theoretic properties of the information feedback structure. We also
provide partially-matching lower bounds.
"
"  The problem of motif detection can be formulated as the construction of a
discriminant function to separate sequences of a specific pattern from
background. In computational biology, motif detection is used to predict DNA
binding sites of a transcription factor (TF), mostly based on the weight matrix
(WM) model or the Gibbs free energy (FE) model. However, despite the wide
applications, theoretical analysis of these two models and their predictions is
still lacking. We derive asymptotic error rates of prediction procedures based
on these models under different data generation assumptions. This allows a
theoretical comparison between the WM-based and the FE-based predictions in
terms of asymptotic efficiency. Applications of the theoretical results are
demonstrated with empirical studies on ChIP-seq data and protein binding
microarray data. We find that, irrespective of underlying data generation
mechanisms, the FE approach shows higher or comparable predictive power
relative to the WM approach when the number of observed binding sites used for
constructing a discriminant decision is not too small.
"
"  The simulation of the continuation of a given time series is useful for many
practical applications. But no standard procedure for this task is suggested in
the literature. It is therefore demonstrated how to use the seasonal ARIMA
process to simulate the continuation of an observed time series. The R-code
presented uses well-known modeling procedures for ARIMA models and conditional
simulation of a SARIMA model with known parameters. A small example
demonstrates the correctness and practical relevance of the new idea.
"
"  We present a consensus-based distributed particle filter (PF) for wireless
sensor networks. Each sensor runs a local PF to compute a global state estimate
that takes into account the measurements of all sensors. The local PFs use the
joint (all-sensors) likelihood function, which is calculated in a distributed
way by a novel generalization of the likelihood consensus scheme. A performance
improvement (or a reduction of the required number of particles) is achieved by
a novel distributed, consensus-based method for adapting the proposal densities
of the local PFs. The performance of the proposed distributed PF is
demonstrated for a target tracking problem.
"
"  Current analysis of astronomical data are confronted with the daunting task
of modeling the awkward features of astronomical data, among which
heteroscedastic (point-dependent) errors, intrinsic scatter, non-ignorable data
collection (selection effects), data structure, non-uniform populations (often
called Malmquist bias), non-Gaussian data, and upper/lower limits. This chapter
shows, by examples, how modeling all these features using Bayesian methods. In
short, one just need to formalize, using maths, the logical link between the
involved quantities, how the data arise and what we already known on the
quantities we want to study. The posterior probability distribution summarizes
what we known on the studied quantities after the data, and we should not be
afraid about their actual numerical computation, because it is left to
(special) Monte Carlo programs such as JAGS. As examples, we show how to
predict the mass of a new object disposing of a calibrating sample, how to
constraint cosmological parameters from supernovae data and how to check if the
fitted data are in tension with the adopted fitting model. Examples are given
with their coding. These examples can be easily used as template for completely
different analysis, on totally unrelated astronomical objects, requiring to
model the same awkward data features.
"
"  In 2007, and in a series of later papers, Joy Christian claimed to refute
Bell's theorem, presenting an alleged local realistic model of the singlet
correlations using techniques from Geometric Algebra (GA). Several authors
published papers refuting his claims, and Christian's ideas did not gain
acceptance. However, he recently succeeded in publishing yet more ambitious and
complex versions of his theory in fairly mainstream journals. How could this
be? The mathematics and logic of Bell's theorem is simple and transparent and
has been intensely studied and debated for over 50 years. Christian claims to
have a mathematical counterexample to a purely mathematical theorem. Each new
version of Christian's model used new devices to circumvent Bell's theorem or
depended on a new way to misunderstand Bell's work. These devices and
misinterpretations are in common use by other Bell critics, so it useful to
identify and name them. I hope that this paper can serve as a useful resource
to those who need to evaluate new ""disproofs of Bell's theorem"". Christian's
fundamental idea is simple and quite original: he gives a probabilistic
interpretation of the fundamental GA equation a.b = (ab + ba)/2. After that,
ambiguous notation and technical complexity allow sign errors to be hidden from
sight, and new mathematical errors can be introduced.
"
"  We consider $\alpha$-mixing observations and deal with the estimation of the
conditional mode of a scalar response variable $Y$ given a random variable $X$
taking values in a semi-metric space. We provide a convergence rate in $L^p$
norm of the estimator. A useful and typical application to functional times
series prediction is given.
"
"  Nonlinear regression is a useful statistical tool, relating observed data and
a nonlinear function of unknown parameters. When the parameter-dependent
nonlinear function is computationally intensive, a straightforward regression
analysis by maximum likelihood is not feasible. The method presented in this
paper proposes to construct a faster running surrogate for such a
computationally intensive nonlinear function, and to use it in a related
nonlinear statistical model that accounts for the uncertainty associated with
this surrogate. A pivotal quantity in the Earth's climate system is the climate
sensitivity: the change in global temperature due to doubling of atmospheric
$\mathrm{CO}_2$ concentrations. This, along with other climate parameters, are
estimated by applying the statistical method developed in this paper, where the
computationally intensive nonlinear function is the MIT 2D climate model.
"
"  Statistical models of natural stimuli provide an important tool for
researchers in the fields of machine learning and computational neuroscience. A
canonical way to quantitatively assess and compare the performance of
statistical models is given by the likelihood. One class of statistical models
which has recently gained increasing popularity and has been applied to a
variety of complex data are deep belief networks. Analyses of these models,
however, have been typically limited to qualitative analyses based on samples
due to the computationally intractable nature of the model likelihood.
Motivated by these circumstances, the present article provides a consistent
estimator for the likelihood that is both computationally tractable and simple
to apply in practice. Using this estimator, a deep belief network which has
been suggested for the modeling of natural image patches is quantitatively
investigated and compared to other models of natural image patches. Contrary to
earlier claims based on qualitative results, the results presented in this
article provide evidence that the model under investigation is not a
particularly good model for natural images
"
"  Log-normal continuous random cascades form a class of multifractal processes
that has already been successfully used in various fields. Several statistical
issues related to this model are studied. We first make a quick but extensive
review of their main properties and show that most of these properties can be
analytically studied. We then develop an approximation theory of these
processes in the limit of small intermittency $\lambda^2\ll 1$, i.e., when the
degree of multifractality is small. This allows us to prove that the
probability distributions associated with these processes possess some very
simple aggregation properties accross time scales. Such a control of the
process properties at different time scales, allows us to address the problem
of parameter estimation. We show that one has to distinguish two different
asymptotic regimes: the first one, referred to as the ''low frequency regime'',
corresponds to taking a sample whose overall size increases whereas the second
one, referred to as the ''high frequency regime'', corresponds to sampling the
process at an increasing sampling rate. We show that, the first regime leads to
convergent estimators whereas, in the high frequency regime, the situation is
much more intricate : only the intermittency coefficient $\lambda^2$ can be
estimated using a consistent estimator. However, we show that, in practical
situations, one can detect the nature of the asymptotic regime (low frequency
versus high frequency) and consequently decide whether the estimations of the
other parameters are reliable or not. We finally illustrate how both our
results on parameter estimation and on aggregation properties, allow one to
successfully use these models for modelization and prediction of financial time
series.
"
"  We introduce new methods of analysing time to event data via extended
versions of the proportional hazards and accelerated failure time (AFT) models.
In many time to event studies, the time of first observation is arbitrary, in
the sense that no risk modifying event occurs. This is particularly common in
epidemiological studies. We show formally that, in these situations, it is not
sensible to take the first observation as the time origin, either in AFT or
proportional hazards type models. Instead, we advocate using age of the subject
as the time scale. We account for the fact that baseline observations may be
made at different ages in different patients via a two stage procedure. First,
we marginally regress any potentially age-varying covariates against age,
retaining the residuals. These residuals are then used as covariates in the
fitting of either an AFT model or a proportional hazards model. We call the
procedures residual accelerated failure time (RAFT) regression and residual
proportional hazards (RPH) regression respectively. We compare standard AFT
with RAFT, and demonstrate superior predictive ability of RAFT in real
examples. In epidemiology, this has real implications in terms of risk
communication to both patients and policy makers.
"
"  A given set of data-points in some feature space may be associated with a
Schrodinger equation whose potential is determined by the data. This is known
to lead to good clustering solutions. Here we extend this approach into a
full-fledged dynamical scheme using a time-dependent Schrodinger equation.
Moreover, we approximate this Hamiltonian formalism by a truncated calculation
within a set of Gaussian wave functions (coherent states) centered around the
original points. This allows for analytic evaluation of the time evolution of
all such states, opening up the possibility of exploration of relationships
among data-points through observation of varying dynamical-distances among
points and convergence of points into clusters. This formalism may be further
supplemented by preprocessing, such as dimensional reduction through singular
value decomposition or feature filtering.
"
"  Biological sequences may contain patterns that are signal important
biomolecular functions; a classical example is regulation of gene expression by
transcription factors that bind to specific patterns in genomic promoter
regions. In motif discovery we are given a set of sequences that share a common
motif and aim to identify not only the motif composition, but also the binding
sites in each sequence of the set. We present a Bayesian model that is an
extended version of the model adopted by the Gibbs motif sampler, and propose a
new centroid estimator that arises from a refined and meaningful loss function
for binding site inference. We discuss the main advantages of centroid
estimation for motif discovery, including computational convenience, and how
its principled derivation offers further insights about the posterior
distribution of binding site configurations. We also illustrate, using
simulated and real datasets, that the centroid estimator can differ from the
maximum a posteriori estimator.
"
"  A virologic marker, the number of HIV RNA copies or viral load, is currently
used to evaluate antiretroviral (ARV) therapies in AIDS clinical trials. This
marker can be used to assess the ARV potency of therapies, but is easily
affected by drug exposures, drug resistance and other factors during the
long-term treatment evaluation process. HIV dynamic studies have significantly
contributed to the understanding of HIV pathogenesis and ARV treatment
strategies. However, the models of these studies are used to quantify
short-term HIV dynamics ($<$ 1 month), and are not applicable to describe
long-term virological response to ARV treatment due to the difficulty of
establishing a relationship of antiviral response with multiple treatment
factors such as drug exposure and drug susceptibility during long-term
treatment. Long-term therapy with ARV agents in HIV-infected patients often
results in failure to suppress the viral load. Pharmacokinetics (PK), drug
resistance and imperfect adherence to prescribed antiviral drugs are important
factors explaining the resurgence of virus. To better understand the factors
responsible for the virological failure, this paper develops the
mechanism-based nonlinear differential equation models for characterizing
long-term viral dynamics with ARV therapy. The models directly incorporate drug
concentration, adherence and drug susceptibility into a function of treatment
efficacy and, hence, fully integrate virologic, PK, drug adherence and
resistance from an AIDS clinical trial into the analysis. A Bayesian nonlinear
mixed-effects modeling approach in conjunction with the rescaled version of
dynamic differential equations is investigated to estimate dynamic parameters
and make inference. In addition, the correlations of baseline factors with
estimated dynamic parameters are explored and some biologically meaningful
correlation results are presented. Further, the estimated dynamic parameters in
patients with virologic success were compared to those in patients with
virologic failure and significantly important findings were summarized. These
results suggest that viral dynamic parameters may play an important role in
understanding HIV pathogenesis, designing new treatment strategies for
long-term care of AIDS patients.
"
"  In recent years, kernel density estimation has been exploited by computer
scientists to model machine learning problems. The kernel density estimation
based approaches are of interest due to the low time complexity of either O(n)
or O(n*log(n)) for constructing a classifier, where n is the number of sampling
instances. Concerning design of kernel density estimators, one essential issue
is how fast the pointwise mean square error (MSE) and/or the integrated mean
square error (IMSE) diminish as the number of sampling instances increases. In
this article, it is shown that with the proposed kernel function it is feasible
to make the pointwise MSE of the density estimator converge at O(n^-2/3)
regardless of the dimension of the vector space, provided that the probability
density function at the point of interest meets certain conditions.
"
"  A software library for constructing and learning probabilistic models is
presented. The library offers a set of building blocks from which a large
variety of static and dynamic models can be built. These include hierarchical
models for variances of other variables and many nonlinear models. The
underlying variational Bayesian machinery, providing for fast and robust
estimation but being mathematically rather involved, is almost completely
hidden from the user thus making it very easy to use the library. The building
blocks include Gaussian, rectified Gaussian and mixture-of-Gaussians variables
and computational nodes which can be combined rather freely.
"
"  In standard clinical within-subject analyses of event-related fMRI data, two
steps are usually performed separately: detection of brain activity and
estimation of the hemodynamic response. Because these two steps are inherently
linked, we adopt the so-called region-based Joint Detection-Estimation (JDE)
framework that addresses this joint issue using a multivariate inference for
detection and estimation. JDE is built by making use of a regional bilinear
generative model of the BOLD response and constraining the parameter estimation
by physiological priors using temporal and spatial information in a Markovian
modeling. In contrast to previous works that use Markov Chain Monte Carlo
(MCMC) techniques to approximate the resulting intractable posterior
distribution, we recast the JDE into a missing data framework and derive a
Variational Expectation-Maximization (VEM) algorithm for its inference. A
variational approximation is used to approximate the Markovian model in the
unsupervised spatially adaptive JDE inference, which allows fine automatic
tuning of spatial regularisation parameters. It follows a new algorithm that
exhibits interesting properties compared to the previously used MCMC-based
approach. Experiments on artificial and real data show that VEM-JDE is robust
to model mis-specification and provides computational gain while maintaining
good performance in terms of activation detection and hemodynamic shape
recovery.
"
"  This paper introduces a general multi-class approach to weakly supervised
classification. Inferring the labels and learning the parameters of the model
is usually done jointly through a block-coordinate descent algorithm such as
expectation-maximization (EM), which may lead to local minima. To avoid this
problem, we propose a cost function based on a convex relaxation of the
soft-max loss. We then propose an algorithm specifically designed to
efficiently solve the corresponding semidefinite program (SDP). Empirically,
our method compares favorably to standard ones on different datasets for
multiple instance learning and semi-supervised learning as well as on
clustering tasks.
"
"  R\'enyi divergence is related to R\'enyi entropy much like Kullback-Leibler
divergence is related to Shannon's entropy, and comes up in many settings. It
was introduced by R\'enyi as a measure of information that satisfies almost the
same axioms as Kullback-Leibler divergence, and depends on a parameter that is
called its order. In particular, the R\'enyi divergence of order 1 equals the
Kullback-Leibler divergence.
  We review and extend the most important properties of R\'enyi divergence and
Kullback-Leibler divergence, including convexity, continuity, limits of
$\sigma$-algebras and the relation of the special order 0 to the Gaussian
dichotomy and contiguity. We also show how to generalize the Pythagorean
inequality to orders different from 1, and we extend the known equivalence
between channel capacity and minimax redundancy to continuous channel inputs
(for all orders) and present several other minimax results.
"
"  For many diseases, logistic and other constraints often render large
incidence studies difficult, if not impossible, to carry out. This becomes a
drawback, particularly when a new incidence study is needed each time the
disease incidence rate is investigated in a different population. However, by
carrying out a prevalent cohort study with follow-up it is possible to estimate
the incidence rate if it is constant. In this paper we derive the maximum
likelihood estimator (MLE) of the overall incidence rate, $\lambda$, as well as
age-specific incidence rates, by exploiting the well known epidemiologic
relationship, prevalence = incidence $\times$ mean duration ($P = \lambda
\times \mu$). We establish the asymptotic distributions of the MLEs, provide
approximate confidence intervals for the parameters, and point out that the MLE
of $\lambda$ is asymptotically most efficient. Moreover, the MLE of $\lambda$
is the natural estimator obtained by substituting the marginal maximum
likelihood estimators for P and $\mu$, respectively, in the expression $P =
\lambda \times \mu$. Our work is related to that of Keiding (1991, 2006), who,
using a Markov process model, proposed estimators for the incidence rate from a
prevalent cohort study \emph{without} follow-up, under three different
scenarios. However, each scenario requires assumptions that are both disease
specific and depend on the availability of epidemiologic data at the population
level. With follow-up, we are able to remove these restrictions, and our
results apply in a wide range of circumstances. We apply our methods to data
collected as part of the Canadian Study of Health and Ageing to estimate the
incidence rate of dementia amongst elderly Canadians.
"
"  Detection of emerging topics are now receiving renewed interest motivated by
the rapid growth of social networks. Conventional term-frequency-based
approaches may not be appropriate in this context, because the information
exchanged are not only texts but also images, URLs, and videos. We focus on the
social aspects of theses networks. That is, the links between users that are
generated dynamically intentionally or unintentionally through replies,
mentions, and retweets. We propose a probability model of the mentioning
behaviour of a social network user, and propose to detect the emergence of a
new topic from the anomaly measured through the model. We combine the proposed
mention anomaly score with a recently proposed change-point detection technique
based on the Sequentially Discounting Normalized Maximum Likelihood (SDNML), or
with Kleinberg's burst model. Aggregating anomaly scores from hundreds of
users, we show that we can detect emerging topics only based on the
reply/mention relationships in social network posts. We demonstrate our
technique in a number of real data sets we gathered from Twitter. The
experiments show that the proposed mention-anomaly-based approaches can detect
new topics at least as early as the conventional term-frequency-based approach,
and sometimes much earlier when the keyword is ill-defined.
"
"  In this paper we consider the problem of inference on a class of sets
describing a collection of admissible models as solutions to a single smooth
inequality. Classical and recent examples include, among others, the
Hansen-Jagannathan (HJ) sets of admissible stochastic discount factors,
Markowitz-Fama (MF) sets of mean-variances for asset portfolio returns, and the
set of structural elasticities in Chetty (2012)'s analysis of demand with
optimization frictions. We show that the econometric structure of the problem
allows us to construct convenient and powerful confidence regions based upon
the weighted likelihood ratio and weighted Wald (directed weighted Hausdorff)
statistics. The statistics we formulate differ (in part) from existing
statistics in that they enforce either exact or first order equivariance to
transformations of parameters, making them especially appealing in the target
applications. Moreover, the resulting inference procedures are also more
powerful than the structured projection methods, which rely upon building
confidence sets for the frontier-determining sufficient parameters (e.g.
frontier-spanning portfolios), and then projecting them to obtain confidence
sets for HJ sets or MF sets. Lastly, the framework we put forward is also
useful for analyzing intersection bounds, namely sets defined as solutions to
multiple smooth inequalities, since multiple inequalities can be conservatively
approximated by a single smooth inequality. We present two empirical examples
that show how the new econometric methods are able to generate sharp economic
conclusions.
"
"  We consider 1-qubit mixed quantum state estimation by adaptively updating
measurements according to previously obtained outcomes and measurement
settings. Updates are determined by the average-variance-optimality
(A-optimality) criterion, known in the classical theory of experimental design
and applied here to quantum state estimation. In general, A-optimization is a
nonlinear minimization problem; however, we find an analytic solution for
1-qubit state estimation using projective measurements, reducing computational
effort. We compare numerically two adaptive and two nonadaptive schemes for
finite data sets and show that the A-optimality criterion gives more precise
estimates than standard quantum tomography.
"
"  In this paper we address the problem of pool based active learning, and
provide an algorithm, called UPAL, that works by minimizing the unbiased
estimator of the risk of a hypothesis in a given hypothesis space. For the
space of linear classifiers and the squared loss we show that UPAL is
equivalent to an exponentially weighted average forecaster. Exploiting some
recent results regarding the spectra of random matrices allows us to establish
consistency of UPAL when the true hypothesis is a linear hypothesis. Empirical
comparison with an active learner implementation in Vowpal Wabbit, and a
previously proposed pool based active learner implementation show good
empirical performance and better scalability.
"
"  In this paper authors present a general methodology for age dependent
reliability analysis of degrading or ageing systems, structures and
components.The methodology is based on Bayesian methods and inference, its
ability to incorporate prior information and on idea that ageing can be thought
as age dependent change of believes about reliability parameters, when change
of belief occurs not just due to new failure data or other information which
becomes available in time, but also it continuously changes due to flow of time
and beliefs evolution. The main objective of this paper is to present the clear
way of how Bayesian methods can be applied by practitioners to deal with risk
and reliability analysis considering ageing phenomena. The methodology
describes step by step failure rate analysis of ageing systems: from the
Bayesian model building to its verification and generalization with Bayesian
model averaging which, as authors suggest in this paper, could serve as
alternative for various goodness of fit assessment tools and as universal tool
to cope with various sources of uncertainty.
"
"  Estimation of the allele frequency at genetic markers is a key ingredient in
biological and biomedical research, such as studies of human genetic variation
or of the genetic etiology of heritable traits. As genetic data becomes
increasingly available, investigators face a dilemma: when should data from
other studies and population subgroups be pooled with the primary data? Pooling
additional samples will generally reduce the variance of the frequency
estimates; however, used inappropriately, pooled estimates can be severely
biased due to population stratification. Because of this potential bias, most
investigators avoid pooling, even for samples with the same ethnic background
and residing on the same continent. Here, we propose an empirical Bayes
approach for estimating allele frequencies of single nucleotide polymorphisms.
This procedure adaptively incorporates genotypes from related samples, so that
more similar samples have a greater influence on the estimates. In every
example we have considered, our estimator achieves a mean squared error (MSE)
that is smaller than either pooling or not, and sometimes substantially
improves over both extremes. The bias introduced is small, as is shown by a
simulation study that is carefully matched to a real data example. Our method
is particularly useful when small groups of individuals are genotyped at a
large number of markers, a situation we are likely to encounter in a
genome-wide association study.
"
"  How does dynamic price information flow among Northern European electricity
spot prices and prices of major electricity generation fuel sources? We use
time series models combined with new advances in causal inference to answer
these questions. Applying our methods to weekly Nordic and German electricity
prices, and oil, gas and coal prices, with German wind power and Nordic water
reservoir levels as exogenous variables, we estimate a causal model for the
price dynamics, both for contemporaneous and lagged relationships. In
contemporaneous time, Nordic and German electricity prices are interlinked
through gas prices. In the long run, electricity prices and British gas prices
adjust themselves to establish the equlibrium price level, since oil, coal,
continental gas and EUR/USD are found to be weakly exogenous.
"
"  Concave regularization methods provide natural procedures for sparse
recovery. However, they are difficult to analyze in the high dimensional
setting. Only recently a few sparse recovery results have been established for
some specific local solutions obtained via specialized numerical procedures.
Still, the fundamental relationship between these solutions such as whether
they are identical or their relationship to the global minimizer of the
underlying nonconvex formulation is unknown. The current paper fills this
conceptual gap by presenting a general theoretical framework showing that under
appropriate conditions, the global solution of nonconvex regularization leads
to desirable recovery performance; moreover, under suitable conditions, the
global solution corresponds to the unique sparse local solution, which can be
obtained via different numerical procedures. Under this unified framework, we
present an overview of existing results and discuss their connections. The
unified view of this work leads to a more satisfactory treatment of concave
high dimensional sparse estimation procedures, and serves as guideline for
developing further numerical procedures for concave regularization.
"
"  In this short report, we discuss how coordinate-wise descent algorithms can
be used to solve minimum variance portfolio (MVP) problems in which the
portfolio weights are constrained by $l_{q}$ norms, where $1\leq q \leq 2$. A
portfolio which weights are regularised by such norms is called a sparse
portfolio (Brodie et al.), since these constraints facilitate sparsity (zero
components) of the weight vector. We first consider a case when the portfolio
weights are regularised by a weighted $l_{1}$ and squared $l_{2}$ norm. Then
two benchmark data sets (Fama and French 48 industries and 100 size and BM
ratio portfolios) are used to examine performances of the sparse portfolios.
When the sample size is not relatively large to the number of assets, sparse
portfolios tend to have lower out-of-sample portfolio variances, turnover
rates, active assets, short-sale positions, but higher Sharpe ratios than the
unregularised MVP. We then show some possible extensions; particularly we
derive an efficient algorithm for solving an MVP problem in which assets are
allowed to be chosen grouply.
"
"  Existing state-wide data bases on prosecutors' decisions about juvenile
offenders are important, yet often un-explored resources for understanding
changes in patterns of judicial decisions over time. We investigate the extent
and nature of change in judicial behavior toward juveniles following the
enactment of a new set of mandatory registration policies between 1992 and 1996
via analyzing the data on prosecutors' decisions of moving forward for youths
repeatedly charged with sexual violence in South Carolina. To analyze this
longitudinal binary data, we use a random effects logistic regression model via
incorporating an unknown change-point year. For convenient physical
interpretation, our models allow the proportional odds interpretation of
effects of the explanatory variables and the change-point year with and without
conditioning on the youth-specific random effects. As a consequence, the
effects of the unknown change-point year and other factors can be interpreted
as changes in both within youth and population averaged odds of moving forward.
Using a Bayesian paradigm, we consider various prior opinions about the unknown
year of the change in the pattern of prosecutors' decision. Based on the
available data, we make posteriori conclusions about whether a change-point has
occurred between 1992 and 1996 (inclusive), evaluate the degree of confidence
about the year of change-point, estimate the magnitude of the effects of the
change-point and other factors, and investigate other provocative questions
about patterns of prosecutors' decisions over time.
"
"  We consider the problem of high-dimensional Ising (graphical) model
selection. We propose a simple algorithm for structure estimation based on the
thresholding of the empirical conditional variation distances. We introduce a
novel criterion for tractable graph families, where this method is efficient,
based on the presence of sparse local separators between node pairs in the
underlying graph. For such graphs, the proposed algorithm has a sample
complexity of $n=\Omega(J_{\min}^{-2}\log p)$, where $p$ is the number of
variables, and $J_{\min}$ is the minimum (absolute) edge potential in the
model. We also establish nonasymptotic necessary and sufficient conditions for
structure estimation.
"
"  We study computational and sample complexity of parameter and structure
learning in graphical models. Our main result shows that the class of factor
graphs with bounded factor size and bounded connectivity can be learned in
polynomial time and polynomial number of samples, assuming that the data is
generated by a network in this class. This result covers both parameter
estimation for a known network structure and structure learning. It implies as
a corollary that we can learn factor graphs for both Bayesian networks and
Markov networks of bounded degree, in polynomial time and sample complexity.
Unlike maximum likelihood estimation, our method does not require inference in
the underlying network, and so applies to networks where inference is
intractable. We also show that the error of our learned model degrades
gracefully when the generating distribution is not a member of the target class
of networks.
"
"  In several application domains, high-dimensional observations are collected
and then analysed in search for naturally occurring data clusters which might
provide further insights about the nature of the problem. In this paper we
describe a new approach for partitioning such high-dimensional data. Our
assumption is that, within each cluster, the data can be approximated well by a
linear subspace estimated by means of a principal component analysis (PCA). The
proposed algorithm, Predictive Subspace Clustering (PSC) partitions the data
into clusters while simultaneously estimating cluster-wise PCA parameters. The
algorithm minimises an objective function that depends upon a new measure of
influence for PCA models. A penalised version of the algorithm is also
described for carrying our simultaneous subspace clustering and variable
selection. The convergence of PSC is discussed in detail, and extensive
simulation results and comparisons to competing methods are presented. The
comparative performance of PSC has been assessed on six real gene expression
data sets for which PSC often provides state-of-art results.
"
"  Many widely studied graphical models with latent variables lead to nontrivial
constraints on the distribution of the observed variables. Inspired by the Bell
inequalities in quantum mechanics, we refer to any linear inequality whose
violation rules out some latent variable model as a ""hidden variable test"" for
that model. Our main contribution is to introduce a sequence of relaxations
which provides progressively tighter hidden variable tests. We demonstrate
applicability to mixtures of sequences of i.i.d. variables, Bell inequalities,
and homophily models in social networks. For the last, we demonstrate that our
method provides a test that is able to rule out latent homophily as the sole
explanation for correlations on a real social network that are known to be due
to influence.
"
"  This work considers a computationally and statistically efficient parameter
estimation method for a wide class of latent variable models---including
Gaussian mixture models, hidden Markov models, and latent Dirichlet
allocation---which exploits a certain tensor structure in their low-order
observable moments (typically, of second- and third-order). Specifically,
parameter estimation is reduced to the problem of extracting a certain
(orthogonal) decomposition of a symmetric tensor derived from the moments; this
decomposition can be viewed as a natural generalization of the singular value
decomposition for matrices. Although tensor decompositions are generally
intractable to compute, the decomposition of these specially structured tensors
can be efficiently obtained by a variety of approaches, including power
iterations and maximization approaches (similar to the case of matrices). A
detailed analysis of a robust tensor power method is provided, establishing an
analogue of Wedin's perturbation theorem for the singular vectors of matrices.
This implies a robust and computationally tractable estimation approach for
several popular latent variable models.
"
"  We give an overview of two approaches to probability theory where lower and
upper probabilities, rather than probabilities, are used: Walley's behavioural
theory of imprecise probabilities, and Shafer and Vovk's game-theoretic account
of probability. We show that the two theories are more closely related than
would be suspected at first sight, and we establish a correspondence between
them that (i) has an interesting interpretation, and (ii) allows us to freely
import results from one theory into the other. Our approach leads to an account
of probability trees and random processes in the framework of Walley's theory.
We indicate how our results can be used to reduce the computational complexity
of dealing with imprecision in probability trees, and we prove an interesting
and quite general version of the weak law of large numbers.
"
"  Recent reports have described that learning Bayesian networks are highly
sensitive to the chosen equivalent sample size (ESS) in the Bayesian Dirichlet
equivalence uniform (BDeu). This sensitivity often engenders some unstable or
undesirable results. This paper describes some asymptotic analyses of BDeu to
explain the reasons for the sensitivity and its effects. Furthermore, this
paper presents a proposal for a robust learning score for ESS by eliminating
the sensitive factors from the approximation of log-BDeu.
"
"  Standard multivariate analysis methods aim to identify and summarize the main
structures in large data sets containing the description of a number of
observations by several variables. In many cases, spatial information is also
available for each observation, so that a map can be associated to the
multivariate data set. Two main objectives are relevant in the analysis of
spatial multivariate data: summarizing covariation structures and identifying
spatial patterns. In practice, achieving both goals simultaneously is a
statistical challenge, and a range of methods have been developed that offer
trade-offs between these two objectives. In an applied context, this
methodological question has been and remains a major issue in community
ecology, where species assemblages (i.e., covariation between species
abundances) are often driven by spatial processes (and thus exhibit spatial
patterns). In this paper we review a variety of methods developed in community
ecology to investigate multivariate spatial patterns. We present different ways
of incorporating spatial constraints in multivariate analysis and illustrate
these different approaches using the famous data set on moral statistics in
France published by Andr\'{e}-Michel Guerry in 1833. We discuss and compare the
properties of these different approaches both from a practical and theoretical
viewpoint.
"
"  To control for multiscale effects in networks, one can transform the matrix
of (in general) weighted, directed internodal flows to bistochastic
(doubly-stochastic) form, using the iterative proportional fitting
(Sinkhorn-Knopp) procedure, which alternatively scales row and column sums to
all equal 1. The dominant entries in the bistochasticized table can then be
employed for network reduction, using strong component hierarchical clustering.
We illustrate various facets of this well-established, widely-applied two-stage
algorithm with the 3, 107 x 3, 107 (asymmetric) 1995-2000 intercounty migration
flow table for the United States. We compare the results obtained with ones
using the disparity filter, for ""extracting the ""multiscale backbone of complex
weighted networks"", recently put forth by Serrano, Boguna and Vespignani (SBV)
(Proc. Natl. Acad. Sci. 106 [2009], 6483), upon which we have briefly commented
(Proc. Natl. Acad. Sci. 106 [2009], E66). The performance of the bistochastic
filter appears to be superior-at least in this specific case-in two respects:
(1) it requires far fewer links to complete a stongly-connected network
backbone; and (2) it ""belittles"" small flows and nodes less-a principal
desideratum of SBV-in the sense that the correlations of the nonzero raw flows
are considerably weaker with the corresponding bistochastized links than with
the significance levels yielded by the disparity filter. Additional comparative
studies--as called for by SBV-of these two filtering procedures, in particular
as regards their topological properties, should be of considerable interest.
Relatedly, in its many geographic applications, the two-stage procedure
has--with rare exceptions-clustered contiguous areas, often reconstructing
traditional regions (islands, for example), even though no contiguity
constraints, at all, are imposed beforehand.
"
"  In this paper, we propose new sequential methods for detecting port-scan
attackers which routinely perform random ""portscans"" of IP addresses to find
vulnerable servers to compromise. In addition to rigorously control the
probability of falsely implicating benign remote hosts as malicious, our method
performs significantly faster than other current solutions. Moreover, our
method guarantees that the maximum amount of observational time is bounded. In
contrast to the previous most effective method, Threshold Random Walk
Algorithm, which is explicit and analytical in nature, our proposed algorithm
involve parameters to be determined by numerical methods. We have developed
computational techniques such as iterative minimax optimization for quick
determination of the parameters of the new detection algorithm. A framework of
multi-valued decision for testing portscanners is also proposed.
"
"  Disease maps display the spatial pattern in disease risk, so that high-risk
clusters can be identified. The spatial structure in the risk map is typically
represented by a set of random effects, which are modelled with a conditional
autoregressive (CAR) prior. Such priors include a global spatial smoothing
parameter, whereas real risk surfaces are likely to include areas of smooth
evolution as well as discontinuities, the latter of which are known as risk
boundaries. Therefore, this paper proposes an extension to the class of CAR
priors, which can identify both areas of localised spatial smoothness and risk
boundaries. However, allowing for this localised smoothing requires large
numbers of correlation parameters to be estimated, which are unlikely to be
well identified from the data. To address this problem we propose eliciting an
informative prior about the locations of such boundaries, which can be combined
with the information from the data to provide more precise posterior inference.
We test our approach by simulation, before applying it to a study of the risk
of emergency admission to hospital in Greater Glasgow, Scotland.
"
"  Marginal MAP problems are notoriously difficult tasks for graphical models.
We derive a general variational framework for solving marginal MAP problems, in
which we apply analogues of the Bethe, tree-reweighted, and mean field
approximations. We then derive a ""mixed"" message passing algorithm and a
convergent alternative using CCCP to solve the BP-type approximations.
Theoretically, we give conditions under which the decoded solution is a global
or local optimum, and obtain novel upper bounds on solutions. Experimentally we
demonstrate that our algorithms outperform related approaches. We also show
that EM and variational EM comprise a special case of our framework.
"
"  The question of aggregating pair-wise comparisons to obtain a global ranking
over a collection of objects has been of interest for a very long time: be it
ranking of online gamers (e.g. MSR's TrueSkill system) and chess players,
aggregating social opinions, or deciding which product to sell based on
transactions. In most settings, in addition to obtaining a ranking, finding
`scores' for each object (e.g. player's rating) is of interest for
understanding the intensity of the preferences.
  In this paper, we propose Rank Centrality, an iterative rank aggregation
algorithm for discovering scores for objects (or items) from pair-wise
comparisons. The algorithm has a natural random walk interpretation over the
graph of objects with an edge present between a pair of objects if they are
compared; the score, which we call Rank Centrality, of an object turns out to
be its stationary probability under this random walk. To study the efficacy of
the algorithm, we consider the popular Bradley-Terry-Luce (BTL) model
(equivalent to the Multinomial Logit (MNL) for pair-wise comparisons) in which
each object has an associated score which determines the probabilistic outcomes
of pair-wise comparisons between objects. In terms of the pair-wise marginal
probabilities, which is the main subject of this paper, the MNL model and the
BTL model are identical. We bound the finite sample error rates between the
scores assumed by the BTL model and those estimated by our algorithm. In
particular, the number of samples required to learn the score well with high
probability depends on the structure of the comparison graph. When the
Laplacian of the comparison graph has a strictly positive spectral gap, e.g.
each item is compared to a subset of randomly chosen items, this leads to
dependence on the number of samples that is nearly order-optimal.
"
"  In this paper we consider the problem of grouped variable selection in
high-dimensional regression using $\ell_1-\ell_q$ regularization ($1\leq q \leq
\infty$), which can be viewed as a natural generalization of the
$\ell_1-\ell_2$ regularization (the group Lasso). The key condition is that the
dimensionality $p_n$ can increase much faster than the sample size $n$, i.e.
$p_n \gg n$ (in our case $p_n$ is the number of groups), but the number of
relevant groups is small. The main conclusion is that many good properties from
$\ell_1-$regularization (Lasso) naturally carry on to the $\ell_1-\ell_q$ cases
($1 \leq q \leq \infty$), even if the number of variables within each group
also increases with the sample size. With fixed design, we show that the whole
family of estimators are both estimation consistent and variable selection
consistent under different conditions. We also show the persistency result with
random design under a much weaker condition. These results provide a unified
treatment for the whole family of estimators ranging from $q=1$ (Lasso) to
$q=\infty$ (iCAP), with $q=2$ (group Lasso)as a special case. When there is no
group structure available, all the analysis reduces to the current results of
the Lasso estimator ($q=1$).
"
"  We study the task of online boosting--combining online weak learners into an
online strong learner. While batch boosting has a sound theoretical foundation,
online boosting deserves more study from the theoretical perspective. In this
paper, we carefully compare the differences between online and batch boosting,
and propose a novel and reasonable assumption for the online weak learner.
Based on the assumption, we design an online boosting algorithm with a strong
theoretical guarantee by adapting from the offline SmoothBoost algorithm that
matches the assumption closely. We further tackle the task of deciding the
number of weak learners using established theoretical results for online convex
programming and predicting with expert advice. Experiments on real-world data
sets demonstrate that the proposed algorithm compares favorably with existing
online boosting algorithms.
"
"  Sparse coding, which is the decomposition of a vector using only a few basis
elements, is widely used in machine learning and image processing. The basis
set, also called dictionary, is learned to adapt to specific data. This
approach has proven to be very effective in many image processing tasks.
Traditionally, the dictionary is an unstructured ""flat"" set of atoms. In this
paper, we study structured dictionaries which are obtained from an epitome, or
a set of epitomes. The epitome is itself a small image, and the atoms are all
the patches of a chosen size inside this image. This considerably reduces the
number of parameters to learn and provides sparse image decompositions with
shiftinvariance properties. We propose a new formulation and an algorithm for
learning the structured dictionaries associated with epitomes, and illustrate
their use in image denoising tasks.
"
"  Clustering with fast algorithms large samples of high dimensional data is an
important challenge in computational statistics. Borrowing ideas from MacQueen
(1967) who introduced a sequential version of the $k$-means algorithm, a new
class of recursive stochastic gradient algorithms designed for the $k$-medians
loss criterion is proposed. By their recursive nature, these algorithms are
very fast and are well adapted to deal with large samples of data that are
allowed to arrive sequentially. It is proved that the stochastic gradient
algorithm converges almost surely to the set of stationary points of the
underlying loss criterion. A particular attention is paid to the averaged
versions, which are known to have better performances, and a data-driven
procedure that allows automatic selection of the value of the descent step is
proposed.
  The performance of the averaged sequential estimator is compared on a
simulation study, both in terms of computation speed and accuracy of the
estimations, with more classical partitioning techniques such as $k$-means,
trimmed $k$-means and PAM (partitioning around medoids). Finally, this new
online clustering technique is illustrated on determining television audience
profiles with a sample of more than 5000 individual television audiences
measured every minute over a period of 24 hours.
"
"  Recently, the method of b-bit minwise hashing has been applied to large-scale
linear learning and sublinear time near-neighbor search. The major drawback of
minwise hashing is the expensive preprocessing cost, as the method requires
applying (e.g.,) k=200 to 500 permutations on the data. The testing time can
also be expensive if a new data point (e.g., a new document or image) has not
been processed, which might be a significant issue in user-facing applications.
  We develop a very simple solution based on one permutation hashing.
Conceptually, given a massive binary data matrix, we permute the columns only
once and divide the permuted columns evenly into k bins; and we simply store,
for each data vector, the smallest nonzero location in each bin. The
interesting probability analysis (which is validated by experiments) reveals
that our one permutation scheme should perform very similarly to the original
(k-permutation) minwise hashing. In fact, the one permutation scheme can be
even slightly more accurate, due to the ""sample-without-replacement"" effect.
  Our experiments with training linear SVM and logistic regression on the
webspam dataset demonstrate that this one permutation hashing scheme can
achieve the same (or even slightly better) accuracies compared to the original
k-permutation scheme. To test the robustness of our method, we also experiment
with the small news20 dataset which is very sparse and has merely on average
500 nonzeros in each data vector. Interestingly, our one permutation scheme
noticeably outperforms the k-permutation scheme when k is not too small on the
news20 dataset. In summary, our method can achieve at least the same accuracy
as the original k-permutation scheme, at merely 1/k of the original
preprocessing cost.
"
"  The measured properties of stellar oscillations can provide powerful
constraints on the internal structure and composition of stars. To begin this
process, oscillation frequencies must be extracted from the observational data,
typically time series of the star's brightness or radial velocity. In this
paper, a probabilistic model is introduced for inferring the frequencies and
amplitudes of stellar oscillation modes from data, assuming that there is some
periodic character to the oscillations, but that they may not be exactly
sinusoidal. Effectively we fit damped oscillations to the time series, and
hence the mode lifetime is also recovered. While this approach is
computationally demanding for large time series (> 1500 points), it should at
least allow improved analysis of observations of solar-like oscillations in
subgiant and red giant stars, as well as sparse observations of semiregular
stars, where the number of points in the time series is often low. The method
is demonstrated on simulated data and then applied to radial velocity
measurements of the red giant star xi Hydrae, yielding a mode lifetime between
0.41 and 2.65 days with 95% posterior probability. The large frequency
separation between modes is ambiguous, however we argue that the most plausible
value is 6.3 microHz, based on the radial velocity data and the star's position
in the HR diagram.
"
"  Longitudinal studies could be complicated by left-censored repeated measures.
For example, in Human Immunodeficiency Virus infection, there is a detection
limit of the assay used to quantify the plasma viral load. Simple imputation of
the limit of the detection or of half of this limit for left-censored measures
biases estimations and their standard errors. In this paper, we review two
likelihood-based methods proposed to handle left-censoring of the outcome in
linear mixed model. We show how to fit these models using SAS Proc NLMIXED and
we compare this tool with other programs. Indications and limitations of the
programs are discussed and an example in the field of HIV infection is shown.
"
"  We first pursue the study of how hierarchy provides a well-adapted tool for
the analysis of change. Then, using a time sequence-constrained hierarchical
clustering, we develop the practical aspects of a new approach to wavelet
regression. This provides a new way to link hierarchical relationships in a
multivariate time series data set with external signals. Violence data from the
Colombian conflict in the years 1990 to 2004 is used throughout. We conclude
with some proposals for further study on the relationship between social
violence and market forces, viz. between the Colombian conflict and the US
narcotics market.
"
"  Alternative splicing of gene transcripts greatly expands the functional
capacity of the genome, and certain splice isoforms may indicate specific
disease states such as cancer. Splice junction microarrays interrogate
thousands of splice junctions, but data analysis is difficult and error prone
because of the increased complexity compared to differential gene expression
analysis. We present Rank Change Detection (RCD) as a method to identify
differential splicing events based upon a straightforward probabilistic model
comparing the over- or underrepresentation of two or more competing isoforms.
RCD has advantages over commonly used methods because it is robust to false
positive errors due to nonlinear trends in microarray measurements. Further,
RCD does not depend on prior knowledge of splice isoforms, yet it takes
advantage of the inherent structure of mutually exclusive junctions, and it is
conceptually generalizable to other types of splicing arrays or RNA-Seq. RCD
specifically identifies the biologically important cases when a splice junction
becomes more or less prevalent compared to other mutually exclusive junctions.
The example data is from different cell lines of glioblastoma tumors assayed
with Agilent microarrays.
"
"  Given a predictor of outcome derived from a high-dimensional dataset,
pre-validation is a useful technique for comparing it to competing predictors
on the same dataset. For microarray data, it allows one to compare a newly
derived predictor for disease outcome to standard clinical predictors on the
same dataset. We study pre-validation analytically to determine if the
inferences drawn from it are valid. We show that while pre-validation generally
works well, the straightforward ""one degree of freedom"" analytical test from
pre-validation can be biased and we propose a permutation test to remedy this
problem. In simulation studies, we show that the permutation test has the
nominal level and achieves roughly the same power as the analytical test.
"
"  We propose a comprehensive Bayesian approach for graphical model
determination in observational studies that can accommodate binary, ordinal or
continuous variables simultaneously. Our new models are called copula Gaussian
graphical models (CGGMs) and embed graphical model selection inside a
semiparametric Gaussian copula. The domain of applicability of our methods is
very broad and encompasses many studies from social science and economics. We
illustrate the use of the copula Gaussian graphical models in the analysis of a
16-dimensional functional disability contingency table.
"
"  This paper describes a novel approach to change-point detection when the
observed high-dimensional data may have missing elements. The performance of
classical methods for change-point detection typically scales poorly with the
dimensionality of the data, so that a large number of observations are
collected after the true change-point before it can be reliably detected.
Furthermore, missing components in the observed data handicap conventional
approaches. The proposed method addresses these challenges by modeling the
dynamic distribution underlying the data as lying close to a time-varying
low-dimensional submanifold embedded within the ambient observation space.
Specifically, streaming data is used to track a submanifold approximation,
measure deviations from this approximation, and calculate a series of
statistics of the deviations for detecting when the underlying manifold has
changed in a sharp or unexpected manner. The approach described in this paper
leverages several recent results in the field of high-dimensional data
analysis, including subspace tracking with missing data, multiscale analysis
techniques for point clouds, online optimization, and change-point detection
performance analysis. Simulations and experiments highlight the robustness and
efficacy of the proposed approach in detecting an abrupt change in an otherwise
slowly varying low-dimensional manifold.
"
"  We consider a class of sparsity-inducing regularization terms based on
submodular functions. While previous work has focused on non-decreasing
functions, we explore symmetric submodular functions and their \lova
extensions. We show that the Lovasz extension may be seen as the convex
envelope of a function that depends on level sets (i.e., the set of indices
whose corresponding components of the underlying predictor are greater than a
given constant): this leads to a class of convex structured regularization
terms that impose prior knowledge on the level sets, and not only on the
supports of the underlying predictors. We provide a unified set of optimization
algorithms, such as proximal operators, and theoretical guarantees (allowed
level sets and recovery conditions). By selecting specific submodular
functions, we give a new interpretation to known norms, such as the total
variation; we also define new norms, in particular ones that are based on order
statistics with application to clustering and outlier detection, and on noisy
cuts in graphs with application to change point detection in the presence of
outliers.
"
"  The value-at-risk of a delta-gamma approximated derivatives portfolio can be
computed by numerical integration of the characteristic function. However,
while the choice of parameters in any numerical integration scheme is
paramount, in practice it often relies on ad hoc procedures of trial and error.
For normal and multivariate $t$-distributed risk factors, we show how to
calculate the necessary parameters for one particular integration scheme as a
function of the data (the distribution of risk factors, and delta and gamma)
\emph{in order to satisfy a given error tolerance}. This allows for
implementation in a fully automated risk management system. We also demonstrate
in simulations that the method is significantly faster than the Monte Carlo
method, for a given error tolerance.
"
"  A variety of genome-wide profiling techniques are available to probe
complementary aspects of genome structure and function. Integrative analysis of
heterogeneous data sources can reveal higher-level interactions that cannot be
detected based on individual observations. A standard integration task in
cancer studies is to identify altered genomic regions that induce changes in
the expression of the associated genes based on joint analysis of genome-wide
gene expression and copy number profiling measurements. In this review, we
provide a comparison among various modeling procedures for integrating
genome-wide profiling data of gene copy number and transcriptional alterations
and highlight common approaches to genomic data integration. A transparent
benchmarking procedure is introduced to quantitatively compare the cancer gene
prioritization performance of the alternative methods. The benchmarking
algorithms and data sets are available at http://intcomp.r-forge.r-project.org
"
"  We present a novel spectral learning algorithm for simultaneous localization
and mapping (SLAM) from range data with known correspondences. This algorithm
is an instance of a general spectral system identification framework, from
which it inherits several desirable properties, including statistical
consistency and no local optima. Compared with popular batch optimization or
multiple-hypothesis tracking (MHT) methods for range-only SLAM, our spectral
approach offers guaranteed low computational requirements and good tracking
performance. Compared with popular extended Kalman filter (EKF) or extended
information filter (EIF) approaches, and many MHT ones, our approach does not
need to linearize a transition or measurement model; such linearizations can
cause severe errors in EKFs and EIFs, and to a lesser extent MHT, particularly
for the highly non-Gaussian posteriors encountered in range-only SLAM. We
provide a theoretical analysis of our method, including finite-sample error
bounds. Finally, we demonstrate on a real-world robotic SLAM problem that our
algorithm is not only theoretically justified, but works well in practice: in a
comparison of multiple methods, the lowest errors come from a combination of
our algorithm with batch optimization, but our method alone produces nearly as
good a result at far lower computational cost.
"
"  This paper estimates models of high frequency index futures returns using
`around the clock' 5-minute returns that incorporate the following key
features: multiple persistent stochastic volatility factors, jumps in prices
and volatilities, seasonal components capturing time of the day patterns,
correlations between return and volatility shocks, and announcement effects. We
develop an integrated MCMC approach to estimate interday and intraday
parameters and states using high-frequency data without resorting to various
aggregation measures like realized volatility. We provide a case study using
financial crisis data from 2007 to 2009, and use particle filters to construct
likelihood functions for model comparison and out-of-sample forecasting from
2009 to 2012. We show that our approach improves realized volatility forecasts
by up to 50% over existing benchmarks.
"
"  The bootstrap provides a simple and powerful means of assessing the quality
of estimators. However, in settings involving large datasets, the computation
of bootstrap-based quantities can be prohibitively demanding. As an
alternative, we present the Bag of Little Bootstraps (BLB), a new procedure
which incorporates features of both the bootstrap and subsampling to obtain a
robust, computationally efficient means of assessing estimator quality. BLB is
well suited to modern parallel and distributed computing architectures and
retains the generic applicability, statistical efficiency, and favorable
theoretical properties of the bootstrap. We provide the results of an extensive
empirical and theoretical investigation of BLB's behavior, including a study of
its statistical correctness, its large-scale implementation and performance,
selection of hyperparameters, and performance on real data.
"
"  We consider the problem of online linear regression on arbitrary
deterministic sequences when the ambient dimension d can be much larger than
the number of time rounds T. We introduce the notion of sparsity regret bound,
which is a deterministic online counterpart of recent risk bounds derived in
the stochastic setting under a sparsity scenario. We prove such regret bounds
for an online-learning algorithm called SeqSEW and based on exponential
weighting and data-driven truncation. In a second part we apply a
parameter-free version of this algorithm to the stochastic setting (regression
model with random design). This yields risk bounds of the same flavor as in
Dalalyan and Tsybakov (2011) but which solve two questions left open therein.
In particular our risk bounds are adaptive (up to a logarithmic factor) to the
unknown variance of the noise if the latter is Gaussian. We also address the
regression model with fixed design.
"
"  In this paper we describe a general probabilistic framework for modeling
waveforms such as heartbeats from ECG data. The model is based on segmental
hidden Markov models (as used in speech recognition) with the addition of
random effects to the generative model. The random effects component of the
model handles shape variability across different waveforms within a general
class of waveforms of similar shape. We show that this probabilistic model
provides a unified framework for learning these models from sets of waveform
data as well as parsing, classification, and prediction of new waveforms. We
derive a computationally efficient EM algorithm to fit the model on multiple
waveforms, and introduce a scoring method that evaluates a test waveform based
on its shape. Results on two real-world data sets demonstrate that the random
effects methodology leads to improved accuracy (compared to alternative
approaches) on classification and segmentation of real-world waveforms.
"
"  This paper presents a new anytime algorithm for the marginal MAP problem in
graphical models. The algorithm is described in detail, its complexity and
convergence rate are studied, and relations to previous theoretical results for
the problem are discussed. It is shown that the algorithm runs in
polynomial-time if the underlying graph of the model has bounded tree-width,
and that it provides guarantees to the lower and upper bounds obtained within a
fixed amount of computational resources. Experiments with both real and
synthetic generated models highlight its main characteristics and show that it
compares favorably against Park and Darwiche's systematic search, particularly
in the case of problems with many MAP variables and moderate tree-width.
"
"  To maximize its success, an AGI typically needs to explore its initially
unknown world. Is there an optimal way of doing so? Here we derive an
affirmative answer for a broad class of environments.
"
"  We consider daily rainfall observations at 32 stations in the province of
North Holland (the Netherlands) during 30 years. Let $T$ be the total rainfall
in this area on one day. An important question is: what is the amount of
rainfall $T$ that is exceeded once in 100 years? This is clearly a problem
belonging to extreme value theory. Also, it is a genuinely spatial problem.
Recently, a theory of extremes of continuous stochastic processes has been
developed. Using the ideas of that theory and much computer power
(simulations), we have been able to come up with a reasonable answer to the
question above.
"
"  We survey agglomerative hierarchical clustering algorithms and discuss
efficient implementations that are available in R and other software
environments. We look at hierarchical self-organizing maps, and mixture models.
We review grid-based clustering, focusing on hierarchical density-based
approaches. Finally we describe a recently developed very efficient (linear
time) hierarchical clustering algorithm, which can also be viewed as a
hierarchical grid-based algorithm.
"
"  Robust low-rank matrix estimation is a topic of increasing interest, with
promising applications in a variety of fields, from computer vision to data
mining and recommender systems. Recent theoretical results establish the
ability of such data models to recover the true underlying low-rank matrix when
a large portion of the measured matrix is either missing or arbitrarily
corrupted. However, if low rank is not a hypothesis about the true nature of
the data, but a device for extracting regularity from it, no current guidelines
exist for choosing the rank of the estimated matrix. In this work we address
this problem by means of the Minimum Description Length (MDL) principle -- a
well established information-theoretic approach to statistical inference -- as
a guideline for selecting a model for the data at hand. We demonstrate the
practical usefulness of our formal approach with results for complex background
extraction in video sequences.
"
"  We study the problem of learning a sparse linear regression vector under
additional conditions on the structure of its sparsity pattern. This problem is
relevant in machine learning, statistics and signal processing. It is well
known that a linear regression can benefit from knowledge that the underlying
regression vector is sparse. The combinatorial problem of selecting the nonzero
components of this vector can be ""relaxed"" by regularizing the squared error
with a convex penalty function like the $\ell_1$ norm. However, in many
applications, additional conditions on the structure of the regression vector
and its sparsity pattern are available. Incorporating this information into the
learning method may lead to a significant decrease of the estimation error. In
this paper, we present a family of convex penalty functions, which encode prior
knowledge on the structure of the vector formed by the absolute values of the
regression coefficients. This family subsumes the $\ell_1$ norm and is flexible
enough to include different models of sparsity patterns, which are of practical
and theoretical importance. We establish the basic properties of these penalty
functions and discuss some examples where they can be computed explicitly.
Moreover, we present a convergent optimization algorithm for solving
regularized least squares with these penalty functions. Numerical simulations
highlight the benefit of structured sparsity and the advantage offered by our
approach over the Lasso method and other related methods.
"
"  North Pacific subsurface temperature data from the Simple Ocean Data
Assimilation model at 10m, 50m, 75m, 100m and 150m depths, are analyzed using a
combination of state-space decomposition and subspace identification techniques
to examine the spatial structure of thermal variability within the upper water
column. We identify four common trends from our analysis that display the major
broad-scale patterns in the North Pacific over a 47 year period (1958-2004):
(1) a basin-wide near-surface warming trend that identifies the mid 1980's as a
change point from a cooling to a warming trend; (2) a contrasting cooling in
the central basin and warming along the coast of North America that began in
the early 1970's; (3) a cooling along the transition zone and the west coast of
North America that becomes dominant around 1998; (4) and contrasting
differences in the subarctic and subtropical gyres displaying differences in
processes at each depth. We also provide a detailed analysis of the temperature
variability at four chosen locations: 52.5N 142.5W (Gulf of Alaska), 37.5N
172.5W (central basin), 37.5N 137.5W (off coast of California), and 27.5N
137.5W (off coast of Baja California) for both 10m and 150m depths. These
results identify subsurface structure, regional heterogeneity, and they also
display important differences and similarities in the patterns of subsurface
temperature variability when compared to previously published temperature
patterns.
"
"  I argue that we must distinguish between:
  (0) the Three-Doors-Problem Problem [sic], which is to make sense of some
real world question of a real person.
  (1) a large number of solutions to this meta-problem, i.e., many specific
Three-Doors-Problem problems, which are competing mathematizations of the
meta-problem (0).
  Each of the solutions at level (1) can well have a number of different
solutions: nice ones and ugly ones; correct ones and incorrect ones. I discuss
three level (1) solutions, i.e., three different Monty Hall problems; and try
to give three short correct and attractive solutions. These are: an
unconditional probability question; a conditional probability question; and a
game-theory question.
  The meta-message of the article is that applied statisticians should beware
of solution-driven science.
"
"  Information mapping is a popular application of Multivoxel Pattern Analysis
(MVPA) to fMRI. Information maps are constructed using the so called
searchlight method, where the spherical multivoxel neighborhood of every voxel
(i.e., a searchlight) in the brain is evaluated for the presence of
task-relevant response patterns. Despite their widespread use, information maps
present several challenges for interpretation. One such challenge has to do
with inferring the size and shape of a multivoxel pattern from its signature on
the information map. To address this issue, we formally examined the geometric
basis of this mapping relationship. Based on geometric considerations, we show
how and why small patterns (i.e., having smaller spatial extents) can produce a
larger signature on the information map as compared to large patterns,
independent of the size of the searchlight radius. Furthermore, we show that
the number of informative searchlights over the brain increase as a function of
searchlight radius, even in the complete absence of any multivariate response
patterns. These properties are unrelated to the statistical capabilities of the
pattern-analysis algorithms used but are obligatory geometric properties
arising from using the searchlight procedure.
"
"  We present a new analysis of the problem of learning with drifting
distributions in the batch setting using the notion of discrepancy. We prove
learning bounds based on the Rademacher complexity of the hypothesis set and
the discrepancy of distributions both for a drifting PAC scenario and a
tracking scenario. Our bounds are always tighter and in some cases
substantially improve upon previous ones based on the $L_1$ distance. We also
present a generalization of the standard on-line to batch conversion to the
drifting scenario in terms of the discrepancy and arbitrary convex combinations
of hypotheses. We introduce a new algorithm exploiting these learning
guarantees, which we show can be formulated as a simple QP. Finally, we report
the results of preliminary experiments demonstrating the benefits of this
algorithm.
"
"  The problem of structure estimation in graphical models with latent variables
is considered. We characterize conditions for tractable graph estimation and
develop efficient methods with provable guarantees. We consider models where
the underlying Markov graph is locally tree-like, and the model is in the
regime of correlation decay. For the special case of the Ising model, the
number of samples $n$ required for structural consistency of our method scales
as $n=\Omega(\theta_{\min}^{-\delta\eta(\eta+1)-2}\log p)$, where p is the
number of variables, $\theta_{\min}$ is the minimum edge potential, $\delta$ is
the depth (i.e., distance from a hidden node to the nearest observed nodes),
and $\eta$ is a parameter which depends on the bounds on node and edge
potentials in the Ising model. Necessary conditions for structural consistency
under any algorithm are derived and our method nearly matches the lower bound
on sample requirements. Further, the proposed method is practical to implement
and provides flexibility to control the number of latent variables and the
cycle lengths in the output graph.
"
"  We analyze the size of the dictionary constructed from online kernel
sparsification, using a novel formula that expresses the expected determinant
of the kernel Gram matrix in terms of the eigenvalues of the covariance
operator. Using this formula, we are able to connect the cardinality of the
dictionary with the eigen-decay of the covariance operator. In particular, we
show that under certain technical conditions, the size of the dictionary will
always grow sub-linearly in the number of data points, and, as a consequence,
the kernel linear regressor constructed from the resulting dictionary is
consistent.
"
"  We obtain an index of the complexity of a random sequence by allowing the
role of the measure in classical probability theory to be played by a function
we call the generating mechanism. Typically, this generating mechanism will be
a finite automata. We generate a set of biased sequences by applying a finite
state automata with a specified number, $m$, of states to the set of all binary
sequences. Thus we can index the complexity of our random sequence by the
number of states of the automata. We detail optimal algorithms to predict
sequences generated in this way.
"
"  Discussion on ""Brownian distance covariance"" by G\'abor J. Sz\'ekely and
Maria L. Rizzo [arXiv:1010.0297]
"
"  We consider the problem of analyzing the heterogeneity of clustering
distributions for multiple groups of observed data, each of which is indexed by
a covariate value, and inferring global clusters arising from observations
aggregated over the covariate domain. We propose a novel Bayesian nonparametric
method reposing on the formalism of spatial modeling and a nested hierarchy of
Dirichlet processes. We provide an analysis of the model properties, relating
and contrasting the notions of local and global clusters. We also provide an
efficient inference algorithm, and demonstrate the utility of our method in
several data examples, including the problem of object tracking and a global
clustering analysis of functional data where the functional identity
information is not available.
"
"  We propose a generative model of a group EEG analysis, based on appropriate
kernel assumptions on EEG data. We derive the variational inference update rule
using various approximation techniques. The proposed model outperforms the
current state-of-the-art algorithms in terms of common pattern extraction. The
validity of the proposed model is tested on the BCI competition dataset.
"
"  Many structured data-fitting applications require the solution of an
optimization problem involving a sum over a potentially large number of
measurements. Incremental gradient algorithms offer inexpensive iterations by
sampling a subset of the terms in the sum. These methods can make great
progress initially, but often slow as they approach a solution. In contrast,
full-gradient methods achieve steady convergence at the expense of evaluating
the full objective and gradient on each iteration. We explore hybrid methods
that exhibit the benefits of both approaches. Rate-of-convergence analysis
shows that by controlling the sample size in an incremental gradient algorithm,
it is possible to maintain the steady convergence rates of full-gradient
methods. We detail a practical quasi-Newton implementation based on this
approach. Numerical experiments illustrate its potential benefits.
"
"  The aim of this study is to compare two supervised classification methods on
a crucial meteorological problem. The data consist of satellite measurements of
cloud systems which are to be classified either in convective or non convective
systems. Convective cloud systems correspond to lightning and detecting such
systems is of main importance for thunderstorm monitoring and warning. Because
the problem is highly unbalanced, we consider specific performance criteria and
different strategies. This case study can be used in an advanced course of data
mining in order to illustrate the use of logistic regression and random forest
on a real data set with unbalanced classes.
"
"  We propose a multi-wing harmonium model for mining multimedia data that
extends and improves on earlier models based on two-layer random fields, which
capture bidirectional dependencies between hidden topic aspects and observed
inputs. This model can be viewed as an undirected counterpart of the two-layer
directed models such as LDA for similar tasks, but bears significant difference
in inference/learning cost tradeoffs, latent topic representations, and topic
mixing mechanisms. In particular, our model facilitates efficient inference and
robust topic mixing, and potentially provides high flexibilities in modeling
the latent topic spaces. A contrastive divergence and a variational algorithm
are derived for learning. We specialized our model to a dual-wing harmonium for
captioned images, incorporating a multivariate Poisson for word-counts and a
multivariate Gaussian for color histogram. We present empirical results on the
applications of this model to classification, retrieval and image annotation on
news video collections, and we report an extensive comparison with various
extant models.
"
"  We introduce a doubly stochastic marked point process model for supervised
classification problems. Regardless of the number of classes or the dimension
of the feature space, the model requires only 2--3 parameters for the
covariance function. The classification criterion involves a permanental ratio
for which an approximation using a polynomial-time cyclic expansion is
proposed. The approximation is effective even if the feature region occupied by
one class is a patchwork interlaced with regions occupied by other classes. An
application to DNA microarray analysis indicates that the cyclic approximation
is effective even for high-dimensional data. It can employ feature variables in
an efficient way to reduce the prediction error significantly. This is critical
when the true classification relies on non-reducible high-dimensional features.
"
"  An autonomous variational inference algorithm for arbitrary graphical models
requires the ability to optimize variational approximations over the space of
model parameters as well as over the choice of tractable families used for the
variational approximation. In this paper, we present a novel combination of
graph partitioning algorithms with a generalized mean field (GMF) inference
algorithm. This combination optimizes over disjoint clustering of variables and
performs inference using those clusters. We provide a formal analysis of the
relationship between the graph cut and the GMF approximation, and explore
several graph partition strategies empirically. Our empirical results provide
rather clear support for a weighted version of MinCut as a useful clustering
algorithm for GMF inference, which is consistent with the implications from the
formal analysis.
"
"  This paper presents a novel study on gas-like models for economic systems.
The interacting agents and the amount of exchanged money at each trade are
selected with different levels of randomness, from a purely random way to a
more chaotic one. Depending on the interaction rules, these statistical models
can present different asymptotic distributions of money in a community of
individuals with a closed economy.
"
"  Rapid developments in geographical information systems (GIS) continue to
generate interest in analyzing complex spatial datasets. One area of activity
is in creating smoothed disease maps to describe the geographic variation of
disease and generate hypotheses for apparent differences in risk. With multiple
diseases, a multivariate conditionally autoregressive (MCAR) model is often
used to smooth across space while accounting for associations between the
diseases. The MCAR, however, imposes complex covariance structures that are
difficult to interpret and estimate. This article develops a much simpler
alternative approach building upon the techniques of smoothed ANOVA (SANOVA).
Instead of simply shrinking effects without any structure, here we use SANOVA
to smooth spatial random effects by taking advantage of the spatial structure.
We extend SANOVA to cases in which one factor is a spatial lattice, which is
smoothed using a CAR model, and a second factor is, for example, type of
cancer. Datasets routinely lack enough information to identify the additional
structure of MCAR. SANOVA offers a simpler and more intelligible structure than
the MCAR while performing as well. We demonstrate our approach with simulation
studies designed to compare SANOVA with different design matrices versus MCAR
with different priors. Subsequently a cancer-surveillance dataset, describing
incidence of 3-cancers in Minnesota's 87 counties, is analyzed using both
approaches, showing the competitiveness of the SANOVA approach.
"
"  Partial feedback in multiple-input multiple-output (MIMO) communication
systems provides tremendous capacity gain and enables the transmitter to
exploit channel condition and to eliminate channel interference. In the case of
severely limited feedback, constructing a quantized partial feedback is an
important issue. To reduce the computational complexity of the feedback system,
in this paper we introduce an adaptive partial method in which at the
transmitter, an easy to implement least square adaptive algorithm is engaged to
compute the channel state information. In this scheme at the receiver, the time
varying step-size is replied to the transmitter via a reliable feedback
channel. The transmitter iteratively employs this feedback information to
estimate the channel weights. This method is independent of the employed
space-time coding schemes and gives all channel components. Simulation examples
are given to evaluate the performance of the proposed method.
"
"  Acute respiratory diseases are transmitted over networks of social contacts.
Large-scale simulation models are used to predict epidemic dynamics and
evaluate the impact of various interventions, but the contact behavior in these
models is based on simplistic and strong assumptions which are not informed by
survey data. These assumptions are also used for estimating transmission
measures such as the basic reproductive number and secondary attack rates.
Development of methodology to infer contact networks from survey data could
improve these models and estimation methods. We contribute to this area by
developing a model of within-household social contacts and using it to analyze
the Belgian POLYMOD data set, which contains detailed diaries of social
contacts in a 24-hour period. We model dependency in contact behavior through a
latent variable indicating which household members are at home. We estimate
age-specific probabilities of being at home and age-specific probabilities of
contact conditional on two members being at home. Our results differ from the
standard random mixing assumption. In addition, we find that the probability
that all members contact each other on a given day is fairly low: 0.49 for
households with two 0--5 year olds and two 19--35 year olds, and 0.36 for
households with two 12--18 year olds and two 36+ year olds. We find higher
contact rates in households with 2--3 members, helping explain the higher
influenza secondary attack rates found in households of this size.
"
"  Consider a dataset of vector-valued observations that consists of noisy
inliers, which are explained well by a low-dimensional subspace, along with
some number of outliers. This work describes a convex optimization problem,
called REAPER, that can reliably fit a low-dimensional model to this type of
data. This approach parameterizes linear subspaces using orthogonal projectors,
and it uses a relaxation of the set of orthogonal projectors to reach the
convex formulation. The paper provides an efficient algorithm for solving the
REAPER problem, and it documents numerical experiments which confirm that
REAPER can dependably find linear structure in synthetic and natural data. In
addition, when the inliers lie near a low-dimensional subspace, there is a
rigorous theory that describes when REAPER can approximate this subspace.
"
"  Model selection in clustering requires (i) to specify a suitable clustering
principle and (ii) to control the model order complexity by choosing an
appropriate number of clusters depending on the noise level in the data. We
advocate an information theoretic perspective where the uncertainty in the
measurements quantizes the set of data partitionings and, thereby, induces
uncertainty in the solution space of clusterings. A clustering model, which can
tolerate a higher level of fluctuations in the measurements than alternative
models, is considered to be superior provided that the clustering solution is
equally informative. This tradeoff between \emph{informativeness} and
\emph{robustness} is used as a model selection criterion. The requirement that
data partitionings should generalize from one data set to an equally probable
second data set gives rise to a new notion of structure induced information.
"
"  This paper defines and implements a non-Bayesian fusion rule for combining
densities of probabilities estimated by local (non-linear) filters for tracking
a moving target by passive sensors. This rule is the restriction to a strict
probabilistic paradigm of the recent and efficient Proportional Conflict
Redistribution rule no 5 (PCR5) developed in the DSmT framework for fusing
basic belief assignments. A sampling method for probabilistic PCR5 (p-PCR5) is
defined. It is shown that p-PCR5 is more robust to an erroneous modeling and
allows to keep the modes of local densities and preserve as much as possible
the whole information inherent to each densities to combine. In particular,
p-PCR5 is able of maintaining multiple hypotheses/modes after fusion, when the
hypotheses are too distant in regards to their deviations. This new p-PCR5 rule
has been tested on a simple example of distributed non-linear filtering
application to show the interest of such approach for future developments. The
non-linear distributed filter is implemented through a basic particles
filtering technique. The results obtained in our simulations show the ability
of this p-PCR5-based filter to track the target even when the models are not
well consistent in regards to the initialization and real cinematic.
"
"  We propose a restricted collapsed draw (RCD) sampler, a general Markov chain
Monte Carlo sampler of simultaneous draws from a hierarchical Chinese
restaurant process (HCRP) with restriction. Models that require simultaneous
draws from a hierarchical Dirichlet process with restriction, such as infinite
Hidden markov models (iHMM), were difficult to enjoy benefits of \markerg{the}
HCRP due to combinatorial explosion in calculating distributions of coupled
draws. By constructing a proposal of seating arrangements (partitioning) and
stochastically accepts the proposal by the Metropolis-Hastings algorithm, the
RCD sampler makes accurate sampling for complex combination of draws while
retaining efficiency of HCRP representation. Based on the RCD sampler, we
developed a series of sophisticated sampling algorithms for iHMMs, including
blocked Gibbs sampling, beam sampling, and split-merge sampling, that
outperformed conventional iHMM samplers in experiments
"
"  In this paper we propose the use of $\phi$-divergences as test statistics to
verify simple hypotheses about a one-dimensional parametric diffusion process
$\de X_t = b(X_t, \theta)\de t + \sigma(X_t, \theta)\de W_t$, from discrete
observations $\{X_{t_i}, i=0, ..., n\}$ with $t_i = i\Delta_n$, $i=0, 1, >...,
n$, under the asymptotic scheme $\Delta_n\to0$, $n\Delta_n\to\infty$ and
$n\Delta_n^2\to 0$. The class of $\phi$-divergences is wide and includes
several special members like Kullback-Leibler, R\'enyi, power and
$\alpha$-divergences. We derive the asymptotic distribution of the test
statistics based on $\phi$-divergences. The limiting law takes different forms
depending on the regularity of $\phi$. These convergence differ from the
classical results for independent and identically distributed random variables.
Numerical analysis is used to show the small sample properties of the test
statistics in terms of estimated level and power of the test.
"
"  The paper presents a new copula based method for measuring dependence between
random variables. Our approach extends the Maximum Mean Discrepancy to the
copula of the joint distribution. We prove that this approach has several
advantageous properties. Similarly to Shannon mutual information, the proposed
dependence measure is invariant to any strictly increasing transformation of
the marginal variables. This is important in many applications, for example in
feature selection. The estimator is consistent, robust to outliers, and uses
rank statistics only. We derive upper bounds on the convergence rate and
propose independence tests too. We illustrate the theoretical contributions
through a series of experiments in feature selection and low-dimensional
embedding of distributions.
"
"  This paper proposes a novel kernel approach to linear dimension reduction for
supervised learning. The purpose of the dimension reduction is to find
directions in the input space to explain the output as effectively as possible.
The proposed method uses an estimator for the gradient of regression function,
based on the covariance operators on reproducing kernel Hilbert spaces. In
comparison with other existing methods, the proposed one has wide applicability
without strong assumptions on the distributions or the type of variables, and
uses computationally simple eigendecomposition. Experimental results show that
the proposed method successfully finds the effective directions with efficient
computation.
"
"  This paper presents regression models obtained from a process of blind
prediction of peptide binding affinity from provided descriptors for several
distinct datasets as part of the 2006 Comparative Evaluation of Prediction
Algorithms (COEPRA) contest. This paper finds that kernel partial least
squares, a nonlinear partial least squares (PLS) algorithm, outperforms PLS,
and that the incorporation of transferable atom equivalent features improves
predictive capability.
"
"  Generalized Linear Models (GLMs) and Single Index Models (SIMs) provide
powerful generalizations of linear regression, where the target variable is
assumed to be a (possibly unknown) 1-dimensional function of a linear
predictor. In general, these problems entail non-convex estimation procedures,
and, in practice, iterative local search heuristics are often used. Kalai and
Sastry (2009) recently provided the first provably efficient method for
learning SIMs and GLMs, under the assumptions that the data are in fact
generated under a GLM and under certain monotonicity and Lipschitz constraints.
However, to obtain provable performance, the method requires a fresh sample
every iteration. In this paper, we provide algorithms for learning GLMs and
SIMs, which are both computationally and statistically efficient. We also
provide an empirical study, demonstrating their feasibility in practice.
"
"  In recent years, the number of pulsars with secure mass measurements has
increased to a level that allows us to probe the underlying neutron star mass
distribution in detail. We critically review radio pulsar mass measurements and
present a detailed examination through which we are able to put stringent
constraints on the underlying neutron star mass distribution. For the first
time, we are able to analyze a sizable population of neutron star-white dwarf
systems in addition to double neutron star systems with a technique that
accounts for systematically different measurement errors. We find that neutron
stars that have evolved through different evolutionary paths reflect
distinctive signatures through dissimilar distribution peak and mass cutoff
values. Neutron stars in double neutron star and neutron star-white dwarf
systems show consistent respective peaks at 1.35 Msun and 1.50 Msun which
suggest significant mass accretion (Delta m~0.15 Msun) has occurred during the
spin up phase. The width of the mass distribution implied by double neutron
star systems is indicative of a tight initial mass function while the inferred
mass range is significantly wider for neutron stars that have gone through
recycling. We find a mass cutoff at 2 Msun for neutron stars with white dwarf
companions which establishes a firm lower bound for the maximum neutron star
mass. This rules out the majority of strange quark and soft equation of state
models as viable configurations for neutron star matter. The lack of truncation
close to the maximum mass cutoff suggests that the 2 Msun limit is set by
evolutionary constraints rather than nuclear physics or general relativity, and
the existence of rare super-massive neutron stars is possible.
"
"  In this paper, we propose a second order optimization method to learn models
where both the dimensionality of the parameter space and the number of training
samples is high. In our method, we construct on each iteration a Krylov
subspace formed by the gradient and an approximation to the Hessian matrix, and
then use a subset of the training data samples to optimize over this subspace.
As with the Hessian Free (HF) method of [7], the Hessian matrix is never
explicitly constructed, and is computed using a subset of data. In practice, as
in HF, we typically use a positive definite substitute for the Hessian matrix
such as the Gauss-Newton matrix. We investigate the effectiveness of our
proposed method on deep neural networks, and compare its performance to widely
used methods such as stochastic gradient descent, conjugate gradient descent
and L-BFGS, and also to HF. Our method leads to faster convergence than either
L-BFGS or HF, and generally performs better than either of them in
cross-validation accuracy. It is also simpler and more general than HF, as it
does not require a positive semi-definite approximation of the Hessian matrix
to work well nor the setting of a damping parameter. The chief drawback versus
HF is the need for memory to store a basis for the Krylov subspace.
"
"  We give a hybrid two stage design which can be useful to estimate the
reliability of a parallel-series and/or by duality a series-parallel system,
when the component reliabilities are unknown as well as the total numbers of
units allowed to be tested in each subsystem. When a total sample size is fixed
large, asymptotic optimality is proved systematically and validated via Monte
Carlo simulation.
"
"  We offer a novel view of AdaBoost in a statistical setting. We propose a
Bayesian model for binary classification in which label noise is modeled
hierarchically. Using variational inference to optimize a dynamic evidence
lower bound, we derive a new boosting-like algorithm called VIBoost. We show
its close connections to AdaBoost and give experimental results from four
datasets.
"
"  We define and discuss the first sparse coding algorithm based on closed-form
EM updates and continuous latent variables. The underlying generative model
consists of a standard `spike-and-slab' prior and a Gaussian noise model.
Closed-form solutions for E- and M-step equations are derived by generalizing
probabilistic PCA. The resulting EM algorithm can take all modes of a
potentially multi-modal posterior into account. The computational cost of the
algorithm scales exponentially with the number of hidden dimensions. However,
with current computational resources, it is still possible to efficiently learn
model parameters for medium-scale problems. Thus the model can be applied to
the typical range of source separation tasks. In numerical experiments on
artificial data we verify likelihood maximization and show that the derived
algorithm recovers the sparse directions of standard sparse coding
distributions. On source separation benchmarks comprised of realistic data we
show that the algorithm is competitive with other recent methods.
"
"  Joint analysis of data from multiple sources has the potential to improve our
understanding of the underlying structures in complex data sets. For instance,
in restaurant recommendation systems, recommendations can be based on rating
histories of customers. In addition to rating histories, customers' social
networks (e.g., Facebook friendships) and restaurant categories information
(e.g., Thai or Italian) can also be used to make better recommendations. The
task of fusing data, however, is challenging since data sets can be incomplete
and heterogeneous, i.e., data consist of both matrices, e.g., the person by
person social network matrix or the restaurant by category matrix, and
higher-order tensors, e.g., the ""ratings"" tensor of the form restaurant by meal
by person.
  In this paper, we are particularly interested in fusing data sets with the
goal of capturing their underlying latent structures. We formulate this problem
as a coupled matrix and tensor factorization (CMTF) problem where heterogeneous
data sets are modeled by fitting outer-product models to higher-order tensors
and matrices in a coupled manner. Unlike traditional approaches solving this
problem using alternating algorithms, we propose an all-at-once optimization
approach called CMTF-OPT (CMTF-OPTimization), which is a gradient-based
optimization approach for joint analysis of matrices and higher-order tensors.
We also extend the algorithm to handle coupled incomplete data sets. Using
numerical experiments, we demonstrate that the proposed all-at-once approach is
more accurate than the alternating least squares approach.
"
"  The aim of this paper is to generalize the PAC-Bayesian theorems proved by
Catoni in the classification setting to more general problems of statistical
inference. We show how to control the deviations of the risk of randomized
estimators. A particular attention is paid to randomized estimators drawn in a
small neighborhood of classical estimators, whose study leads to control the
risk of the latter. These results allow to bound the risk of very general
estimation procedures, as well as to perform model selection.
"
"  We introduce Clique Matrices as an alternative representation of undirected
graphs, being a generalisation of the incidence matrix representation. Here we
use clique matrices to decompose a graph into a set of possibly overlapping
clusters, de ned as well-connected subsets of vertices. The decomposition is
based on a statistical description which encourages clusters to be well
connected and few in number. Inference is carried out using a variational
approximation. Clique matrices also play a natural role in parameterising
positive de nite matrices under zero constraints on elements of the matrix. We
show that clique matrices can parameterise all positive de nite matrices
restricted according to a decomposable graph and form a structured Factor
Analysis approximation in the non-decomposable case.
"
"  Observational data usually comes with a multimodal nature, which means that
it can be naturally represented by a multi-layer graph whose layers share the
same set of vertices (users) with different edges (pairwise relationships). In
this paper, we address the problem of combining different layers of the
multi-layer graph for improved clustering of the vertices compared to using
layers independently. We propose two novel methods, which are based on joint
matrix factorization and graph regularization framework respectively, to
efficiently combine the spectrum of the multiple graph layers, namely the
eigenvectors of the graph Laplacian matrices. In each case, the resulting
combination, which we call a ""joint spectrum"" of multiple graphs, is used for
clustering the vertices. We evaluate our approaches by simulations with several
real world social network datasets. Results demonstrate the superior or
competitive performance of the proposed methods over state-of-the-art technique
and common baseline methods, such as co-regularization and summation of
information from individual graphs.
"
"  In this paper, we investigate the stationarity of stochastic processes in the
fractional Fourier domains. We study the stationarity of a stochastic process
after performing fractional Fourier transform (FRFT), and discrete fractional
Fourier transform (DFRT) on both continuous and discrete stochastic processes,
respectively. Also we investigate the stationarity of the fractional Fourier
series (FRFS) coefficients of a continuous time stochastic process, and the
stationarity of the discrete time fractional Fourier transform (DTFRFT) of a
discrete time stochastic process. Closed formulas of the input process
autocorrelation function and pseudo-autocorrelation function after performing
the fractional Fourier transform are derived given that the input is a
stationary stochastic process. We derive a formula for the output
autocorrelation as a function of the $a^{th}$ power spectral density of the
input stochastic process, also we derived a formula for the input fractional
power spectral density as a function of the fractional Fourier transform of the
output process autocorrelation function. We proved that, the input stochastic
process must be zero mean to satisfy a necessary but not a sufficient condition
of stationarity in the fractional domains. Closed formulas of the resultant
statistics are also shown. It is shown that, in case of real input process, the
output process is stationary if and only if the input process is white. On the
other hand, if the input process is a complex process, it should be proper
white process to obtain a stationary output process.
"
"  We introduce the optimal obstacle placement with disambiguations problem
wherein the goal is to place true obstacles in an environment cluttered with
false obstacles so as to maximize the total traversal length of a navigating
agent (NAVA). Prior to the traversal, the NAVA is given location information
and probabilistic estimates of each disk-shaped hindrance (hereinafter referred
to as disk) being a true obstacle. The NAVA can disambiguate a disk's status
only when situated on its boundary. There exists an obstacle placing agent
(OPA) that locates obstacles prior to the NAVA's traversal. The goal of the OPA
is to place true obstacles in between the clutter in such a way that the NAVA's
traversal length is maximized in a game-theoretic sense. We assume the OPA
knows the clutter spatial distribution type, but not the exact locations of
clutter disks. We analyze the traversal length using repeated measures analysis
of variance for various obstacle number, obstacle placing scheme and clutter
spatial distribution type combinations in order to identify the optimal
combination. Our results indicate that as the clutter becomes more regular
(clustered), the NAVA's traversal length gets longer (shorter). On the other
hand, the traversal length tends to follow a concave-down trend as the number
of obstacles increases. We also provide a case study on a real-world maritime
minefield data set.
"
"  A popular approach within the signal processing and machine learning
communities consists in modelling signals as sparse linear combinations of
atoms selected from a learned dictionary. While this paradigm has led to
numerous empirical successes in various fields ranging from image to audio
processing, there have only been a few theoretical arguments supporting these
evidences. In particular, sparse coding, or sparse dictionary learning, relies
on a non-convex procedure whose local minima have not been fully analyzed yet.
In this paper, we consider a probabilistic model of sparse signals, and show
that, with high probability, sparse coding admits a local minimum around the
reference dictionary generating the signals. Our study takes into account the
case of over-complete dictionaries and noisy signals, thus extending previous
work limited to noiseless settings and/or under-complete dictionaries. The
analysis we conduct is non-asymptotic and makes it possible to understand how
the key quantities of the problem, such as the coherence or the level of noise,
can scale with respect to the dimension of the signals, the number of atoms,
the sparsity and the number of observations.
"
"  Assigning significance in high-dimensional regression is challenging. Most
computationally efficient selection algorithms cannot guard against inclusion
of noise variables. Asymptotically valid p-values are not available. An
exception is a recent proposal by Wasserman and Roeder (2008) which splits the
data into two parts. The number of variables is then reduced to a manageable
size using the first split, while classical variable selection techniques can
be applied to the remaining variables, using the data from the second split.
This yields asymptotic error control under minimal conditions. It involves,
however, a one-time random split of the data. Results are sensitive to this
arbitrary choice: it amounts to a `p-value lottery' and makes it difficult to
reproduce results. Here, we show that inference across multiple random splits
can be aggregated, while keeping asymptotic control over the inclusion of noise
variables. We show that the resulting p-values can be used for control of both
family-wise error (FWER) and false discovery rate (FDR). In addition, the
proposed aggregation is shown to improve power while reducing the number of
falsely selected variables substantially.
"
"  In this tutorial, I will discuss the details about how Probabilistic Latent
Semantic Analysis (PLSA) is formalized and how different learning algorithms
are proposed to learn the model.
"
"  This paper provides an analysis of the effects of attrition and non-response
on employment and wages using the Canadian Survey of Labour and Income
Dynamics. We consider a structural model composed of three freely correlated
equations for nonattrition/response, employment and wages. The model is
estimated using microdata from 22,990 individuals who provided sufficient
information in the first wave of the 1996-2001 panel. The main findings of the
paper are that attrition is not random. Attritors and non-respondents likely
are less attached to employment and come from low-income population. The
correlation between non-attrition and employment is positive and statistically
significant, though small. Also, wage estimates are biased upwards. Observed
wages are on average higher than wages that would be observed if all the
individuals initially selected in the panel remained in the sample.
"
"  Inferring the causal structure of a set of random variables from a finite
sample of the joint distribution is an important problem in science. Recently,
methods using additive noise models have been suggested to approach the case of
continuous variables. In many situations, however, the variables of interest
are discrete or even have only finitely many states. In this work we extend the
notion of additive noise models to these cases. We prove that whenever the
joint distribution $\prob^{(X,Y)}$ admits such a model in one direction, e.g.
$Y=f(X)+N, N \independent X$, it does not admit the reversed model
$X=g(Y)+\tilde N, \tilde N \independent Y$ as long as the model is chosen in a
generic way. Based on these deliberations we propose an efficient new algorithm
that is able to distinguish between cause and effect for a finite sample of
discrete variables. In an extensive experimental study we show that this
algorithm works both on synthetic and real data sets.
"
"  Complex computer codes, for instance simulating physical phenomena, are often
too time expensive to be directly used to perform uncertainty, sensitivity,
optimization and robustness analyses. A widely accepted method to circumvent
this problem consists in replacing cpu time expensive computer models by cpu
inexpensive mathematical functions, called metamodels. In this paper, we focus
on the Gaussian process metamodel and two essential steps of its definition
phase. First, the initial design of the computer code input variables (which
allows to fit the metamodel) has to honor adequate space filling properties. We
adopt a numerical approach to compare the performance of different types of
space filling designs, in the class of the optimal Latin hypercube samples, in
terms of the predictivity of the subsequent fitted metamodel. We conclude that
such samples with minimal wrap-around discrepancy are particularly well-suited
for the Gaussian process metamodel fitting. Second, the metamodel validation
process consists in evaluating the metamodel predictivity with respect to the
initial computer code. We propose and test an algorithm which optimizes the
distance between the validation points and the metamodel learning points in
order to estimate the true metamodel predictivity with a minimum number of
validation points. Comparisons with classical validation algorithms and
application to a nuclear safety computer code show the relevance of this new
sequential validation design.
"
"  This paper explores a new framework for reinforcement learning based on
online convex optimization, in particular mirror descent and related
algorithms. Mirror descent can be viewed as an enhanced gradient method,
particularly suited to minimization of convex functions in highdimensional
spaces. Unlike traditional gradient methods, mirror descent undertakes gradient
updates of weights in both the dual space and primal space, which are linked
together using a Legendre transform. Mirror descent can be viewed as a proximal
algorithm where the distance generating function used is a Bregman divergence.
A new class of proximal-gradient based temporal-difference (TD) methods are
presented based on different Bregman divergences, which are more powerful than
regular TD learning. Examples of Bregman divergences that are studied include
p-norm functions, and Mahalanobis distance based on the covariance of sample
gradients. A new family of sparse mirror-descent reinforcement learning methods
are proposed, which are able to find sparse fixed points of an l1-regularized
Bellman equation at significantly less computational cost than previous methods
based on second-order matrix methods. An experimental study of mirror-descent
reinforcement learning is presented using discrete and continuous Markov
decision processes.
"
"  Gaussian mixtures are a common density representation in nonlinear,
non-Gaussian Bayesian state estimation. Selecting an appropriate number of
Gaussian components, however, is difficult as one has to trade of computational
complexity against estimation accuracy. In this paper, an adaptive Gaussian
mixture filter based on statistical linearization is proposed. Depending on the
nonlinearity of the considered estimation problem, this filter dynamically
increases the number of components via splitting. For this purpose, a measure
is introduced that allows for quantifying the locally induced linearization
error at each Gaussian mixture component. The deviation between the nonlinear
and the linearized state space model is evaluated for determining the splitting
direction. The proposed approach is not restricted to a specific statistical
linearization method. Simulations show the superior estimation performance
compared to related approaches and common filtering algorithms.
"
"  The goal of supervised feature selection is to find a subset of input
features that are responsible for predicting output values. The least absolute
shrinkage and selection operator (Lasso) allows computationally efficient
feature selection based on linear dependency between input features and output
values. In this paper, we consider a feature-wise kernelized Lasso for
capturing non-linear input-output dependency. We first show that, with
particular choices of kernel functions, non-redundant features with strong
statistical dependence on output values can be found in terms of kernel-based
independence measures. We then show that the globally optimal solution can be
efficiently computed; this makes the approach scalable to high-dimensional
problems. The effectiveness of the proposed method is demonstrated through
feature selection experiments with thousands of features.
"
"  In recent years a number of methods have been developed for automatically
learning the (sparse) connectivity structure of Markov Random Fields. These
methods are mostly based on L1-regularized optimization which has a number of
disadvantages such as the inability to assess model uncertainty and expensive
cross-validation to find the optimal regularization parameter. Moreover, the
model's predictive performance may degrade dramatically with a suboptimal value
of the regularization parameter (which is sometimes desirable to induce
sparseness). We propose a fully Bayesian approach based on a ""spike and slab""
prior (similar to L0 regularization) that does not suffer from these
shortcomings. We develop an approximate MCMC method combining Langevin dynamics
and reversible jump MCMC to conduct inference in this model. Experiments show
that the proposed model learns a good combination of the structure and
parameter values without the need for separate hyper-parameter tuning.
Moreover, the model's predictive performance is much more robust than L1-based
methods with hyper-parameter settings that induce highly sparse model
structures.
"
"  The number of extant individuals within a lineage, as exemplified by counts
of species numbers across genera in a higher taxonomic category, is known to be
a highly skewed distribution. Because the sublineages (such as genera in a
clade) themselves follow a random birth process, deriving the distribution of
lineage sizes involves averaging the solutions to a birth and death process
over the distribution of time intervals separating the origin of the lineages.
In this paper, we show that the resulting distributions can be represented by
hypergeometric functions of the second kind. We also provide approximations of
these distributions up to the second order, and compare these results to the
asymptotic distributions and numerical approximations used in previous studies.
For two limiting cases, one with a relatively high rate of lineage origin, one
with a low rate, the cumulative probability densities and percentiles are
compared to show that the approximations are robust over a wide rane of
parameters. It is proposed that the probability density distributions of
lineage size may have a number of relevant applications to biological problems
such as the coalescence of genetic lineages and in predicting the number of
species in living and extinct higher taxa, as these systems are special
instances of the underlying process analyzed in this paper.
"
"  We propose a new analytical approximation to the $\chi^2$ kernel that
converges geometrically. The analytical approximation is derived with
elementary methods and adapts to the input distribution for optimal convergence
rate. Experiments show the new approximation leads to improved performance in
image classification and semantic segmentation tasks using a random Fourier
feature approximation of the $\exp-\chi^2$ kernel. Besides, out-of-core
principal component analysis (PCA) methods are introduced to reduce the
dimensionality of the approximation and achieve better performance at the
expense of only an additional constant factor to the time complexity. Moreover,
when PCA is performed jointly on the training and unlabeled testing data,
further performance improvements can be obtained. Experiments conducted on the
PASCAL VOC 2010 segmentation and the ImageNet ILSVRC 2010 datasets show
statistically significant improvements over alternative approximation methods.
"
"  Min-cut clustering, based on minimizing one of two heuristic cost-functions
proposed by Shi and Malik, has spawned tremendous research, both analytic and
algorithmic, in the graph partitioning and image segmentation communities over
the last decade. It is however unclear if these heuristics can be derived from
a more general principle facilitating generalization to new problem settings.
Motivated by an existing graph partitioning framework, we derive relationships
between optimizing relevance information, as defined in the Information
Bottleneck method, and the regularized cut in a K-partitioned graph. For fast
mixing graphs, we show that the cost functions introduced by Shi and Malik can
be well approximated as the rate of loss of predictive information about the
location of random walkers on the graph. For graphs generated from a stochastic
algorithm designed to model community structure, the optimal information
theoretic partition and the optimal min-cut partition are shown to be the same
with high probability.
"
"  Identifying overlapping communities in networks is a challenging task. In
this work we present a novel approach to community detection that utilises the
Bayesian non-negative matrix factorisation (NMF) model to produce a
probabilistic output for node memberships. The scheme has the advantage of
computational efficiency, soft community membership and an intuitive
foundation. We present the performance of the method against a variety of
benchmark problems and compare and contrast it to several other algorithms for
community detection. Our approach performs favourably compared to other methods
at a fraction of the computational costs.
"
"  Sequences of events in noise-driven excitable systems with slow variables
often show serial correlations among their intervals of events. Here, we employ
a master equation for general non-renewal processes to calculate the interval
and count statistics of superimposed processes governed by a slow adaptation
variable. For an ensemble of spike-frequency adapting neurons this results in
the regularization of the population activity and an enhanced post-synaptic
signal decoding. We confirm our theoretical results in a population of cortical
neurons.
"
"  Two challenging problems in the clinical study of cancer are the
characterization of cancer subtypes and the classification of individual
patients according to those subtypes. Statistical approaches addressing these
problems are hampered by population heterogeneity and challenges inherent in
data integration across high-dimensional, diverse covariates. We have developed
a survival-supervised latent Dirichlet allocation (survLDA) modeling framework
to address these concerns. LDA models have proven extremely effective at
identifying themes common across large collections of text, but applications to
genomics have been limited. Our framework extends LDA to the genome by
considering each patient as a `document' with `text' constructed from clinical
and high-dimensional genomic measurements. We then further extend the framework
to allow for supervision by a time-to-event response. The model enables the
efficient identification of collections of clinical and genomic features that
co-occur within patient subgroups, and then characterizes each patient by those
features. An application of survLDA to The Cancer Genome Atlas (TCGA) ovarian
project identifies informative patient subgroups that are characterized by
different propensities for exhibiting abnormal mRNA expression and
methylations, corresponding to differential rates of survival from primary
therapy.
"
"  This paper addresses the issues of conservativeness and computational
complexity of probabilistic robustness analysis. We solve both issues by
defining a new sampling strategy and robustness measure. The new measure is
shown to be much less conservative than the existing one. The new sampling
strategy enables the definition of efficient hierarchical sample reuse
algorithms that reduce significantly the computational complexity and make it
independent of the dimension of the uncertainty space. Moreover, we show that
there exists a one to one correspondence between the new and the existing
robustness measures and provide a computationally simple algorithm to derive
one from the other.
"
"  We consider the problem of estimating a sparse multi-response regression
function, with an application to expression quantitative trait locus (eQTL)
mapping, where the goal is to discover genetic variations that influence
gene-expression levels. In particular, we investigate a shrinkage technique
capable of capturing a given hierarchical structure over the responses, such as
a hierarchical clustering tree with leaf nodes for responses and internal nodes
for clusters of related responses at multiple granularity, and we seek to
leverage this structure to recover covariates relevant to each
hierarchically-defined cluster of responses. We propose a tree-guided group
lasso, or tree lasso, for estimating such structured sparsity under
multi-response regression by employing a novel penalty function constructed
from the tree. We describe a systematic weighting scheme for the overlapping
groups in the tree-penalty such that each regression coefficient is penalized
in a balanced manner despite the inhomogeneous multiplicity of group
memberships of the regression coefficients due to overlaps among groups. For
efficient optimization, we employ a smoothing proximal gradient method that was
originally developed for a general class of structured-sparsity-inducing
penalties. Using simulated and yeast data sets, we demonstrate that our method
shows a superior performance in terms of both prediction errors and recovery of
true sparsity patterns, compared to other methods for learning a
multivariate-response regression.
"
"  We consider a class of sparse learning problems in high dimensional feature
space regularized by a structured sparsity-inducing norm which incorporates
prior knowledge of the group structure of the features. Such problems often
pose a considerable challenge to optimization algorithms due to the
non-smoothness and non-separability of the regularization term. In this paper,
we focus on two commonly adopted sparsity-inducing regularization terms, the
overlapping Group Lasso penalty $l_1/l_2$-norm and the $l_1/l_\infty$-norm. We
propose a unified framework based on the augmented Lagrangian method, under
which problems with both types of regularization and their variants can be
efficiently solved. As the core building-block of this framework, we develop
new algorithms using an alternating partial-linearization/splitting technique,
and we prove that the accelerated versions of these algorithms require
$O(\frac{1}{\sqrt{\epsilon}})$ iterations to obtain an $\epsilon$-optimal
solution. To demonstrate the efficiency and relevance of our algorithms, we
test them on a collection of data sets and apply them to two real-world
problems to compare the relative merits of the two norms.
"
"  We illustrate a class of Item Response Theory (IRT) models for binary and
ordinal polythomous items and we describe an R package for dealing with these
models, which is named MultiLCIRT. The models at issue extend traditional IRT
models allowing for (i) multidimensionality and (ii) discreteness of latent
traits. This class of models also allows for different parameterizations for
the conditional distribution of the response variables given the latent traits,
depending on both the type of link function and the constraints imposed on the
discriminating and the difficulty item parameters. We illustrate how the
proposed class of models may be estimated by the maximum likelihood approach
via an Expectation-Maximization algorithm, which is implemented in the
MultiLCIRT package, and we discuss in detail issues related to model selection.
In order to illustrate this package, we analyze two datasets: one concerning
binary items and referred to the measurement of ability in mathematics and the
other one coming from the administration of ordinal polythomous items for the
assessment of anxiety and depression. In the first application, we illustrate
how aggregating items in homogeneous groups through a model-based hierarchical
clustering procedure which is implemented in the proposed package. In the
second application, we describe the steps to select a specific model having the
best fit in our class of IRT models.
"
"  Reversible jump Markov chain Monte Carlo (RJMCMC) extends ordinary MCMC
methods for use in Bayesian multimodel inference. We show that RJMCMC can be
implemented as Gibbs sampling with alternating updates of a model indicator and
a vector-valued ""palette"" of parameters denoted $\bm \psi$. Like an artist uses
the palette to mix dabs of color for specific needs, we create model-specific
parameters from the set available in $\bm \psi$. This description not only
removes some of the mystery of RJMCMC, but also provides a basis for fitting
models one at a time using ordinary MCMC and computing model weights or Bayes
factors by post-processing the Monte Carlo output. We illustrate our procedure
using several examples.
"
"  We present a method to estimate block membership of nodes in a random graph
generated by a stochastic blockmodel. We use an embedding procedure motivated
by the random dot product graph model, a particular example of the latent
position model. The embedding associates each node with a vector; these vectors
are clustered via minimization of a square error criterion. We prove that this
method is consistent for assigning nodes to blocks, as only a negligible number
of nodes will be mis-assigned. We prove consistency of the method for directed
and undirected graphs. The consistent block assignment makes possible
consistent parameter estimation for a stochastic blockmodel. We extend the
result in the setting where the number of blocks grows slowly with the number
of nodes. Our method is also computationally feasible even for very large
graphs. We compare our method to Laplacian spectral clustering through analysis
of simulated data and a graph derived from Wikipedia documents.
"
"  The performance of sparse signal recovery from noise corrupted,
underdetermined measurements can be improved if both sparsity and correlation
structure of signals are exploited. One typical correlation structure is the
intra-block correlation in block sparse signals. To exploit this structure, a
framework, called block sparse Bayesian learning (BSBL), has been proposed
recently. Algorithms derived from this framework showed superior performance
but they are not very fast, which limits their applications. This work derives
an efficient algorithm from this framework, using a marginalized likelihood
maximization method. Compared to existing BSBL algorithms, it has close
recovery performance but is much faster. Therefore, it is more suitable for
large scale datasets and applications requiring real-time implementation.
"
"  The assassination of President John Fitzgerald Kennedy (JFK) traumatized the
nation. In this paper we show that evidence used to rule out a second assassin
is fundamentally flawed. This paper discusses new compositional analyses of
bullets reportedly to have been derived from the same batch as those used in
the assassination. The new analyses show that the bullet fragments involved in
the assassination are not nearly as rare as previously reported. In particular,
the new test results are compared to key bullet composition testimony presented
before the House Select Committee on Assassinations (HSCA). Matches of bullets
within the same box of bullets are shown to be much more likely than indicated
in the House Select Committee on Assassinations' testimony. Additionally, we
show that one of the ten test bullets is considered a match to one or more
assassination fragments. This finding means that the bullet fragments from the
assassination that match could have come from three or more separate bullets.
Finally, this paper presents a case for reanalyzing the assassination bullet
fragments and conducting the necessary supporting scientific studies. These
analyses will shed light on whether the five bullet fragments constitute three
or more separate bullets. If the assassination fragments are derived from three
or more separate bullets, then a second assassin is likely, as the additional
bullet would not easily be attributable to the main suspect, Mr. Oswald, under
widely accepted shooting scenarios [see Posner (1993), Case Closed, Bantam, New
York].
"
"  Online learning algorithms have impressive convergence properties when it
comes to risk minimization and convex games on very large problems. However,
they are inherently sequential in their design which prevents them from taking
advantage of modern multi-core architectures. In this paper we prove that
online learning with delayed updates converges well, thereby facilitating
parallel online learning.
"
"  We develop a simulation tool to support policy-decisions about healthcare for
chronic diseases in defined populations. Incident disease-cases are generated
in-silico from an age-sex characterised general population using standard
epidemiological approaches. A novel disease-treatment model then simulates
continuous life courses for each patient using discrete event simulation.
Ideally, the discrete event simulation model would be inferred from complete
longitudinal healthcare data via a likelihood or Bayesian approach. Such data
is seldom available for relevant populations, therefore an innovative approach
to evidence synthesis is required. We propose a novel entropy-based approach to
fit survival densities. This method provides a fully flexible way to
incorporate the available information, which can be derived from arbitrary
sources. Discrete event simulation then takes place on the fitted model using a
competing hazards framework. The output is then used to help evaluate the
potential impacts of policy options for a given population.
"
"  It is possible to approach regression analysis with random covariates from a
semiparametric perspective where information is combined from multiple
multivariate sources. The approach assumes a semiparametric density ratio model
where multivariate distributions are ""regressed"" on a reference distribution. A
kernel density estimator can be constructed from many data sources in
conjunction with the semiparametric model. The estimator is shown to be more
efficient than the traditional single-sample kernel density estimator, and its
optimal bandwidth is discussed in some detail. Each multivariate distribution
and the corresponding conditional expectation (regression) of interest are
estimated from the combined data using all sources. Graphical and quantitative
diagnostic tools are suggested to assess model validity. The method is applied
in quantifying the effect of height and age on weight of germ cell testicular
cancer patients. Comparisons are made with multiple regression, generalized
additive models (GAM) and nonparametric kernel regression.
"
"  Due to globalization and relaxed market regulation, we have assisted to an
increasing of extremal dependence in international markets. As a consequence,
several measures of tail dependence have been stated in literature in recent
years, based on multivariate extreme-value theory. In this paper we present a
tail dependence function and an extremal coefficient of dependence between two
random vectors that extend existing ones. We shall see that in weakening the
usual required dependence allows to assess the amount of dependence in
$d$-variate random vectors based on bidimensional techniques. Very simple
estimators will be stated and can be applied to the well-known \emph{stable
tail dependence function}. Asymptotic normality and strong consistency will be
derived too. An application to financial markets will be presented at the end.
"
"  Monte Carlo (MC) techniques are often used to estimate integrals of a
multivariate function using randomly generated samples of the function. In
light of the increasing interest in uncertainty quantification and robust
design applications in aerospace engineering, the calculation of expected
values of such functions (e.g. performance measures) becomes important.
However, MC techniques often suffer from high variance and slow convergence as
the number of samples increases. In this paper we present Stacked Monte Carlo
(StackMC), a new method for post-processing an existing set of MC samples to
improve the associated integral estimate. StackMC is based on the supervised
learning techniques of fitting functions and cross validation. It should reduce
the variance of any type of Monte Carlo integral estimate (simple sampling,
importance sampling, quasi-Monte Carlo, MCMC, etc.) without adding bias. We
report on an extensive set of experiments confirming that the StackMC estimate
of an integral is more accurate than both the associated unprocessed Monte
Carlo estimate and an estimate based on a functional fit to the MC samples.
These experiments run over a wide variety of integration spaces, numbers of
sample points, dimensions, and fitting functions. In particular, we apply
StackMC in estimating the expected value of the fuel burn metric of future
commercial aircraft and in estimating sonic boom loudness measures. We compare
the efficiency of StackMC with that of more standard methods and show that for
negligible additional computational cost significant increases in accuracy are
gained.
"
"  In this study we illustrate a statistical approach to questioned document
examination. Specifically, we consider the construction of three classifiers
that predict the writer of a sample document based on categorical data. To
evaluate these classifiers, we use a data set with a large number of writers
and a small number of writing samples per writer. Since the resulting
classifiers were found to have near perfect accuracy using leave-one-out
cross-validation, we propose a novel Bayesian-based cross-validation method for
evaluating the classifiers.
"
"  The Adaptive Multiple Importance Sampling (AMIS) algorithm is aimed at an
optimal recycling of past simulations in an iterated importance sampling
scheme. The difference with earlier adaptive importance sampling
implementations like Population Monte Carlo is that the importance weights of
all simulated values, past as well as present, are recomputed at each
iteration, following the technique of the deterministic multiple mixture
estimator of Owen and Zhou (2000). Although the convergence properties of the
algorithm cannot be fully investigated, we demonstrate through a challenging
banana shape target distribution and a population genetics example that the
improvement brought by this technique is substantial.
"
"  Least mean square-partial parallel interference cancelation (LMS-PPIC) is a
partial interference cancelation using adaptive multistage structure in which
the normalized least mean square (NLMS) adaptive algorithm is engaged to obtain
the cancelation weights. The performance of the NLMS algorithm is mostly
dependent to its step-size. A fixed and non-optimized step-size causes the
propagation of error from one stage to the next one. When all user channels are
balanced, the unit magnitude is the principal property of the cancelation
weight elements. Based on this fact and using a set of NLMS algorithms with
different step-sizes, the parallel LMS-PPIC (PLMS-PPIC) method is proposed. In
each iteration of the algorithm, the parameter estimate of the NLMS algorithm
is chosen to match the elements' magnitudes of the cancelation weight estimate
with unity. Simulation results are given to compare the performance of our
method with the LMS-PPIC algorithm in three cases: balanced channel, unbalanced
channel and time varying channel.
"
"  In a recent work (arXiv:0910.2517), for nonlinear models with sparse
underlying linear structures, we studied the error bounds of
$\ell_0$-regularized estimation. In this note, we show that
$\ell_1$-regularized estimation in some important cases can achieve the same
order of error bounds as those in the aforementioned work.
"
"  Under what conditions is an edge present in a social network at time t likely
to decay or persist by some future time t + Delta(t)? Previous research
addressing this issue suggests that the network range of the people involved in
the edge, the extent to which the edge is embedded in a surrounding structure,
and the age of the edge all play a role in edge decay. This paper uses weighted
data from a large-scale social network built from cell-phone calls in an 8-week
period to determine the importance of edge weight for the decay/persistence
process. In particular, we study the relative predictive power of directed
weight, embeddedness, newness, and range (measured as outdegree) with respect
to edge decay and assess the effectiveness with which a simple decision tree
and logistic regression classifier can accurately predict whether an edge that
was active in one time period continues to be so in a future time period. We
find that directed edge weight, weighted reciprocity and time-dependent
measures of edge longevity are highly predictive of whether we classify an edge
as persistent or decayed, relative to the other types of factors at the dyad
and neighborhood level.
"
"  Vapnik-Chervonenkis (VC) dimension is a fundamental measure of the
generalization capacity of learning algorithms. However, apart from a few
special cases, it is hard or impossible to calculate analytically. Vapnik et
al. [10] proposed a technique for estimating the VC dimension empirically.
While their approach behaves well in simulations, it could not be used to bound
the generalization risk of classifiers, because there were no bounds for the
estimation error of the VC dimension itself. We rectify this omission,
providing high probability concentration results for the proposed estimator and
deriving corresponding generalization bounds.
"
"  In this paper we propose an algorithm that builds sparse decision DAGs
(directed acyclic graphs) from a list of base classifiers provided by an
external learning method such as AdaBoost. The basic idea is to cast the DAG
design task as a Markov decision process. Each instance can decide to use or to
skip each base classifier, based on the current state of the classifier being
built. The result is a sparse decision DAG where the base classifiers are
selected in a data-dependent way. The method has a single hyperparameter with a
clear semantics of controlling the accuracy/speed trade-off. The algorithm is
competitive with state-of-the-art cascade detectors on three object-detection
benchmarks, and it clearly outperforms them when there is a small number of
base classifiers. Unlike cascades, it is also readily applicable for
multi-class classification. Using the multi-class setup, we show on a benchmark
web page ranking data set that we can significantly improve the decision speed
without harming the performance of the ranker.
"
"  The volatility of financial instruments is rarely constant, and usually
varies over time. This creates a phenomenon called volatility clustering, where
large price movements on one day are followed by similarly large movements on
successive days, creating temporal clusters. The GARCH model, which treats
volatility as a drift process, is commonly used to capture this behavior.
However research suggests that volatility is often better described by a
structural break model, where the volatility undergoes abrupt jumps in addition
to drift. Most efforts to integrate these jumps into the GARCH methodology have
resulted in models which are either very computationally demanding, or which
make problematic assumptions about the distribution of the instruments, often
assuming that they are Gaussian. We present a new approach which uses ideas
from nonparametric statistics to identify structural break points without
making such distributional assumptions, and then models drift separately within
each identified regime. Using our method, we investigate the volatility of
several major stock indexes, and find that our approach can potentially give an
improved fit compared to more commonly used techniques.
"
"  Researchers in many scientific fields make inferences from individuals to
larger groups. For many groups however, there is no list of members from which
to take a random sample. Respondent-driven sampling (RDS) is a relatively new
sampling methodology that circumvents this difficulty by using the social
networks of the groups under study. The RDS method has been shown to provide
unbiased estimates of population proportions given certain conditions. The
method is now widely used in the study of HIV-related high-risk populations
globally. In this paper, we test the RDS methodology by simulating RDS studies
on the social networks of a large LGBT web community. The robustness of the RDS
method is tested by violating, one by one, the conditions under which the
method provides unbiased estimates. Results reveal that the risk of bias is
large if networks are directed, or respondents choose to invite persons based
on characteristics that are correlated with the study outcomes. If these two
problems are absent, the RDS method shows strong resistance to low response
rates and certain errors in the participants' reporting of their network sizes.
Other issues that might affect the RDS estimates, such as the method for
choosing initial participants, the maximum number of recruitments per
participant, sampling with or without replacement and variations in network
structures, are also simulated and discussed.
"
"  This paper describes a new method for generating stationary integer-valued
time series from renewal processes. We prove that if the lifetime distribution
of renewal processes is nonlattice and the probability generating function is
rational, then the generated time series satisfy causal and invertible ARMA
type stochastic difference equations. The result provides an easy method for
generating integer-valued time series with ARMA type autocovariance functions.
Examples of generating binomial ARMA(p,p-1) series from lifetime distributions
with constant hazard rates after lag p are given as an illustration. An
estimation method is developed for the AR(p) cases.
"
"  Often it is not easy to choose between estimators, based on the estimated MSE
and bias using simulation studies. Normality in small samples and a variance of
the estimator, which is correct and easy to calculate using a single sample,
give the added advantage that hypotheses concerning the parameter can be tested
in new samples. A procedure to check normality is proposed where previously
published MSE and bias are used to perform a test for normality. A confidence
interval for the index of the S&P500 index is found by applying the results to
estimators of the generalized Pareto distribution.
"
"  Recent work suggests that some auto-encoder variants do a good job of
capturing the local manifold structure of the unknown data generating density.
This paper contributes to the mathematical understanding of this phenomenon and
helps define better justified sampling algorithms for deep learning based on
auto-encoder variants. We consider an MCMC where each step samples from a
Gaussian whose mean and covariance matrix depend on the previous state, defines
through its asymptotic distribution a target density. First, we show that good
choices (in the sense of consistency) for these mean and covariance functions
are the local expected value and local covariance under that target density.
Then we show that an auto-encoder with a contractive penalty captures
estimators of these local moments in its reconstruction function and its
Jacobian. A contribution of this work is thus a novel alternative to
maximum-likelihood density estimation, which we call local moment matching. It
also justifies a recently proposed sampling algorithm for the Contractive
Auto-Encoder and extends it to the Denoising Auto-Encoder.
"
"  In this work, we develop a simple algorithm for semi-supervised regression.
The key idea is to use the top eigenfunctions of integral operator derived from
both labeled and unlabeled examples as the basis functions and learn the
prediction function by a simple linear regression. We show that under
appropriate assumptions about the integral operator, this approach is able to
achieve an improved regression error bound better than existing bounds of
supervised learning. We also verify the effectiveness of the proposed algorithm
by an empirical study.
"
"  Discriminative linear models are a popular tool in machine learning. These
can be generally divided into two types: The first is linear classifiers, such
as support vector machines, which are well studied and provide state-of-the-art
results. One shortcoming of these models is that their output (known as the
'margin') is not calibrated, and cannot be translated naturally into a
distribution over the labels. Thus, it is difficult to incorporate such models
as components of larger systems, unlike probabilistic based approaches. The
second type of approach constructs class conditional distributions using a
nonlinearity (e.g. log-linear models), but is occasionally worse in terms of
classification error. We propose a supervised learning method which combines
the best of both approaches. Specifically, our method provides a distribution
over the labels, which is a linear function of the model parameters. As a
consequence, differences between probabilities are linear functions, a property
which most probabilistic models (e.g. log-linear) do not have.
  Our model assumes that classes correspond to linear subspaces (rather than to
half spaces). Using a relaxed projection operator, we construct a measure which
evaluates the degree to which a given vector 'belongs' to a subspace, resulting
in a distribution over labels. Interestingly, this view is closely related to
similar concepts in quantum detection theory. The resulting models can be
trained either to maximize the margin or to optimize average likelihood
measures. The corresponding optimization problems are semidefinite programs
which can be solved efficiently. We illustrate the performance of our algorithm
on real world datasets, and show that it outperforms 2nd order kernel methods.
"
"  For high-dimensional classification, it is well known that naively performing
the Fisher discriminant rule leads to poor results due to diverging spectra and
noise accumulation. Therefore, researchers proposed independence rules to
circumvent the diverse spectra, and sparse independence rules to mitigate the
issue of noise accumulation. However, in biological applications, there are
often a group of correlated genes responsible for clinical outcomes, and the
use of the covariance information can significantly reduce misclassification
rates. The extent of such error rate reductions is unveiled by comparing the
misclassification rates of the Fisher discriminant rule and the independence
rule. To materialize the gain based on finite samples, a Regularized Optimal
Affine Discriminant (ROAD) is proposed based on a covariance penalty. ROAD
selects an increasing number of features as the penalization relaxes. Further
benefits can be achieved when a screening method is employed to narrow the
feature pool before hitting the ROAD. An efficient Constrained Coordinate
Descent algorithm (CCD) is also developed to solve the associated optimization
problems. Sampling properties of oracle type are established. Simulation
studies and real data analysis support our theoretical results and demonstrate
the advantages of the new classification procedure under a variety of
correlation structures. A delicate result on continuous piecewise linear
solution path for the ROAD optimization problem at the population level
justifies the linear interpolation of the CCD algorithm.
"
"  We theoretically investigate the convergence rate and support consistency
(i.e., correctly identifying the subset of non-zero coefficients in the large
sample limit) of multiple kernel learning (MKL). We focus on MKL with block-l1
regularization (inducing sparse kernel combination), block-l2 regularization
(inducing uniform kernel combination), and elastic-net regularization
(including both block-l1 and block-l2 regularization). For the case where the
true kernel combination is sparse, we show a sharper convergence rate of the
block-l1 and elastic-net MKL methods than the existing rate for block-l1 MKL.
We further show that elastic-net MKL requires a milder condition for being
consistent than block-l1 MKL. For the case where the optimal kernel combination
is not exactly sparse, we prove that elastic-net MKL can achieve a faster
convergence rate than the block-l1 and block-l2 MKL methods by carefully
controlling the balance between the block-l1and block-l2 regularizers. Thus,
our theoretical results overall suggest the use of elastic-net regularization
in MKL.
"
"  Data from spectrophotometers form vectors of a large number of exploitable
variables. Building quantitative models using these variables most often
requires using a smaller set of variables than the initial one. Indeed, a too
large number of input variables to a model results in a too large number of
parameters, leading to overfitting and poor generalization abilities. In this
paper, we suggest the use of the mutual information measure to select variables
from the initial set. The mutual information measures the information content
in input variables with respect to the model output, without making any
assumption on the model that will be used; it is thus suitable for nonlinear
modelling. In addition, it leads to the selection of variables among the
initial set, and not to linear or nonlinear combinations of them. Without
decreasing the model performances compared to other variable projection
methods, it allows therefore a greater interpretability of the results.
"
"  We provide a simple method and relevant theoretical analysis for efficiently
estimating higher-order lp distances. While the analysis mainly focuses on l4,
our methodology extends naturally to p = 6,8,10..., (i.e., when p is even).
Distance-based methods are popular in machine learning. In large-scale
applications, storing, computing, and retrieving the distances can be both
space and time prohibitive. Efficient algorithms exist for estimating lp
distances if 0 < p <= 2. The task for p > 2 is known to be difficult. Our work
partially fills this gap.
"
"  Continuous-time linear birth-death-immigration (BDI) processes are frequently
used in ecology and epidemiology to model stochastic dynamics of the population
of interest. In clinical settings, multiple birth-death processes can describe
disease trajectories of individual patients, allowing for estimation of the
effects of individual covariates on the birth and death rates of the process.
Such estimation is usually accomplished by analyzing patient data collected at
unevenly spaced time points, referred to as panel data in the biostatistics
literature. Fitting linear BDI processes to panel data is a nontrivial
optimization problem because birth and death rates can be functions of many
parameters related to the covariates of interest. We propose a novel
expectation--maximization (EM) algorithm for fitting linear BDI models with
covariates to panel data. We derive a closed-form expression for the joint
generating function of some of the BDI process statistics and use this
generating function to reduce the E-step of the EM algorithm, as well as
calculation of the Fisher information, to one-dimensional integration. This
analytical technique yields a computationally efficient and robust optimization
algorithm that we implemented in an open-source R package. We apply our method
to DNA fingerprinting of Mycobacterium tuberculosis, the causative agent of
tuberculosis, to study intrapatient time evolution of IS6110 copy number, a
genetic marker frequently used during estimation of epidemiological clusters of
Mycobacterium tuberculosis infections. Our analysis reveals previously
undocumented differences in IS6110 birth-death rates among three major lineages
of Mycobacterium tuberculosis, which has important implications for
epidemiologists that use IS6110 for DNA fingerprinting of Mycobacterium
tuberculosis.
"
"  We derive generalization error bounds for stationary univariate
autoregressive (AR) models. We show that imposing stationarity is enough to
control the Gaussian complexity without further regularization. This lets us
use structural risk minimization for model selection. We demonstrate our
methods by predicting interest rate movements.
"
"  Mining association rules is an important technique for discovering meaningful
patterns in transaction databases. Many different measures of interestingness
have been proposed for association rules. However, these measures fail to take
the probabilistic properties of the mined data into account. In this paper, we
start with presenting a simple probabilistic framework for transaction data
which can be used to simulate transaction data when no associations are
present. We use such data and a real-world database from a grocery outlet to
explore the behavior of confidence and lift, two popular interest measures used
for rule mining. The results show that confidence is systematically influenced
by the frequency of the items in the left hand side of rules and that lift
performs poorly to filter random noise in transaction data. Based on the
probabilistic framework we develop two new interest measures, hyper-lift and
hyper-confidence, which can be used to filter or order mined association rules.
The new measures show significantly better performance than lift for
applications where spurious rules are problematic.
"
"  There is a growing interest in using a longitudinal observational databases
to detect drug safety signal. In this paper we present a novel method, which we
used online during the OMOP Cup. We consider homogeneous ensembling, which is
based on random re-sampling (known, also, as bagging) as a main innovation
compared to the previous publications in the related field. This study is based
on a very large simulated database of the 10 million patients records, which
was created by the Observational Medical Outcomes Partnership (OMOP). Compared
to the traditional classification problem, the given data are unlabelled. The
objective of this study is to discover hidden associations between drugs and
conditions. The main idea of the approach, which we used during the OMOP Cup is
to compare the numbers of observed and expected patterns. This comparison may
be organised in several different ways, and the outcomes (base learners) may be
quite different as well. It is proposed to construct the final decision
function as an ensemble of the base learners. Our method was recognised
formally by the Organisers of the OMOP Cup as a top performing method for the
Challenge N2.
"
"  In this paper, we investigate a new compressive sensing model for
multi-channel sparse data where each channel can be represented as a
hierarchical tree and different channels are highly correlated. Therefore, the
full data could follow the forest structure and we call this property as
\emph{forest sparsity}. It exploits both intra- and inter- channel correlations
and enriches the family of existing model-based compressive sensing theories.
The proposed theory indicates that only $\mathcal{O}(Tk+\log(N/k))$
measurements are required for multi-channel data with forest sparsity, where
$T$ is the number of channels, $N$ and $k$ are the length and sparsity number
of each channel respectively. This result is much better than
$\mathcal{O}(Tk+T\log(N/k))$ of tree sparsity, $\mathcal{O}(Tk+k\log(N/k))$ of
joint sparsity, and far better than $\mathcal{O}(Tk+Tk\log(N/k))$ of standard
sparsity. In addition, we extend the forest sparsity theory to the multiple
measurement vectors problem, where the measurement matrix is a block-diagonal
matrix. The result shows that the required measurement bound can be the same as
that for dense random measurement matrix, when the data shares equal energy in
each channel. A new algorithm is developed and applied on four example
applications to validate the benefit of the proposed model. Extensive
experiments demonstrate the effectiveness and efficiency of the proposed theory
and algorithm.
"
"  In this letter we borrow from the inference techniques developed for
unbounded state-cardinality (nonparametric) variants of the HMM and use them to
develop a tuning-parameter free, black-box inference procedure for
Explicit-state-duration hidden Markov models (EDHMM). EDHMMs are HMMs that have
latent states consisting of both discrete state-indicator and discrete
state-duration random variables. In contrast to the implicit geometric state
duration distribution possessed by the standard HMM, EDHMMs allow the direct
parameterisation and estimation of per-state duration distributions. As most
duration distributions are defined over the positive integers, truncation or
other approximations are usually required to perform EDHMM inference.
"
"  Slow feature analysis (SFA) is a method for extracting slowly varying
features from a quickly varying multidimensional signal. An open source
Matlab-implementation sfa-tk makes SFA easily useable. We show here that under
certain circumstances, namely when the covariance matrix of the nonlinearly
expanded data does not have full rank, this implementation runs into numerical
instabilities. We propse a modified algorithm based on singular value
decomposition (SVD) which is free of those instabilities even in the case where
the rank of the matrix is only less than 10% of its size. Furthermore we show
that an alternative way of handling the numerical problems is to inject a small
amount of noise into the multidimensional input signal which can restore a
rank-deficient covariance matrix to full rank, however at the price of
modifying the original data and the need for noise parameter tuning.
"
"  The separability assumption (Donoho & Stodden, 2003; Arora et al., 2012)
turns non-negative matrix factorization (NMF) into a tractable problem.
Recently, a new class of provably-correct NMF algorithms have emerged under
this assumption. In this paper, we reformulate the separable NMF problem as
that of finding the extreme rays of the conical hull of a finite set of
vectors. From this geometric perspective, we derive new separable NMF
algorithms that are highly scalable and empirically noise robust, and have
several other favorable properties in relation to existing methods. A parallel
implementation of our algorithm demonstrates high scalability on shared- and
distributed-memory machines.
"
"  We present here an introduction to Brainstorming approach, that was recently
proposed as a consensus meta-learning technique, and used in several practical
applications in bioinformatics and chemoinformatics. The consensus learning
denotes heterogeneous theoretical classification method, where one trains an
ensemble of machine learning algorithms using different types of input training
data representations. In the second step all solutions are gathered and the
consensus is build between them. Therefore no early solution, given even by a
generally low performing algorithm, is not discarder until the late phase of
prediction, when the final conclusion is drawn by comparing different machine
learning models. This final phase, i.e. consensus learning, is trying to
balance the generality of solution and the overall performance of trained
model.
"
"  Section 1.3 was incorrect, and 2.1 will be removed from further submissions.
A rewritten version will be posted in the future.
"
"  We propose a deconvolution algorithm for images blurred and degraded by a
Poisson noise. The algorithm uses a fast proximal backward-forward splitting
iteration. This iteration minimizes an energy which combines a
\textit{non-linear} data fidelity term, adapted to Poisson noise, and a
non-smooth sparsity-promoting regularization (e.g $\ell_1$-norm) over the image
representation coefficients in some dictionary of transforms (e.g. wavelets,
curvelets). Our results on simulated microscopy images of neurons and cells are
confronted to some state-of-the-art algorithms. They show that our approach is
very competitive, and as expected, the importance of the non-linearity due to
Poisson noise is more salient at low and medium intensities. Finally an
experiment on real fluorescent confocal microscopy data is reported.
"
"  Orthogonal Matching Pursuit (OMP) has long been considered a powerful
heuristic for attacking compressive sensing problems; however, its theoretical
development is, unfortunately, somewhat lacking. This paper presents an
improved Restricted Isometry Property (RIP) based performance guarantee for
T-sparse signal reconstruction that asymptotically approaches the conjectured
lower bound given in Davenport et al. We also further extend the
state-of-the-art by deriving reconstruction error bounds for the case of
general non-sparse signals subjected to measurement noise. We then generalize
our results to the case of K-fold Orthogonal Matching Pursuit (KOMP). We finish
by presenting an empirical analysis suggesting that OMP and KOMP outperform
other compressive sensing algorithms in average case scenarios. This turns out
to be quite surprising since RIP analysis (i.e. worst case scenario) suggests
that these matching pursuits should perform roughly T^0.5 times worse than
convex optimization, CoSAMP, and Iterative Thresholding.
"
"  The purpose of this paper is to propose a time-varying vector autoregressive
model (TV-VAR) for forecasting multivariate time series. The model is casted
into a state-space form that allows flexible description and analysis. The
volatility covariance matrix of the time series is modelled via inverted
Wishart and singular multivariate beta distributions allowing a fully conjugate
Bayesian inference. Model performance and model comparison is done via the
likelihood function, sequential Bayes factors, the mean of squared standardized
forecast errors, the mean of absolute forecast errors (known also as mean
absolute deviation), and the mean forecast error. Bayes factors are also used
in order to choose the autoregressive order of the model. Multi-step
forecasting is discussed in detail and a flexible formula is proposed to
approximate the forecast function. Two examples, consisting of bivariate data
of IBM shares and of foreign exchange (FX) rates for 8 currencies, illustrate
the methods. For the IBM data we discuss model performance and multi-step
forecasting in some detail. For the FX data we discuss sequential portfolio
allocation; for both data sets our empirical findings suggest that the TV-VAR
models outperform the widely used VAR models.
"
"  We consider applying Bayesian Variable Selection Regression, or BVSR, to
genome-wide association studies and similar large-scale regression problems.
Currently, typical genome-wide association studies measure hundreds of
thousands, or millions, of genetic variants (SNPs), in thousands or tens of
thousands of individuals, and attempt to identify regions harboring SNPs that
affect some phenotype or outcome of interest. This goal can naturally be cast
as a variable selection regression problem, with the SNPs as the covariates in
the regression. Characteristic features of genome-wide association studies
include the following: (i) a focus primarily on identifying relevant variables,
rather than on prediction; and (ii) many relevant covariates may have tiny
effects, making it effectively impossible to confidently identify the complete
""correct"" subset of variables. Taken together, these factors put a premium on
having interpretable measures of confidence for individual covariates being
included in the model, which we argue is a strength of BVSR compared with
alternatives such as penalized regression methods. Here we focus primarily on
analysis of quantitative phenotypes, and on appropriate prior specification for
BVSR in this setting, emphasizing the idea of considering what the priors imply
about the total proportion of variance in outcome explained by relevant
covariates. We also emphasize the potential for BVSR to estimate this
proportion of variance explained, and hence shed light on the issue of ""missing
heritability"" in genome-wide association studies.
"
"  When analyzing interaction networks, it is common to interpret the amount of
interaction between two nodes as the strength of their relationship. We argue
that this interpretation may not be appropriate, since the interaction between
a pair of nodes could potentially be explained only by characteristics of the
nodes that compose the pair and, however, not by pair-specific features. In
interaction networks, where edges or arcs are count-valued, the above scenario
corresponds to a model of independence for the expected interaction in the
network, and consequently we propose the notions of arc strength, and edge
strength to be understood as departures from this model of independence. We
discuss how our notion of arc/edge strength can be used as a guidance to study
network structure, and in particular we develop a latent arc strength
stochastic blockmodel for directed interaction networks. We illustrate our
approach studying the interaction between the Kolkata users of the myGamma
mobile network.
"
"  The authors apply three methods of prospective modelling to high resolution
georeferenced land cover data in a Mediterranean mountain area: GIS approach,
non linear parametric model and neuronal network. Land cover prediction to the
latest known date is used to validate the models. In the frame of
spatial-temporal dynamics in open systems results are encouraging and
comparable. Correct prediction scores are about 73 %. The results analysis
focuses on geographic location, land cover categories and parametric distance
to reality of the residues. Crossing the three models show the high degree of
convergence and a relative similitude of the results obtained by the two
statistic approaches compared to the GIS supervised model. Steps under work are
the application of the models to other test areas and the identification of
respective advantages to develop an integrated model.
"
"  We study the basic problem of robust subspace recovery. That is, we assume a
data set that some of its points are sampled around a fixed subspace and the
rest of them are spread in the whole ambient space, and we aim to recover the
fixed underlying subspace. We first estimate ""robust inverse sample covariance""
by solving a convex minimization procedure; we then recover the subspace by the
bottom eigenvectors of this matrix (their number correspond to the number of
eigenvalues close to 0). We guarantee exact subspace recovery under some
conditions on the underlying data. Furthermore, we propose a fast iterative
algorithm, which linearly converges to the matrix minimizing the convex
problem. We also quantify the effect of noise and regularization and discuss
many other practical and theoretical issues for improving the subspace recovery
in various settings. When replacing the sum of terms in the convex energy
function (that we minimize) with the sum of squares of terms, we obtain that
the new minimizer is a scaled version of the inverse sample covariance (when
exists). We thus interpret our minimizer and its subspace (spanned by its
bottom eigenvectors) as robust versions of the empirical inverse covariance and
the PCA subspace respectively. We compare our method with many other algorithms
for robust PCA on synthetic and real data sets and demonstrate state-of-the-art
speed and accuracy.
"
"  We propose a hierarchy for approximate inference based on the Dobrushin,
Lanford, Ruelle (DLR) equations. This hierarchy includes existing algorithms,
such as belief propagation, and also motivates novel algorithms such as
factorized neighbors (FN) algorithms and variants of mean field (MF)
algorithms. In particular, we show that extrema of the Bethe free energy
correspond to approximate solutions of the DLR equations. In addition, we
demonstrate a close connection between these approximate algorithms and Gibbs
sampling. Finally, we compare and contrast various of the algorithms in the DLR
hierarchy on spin-glass problems. The experiments show that algorithms higher
up in the hierarchy give more accurate results when they converge but tend to
be less stable.
"
"  Exemplar-based clustering methods have been shown to produce state-of-the-art
results on a number of synthetic and real-world clustering problems. They are
appealing because they offer computational benefits over latent-mean models and
can handle arbitrary pairwise similarity measures between data points. However,
when trying to recover underlying structure in clustering problems, tailored
similarity measures are often not enough; we also desire control over the
distribution of cluster sizes. Priors such as Dirichlet process priors allow
the number of clusters to be unspecified while expressing priors over data
partitions. To our knowledge, they have not been applied to exemplar-based
models. We show how to incorporate priors, including Dirichlet process priors,
into the recently introduced affinity propagation algorithm. We develop an
efficient maxproduct belief propagation algorithm for our new model and
demonstrate experimentally how the expanded range of clustering priors allows
us to better recover true clusterings in situations where we have some
information about the generating process.
"
"  We develop a coherent framework for integrative simultaneous analysis of the
exploration-exploitation and model order selection trade-offs. We improve over
our preceding results on the same subject (Seldin et al., 2011) by combining
PAC-Bayesian analysis with Bernstein-type inequality for martingales. Such a
combination is also of independent interest for studies of multiple
simultaneously evolving martingales.
"
"  Genetical genomics experiments have now been routinely conducted to measure
both the genetic markers and gene expression data on the same subjects. The
gene expression levels are often treated as quantitative traits and are subject
to standard genetic analysis in order to identify the gene expression
quantitative loci (eQTL). However, the genetic architecture for many gene
expressions may be complex, and poorly estimated genetic architecture may
compromise the inferences of the dependency structures of the genes at the
transcriptional level. In this paper we introduce a sparse conditional Gaussian
graphical model for studying the conditional independent relationships among a
set of gene expressions adjusting for possible genetic effects where the gene
expressions are modeled with seemingly unrelated regressions. We present an
efficient coordinate descent algorithm to obtain the penalized estimation of
both the regression coefficients and the sparse concentration matrix. The
corresponding graph can be used to determine the conditional independence among
a group of genes while adjusting for shared genetic effects. Simulation
experiments and asymptotic convergence rates and sparsistency are used to
justify our proposed methods. By sparsistency, we mean the property that all
parameters that are zero are actually estimated as zero with probability
tending to one. We apply our methods to the analysis of a yeast eQTL data set
and demonstrate that the conditional Gaussian graphical model leads to a more
interpretable gene network than a standard Gaussian graphical model based on
gene expression data alone.
"
"  We present a general construction for dependent random measures based on
thinning Poisson processes on an augmented space. The framework is not
restricted to dependent versions of a specific nonparametric model, but can be
applied to all models that can be represented using completely random measures.
Several existing dependent random measures can be seen as specific cases of
this framework. Interesting properties of the resulting measures are derived
and the efficacy of the framework is demonstrated by constructing a
covariate-dependent latent feature model and topic model that obtain superior
predictive performance.
"
"  The estimation of asset return distributions is crucial for determining
optimal trading strategies. In this paper we describe the constrained mixture
model, based on a mixture of Gamma and Gaussian distributions, to provide an
accurate description of price trends as being clearly positive, negative or
ranging while accounting for heavy tails and high kurtosis. The model is
estimated in the Expectation Maximisation framework and model order estimation
also respects the model's constraints.
"
"  North Pacific sea surface temperatures (SST), as used in estimating the PDO,
are reanalyzed using state-space decomposition and subspace identification
techniques. The reanalysis presents a very different picture of SST in this
region. The first common trend reflects a global warming signal. The second
common trend modifies this for areas that underwent a sharper warming (cooling)
starting in the early 1970's. This trend is also related to dynamics in the
tropics and in Arctic sea ice extent. The third common trend is a superposition
of changes in pressure centers on the long-term global warming signal. The
fourth common trend is the trend that is contained in the original PDO series
if analyzed by state-space techniques, and is identical to the trend in the
North Pacific High. The first two common stochastic cycles capture the original
PDO and so-called ""Victoria mode"", showing that these series are dominated by
stationary behavior..
"
"  The paper describes an application of Aggregating Algorithm to the problem of
regression. It generalizes earlier results concerned with plain linear
regression to kernel techniques and presents an on-line algorithm which
performs nearly as well as any oblivious kernel predictor. The paper contains
the derivation of an estimate on the performance of this algorithm. The
estimate is then used to derive an application of the Complexity Approximation
Principle to kernel methods.
"
"  We develop the relational topic model (RTM), a hierarchical model of both
network structure and node attributes. We focus on document networks, where the
attributes of each document are its words, that is, discrete observations taken
from a fixed vocabulary. For each pair of documents, the RTM models their link
as a binary random variable that is conditioned on their contents. The model
can be used to summarize a network of documents, predict links between them,
and predict words within them. We derive efficient inference and estimation
algorithms based on variational methods that take advantage of sparsity and
scale with the number of links. We evaluate the predictive performance of the
RTM for large networks of scientific abstracts, web documents, and
geographically tagged news.
"
"  We propose an active set selection framework for Gaussian process
classification for cases when the dataset is large enough to render its
inference prohibitive. Our scheme consists of a two step alternating procedure
of active set update rules and hyperparameter optimization based upon marginal
likelihood maximization. The active set update rules rely on the ability of the
predictive distributions of a Gaussian process classifier to estimate the
relative contribution of a datapoint when being either included or removed from
the model. This means that we can use it to include points with potentially
high impact to the classifier decision process while removing those that are
less relevant. We introduce two active set rules based on different criteria,
the first one prefers a model with interpretable active set parameters whereas
the second puts computational complexity first, thus a model with active set
parameters that directly control its complexity. We also provide both
theoretical and empirical support for our active set selection strategy being a
good approximation of a full Gaussian process classifier. Our extensive
experiments show that our approach can compete with state-of-the-art
classification techniques with reasonable time complexity. Source code publicly
available at http://cogsys.imm.dtu.dk/passgp.
"
"  In multivariate regression, a $K$-dimensional response vector is regressed
upon a common set of $p$ covariates, with a matrix $B^*\in\mathbb{R}^{p\times
K}$ of regression coefficients. We study the behavior of the multivariate group
Lasso, in which block regularization based on the $\ell_1/\ell_2$ norm is used
for support union recovery, or recovery of the set of $s$ rows for which $B^*$
is nonzero. Under high-dimensional scaling, we show that the multivariate group
Lasso exhibits a threshold for the recovery of the exact row pattern with high
probability over the random design and noise that is specified by the sample
complexity parameter $\theta(n,p,s):=n/[2\psi(B^*)\log(p-s)]$. Here $n$ is the
sample size, and $\psi(B^*)$ is a sparsity-overlap function measuring a
combination of the sparsities and overlaps of the $K$-regression coefficient
vectors that constitute the model. We prove that the multivariate group Lasso
succeeds for problem sequences $(n,p,s)$ such that $\theta(n,p,s)$ exceeds a
critical level $\theta_u$, and fails for sequences such that $\theta(n,p,s)$
lies below a critical level $\theta_{\ell}$. For the special case of the
standard Gaussian ensemble, we show that $\theta_{\ell}=\theta_u$ so that the
characterization is sharp. The sparsity-overlap function $\psi(B^*)$ reveals
that, if the design is uncorrelated on the active rows, $\ell_1/\ell_2$
regularization for multivariate regression never harms performance relative to
an ordinary Lasso approach and can yield substantial improvements in sample
complexity (up to a factor of $K$) when the coefficient vectors are suitably
orthogonal. For more general designs, it is possible for the ordinary Lasso to
outperform the multivariate group Lasso. We complement our analysis with
simulations that demonstrate the sharpness of our theoretical results, even for
relatively small problems.
"
"  Within the unmanageably large class of nonconvex optimization, we consider
the rich subclass of nonsmooth problems that have composite objectives---this
already includes the extensively studied convex, composite objective problems
as a special case. For this subclass, we introduce a powerful, new framework
that permits asymptotically non-vanishing perturbations. In particular, we
develop perturbation-based batch and incremental (online like) nonconvex
proximal splitting algorithms. To our knowledge, this is the first time that
such perturbation-based nonconvex splitting algorithms are being proposed and
analyzed. While the main contribution of the paper is the theoretical
framework, we complement our results by presenting some empirical results on
matrix factorization.
"
"  Self-Organizing Maps (SOM) are popular unsupervised artificial neural network
used to reduce dimensions and visualize data. Visual interpretation from
Self-Organizing Maps (SOM) has been limited due to grid approach of data
representation, which makes inter-scenario analysis impossible. The paper
proposes a new way to structure SOM. This model reconstructs SOM to show
strength between variables as the threads of a cobweb and illuminate
inter-scenario analysis. While Radar Graphs are very crude representation of
spider web, this model uses more lively and realistic cobweb representation to
take into account the difference in strength and length of threads. This model
allows for visualization of highly unstructured dataset with large number of
dimensions, common in Bigdata sources.
"
"  In the year 2005 Jorge Hirsch introduced the h index for quantifying the
research output of scientists. Today, the h index is a widely accepted
indicator of research performance. The h index has been criticized for its
insufficient reliability - the ability to discriminate reliably between
meaningful amounts of research performance. Taking as an example an extensive
data set with bibliometric data on scientists working in the field of molecular
biology, we compute h2 lower, h2 upper, and sRM values and present them as
complementary approaches that improve the reliability of the h index research
performance measurement.
"
"  Hawley-Dolan and Winner had asked the art students to compare paintings by
abstract artists with paintings made by a child or by an animal. In 67% of the
cases, art students said that the painting by a renowned artist is better. I
compare this with the winning probability of the chessplayers of different
ratings. I conclude that the great artists score on the level of class D
amateurs.
"
"  This article aims at summarizing the existing methods for sampling social
networking services and proposing a faster confidence interval for related
sampling methods. It also includes comparisons of common network sampling
techniques.
"
"  We analyze how an observer synchronizes to the internal state of a
finite-state information source, using the epsilon-machine causal
representation. Here, we treat the case of exact synchronization, when it is
possible for the observer to synchronize completely after a finite number of
observations. The more difficult case of strictly asymptotic synchronization is
treated in a sequel. In both cases, we find that an observer, on average, will
synchronize to the source state exponentially fast and that, as a result, the
average accuracy in an observer's predictions of the source output approaches
its optimal level exponentially fast as well. Additionally, we show here how to
analytically calculate the synchronization rate for exact epsilon-machines and
provide an efficient polynomial-time algorithm to test epsilon-machines for
exactness.
"
"  This question is raised by Cason, Friedman and Hopkins (CFH, 2012) after they
firstly found and indexed quantitatively the cycles in a continuous time
experiment. To answer this question, we use the data from standard RPS
experiment. Our experiments are of the traditional setting - in each of
repeated rounds, the subjects are paired with random matching, using pure
strategy and must choose simultaneously, and after each round, each subject
obtains only private information. This economics environment is a decartelized
and low-information one.
  Using the cycle rotation indexes (CRI, developed by CFH) method, we find, the
cycles not only exist but also persist in our experiment. Meanwhile, the
cycles' direction are consistent with 'standard' learning models. That is the
answer to the CHF question: Cycles do not dissipate in the simultaneously
choose game.
  In addtion, we discuss three questions (1) why significant cycles are uneasy
to be obtained in traditional setting experiments; (2) why CRI can be an iconic
indexing-method for 'standard' evolution dynamics; and (3) where more cycles
could be expected.
"
"  Kernel methods are among the most popular techniques in machine learning.
From a frequentist/discriminative perspective they play a central role in
regularization theory as they provide a natural choice for the hypotheses space
and the regularization functional through the notion of reproducing kernel
Hilbert spaces. From a Bayesian/generative perspective they are the key in the
context of Gaussian processes, where the kernel function is also known as the
covariance function. Traditionally, kernel methods have been used in supervised
learning problem with scalar outputs and indeed there has been a considerable
amount of work devoted to designing and learning kernels. More recently there
has been an increasing interest in methods that deal with multiple outputs,
motivated partly by frameworks like multitask learning. In this paper, we
review different methods to design or learn valid kernel functions for multiple
outputs, paying particular attention to the connection between probabilistic
and functional methods.
"
"  Locally adapted parameterizations of a model (such as locally weighted
regression) are expressive but often suffer from high variance. We describe an
approach for reducing the variance, based on the idea of estimating
simultaneously a transformed space for the model, as well as locally adapted
parameterizations in this new space. We present a new problem formulation that
captures this idea and illustrate it in the important context of time varying
models. We develop an algorithm for learning a set of bases for approximating a
time varying sparse network; each learned basis constitutes an archetypal
sparse network structure. We also provide an extension for learning task-driven
bases. We present empirical results on synthetic data sets, as well as on a BCI
EEG classification task.
"
"  When dealing with large scale gene expression studies, observations are
commonly contaminated by unwanted variation factors such as platforms or
batches. Not taking this unwanted variation into account when analyzing the
data can lead to spurious associations and to missing important signals. When
the analysis is unsupervised, e.g., when the goal is to cluster the samples or
to build a corrected version of the dataset - as opposed to the study of an
observed factor of interest - taking unwanted variation into account can become
a difficult task. The unwanted variation factors may be correlated with the
unobserved factor of interest, so that correcting for the former can remove the
latter if not done carefully. We show how negative control genes and replicate
samples can be used to estimate unwanted variation in gene expression, and
discuss how this information can be used to correct the expression data or
build estimators for unsupervised problems. The proposed methods are then
evaluated on three gene expression datasets. They generally manage to remove
unwanted variation without losing the signal of interest and compare favorably
to state of the art corrections.
"
"  Given i.i.d. observations of a random vector $X \in \mathbb{R}^p$, we study
the problem of estimating both its covariance matrix $\Sigma^*$, and its
inverse covariance or concentration matrix {$\Theta^* = (\Sigma^*)^{-1}$.} We
estimate $\Theta^*$ by minimizing an $\ell_1$-penalized log-determinant Bregman
divergence; in the multivariate Gaussian case, this approach corresponds to
$\ell_1$-penalized maximum likelihood, and the structure of $\Theta^*$ is
specified by the graph of an associated Gaussian Markov random field. We
analyze the performance of this estimator under high-dimensional scaling, in
which the number of nodes in the graph $p$, the number of edges $s$ and the
maximum node degree $d$, are allowed to grow as a function of the sample size
$n$. In addition to the parameters $(p,s,d)$, our analysis identifies other key
quantities covariance matrix $\Sigma^*$; and (b) the $\ell_\infty$ operator
norm of the sub-matrix $\Gamma^*_{S S}$, where $S$ indexes the graph edges, and
$\Gamma^* = (\Theta^*)^{-1} \otimes (\Theta^*)^{-1}$; and (c) a mutual
incoherence or irrepresentability measure on the matrix $\Gamma^*$ and (d) the
rate of decay $1/f(n,\delta)$ on the probabilities $ \{|\hat{\Sigma}^n_{ij}-
\Sigma^*_{ij}| > \delta \}$, where $\hat{\Sigma}^n$ is the sample covariance
based on $n$ samples. Our first result establishes consistency of our estimate
$\hat{\Theta}$ in the elementwise maximum-norm. This in turn allows us to
derive convergence rates in Frobenius and spectral norms, with improvements
upon existing results for graphs with maximum node degrees $d = o(\sqrt{s})$.
In our second result, we show that with probability converging to one, the
estimate $\hat{\Theta}$ correctly specifies the zero pattern of the
concentration matrix $\Theta^*$.
"
"  An empirical investigation of the interaction of sample size and
discretization - in this case the entropy-based method CAIM (Class-Attribute
Interdependence Maximization) - was undertaken to evaluate the impact and
potential bias introduced into data mining performance metrics due to variation
in sample size as it impacts the discretization process. Of particular interest
was the effect of discretizing within cross-validation folds averse to outside
discretization folds. Previous publications have suggested that discretizing
externally can bias performance results; however, a thorough review of the
literature found no empirical evidence to support such an assertion. This
investigation involved construction of over 117,000 models on seven distinct
datasets from the UCI (University of California-Irvine) Machine Learning
Library and multiple modeling methods across a variety of configurations of
sample size and discretization, with each unique ""setup"" being independently
replicated ten times. The analysis revealed a significant optimistic bias as
sample sizes decreased and discretization was employed. The study also revealed
that there may be a relationship between the interaction that produces such
bias and the numbers and types of predictor attributes, extending the ""curse of
dimensionality"" concept from feature selection into the discretization realm.
Directions for further exploration are laid out, as well some general
guidelines about the proper application of discretization in light of these
results.
"
"  We investigate the computational structure of a paradigmatic example of
distributed social interaction: that of the open-source Wikipedia community. We
examine the statistical properties of its cooperative behavior, and perform
model selection to determine whether this aspect of the system can be described
by a finite-state process, or whether reference to an effectively unbounded
resource allows for a more parsimonious description. We find strong evidence,
in a majority of the most-edited pages, in favor of a collective-state model,
where the probability of a ""revert"" action declines as the square root of the
number of non-revert actions seen since the last revert. We provide evidence
that the emergence of this social counter is driven by collective interaction
effects, rather than properties of individual users.
"
"  If an experimental treatment is experienced by both treated and control group
units, tests of hypotheses about causal effects may be difficult to
conceptualize let alone execute. In this paper, we show how counterfactual
causal models may be written and tested when theories suggest spillover or
other network-based interference among experimental units. We show that the ""no
interference"" assumption need not constrain scholars who have interesting
questions about interference. We offer researchers the ability to model
theories about how treatment given to some units may come to influence outcomes
for other units. We further show how to test hypotheses about these causal
effects, and we provide tools to enable researchers to assess the operating
characteristics of their tests given their own models, designs, test
statistics, and data. The conceptual and methodological framework we develop
here is particularly applicable to social networks, but may be usefully
deployed whenever a researcher wonders about interference between units.
Interference between units need not be an untestable assumption; instead,
interference is an opportunity to ask meaningful questions about theoretically
interesting phenomena.
"
"  This paper reviews the functional aspects of statistical learning theory. The
main point under consideration is the nature of the hypothesis set when no
prior information is available but data. Within this framework we first discuss
about the hypothesis set: it is a vectorial space, it is a set of pointwise
defined functions, and the evaluation functional on this set is a continuous
mapping. Based on these principles an original theory is developed generalizing
the notion of reproduction kernel Hilbert space to non hilbertian sets. Then it
is shown that the hypothesis set of any learning machine has to be a
generalized reproducing set. Therefore, thanks to a general ""representer
theorem"", the solution of the learning problem is still a linear combination of
a kernel. Furthermore, a way to design these kernels is given. To illustrate
this framework some examples of such reproducing sets and kernels are given.
"
"  Machine learning approaches to multi-label document classification have to
date largely relied on discriminative modeling techniques such as support
vector machines. A drawback of these approaches is that performance rapidly
drops off as the total number of labels and the number of labels per document
increase. This problem is amplified when the label frequencies exhibit the type
of highly skewed distributions that are often observed in real-world datasets.
In this paper we investigate a class of generative statistical topic models for
multi-label documents that associate individual word tokens with different
labels. We investigate the advantages of this approach relative to
discriminative models, particularly with respect to classification problems
involving large numbers of relatively rare labels. We compare the performance
of generative and discriminative approaches on document labeling tasks ranging
from datasets with several thousand labels to datasets with tens of labels. The
experimental results indicate that probabilistic generative models can achieve
competitive multi-label classification performance compared to discriminative
methods, and have advantages for datasets with many labels and skewed label
frequencies.
"
"  We introduce two kernels that extend the mean map, which embeds probability
measures in Hilbert spaces. The generative mean map kernel (GMMK) is a smooth
similarity measure between probabilistic models. The latent mean map kernel
(LMMK) generalizes the non-iid formulation of Hilbert space embeddings of
empirical distributions in order to incorporate latent variable models. When
comparing certain classes of distributions, the GMMK exhibits beneficial
regularization and generalization properties not shown for previous generative
kernels. We present experiments comparing support vector machine performance
using the GMMK and LMMK between hidden Markov models to the performance of
other methods on discrete and continuous observation sequence data. The results
suggest that, in many cases, the GMMK has generalization error competitive with
or better than other methods.
"
"  Composite indicators aggregate a set of variables using weights which are
understood to reflect the variables' importance in the index. In this paper we
propose to measure the importance of a given variable within existing composite
indicators via Karl Pearson's `correlation ratio'; we call this measure `main
effect'. Because socio-economic variables are heteroskedastic and correlated,
(relative) nominal weights are hardly ever found to match (relative) main
effects; we propose to summarize their discrepancy with a divergence measure.
We further discuss to what extent the mapping from nominal weights to main
effects can be inverted. This analysis is applied to five composite indicators,
including the Human Development Index and two popular league tables of
university performance. It is found that in many cases the declared importance
of single indicators and their main effect are very different, and that the
data correlation structure often prevents developers from obtaining the stated
importance, even when modifying the nominal weights in the set of nonnegative
numbers with unit sum.
"
"  We formulate a principle for classification with the knowledge of the
marginal distribution over the data points (unlabeled data). The principle is
cast in terms of Tikhonov style regularization where the regularization penalty
articulates the way in which the marginal density should constrain otherwise
unrestricted conditional distributions. Specifically, the regularization
penalty penalizes any information introduced between the examples and labels
beyond what is provided by the available labeled examples. The work extends
Szummer and Jaakkola's information regularization (NIPS 2002) to multiple
dimensions, providing a regularizer independent of the covering of the space
used in the derivation. We show in addition how the information regularizer can
be used as a measure of complexity of the classification task with unlabeled
data and prove a relevant sample-complexity bound. We illustrate the
regularization principle in practice by restricting the class of conditional
distributions to be logistic regression models and constructing the
regularization penalty from a finite set of unlabeled examples.
"
"  We consider a well defined joint detection and parameter estimation problem.
By combining the Baysian formulation of the estimation subproblem with suitable
constraints on the detection subproblem we develop optimum one- and two-step
test for the joint detection/estimation case. The proposed combined strategies
have the very desirable characteristic to allow for the trade-off between
detection power and estimation efficiency. Our theoretical developments are
then applied to the problems of retrospective changepoint detection and MIMO
radar. In the former case we are interested in detecting a change in the
statistics of a set of available data and provide an estimate for the time of
change, while in the latter in detecting a target and estimating its location.
Intense simulations demonstrate that by using the jointly optimum schemes, we
can experience significant improvement in estimation quality with small
sacrifice in detection power.
"
"  The ratio of two probability densities can be used for solving various
machine learning tasks such as covariate shift adaptation (importance
sampling), outlier detection (likelihood-ratio test), and feature selection
(mutual information). Recently, several methods of directly estimating the
density ratio have been developed, e.g., kernel mean matching, maximum
likelihood density ratio estimation, and least-squares density ratio fitting.
In this paper, we consider a kernelized variant of the least-squares method and
investigate its theoretical properties from the viewpoint of the condition
number using smoothed analysis techniques--the condition number of the Hessian
matrix determines the convergence rate of optimization and the numerical
stability. We show that the kernel least-squares method has a smaller condition
number than a version of kernel mean matching and other M-estimators, implying
that the kernel least-squares method has preferable numerical properties. We
further give an alternative formulation of the kernel least-squares estimator
which is shown to possess an even smaller condition number. We show that
numerical studies meet our theoretical analysis.
"
"  Metric learning has attracted a lot of interest over the last decade, but the
generalization ability of such methods has not been thoroughly studied. In this
paper, we introduce an adaptation of the notion of algorithmic robustness
(previously introduced by Xu and Mannor) that can be used to derive
generalization bounds for metric learning. We further show that a weak notion
of robustness is in fact a necessary and sufficient condition for a metric
learning algorithm to generalize. To illustrate the applicability of the
proposed framework, we derive generalization results for a large family of
existing metric learning algorithms, including some sparse formulations that
are not covered by previous results.
"
"  We study the effect of growth on the fingerprints of adolescents, based on
which we suggest a simple method to adjust for growth when trying to recover a
juvenile's fingerprint in a database years later. Based on longitudinal data
sets in juveniles' criminal records, we show that growth essentially leads to
an isotropic rescaling, so that we can use the strong correlation between
growth in stature and limbs to model the growth of fingerprints proportional to
stature growth as documented in growth charts. The proposed rescaling leads to
a 72% reduction of the distances between corresponding minutiae for the data
set analyzed. These findings were corroborated by several verification tests.
In an identification test on a database containing 3.25 million right index
fingers at the Federal Criminal Police Office of Germany, the identification
error rate of 20.8% was reduced to 2.1% by rescaling. The presented method is
of striking simplicity and can easily be integrated into existing automated
fingerprint identification systems.
"
"  We study inference and learning based on a sparse coding model with
`spike-and-slab' prior. As in standard sparse coding, the model used assumes
independent latent sources that linearly combine to generate data points.
However, instead of using a standard sparse prior such as a Laplace
distribution, we study the application of a more flexible `spike-and-slab'
distribution which models the absence or presence of a source's contribution
independently of its strength if it contributes. We investigate two approaches
to optimize the parameters of spike-and-slab sparse coding: a novel truncated
EM approach and, for comparison, an approach based on standard factored
variational distributions. The truncated approach can be regarded as a
variational approach with truncated posteriors as variational distributions. In
applications to source separation we find that both approaches improve the
state-of-the-art in a number of standard benchmarks, which argues for the use
of `spike-and-slab' priors for the corresponding data domains. Furthermore, we
find that the truncated EM approach improves on the standard factored approach
in source separation tasks$-$which hints to biases introduced by assuming
posterior independence in the factored variational approach. Likewise, on a
standard benchmark for image denoising, we find that the truncated EM approach
improves on the factored variational approach. While the performance of the
factored approach saturates with increasing numbers of hidden dimensions, the
performance of the truncated approach improves the state-of-the-art for higher
noise levels.
"
"  Many statistical problems involve data from thousands of parallel cases. Each
case has some associated effect size, and most cases will have no effect. It is
often important to estimate the effect size and the local or tail-area false
discovery rate for each case. Most current methods do this separately, and most
are designed for normal data. This paper uses an empirical Bayes mixture model
approach to estimate both quantities together for exponential family data. The
proposed method yields simple, interpretable models that can still be used
nonparametrically. It can also estimate an empirical null and incorporate it
fully into the model. The method outperforms existing effect size and false
discovery rate estimation procedures in normal data simulations; it nearly
acheives the Bayes error for effect size estimation. The method is implemented
in an R package (mixfdr), freely available from CRAN.
"
"  The optimal selection of experimental conditions is essential to maximizing
the value of data for inference and prediction, particularly in situations
where experiments are time-consuming and expensive to conduct. We propose a
general mathematical framework and an algorithmic approach for optimal
experimental design with nonlinear simulation-based models; in particular, we
focus on finding sets of experiments that provide the most information about
targeted sets of parameters.
  Our framework employs a Bayesian statistical setting, which provides a
foundation for inference from noisy, indirect, and incomplete data, and a
natural mechanism for incorporating heterogeneous sources of information. An
objective function is constructed from information theoretic measures,
reflecting expected information gain from proposed combinations of experiments.
Polynomial chaos approximations and a two-stage Monte Carlo sampling method are
used to evaluate the expected information gain. Stochastic approximation
algorithms are then used to make optimization feasible in computationally
intensive and high-dimensional settings. These algorithms are demonstrated on
model problems and on nonlinear parameter estimation problems arising in
detailed combustion kinetics.
"
"  Recently, considerable research efforts have been devoted to the design of
methods to learn from data overcomplete dictionaries for sparse coding.
However, learned dictionaries require the solution of an optimization problem
for coding new data. In order to overcome this drawback, we propose an
algorithm aimed at learning both a dictionary and its dual: a linear mapping
directly performing the coding. By leveraging on proximal methods, our
algorithm jointly minimizes the reconstruction error of the dictionary and the
coding error of its dual; the sparsity of the representation is induced by an
$\ell_1$-based penalty on its coefficients. The results obtained on synthetic
data and real images show that the algorithm is capable of recovering the
expected dictionaries. Furthermore, on a benchmark dataset, we show that the
image features obtained from the dual matrix yield state-of-the-art
classification performance while being much less computational intensive.
"
"  In biomedical studies it is of substantial interest to develop risk
prediction scores using high-dimensional data such as gene expression data for
clinical endpoints that are subject to censoring. In the presence of
well-established clinical risk factors, investigators often prefer a procedure
that also adjusts for these clinical variables. While accelerated failure time
(AFT) models are a useful tool for the analysis of censored outcome data, it
assumes that covariate effects on the logarithm of time-to-event are linear,
which is often unrealistic in practice. We propose to build risk prediction
scores through regularized rank estimation in partly linear AFT models, where
high-dimensional data such as gene expression data are modeled linearly and
important clinical variables are modeled nonlinearly using penalized regression
splines. We show through simulation studies that our model has better operating
characteristics compared to several existing models. In particular, we show
that there is a nonnegligible effect on prediction as well as feature selection
when nonlinear clinical effects are misspecified as linear. This work is
motivated by a recent prostate cancer study, where investigators collected gene
expression data along with established prognostic clinical variables and the
primary endpoint is time to prostate cancer recurrence.
"
"  Learning from electronic medical records (EMR) is challenging due to their
relational nature and the uncertain dependence between a patient's past and
future health status. Statistical relational learning is a natural fit for
analyzing EMRs but is less adept at handling their inherent latent structure,
such as connections between related medications or diseases. One way to capture
the latent structure is via a relational clustering of objects. We propose a
novel approach that, instead of pre-clustering the objects, performs a
demand-driven clustering during learning. We evaluate our algorithm on three
real-world tasks where the goal is to use EMRs to predict whether a patient
will have an adverse reaction to a medication. We find that our approach is
more accurate than performing no clustering, pre-clustering, and using
expert-constructed medical heterarchies.
"
"  For a variety of regularized optimization problems in machine learning,
algorithms computing the entire solution path have been developed recently.
Most of these methods are quadratic programs that are parameterized by a single
parameter, as for example the Support Vector Machine (SVM). Solution path
algorithms do not only compute the solution for one particular value of the
regularization parameter but the entire path of solutions, making the selection
of an optimal parameter much easier.
  It has been assumed that these piecewise linear solution paths have only
linear complexity, i.e. linearly many bends. We prove that for the support
vector machine this complexity can be exponential in the number of training
points in the worst case. More strongly, we construct a single instance of n
input points in d dimensions for an SVM such that at least \Theta(2^{n/2}) =
\Theta(2^d) many distinct subsets of support vectors occur as the
regularization parameter changes.
"
"  Percolation on complex networks has been used to study computer viruses,
epidemics, and other casual processes. Here, we present conditions for the
existence of a network specific, observation dependent, phase transition in the
updated posterior of node states resulting from actively monitoring the
network. Since traditional percolation thresholds are derived using observation
independent Markov chains, the threshold of the posterior should more
accurately model the true phase transition of a network, as the updated
posterior more accurately tracks the process. These conditions should provide
insight into modeling the dynamic response of the updated posterior to active
intervention and control policies while monitoring large complex networks.
"
"  Discrete choice models are commonly used by applied statisticians in numerous
fields, such as marketing, economics, finance, and operations research. When
agents in discrete choice models are assumed to have differing preferences,
exact inference is often intractable. Markov chain Monte Carlo techniques make
approximate inference possible, but the computational cost is prohibitive on
the large data sets now becoming routinely available. Variational methods
provide a deterministic alternative for approximation of the posterior
distribution. We derive variational procedures for empirical Bayes and fully
Bayesian inference in the mixed multinomial logit model of discrete choice. The
algorithms require only that we solve a sequence of unconstrained optimization
problems, which are shown to be convex. Extensive simulations demonstrate that
variational methods achieve accuracy competitive with Markov chain Monte Carlo,
at a small fraction of the computational cost. Thus, variational methods permit
inferences on data sets that otherwise could not be analyzed without
bias-inducing modifications to the underlying model.
"
"  Large-scale L1-regularized loss minimization problems arise in
high-dimensional applications such as compressed sensing and high-dimensional
supervised learning, including classification and regression problems.
High-performance algorithms and implementations are critical to efficiently
solving these problems. Building upon previous work on coordinate descent
algorithms for L1-regularized problems, we introduce a novel family of
algorithms called block-greedy coordinate descent that includes, as special
cases, several existing algorithms such as SCD, Greedy CD, Shotgun, and
Thread-Greedy. We give a unified convergence analysis for the family of
block-greedy algorithms. The analysis suggests that block-greedy coordinate
descent can better exploit parallelism if features are clustered so that the
maximum inner product between features in different blocks is small. Our
theoretical convergence analysis is supported with experimental re- sults using
data from diverse real-world applications. We hope that algorithmic approaches
and convergence analysis we provide will not only advance the field, but will
also encourage researchers to systematically explore the design space of
algorithms for solving large-scale L1-regularization problems.
"
"  We present a joint message passing approach that combines belief propagation
and the mean field approximation. Our analysis is based on the region-based
free energy approximation method proposed by Yedidia et al. We show that the
message passing fixed-point equations obtained with this combination correspond
to stationary points of a constrained region-based free energy approximation.
Moreover, we present a convergent implementation of these message passing
fixedpoint equations provided that the underlying factor graph fulfills certain
technical conditions. In addition, we show how to include hard constraints in
the part of the factor graph corresponding to belief propagation. Finally, we
demonstrate an application of our method to iterative channel estimation and
decoding in an orthogonal frequency division multiplexing (OFDM) system.
"
"  This paper considers the multi-task learning problem and in the setting where
some relevant features could be shared across few related tasks. Most of the
existing methods assume the extent to which the given tasks are related or
share a common feature space to be known apriori. In real-world applications
however, it is desirable to automatically discover the groups of related tasks
that share a feature space. In this paper we aim at searching the exponentially
large space of all possible groups of tasks that may share a feature space. The
main contribution is a convex formulation that employs a graph-based
regularizer and simultaneously discovers few groups of related tasks, having
close-by task parameters, as well as the feature space shared within each
group. The regularizer encodes an important structure among the groups of tasks
leading to an efficient algorithm for solving it: if there is no feature space
under which a group of tasks has close-by task parameters, then there does not
exist such a feature space for any of its supersets. An efficient active set
algorithm that exploits this simplification and performs a clever search in the
exponentially large space is presented. The algorithm is guaranteed to solve
the proposed formulation (within some precision) in a time polynomial in the
number of groups of related tasks discovered. Empirical results on benchmark
datasets show that the proposed formulation achieves good generalization and
outperforms state-of-the-art multi-task learning algorithms in some cases.
"
"  In this work we consider the stochastic minimization of nonsmooth convex loss
functions, a central problem in machine learning. We propose a novel algorithm
called Accelerated Nonsmooth Stochastic Gradient Descent (ANSGD), which
exploits the structure of common nonsmooth loss functions to achieve optimal
convergence rates for a class of problems including SVMs. It is the first
stochastic algorithm that can achieve the optimal O(1/t) rate for minimizing
nonsmooth loss functions (with strong convexity). The fast rates are confirmed
by empirical comparisons, in which ANSGD significantly outperforms previous
subgradient descent algorithms including SGD.
"
"  We consider penalized estimation in hidden Markov models (HMMs) with
multivariate Normal observations. In the moderate-to-large dimensional setting,
estimation for HMMs remains challenging in practice, due to several concerns
arising from the hidden nature of the states. We address these concerns by
$\ell_1$-penalization of state-specific inverse covariance matrices. Penalized
estimation leads to sparse inverse covariance matrices which can be interpreted
as state-specific conditional independence graphs. Penalization is nontrivial
in this latent variable setting; we propose a penalty that automatically adapts
to the number of states $K$ and the state-specific sample sizes and can cope
with scaling issues arising from the unknown states. The methodology is
adaptive and very general, applying in particular to both low- and
high-dimensional settings without requiring hand tuning. Furthermore, our
approach facilitates exploration of the number of states $K$ by coupling
estimation for successive candidate values $K$. Empirical results on simulated
examples demonstrate the effectiveness of the proposed approach. In a
challenging real data example from genome biology, we demonstrate the ability
of our approach to yield gains in predictive power and to deliver richer
estimates than existing methods.
"
"  To analyze and project age-specific mortality or morbidity rates
age-period-cohort (APC) models are very popular. Bayesian approaches facilitate
estimation and improve predictions by assigning smoothing priors to age, period
and cohort effects. Adjustments for overdispersion are straightforward using
additional random effects. When rates are further stratified, for example, by
countries, multivariate APC models can be used, where differences of
stratum-specific effects are interpretable as log relative risks. Here, we
incorporate correlated stratum-specific smoothing priors and correlated
overdispersion parameters into the multivariate APC model, and use Markov chain
Monte Carlo and integrated nested Laplace approximations for inference.
Compared to a model without correlation, the new approach may lead to more
precise relative risk estimates, as shown in an application to chronic
obstructive pulmonary disease mortality in three regions of England and Wales.
Furthermore, the imputation of missing data for one particular stratum may be
improved, since the new approach takes advantage of the remaining strata if the
corresponding observations are available there. This is shown in an application
to female mortality in Denmark, Sweden and Norway from the 20th century, where
we treat for each country in turn either the first or second half of the
observations as missing and then impute the omitted data. The projections are
compared to those obtained from a univariate APC model and an extended
Lee--Carter demographic forecasting approach using the proper Dawid--Sebastiani
scoring rule.
"
"  The method of extended maximum likelihood is a well known concept of
parameter estimation. One can implement external knowledge on the unknown
parameters by multiplying the likelihood by constraint terms. In this note, we
emphasize that this is also true for yield parameters in an extended maximum
likelihood fit, which is widely used in the particle physics community. We
recommend a way to generate pseudo-experiments in presence of constraint terms
on yield parameters, and point to pitfalls inside the RooFit framework.
"
"  The Earth is continuously showered by charged cosmic ray particles, naturally
produced atomic nuclei moving with velocity close to the speed of light. Among
these are ultra high energy cosmic ray particles with energy exceeding 5x10^19
eV, which is ten million times more energetic than the most energetic particles
produced at the Large Hadron Collider. Astrophysical questions include: what
phenomenon accelerates particles to such high energies, and what sort of nuclei
are energized? Also, the magnetic deflection of the trajectories of the cosmic
rays makes them potential probes of galactic and intergalactic magnetic fields.
We develop a Bayesian hierarchical model that can be used to compare different
association models between the cosmic rays and source population, using Bayes
factors. A measurement model with directional uncertainties and accounting for
non-uniform sky exposure is incoporated into the model. The methodology allows
us to learn about astrophysical parameters, such as those governing the source
luminosity function and the cosmic magnetic field.
"
"  Reconstruction based subspace clustering methods compute a self
reconstruction matrix over the samples and use it for spectral clustering to
obtain the final clustering result. Their success largely relies on the
assumption that the underlying subspaces are independent, which, however, does
not always hold in the applications with increasing number of subspaces. In
this paper, we propose a novel reconstruction based subspace clustering model
without making the subspace independence assumption. In our model, certain
properties of the reconstruction matrix are explicitly characterized using the
latent cluster indicators, and the affinity matrix used for spectral clustering
can be directly built from the posterior of the latent cluster indicators
instead of the reconstruction matrix. Experimental results on both synthetic
and real-world datasets show that the proposed model can outperform the
state-of-the-art methods.
"
"  Information theory is widely accepted as a powerful tool for analyzing
complex systems and it has been applied in many disciplines. Recently, some
central components of information theory - multivariate information measures -
have found expanded use in the study of several phenomena. These information
measures differ in subtle yet significant ways. Here, we will review the
information theory behind each measure, as well as examine the differences
between these measures by applying them to several simple model systems. In
addition to these systems, we will illustrate the usefulness of the information
measures by analyzing neural spiking data from a dissociated culture through
early stages of its development. We hope that this work will aid other
researchers as they seek the best multivariate information measure for their
specific research goals and system. Finally, we have made software available
online which allows the user to calculate all of the information measures
discussed within this paper.
"
"  We provide new results concerning label efficient, polynomial time, passive
and active learning of linear separators. We prove that active learning
provides an exponential improvement over PAC (passive) learning of homogeneous
linear separators under nearly log-concave distributions. Building on this, we
provide a computationally efficient PAC algorithm with optimal (up to a
constant factor) sample complexity for such problems. This resolves an open
question concerning the sample complexity of efficient PAC algorithms under the
uniform distribution in the unit ball. Moreover, it provides the first bound
for a polynomial-time PAC algorithm that is tight for an interesting infinite
class of hypothesis functions under a general and natural class of
data-distributions, providing significant progress towards a longstanding open
question.
  We also provide new bounds for active and passive learning in the case that
the data might not be linearly separable, both in the agnostic case and and
under the Tsybakov low-noise condition. To derive our results, we provide new
structural results for (nearly) log-concave distributions, which might be of
independent interest as well.
"
"  Recent technological advances coupled with large sample sets have uncovered
many factors underlying the genetic basis of traits and the predisposition to
complex disease, but much is left to discover. A common thread to most genetic
investigations is familial relationships. Close relatives can be identified
from family records, and more distant relatives can be inferred from large
panels of genetic markers. Unfortunately these empirical estimates can be
noisy, especially regarding distant relatives. We propose a new method for
denoising genetically - inferred relationship matrices by exploiting the
underlying structure due to hierarchical groupings of correlated individuals.
The approach, which we call Treelet Covariance Smoothing, employs a multiscale
decomposition of covariance matrices to improve estimates of pairwise
relationships. On both simulated and real data, we show that smoothing leads to
better estimates of the relatedness amongst distantly related individuals. We
illustrate our method with a large genome-wide association study and estimate
the ""heritability"" of body mass index quite accurately. Traditionally
heritability, defined as the fraction of the total trait variance attributable
to additive genetic effects, is estimated from samples of closely related
individuals using random effects models. We show that by using smoothed
relationship matrices we can estimate heritability using population-based
samples. Finally, while our methods have been developed for refining genetic
relationship matrices and improving estimates of heritability, they have much
broader potential application in statistics. Most notably, for
error-in-variables random effects models and settings that require
regularization of matrices with block or hierarchical structure.
"
"  Nonparametric estimators of the mean total cost have been proposed in a
variety of settings. In clinical trials it is generally impractical to follow
up patients until all have responded, and therefore censoring of patient
outcomes and total cost will occur in practice. We describe a general
longitudinal framework in which costs emanate from two streams, during sojourn
in health states and in transition from one health state to another. We
consider estimation of net present value for expenditures incurred over a
finite time horizon from medical cost data that might be incompletely
ascertained in some patients. Because patient specific demographic and clinical
characteristics would influence total cost, we use a regression model to
incorporate covariates. We discuss similarities and differences between our net
present value estimator and other widely used estimators of total medical
costs. Our model can accommodate heteroscedasticity, skewness and censoring in
cost data and provides a flexible approach to analyses of health care cost.
"
"  The problem of estimating the parameters of a moving target in multiple-input
multiple-output (MIMO) radar is considered and a new approach for estimating
the moving target parameters by making use of the phase information associated
with each transmit-receive path is introduced. It is required for this
technique that different receive antennas have the same time reference, but no
synchronization of initial phases of the receive antennas is needed and,
therefore, the estimation process is non-coherent. We model the target motion
within a certain processing interval as a polynomial of general order. The
first three coefficients of such a polynomial correspond to the initial
location, velocity, and acceleration of the target, respectively. A new maximum
likelihood (ML) technique for estimating the target motion coefficients is
developed. It is shown that the considered ML problem can be interpreted as the
classic ""overdetermined"" nonlinear least-squares problem. The proposed ML
estimator requires multi-dimensional search over the unknown polynomial
coefficients. The Cram\'er-Rao Bound (CRB) for the proposed parameter
estimation problem is derived. The performance of the proposed estimator is
validated by simulation results and is shown to achieve the CRB.
"
"  Graphical model learning and inference are often performed using Bayesian
techniques. In particular, learning is usually performed in two separate steps.
First, the graph structure is learned from the data; then the parameters of the
model are estimated conditional on that graph structure. While the probability
distributions involved in this second step have been studied in depth, the ones
used in the first step have not been explored in as much detail.
  In this paper, we will study the prior and posterior distributions defined
over the space of the graph structures for the purpose of learning the
structure of a graphical model. In particular, we will provide a
characterisation of the behaviour of those distributions as a function of the
possible edges of the graph. We will then use the properties resulting from
this characterisation to define measures of structural variability for both
Bayesian and Markov networks, and we will point out some of their possible
applications.
"
"  Nonseparable panel models are important in a variety of economic settings,
including discrete choice. This paper gives identification and estimation
results for nonseparable models under time homogeneity conditions that are like
""time is randomly assigned"" or ""time is an instrument."" Partial identification
results for average and quantile effects are given for discrete regressors,
under static or dynamic conditions, in fully nonparametric and in
semiparametric models, with time effects. It is shown that the usual, linear,
fixed-effects estimator is not a consistent estimator of the identified average
effect, and a consistent estimator is given. A simple estimator of identified
quantile treatment effects is given, providing a solution to the important
problem of estimating quantile treatment effects from panel data. Bounds for
overall effects in static and dynamic models are given. The dynamic bounds
provide a partial identification solution to the important problem of
estimating the effect of state dependence in the presence of unobserved
heterogeneity. The impact of $T$, the number of time periods, is shown by
deriving shrinkage rates for the identified set as $T$ grows. We also consider
semiparametric, discrete-choice models and find that semiparametric panel
bounds can be much tighter than nonparametric bounds.
Computationally-convenient methods for semiparametric models are presented. We
propose a novel inference method that applies in panel data and other settings
and show that it produces uniformly valid confidence regions in large samples.
We give empirical illustrations.
"
"  We suggest using the max-norm as a convex surrogate constraint for
clustering. We show how this yields a better exact cluster recovery guarantee
than previously suggested nuclear-norm relaxation, and study the effectiveness
of our method, and other related convex relaxations, compared to other
clustering approaches.
"
"  The objective of this paper is to develop statistical methodology for
planning and evaluating three-armed non-inferiority trials for general
retention of effect hypotheses, where the endpoint of interest may follow any
(regular) parametric distribution family. This generalizes and unifies specific
results for binary, normally and exponentially distributed endpoints. We
propose a Wald-type test procedure for the retention of effect hypothesis
(RET), which assures that the test treatment maintains at least a proportion
$\Delta$ of reference treatment effect compared to placebo. At this, we
distinguish the cases where the variance of the test statistic is estimated
unrestrictedly and restrictedly to the null hypothesis, to improve accuracy of
the nominal level. We present a general valid sample size allocation rule to
achieve optimal power and sample size formulas, which significantly improve
existing ones. Moreover, we propose a general applicable rule of thumb for
sample allocation and give conditions where this rule is theoretically
justified. The presented methodologies are discussed in detail for binary and
for Poisson distributed endpoints by means of two clinical trials in the
treatment of depression and in the treatment of epilepsy, respectively.
$R$-software for implementation of the proposed tests and for sample size
planning accompanies this paper.
"
"  The present work has as objective to show the profile of students who
abandoned the studies in a High School, located in Sao Joao de Meriti city,
municipal district of Rio de Janeiro state, by means of statistical analysis.
The presented indices portray an undesirable reality with almost 20% school
evasion, beyond showing that more the half of the students not standing in
adequate series. Keywords: School evasion, High Schools, Educational
Statistics.
"
"  While statisticians are well-accustomed to performing exploratory analysis in
the modeling stage of an analysis, the notion of conducting preliminary
general-purpose exploratory analysis in the Monte Carlo stage (or more
generally, the model-fitting stage) of an analysis is an area which we feel
deserves much further attention. Towards this aim, this paper proposes a
general-purpose algorithm for automatic density exploration. The proposed
exploration algorithm combines and expands upon components from various
adaptive Markov chain Monte Carlo methods, with the Wang-Landau algorithm at
its heart. Additionally, the algorithm is run on interacting parallel chains --
a feature which both decreases computational cost as well as stabilizes the
algorithm, improving its ability to explore the density. Performance is studied
in several applications. Through a Bayesian variable selection example, the
authors demonstrate the convergence gains obtained with interacting chains. The
ability of the algorithm's adaptive proposal to induce mode-jumping is
illustrated through a trimodal density and a Bayesian mixture modeling
application. Lastly, through a 2D Ising model, the authors demonstrate the
ability of the algorithm to overcome the high correlations encountered in
spatial models.
"
"  An enormous amount of observations on Cosmic Microwave Background radiation
has been collected in the last decade, and much more data are expected in the
near future from planned or operating satellite missions. These datasets are a
goldmine of information for Cosmology and Theoretical Physics; their efficient
exploitation posits several intriguing challenges from the statistical point of
view. In this paper we review a number of open problems in CMB data analysis
and we present applications to observations from the WMAP mission.
"
"  An experimental unit is an opportunity to randomly apply or withhold a
treatment. There is interference between units if the application of the
treatment to one unit may also affect other units. In cognitive neuroscience, a
common form of experiment presents a sequence of stimuli or requests for
cognitive activity at random to each experimental subject and measures
biological aspects of brain activity that follow these requests. Each subject
is then many experimental units, and interference between units within an
experimental subject is likely, in part because the stimuli follow one another
quickly and in part because human subjects learn or become experienced or
primed or bored as the experiment proceeds. We use a recent fMRI experiment
concerned with the inhibition of motor activity to illustrate and further
develop recently proposed methodology for inference in the presence of
interference. A simulation evaluates the power of competing procedures.
"
"  We present in this work a new family of kernels to compare positive measures
on arbitrary spaces $\Xcal$ endowed with a positive kernel $\kappa$, which
translates naturally into kernels between histograms or clouds of points. We
first cover the case where $\Xcal$ is Euclidian, and focus on kernels which
take into account the variance matrix of the mixture of two measures to compute
their similarity. The kernels we define are semigroup kernels in the sense that
they only use the sum of two measures to compare them, and spectral in the
sense that they only use the eigenspectrum of the variance matrix of this
mixture. We show that such a family of kernels has close bonds with the laplace
transforms of nonnegative-valued functions defined on the cone of positive
semidefinite matrices, and we present some closed formulas that can be derived
as special cases of such integral expressions. By focusing further on functions
which are invariant to the addition of a null eigenvalue to the spectrum of the
variance matrix, we can define kernels between atomic measures on arbitrary
spaces $\Xcal$ endowed with a kernel $\kappa$ by using directly the eigenvalues
of the centered Gram matrix of the joined support of the compared measures. We
provide explicit formulas suited for applications and present preliminary
experiments to illustrate the interest of the approach.
"
"  The problem of maximum-likelihood (ML) estimation of discrete tree-structured
distributions is considered. Chow and Liu established that ML-estimation
reduces to the construction of a maximum-weight spanning tree using the
empirical mutual information quantities as the edge weights. Using the theory
of large-deviations, we analyze the exponent associated with the error
probability of the event that the ML-estimate of the Markov tree structure
differs from the true tree structure, given a set of independently drawn
samples. By exploiting the fact that the output of ML-estimation is a tree, we
establish that the error exponent is equal to the exponential rate of decay of
a single dominant crossover event. We prove that in this dominant crossover
event, a non-neighbor node pair replaces a true edge of the distribution that
is along the path of edges in the true tree graph connecting the nodes in the
non-neighbor pair. Using ideas from Euclidean information theory, we then
analyze the scenario of ML-estimation in the very noisy learning regime and
show that the error exponent can be approximated as a ratio, which is
interpreted as the signal-to-noise ratio (SNR) for learning tree distributions.
We show via numerical experiments that in this regime, our SNR approximation is
accurate.
"
"  (ABRIDGED) In previous work, two platforms have been developed for testing
computer-vision algorithms for robotic planetary exploration (McGuire et al.
2004b,2005; Bartolo et al. 2007). The wearable-computer platform has been
tested at geological and astrobiological field sites in Spain (Rivas
Vaciamadrid and Riba de Santiuste), and the phone-camera has been tested at a
geological field site in Malta. In this work, we (i) apply a Hopfield
neural-network algorithm for novelty detection based upon color, (ii) integrate
a field-capable digital microscope on the wearable computer platform, (iii)
test this novelty detection with the digital microscope at Rivas Vaciamadrid,
(iv) develop a Bluetooth communication mode for the phone-camera platform, in
order to allow access to a mobile processing computer at the field sites, and
(v) test the novelty detection on the Bluetooth-enabled phone-camera connected
to a netbook computer at the Mars Desert Research Station in Utah. This systems
engineering and field testing have together allowed us to develop a real-time
computer-vision system that is capable, for example, of identifying lichens as
novel within a series of images acquired in semi-arid desert environments. We
acquired sequences of images of geologic outcrops in Utah and Spain consisting
of various rock types and colors to test this algorithm. The algorithm robustly
recognized previously-observed units by their color, while requiring only a
single image or a few images to learn colors as familiar, demonstrating its
fast learning capability.
"
"  Variable selection in linear models plays a pivotal role in modern
statistics. Hard-thresholding methods such as $l_0$ regularization are
theoretically ideal but computationally infeasible. In this paper, we propose a
new approach, called the LAGS, short for ""least absulute gradient selector"", to
this challenging yet interesting problem by mimicking the discrete selection
process of $l_0$ regularization. To estimate $\beta$ under the influence of
noise, we consider, nevertheless, the following convex program [\hat{\beta} =
\textrm{arg min}\frac{1}{n}\|X^{T}(y - X\beta)\|_1 + \lambda_n\sum_{i =
1}^pw_i(y;X;n)|\beta_i|]
  $\lambda_n > 0$ controls the sparsity and $w_i > 0$ dependent on $y, X$ and
$n$ is the weights on different $\beta_i$; $n$ is the sample size.
Surprisingly, we shall show in the paper, both geometrically and analytically,
that LAGS enjoys two attractive properties: (1) LAGS demonstrates discrete
selection behavior and hard thresholding property as $l_0$ regularization by
strategically chosen $w_i$, we call this property ""pseudo-hard thresholding"";
(2) Asymptotically, LAGS is consistent and capable of discovering the true
model; nonasymptotically, LAGS is capable of identifying the sparsity in the
model and the prediction error of the coefficients is bounded at the noise
level up to a logarithmic factor---$\log p$, where $p$ is the number of
predictors.
  Computationally, LAGS can be solved efficiently by convex program routines
for its convexity or by simplex algorithm after recasting it into a linear
program. The numeric simulation shows that LAGS is superior compared to
soft-thresholding methods in terms of mean squared error and parsimony of the
model.
"
"  Bayesian classification and regression with high order interactions is
largely infeasible because Markov chain Monte Carlo (MCMC) would need to be
applied with a great many parameters, whose number increases rapidly with the
order. In this paper we show how to make it feasible by effectively reducing
the number of parameters, exploiting the fact that many interactions have the
same values for all training cases. Our method uses a single ``compressed''
parameter to represent the sum of all parameters associated with a set of
patterns that have the same value for all training cases. Using symmetric
stable distributions as the priors of the original parameters, we can easily
find the priors of these compressed parameters. We therefore need to deal only
with a much smaller number of compressed parameters when training the model
with MCMC. The number of compressed parameters may have converged before
considering the highest possible order. After training the model, we can split
these compressed parameters into the original ones as needed to make
predictions for test cases. We show in detail how to compress parameters for
logistic sequence prediction models. Experiments on both simulated and real
data demonstrate that a huge number of parameters can indeed be reduced by our
compression method.
"
"  We argue that the time from the onset of infectiousness to infectious
contact, which we call the contact interval, is a better basis for inference in
epidemic data than the generation or serial interval. Since contact intervals
can be right-censored, survival analysis is the natural approach to estimation.
Estimates of the contact interval distribution can be used to estimate R_0 in
both mass-action and network-based models.
"
"  A pair of ecological tables is made of one table containing environmental
variables (in columns) and another table containing species data (in columns).
The rows of these two tables are identical and correspond to the sites where
environmental variables and species data have been measured. Such data are used
to analyze the relationships between species and their environment. If sampling
is repeated over time for both tables, one obtains a sequence of pairs of
ecological tables. Analyzing this type of data is a way to assess changes in
species-environment relationships, which can be important for conservation
Ecology or for global change studies. We present a new data analysis method
adapted to the study of this type of data, and we compare it with two other
methods on the same data set. All three methods are implemented in the ade4
package for the R environment.
"
"  Since the early days of digital communication, hidden Markov models (HMMs)
have now been also routinely used in speech recognition, processing of natural
languages, images, and in bioinformatics. In an HMM $(X_i,Y_i)_{i\ge 1}$,
observations $X_1,X_2,...$ are assumed to be conditionally independent given an
``explanatory'' Markov process $Y_1,Y_2,...$, which itself is not observed;
moreover, the conditional distribution of $X_i$ depends solely on $Y_i$.
Central to the theory and applications of HMM is the Viterbi algorithm to find
{\em a maximum a posteriori} (MAP) estimate $q_{1:n}=(q_1,q_2,...,q_n)$ of
$Y_{1:n}$ given observed data $x_{1:n}$. Maximum {\em a posteriori} paths are
also known as Viterbi paths or alignments. Recently, attempts have been made to
study the behavior of Viterbi alignments when $n\to \infty$. Thus, it has been
shown that in some special cases a well-defined limiting Viterbi alignment
exists. While innovative, these attempts have relied on rather strong
assumptions and involved proofs which are existential. This work proves the
existence of infinite Viterbi alignments in a more constructive manner and for
a very general class of HMMs.
"
"  This work presents a family of parsimonious Gaussian process models which
allow to build, from a finite sample, a model-based classifier in an infinite
dimensional space. The proposed parsimonious models are obtained by
constraining the eigen-decomposition of the Gaussian processes modeling each
class. This allows in particular to use non-linear mapping functions which
project the observations into infinite dimensional spaces. It is also
demonstrated that the building of the classifier can be directly done from the
observation space through a kernel function. The proposed classification method
is thus able to classify data of various types such as categorical data,
functional data or networks. Furthermore, it is possible to classify mixed data
by combining different kernels. The methodology is as well extended to the
unsupervised classification case. Experimental results on various data sets
demonstrate the effectiveness of the proposed method.
"
"  Variational methods are widely used for approximate posterior inference.
However, their use is typically limited to families of distributions that enjoy
particular conjugacy properties. To circumvent this limitation, we propose a
family of variational approximations inspired by nonparametric kernel density
estimation. The locations of these kernels and their bandwidth are treated as
variational parameters and optimized to improve an approximate lower bound on
the marginal likelihood of the data. Using multiple kernels allows the
approximation to capture multiple modes of the posterior, unlike most other
variational approximations. We demonstrate the efficacy of the nonparametric
approximation with a hierarchical logistic regression model and a nonlinear
matrix factorization model. We obtain predictive performance as good as or
better than more specialized variational methods and sample-based
approximations. The method is easy to apply to more general graphical models
for which standard variational methods are difficult to derive.
"
"  Nuclear norm minimization (NNM) has recently gained significant attention for
its use in rank minimization problems. Similar to compressed sensing, using
null space characterizations, recovery thresholds for NNM have been studied in
\cite{arxiv,Recht_Xu_Hassibi}. However simulations show that the thresholds are
far from optimal, especially in the low rank region. In this paper we apply the
recent analysis of Stojnic for compressed sensing \cite{mihailo} to the null
space conditions of NNM. The resulting thresholds are significantly better and
in particular our weak threshold appears to match with simulation results.
Further our curves suggest for any rank growing linearly with matrix size $n$
we need only three times of oversampling (the model complexity) for weak
recovery. Similar to \cite{arxiv} we analyze the conditions for weak, sectional
and strong thresholds. Additionally a separate analysis is given for special
case of positive semidefinite matrices. We conclude by discussing simulation
results and future research directions.
"
"  I do not remember when was the first time that I met Leo, but I have a clear
memory of going to Leo's office on the 4th floor of Evans Hall to talk to him
in my second year in Berkeley's Ph.D. program in 1986. The details of the
conversation are not retained but a visual image of his clean and orderly
office remains, in a stark contrast to a high entropy state of the same office
now being used by myself.
"
"  The ages and masses of neutron stars (NSs) are two fundamental threads that
make pulsars accessible to other sub-disciplines of astronomy and physics. A
realistic and accurate determination of these two derived parameters play an
important role in understanding of advanced stages of stellar evolution and the
physics that govern relevant processes. Here I summarize new constraints on the
ages and masses of NSs with an evolutionary perspective. I show that the
observed P-Pdot demographics is more diverse than what is theoretically
predicted for the standard evolutionary channel. In particular, standard
recycling followed by dipole spin-down fails to reproduce the population of
millisecond pulsars with higher magnetic fields (B > 4 x 10^{8} G) at rates
deduced from observations. A proper inclusion of constraints arising from
binary evolution and mass accretion offers a more realistic insight into the
age distribution. By analytically implementing these constraints, I propose a
""modified"" spin-down age for millisecond pulsars that gives estimates closer to
the true age. Finally, I independently analyze the peak, skewness and cutoff
values of the underlying mass distribution from a comprehensive list of radio
pulsars for which secure mass measurements are available. The inferred mass
distribution shows clear peaks at 1.35 Msun and 1.50 Msun for NSs in double
neutron star (DNS) and neutron star-white dwarf (NS-WD) systems respectively. I
find a mass cutoff at 2 Msun for NSs with WD companions, which establishes a
firm lower bound for the maximum mass of NSs.
"
"  Model-based learning algorithms have been shown to use experience efficiently
when learning to solve Markov Decision Processes (MDPs) with finite state and
action spaces. However, their high computational cost due to repeatedly solving
an internal model inhibits their use in large-scale problems. We propose a
method based on real-time dynamic programming (RTDP) to speed up two
model-based algorithms, RMAX and MBIE (model-based interval estimation),
resulting in computationally much faster algorithms with little loss compared
to existing bounds. Specifically, our two new learning algorithms, RTDP-RMAX
and RTDP-IE, have considerably smaller computational demands than RMAX and
MBIE. We develop a general theoretical framework that allows us to prove that
both are efficient learners in a PAC (probably approximately correct) sense. We
also present an experimental evaluation of these new algorithms that helps
quantify the tradeoff between computational and experience demands.
"
"  Recently two search algorithms, A* and breadth-first branch and bound
(BFBnB), were developed based on a simple admissible heuristic for learning
Bayesian network structures that optimize a scoring function. The heuristic
represents a relaxation of the learning problem such that each variable chooses
optimal parents independently. As a result, the heuristic may contain many
directed cycles and result in a loose bound. This paper introduces an improved
admissible heuristic that tries to avoid directed cycles within small groups of
variables. A sparse representation is also introduced to store only the unique
optimal parent choices. Empirical results show that the new techniques
significantly improved the efficiency and scalability of A* and BFBnB on most
of datasets tested in this paper.
"
"  Identifying promising compounds from a vast collection of feasible compounds
is an important and yet challenging problem in the pharmaceutical industry. An
efficient solution to this problem will help reduce the expenditure at the
early stages of drug discovery. In an attempt to solve this problem, Mandal, Wu
and Johnson [Technometrics 48 (2006) 273--283] proposed the SELC algorithm.
Although powerful, it fails to extract substantial information from the data to
guide the search efficiently, as this methodology is not based on any
statistical modeling. The proposed approach uses Gaussian Process (GP) modeling
to improve upon SELC, and hence named $\mathcal{G}$-SELC. The performance of
the proposed methodology is illustrated using four and five dimensional test
functions. Finally, we implement the new algorithm on a real pharmaceutical
data set for finding a group of chemical compounds with optimal properties.
"
"  In many classification systems, sensing modalities have different acquisition
costs. It is often {\it unnecessary} to use every modality to classify a
majority of examples. We study a multi-stage system in a prediction time cost
reduction setting, where the full data is available for training, but for a
test example, measurements in a new modality can be acquired at each stage for
an additional cost. We seek decision rules to reduce the average measurement
acquisition cost. We formulate an empirical risk minimization problem (ERM) for
a multi-stage reject classifier, wherein the stage $k$ classifier either
classifies a sample using only the measurements acquired so far or rejects it
to the next stage where more attributes can be acquired for a cost. To solve
the ERM problem, we show that the optimal reject classifier at each stage is a
combination of two binary classifiers, one biased towards positive examples and
the other biased towards negative examples. We use this parameterization to
construct stage-by-stage global surrogate risk, develop an iterative algorithm
in the boosting framework and present convergence and generalization results.
We test our work on synthetic, medical and explosives detection datasets. Our
results demonstrate that substantial cost reduction without a significant
sacrifice in accuracy is achievable.
"
"  I studied what role the US stock markets and money markets have possibly
played in the Gross Private Domestic Investment (GPDI) of the United States
from the year 1959 to the year 2001, Gross Private Domestic Investment refers
to the total amount of investment spending by businesses and firms located
within the borders of a nation. It includes both the values of the purchases of
non-residential fixed investment, which include capital goods used for
production, and the values of the purchases of residential fixed investment,
which include construction spending for factories or offices. And I created a
Multiple Linear Regression Model of the GDPI. To see if companies and private
citizens use the stock market and money markets as a way of financing capital
projects (business ventures, buying commercial and noncommercial property,
etc).
  Keywords: Gross Private Domestic Investment, Pearson Correlation, SP 500, TB3
"
"  Stochastic neighbor embedding (SNE) and related nonlinear manifold learning
algorithms achieve high-quality low-dimensional representations of similarity
data, but are notoriously slow to train. We propose a generic formulation of
embedding algorithms that includes SNE and other existing algorithms, and study
their relation with spectral methods and graph Laplacians. This allows us to
define several partial-Hessian optimization strategies, characterize their
global and local convergence, and evaluate them empirically. We achieve up to
two orders of magnitude speedup over existing training methods with a strategy
(which we call the spectral direction) that adds nearly no overhead to the
gradient and yet is simple, scalable and applicable to several existing and
future embedding algorithms.
"
"  Efficient and automated classification of periodic variable stars is becoming
increasingly important as the scale of astronomical surveys grows. Several
recent papers have used methods from machine learning and statistics to
construct classifiers on databases of labeled, multi--epoch sources with the
intention of using these classifiers to automatically infer the classes of
unlabeled sources from new surveys. However, the same source observed with two
different synoptic surveys will generally yield different derived metrics
(features) from the light curve. Since such features are used in classifiers,
this survey-dependent mismatch in feature space will typically lead to degraded
classifier performance. In this paper we show how and why feature distributions
change using OGLE and \textit{Hipparcos} light curves. To overcome survey
systematics, we apply a method, \textit{noisification}, which attempts to
empirically match distributions of features between the labeled sources used to
construct the classifier and the unlabeled sources we wish to classify. Results
from simulated and real--world light curves show that noisification can
significantly improve classifier performance. In a three--class problem using
light curves from \textit{Hipparcos} and OGLE, noisification reduces the
classifier error rate from 27.0% to 7.0%. We recommend that noisification be
used for upcoming surveys such as Gaia and LSST and describe some of the
promises and challenges of applying noisification to these surveys.
"
"  Here we introduce a new design framework for synthetic biology that exploits
the advantages of Bayesian model selection. We will argue that the difference
between inference and design is that in the former we try to reconstruct the
system that has given rise to the data that we observe, while in the latter, we
seek to construct the system that produces the data that we would like to
observe, i.e. the desired behavior. Our approach allows us to exploit methods
from Bayesian statistics, including efficient exploration of models spaces and
high-dimensional parameter spaces, and the ability to rank models with respect
to their ability to generate certain types of data. Bayesian model selection
furthermore automatically strikes a balance between complexity and (predictive
or explanatory) performance of mathematical models. In order to deal with the
complexities of molecular systems we employ an approximate Bayesian computation
scheme which only requires us to simulate from different competing models in
order to arrive at rational criteria for choosing between them. We illustrate
the advantages resulting from combining the design and modeling (or in-silico
prototyping) stages currently seen as separate in synthetic biology by
reference to deterministic and stochastic model systems exhibiting adaptive and
switch-like behavior, as well as bacterial two-component signaling systems.
"
"  We propose a Model-Based Clustering (MBC) method combined with loci selection
using multi-allelic loci genetic data. The loci selection problem is regarded
as a model selection problem and models in competition are compared with the
Bayesian Information Criterion (BIC). The resulting procedure selects the
subset of clustering loci, the number of clusters, estimates the proportion of
each cluster and the allelic frequencies within each cluster. We prove that the
selected model converges in probability to the true model under a single
realistic assumption as the size of the sample tends to infinity. The proposed
method named MixMoGenD (Mixture Model using Genetic Data) was implemented using
c++ programming language. Numerical experiments on simulated data sets was
conducted to highlight the interest of the proposed loci selection procedure.
"
"  Many man-made and natural phenomena, including the intensity of earthquakes,
population of cities and size of international wars, are believed to follow
power-law distributions. The accurate identification of power-law patterns has
significant consequences for correctly understanding and modeling complex
systems. However, statistical evidence for or against the power-law hypothesis
is complicated by large fluctuations in the empirical distribution's tail, and
these are worsened when information is lost from binning the data. We adapt the
statistically principled framework for testing the power-law hypothesis,
developed by Clauset, Shalizi and Newman, to the case of binned data. This
approach includes maximum-likelihood fitting, a hypothesis test based on the
Kolmogorov--Smirnov goodness-of-fit statistic and likelihood ratio tests for
comparing against alternative explanations. We evaluate the effectiveness of
these methods on synthetic binned data with known structure, quantify the loss
of statistical power due to binning, and apply the methods to twelve real-world
binned data sets with heavy-tailed patterns.
"
"  The most direct way to express arbitrary dependencies in datasets is to
estimate the joint distribution and to apply afterwards the argmax-function to
obtain the mode of the corresponding conditional distribution. This method is
in practice difficult, because it requires a global optimization of a
complicated function, the joint distribution by fixed input variables. This
article proposes a method for finding global maxima if the joint distribution
is modeled by a kernel density estimation. Some experiments show advantages and
shortcomings of the resulting regression method in comparison to the standard
Nadaraya-Watson regression technique, which approximates the optimum by the
expectation value.
"
"  There is no known efficient method for selecting k Gaussian features from n
which achieve the lowest Bayesian classification error. We show an example of
how greedy algorithms faced with this task are led to give results that are not
optimal. This motivates us to propose a more robust approach. We present a
Branch and Bound algorithm for finding a subset of k independent Gaussian
features which minimizes the naive Bayesian classification error. Our algorithm
uses additive monotonic distance measures to produce bounds for the Bayesian
classification error in order to exclude many feature subsets from evaluation,
while still returning an optimal solution. We test our method on synthetic data
as well as data obtained from gene expression profiling.
"
"  Propagation in non-line-of-sight (NLOS) conditions is one of the major
impairments in ultrawideband (UWB) wireless localization systems based on
time-of-arrival (TOA) measurements. In this paper the problem of the joint
statistical characterization of the NLOS bias and of the most representative
features of LOS/NLOS UWB waveforms is investigated. In addition, the
performance of various maximum-likelihood (ML) estimators for joint
localization and NLOS bias mitigation is assessed. Our numerical results
evidence that the accuracy of all the considered estimators is appreciably
influenced by the LOS/NLOS conditions of the propagation environment and that a
statistical knowledge of multiple signal features can be exploited to mitigate
the NLOS bias, reducing the overall localization error.
"
"  This paper presents a kernel-based discriminative learning framework on
probability measures. Rather than relying on large collections of vectorial
training examples, our framework learns using a collection of probability
distributions that have been constructed to meaningfully represent training
data. By representing these probability distributions as mean embeddings in the
reproducing kernel Hilbert space (RKHS), we are able to apply many standard
kernel-based learning techniques in straightforward fashion. To accomplish
this, we construct a generalization of the support vector machine (SVM) called
a support measure machine (SMM). Our analyses of SMMs provides several insights
into their relationship to traditional SVMs. Based on such insights, we propose
a flexible SVM (Flex-SVM) that places different kernel functions on each
training example. Experimental results on both synthetic and real-world data
demonstrate the effectiveness of our proposed framework.
"
"  A series of ten plant species belonging to Magnoliopsida - Dicotyledons class
were analyzed in terms of chemical compounds distribution of abundance,
starting from the assumption that these distributions should give a picture of
similarities and differences between plants metabolism. From a pool of
theoretical distributions, log-normal distribution was selected giving the best
accuracy with the modeled phenomena and agreement with the observed data. From
obtained lognormal distributions statistics a classification were constructed
and were compared with the classification based on phylogeny.
"
"  Many networks are complex dynamical systems, where both attributes of nodes
and topology of the network (link structure) can change with time. We propose a
model of co-evolving networks where both node at- tributes and network
structure evolve under mutual influence. Specifically, we consider a mixed
membership stochastic blockmodel, where the probability of observing a link
between two nodes depends on their current membership vectors, while those
membership vectors themselves evolve in the presence of a link between the
nodes. Thus, the network is shaped by the interaction of stochastic processes
describing the nodes, while the processes themselves are influenced by the
changing network structure. We derive an efficient variational inference
procedure for our model, and validate the model on both synthetic and
real-world data.
"
"  In many scientific settings data can be naturally partitioned into variable
groupings called views. Common examples include environmental (1st view) and
genetic information (2nd view) in ecological applications, chemical (1st view)
and biological (2nd view) data in drug discovery. Multi-view data also occur in
text analysis and proteomics applications where one view consists of a graph
with observations as the vertices and a weighted measure of pairwise similarity
between observations as the edges. Further, in several of these applications
the observations can be partitioned into two sets, one where the response is
observed (labeled) and the other where the response is not (unlabeled). The
problem for simultaneously addressing viewed data and incorporating unlabeled
observations in training is referred to as multi-view transductive learning. In
this work we introduce and study a comprehensive generalized fixed point
additive modeling framework for multi-view transductive learning, where any
view is represented by a linear smoother. The problem of view selection is
discussed using a generalized Akaike Information Criterion, which provides an
approach for testing the contribution of each view. An efficient implementation
is provided for fitting these models with both backfitting and local-scoring
type algorithms adjusted to semi-supervised graph-based learning. The proposed
technique is assessed on both synthetic and real data sets and is shown to be
competitive to state-of-the-art co-training and graph-based techniques.
"
"  A modelgenerator is developed that searches for cointegrated models among a
potentially large group of candidate models. The generator employs the first
step of the Engle-Granger procedure and orders cointegrated models according to
the information criterions AIC and BIC. Assisted by the generator, a
cointegrated relation is established between recorded violent crime in the
Netherlands, the number of males aged 15-25 years (split into Western and
non-Western background) and deflated consumption. In-sample forecasts reveal
that the cointegrated model outperforms the best short-run models.
"
"  The proposed approach extends the confidence posterior distribution to the
semi-parametric empirical Bayes setting. Whereas the Bayesian posterior is
defined in terms of a prior distribution conditional on the observed data, the
confidence posterior is defined such that the probability that the parameter
value lies in any fixed subset of parameter space, given the observed data, is
equal to the coverage rate of the corresponding confidence interval. A
confidence posterior that has correct frequentist coverage at each fixed
parameter value is combined with the estimated local false discovery rate to
yield a parameter distribution from which interval and point estimates are
derived within the framework of minimizing expected loss. The point estimates
exhibit suitable shrinkage toward the null hypothesis value, making them
practical for automatically ranking features in order of priority. The
corresponding confidence intervals are also shrunken and tend to be much
shorter than their fixed-parameter counterparts, as illustrated with gene
expression data. Further, simulations confirm a theoretical argument that the
shrunken confidence intervals cover the parameter at a higher-than-nominal
frequency.
"
"  In many scientific disciplines structures in high-dimensional data have to be
found, e.g., in stellar spectra, in genome data, or in face recognition tasks.
In this work we present a novel approach to non-linear dimensionality
reduction. It is based on fitting K-nearest neighbor regression to the
unsupervised regression framework for learning of low-dimensional manifolds.
Similar to related approaches that are mostly based on kernel methods,
unsupervised K-nearest neighbor (UNN) regression optimizes latent variables
w.r.t. the data space reconstruction error employing the K-nearest neighbor
heuristic. The problem of optimizing latent neighborhoods is difficult to
solve, but the UNN formulation allows the design of efficient strategies that
iteratively embed latent points to fixed neighborhood topologies. UNN is well
appropriate for sorting of high-dimensional data. The iterative variants are
analyzed experimentally.
"
"  Most classification algorithms used in high energy physics fall under the
category of supervised machine learning. Such methods require a training set
containing both signal and background events and are prone to classification
errors should this training data be systematically inaccurate for example due
to the assumed MC model. To complement such model-dependent searches, we
propose an algorithm based on semi-supervised anomaly detection techniques,
which does not require a MC training sample for the signal data. We first model
the background using a multivariate Gaussian mixture model. We then search for
deviations from this model by fitting to the observations a mixture of the
background model and a number of additional Gaussians. This allows us to
perform pattern recognition of any anomalous excess over the background. We
show by a comparison to neural network classifiers that such an approach is a
lot more robust against misspecification of the signal MC than supervised
classification. In cases where there is an unexpected signal, a neural network
might fail to correctly identify it, while anomaly detection does not suffer
from such a limitation. On the other hand, when there are no systematic errors
in the training data, both methods perform comparably.
"
"  Motivation: The rapid growth in genome-wide association studies (GWAS) in
plants and animals has brought about the need for a central resource that
facilitates i) performing GWAS, ii) accessing data and results of other GWAS,
and iii) enabling all users regardless of their background to exploit the
latest statistical techniques without having to manage complex software and
computing resources.
  Results: We present easyGWAS, a web platform that provides methods, tools and
dynamic visualizations to perform and analyze GWAS. In addition, easyGWAS makes
it simple to reproduce results of others, validate findings, and access larger
sample sizes through merging of public datasets.
  Availability: Detailed method and data descriptions as well as tutorials are
available in the supplementary materials. easyGWAS is available at
http://easygwas.tuebingen.mpg.de/.
  Contact: dominik.grimm@tuebingen.mpg.de
"
"  Folded concave penalization methods have been shown to enjoy the strong
oracle property for high-dimensional sparse estimation. However, a folded
concave penalization problem usually has multiple local solutions and the
oracle property is established only for one of the unknown local solutions. A
challenging fundamental issue still remains that it is not clear whether the
local optimum computed by a given optimization algorithm possesses those nice
theoretical properties. To close this important theoretical gap in over a
decade, we provide a unified theory to show explicitly how to obtain the oracle
solution via the local linear approximation algorithm. For a folded concave
penalized estimation problem, we show that as long as the problem is
localizable and the oracle estimator is well behaved, we can obtain the oracle
estimator by using the one-step local linear approximation. In addition, once
the oracle estimator is obtained, the local linear approximation algorithm
converges, namely it produces the same estimator in the next iteration. The
general theory is demonstrated by using four classical sparse estimation
problems, that is, sparse linear regression, sparse logistic regression, sparse
precision matrix estimation and sparse quantile regression.
"
"  It is assumed that the solar cell efficiency of PV device is closely related
to the solar irradiance, considered the solar parameter Global Solar Irradiance
(G) and the meteorological parameters like daily data of Earth Skin Temperature
(E), Average Temperature (T), Relative Humidity (H) and Dew Frost Point (D),
for the coastal city Karachi and a non-coastal city Jacobabad, K and J is used
as a subscripts for parameters of Karachi and Jacobabad respectively. All
variables used here are dependent on the location (latitude and longitude) of
our stations except G. To employ ARIMA modeling, the first eighteen years data
is used for modeling and forecast is done for the last five years data. In most
cases results show good correlation among monthly actual and monthly forecasted
values of all the predictors. Next, multiple linear regression is employed to
the data obtained by ARIMA modeling and models for mean monthly observed G
values are constructed. For each station, two equations are constructed the R2
values are above 93% for each model, showing adequacy of the fit. Our
computations show that Solar cell efficiency can be increased if better
modeling for meteorological predictors governs the process.
"
"  We consider the predictive problem of supervised ranking, where the task is
to rank sets of candidate items returned in response to queries. Although there
exist statistical procedures that come with guarantees of consistency in this
setting, these procedures require that individuals provide a complete ranking
of all items, which is rarely feasible in practice. Instead, individuals
routinely provide partial preference information, such as pairwise comparisons
of items, and more practical approaches to ranking have aimed at modeling this
partial preference data directly. As we show, however, such an approach raises
serious theoretical challenges. Indeed, we demonstrate that many commonly used
surrogate losses for pairwise comparison data do not yield consistency;
surprisingly, we show inconsistency even in low-noise settings. With these
negative results as motivation, we present a new approach to supervised ranking
based on aggregation of partial preferences, and we develop $U$-statistic-based
empirical risk minimization procedures. We present an asymptotic analysis of
these new procedures, showing that they yield consistency results that parallel
those available for classification. We complement our theoretical results with
an experiment studying the new procedures in a large-scale web-ranking task.
"
"  Statistical properties of order-driven double-auction markets with Bid-Ask
spread are investigated through the dynamical quantities such as response
function. We first attempt to utilize the so-called {\it
Madhavan-Richardson-Roomans model} (MRR for short) to simulate the stochastic
process of the price-change in empirical data sets (say, EUR/JPY or USD/JPY
exchange rates) in which the Bid-Ask spread fluctuates in time. We find that
the MRR theory apparently fails to simulate so much as the qualitative
behaviour ('non-monotonic' behaviour) of the response function $R(l)$ ($l$
denotes the difference of times at which the response function is evaluated)
calculated from the data. Especially, we confirm that the stochastic nature of
the Bid-Ask spread causes apparent deviations from a linear relationship
between the $R(l)$ and the auto-correlation function $C(l)$, namely, $R(l)
\propto -C(l)$. To make the microscopic model of double-auction markets having
stochastic Bid-Ask spread, we use the minority game with a finite market
history length and find numerically that appropriate extension of the game
shows quite similar behaviour of the response function to the empirical
evidence. We also reveal that the minority game modeling with the adaptive
('annealed') look-up table reproduces the non-linear relationship $R(l) \propto
-f(C(l))$ ($f(x)$ stands for a non-linear function leading to
'$\lambda$-shapes') more effectively than the fixed (`quenched') look-up table
does.
"
"  We describe a Groebner basis of relations among conditional probabilities in
a discrete probability space, with any set of conditioned-upon events. They may
be specialized to the partially-observed random variable case, the purely
conditional case, and other special cases. We also investigate the connection
to generalized permutohedra and describe a conditional probability simplex.
"
"  We consider the problem of learning the structure of a pairwise graphical
model over continuous and discrete variables. We present a new pairwise model
for graphical models with both continuous and discrete variables that is
amenable to structure learning. In previous work, authors have considered
structure learning of Gaussian graphical models and structure learning of
discrete models. Our approach is a natural generalization of these two lines of
work to the mixed case. The penalization scheme involves a novel symmetric use
of the group-lasso norm and follows naturally from a particular parametrization
of the model.
"
"  We show that the sensor self-localization problem can be cast as a static
parameter estimation problem for Hidden Markov Models and we implement fully
decentralized versions of the Recursive Maximum Likelihood and on-line
Expectation-Maximization algorithms to localize the sensor network
simultaneously with target tracking. For linear Gaussian models, our algorithms
can be implemented exactly using a distributed version of the Kalman filter and
a novel message passing algorithm. The latter allows each node to compute the
local derivatives of the likelihood or the sufficient statistics needed for
Expectation-Maximization. In the non-linear case, a solution based on local
linearization in the spirit of the Extended Kalman Filter is proposed. In
numerical examples we demonstrate that the developed algorithms are able to
learn the localization parameters.
"
"  Using fits to numerical simulations, we show that the entire hierarchy of
moments quickly ceases to provide a complete description of the convergence
one-point probability density function leaving the linear regime. This suggests
that the full N-point correlation function hierarchy of the convergence field
becomes quickly generically incomplete and a very poor cosmological probe on
nonlinear scales. At the scale of unit variance, only 5% of the Fisher
information content of the one-point probability density function is still
contained in its hierarchy of moments, making clear that information escaping
the hierarchy is a far stronger effect than information propagating to higher
order moments. It follows that the constraints on cosmological parameters
achievable through extraction of the entire hierarchy become suboptimal by
large amounts. A simple logarithmic mapping makes the moment hierarchy well
suited again for parameter extraction.
"
"  When choosing a suitable technique for regression and classification with
multivariate predictor variables, one is often faced with a tradeoff between
interpretability and high predictive accuracy. To give a classical example,
classification and regression trees are easy to understand and interpret. Tree
ensembles like Random Forests provide usually more accurate predictions. Yet
tree ensembles are also more difficult to analyze than single trees and are
often criticized, perhaps unfairly, as `black box' predictors. Node harvest is
trying to reconcile the two aims of interpretability and predictive accuracy by
combining positive aspects of trees and tree ensembles. Results are very sparse
and interpretable and predictive accuracy is extremely competitive, especially
for low signal-to-noise data. The procedure is simple: an initial set of a few
thousand nodes is generated randomly. If a new observation falls into just a
single node, its prediction is the mean response of all training observation
within this node, identical to a tree-like prediction. A new observation falls
typically into several nodes and its prediction is then the weighted average of
the mean responses across all these nodes. The only role of node harvest is to
`pick' the right nodes from the initial large ensemble of nodes by choosing
node weights, which amounts in the proposed algorithm to a quadratic
programming problem with linear inequality constraints. The solution is sparse
in the sense that only very few nodes are selected with a nonzero weight. This
sparsity is not explicitly enforced. Maybe surprisingly, it is not necessary to
select a tuning parameter for optimal predictive accuracy. Node harvest can
handle mixed data and missing values and is shown to be simple to interpret and
competitive in predictive accuracy on a variety of data sets.
"
"  It is now widely accepted that knowledge can be acquired from networks by
clustering their vertices according to connection profiles. Many methods have
been proposed and in this paper we concentrate on the Stochastic Block Model
(SBM). The clustering of vertices and the estimation of SBM model parameters
have been subject to previous work and numerous inference strategies such as
variational Expectation Maximization (EM) and classification EM have been
proposed. However, SBM still suffers from a lack of criteria to estimate the
number of components in the mixture. To our knowledge, only one model based
criterion, ICL, has been derived for SBM in the literature. It relies on an
asymptotic approximation of the Integrated Complete-data Likelihood and recent
studies have shown that it tends to be too conservative in the case of small
networks. To tackle this issue, we propose a new criterion that we call ILvb,
based on a non asymptotic approximation of the marginal likelihood. We describe
how the criterion can be computed through a variational Bayes EM algorithm.
"
"  The fitness of a biological strategy is typically measured by its expected
reproductive rate, the first moment of its offspring distribution. However,
strategies with high expected rates can also have high probabilities of
extinction. A similar situation is found in gambling and investment, where
strategies with a high expected payoff can also have a high risk of ruin. We
take inspiration from the gambler's ruin problem to examine how extinction is
related to population growth. Using moment theory we demonstrate how higher
moments can impact the probability of extinction. We discuss how moments can be
used to find bounds on the extinction probability, focusing on s-convex
ordering of random variables, a method developed in actuarial science. This
approach generates ""best case"" and ""worst case"" scenarios to provide upper and
lower bounds on the probability of extinction. Our results demonstrate that
even the most fit strategies can have high probabilities of extinction.
"
"  We describe a probabilistic (generative) view of affinity matrices along with
inference algorithms for a subclass of problems associated with data
clustering. This probabilistic view is helpful in understanding different
models and algorithms that are based on affinity functions OF the data. IN
particular, we show how(greedy) inference FOR a specific probabilistic model IS
equivalent TO the spectral clustering algorithm.It also provides a framework
FOR developing new algorithms AND extended models. AS one CASE, we present new
generative data clustering models that allow us TO infer the underlying
distance measure suitable for the clustering problem at hand. These models seem
to perform well in a larger class of problems for which other clustering
algorithms (including spectral clustering) usually fail. Experimental
evaluation was performed in a variety point data sets, showing excellent
performance.
"
"  Approaches for testing sets of variants, such as a set of rare or common
variants within a gene or pathway, for association with complex traits are
important. In particular, set tests allow for aggregation of weak signal within
a set, can capture interplay among variants, and reduce the burden of multiple
hypothesis testing. Until now, these approaches did not address confounding by
family relatedness and population structure, a problem that is becoming more
important as larger data sets are used to increase power.
  Results: We introduce a new approach for set tests that handles confounders.
Our model is based on the linear mixed model and uses two random effects-one to
capture the set association signal and one to capture confounders. We also
introduce a computational speedup for two-random-effects models that makes this
approach feasible even for extremely large cohorts. Using this model with both
the likelihood ratio test and score test, we find that the former yields more
power while controlling type I error. Application of our approach to richly
structured GAW14 data demonstrates that our method successfully corrects for
population structure and family relatedness, while application of our method to
a 15,000 individual Crohn's disease case-control cohort demonstrates that it
additionally recovers genes not recoverable by univariate analysis.
  Availability: A Python-based library implementing our approach is available
at http://mscompbio.codeplex.com
"
"  The Yule-Simpson paradox notes that an association between random variables
can be reversed when averaged over a background variable. Cox and Wermuth
(2003) introduced the concept of distribution dependence between two random
variables X and Y, and developed two dependence conditions, each of which
guarantees that reversal cannot occur. Ma, Xie and Geng (2006) studied the
collapsibility of distribution dependence over a background variable W, under a
rather strong homogeneity condition. Collapsibility ensures the association
remains the same for conditional and marginal models, so that Yule-Simpson
reversal cannot occur. In this paper, we investigate a more general condition
for avoiding effect reversal: A-collapsibility. The conditions of Cox and
Wermuth imply A-collapsibility, without assuming homogeneity. In fact, we show
that, when W is a binary variable, collapsibility is equivalent to
A-collapsibility plus homogeneity, and A-collapsibility is equivalent to the
conditions of Cox and Wermuth. Recently, Cox (2007) extended Cochran's result
on regression coefficients of conditional and marginal models, to quantile
regression coefficients. The conditions of Cox and Wermuth are sufficient for
A-collapsibility of quantile regression coefficients. If the conditional
distribution of W, given Y = y and X = x, belong to one-dimensional natural
exponential family, they are also necessary. Some applications of
A-collapsibility include the analysis of a contingency table, linear regression
models and quantile regression models.
"
"  In the Bayesian approach to structure learning of graphical models, the
equivalent sample size (ESS) in the Dirichlet prior over the model parameters
was recently shown to have an important effect on the maximum-a-posteriori
estimate of the Bayesian network structure. In our first contribution, we
theoretically analyze the case of large ESS-values, which complements previous
work: among other results, we find that the presence of an edge in a Bayesian
network is favoured over its absence even if both the Dirichlet prior and the
data imply independence, as long as the conditional empirical distribution is
notably different from uniform. In our second contribution, we focus on
realistic ESS-values, and provide an analytical approximation to the ""optimal""
ESS-value in a predictive sense (its accuracy is also validated
experimentally): this approximation provides an understanding as to which
properties of the data have the main effect determining the ""optimal""
ESS-value.
"
"  Computer codes are widely used to describe physical processes in lieu of
physical observations. In some cases, more than one computer simulator, each
with different degrees of fidelity, can be used to explore the physical system.
In this work, we combine field observations and model runs from deterministic
multi-fidelity computer simulators to build a predictive model for the real
process. The resulting model can be used to perform sensitivity analysis for
the system, solve inverse problems and make predictions. Our approach is
Bayesian and will be illustrated through a simple example, as well as a real
application in predictive science at the Center for Radiative Shock
Hydrodynamics at the University of Michigan.
"
"  Multi-task learning leverages shared information among data sets to improve
the learning performance of individual tasks. The paper applies this framework
for data where each task is a phase-shifted periodic time series. In
particular, we develop a novel Bayesian nonparametric model capturing a mixture
of Gaussian processes where each task is a sum of a group-specific function and
a component capturing individual variation, in addition to each task being
phase shifted. We develop an efficient \textsc{em} algorithm to learn the
parameters of the model. As a special case we obtain the Gaussian mixture model
and \textsc{em} algorithm for phased-shifted periodic time series. Furthermore,
we extend the proposed model by using a Dirichlet Process prior and thereby
leading to an infinite mixture model that is capable of doing automatic model
selection. A Variational Bayesian approach is developed for inference in this
model. Experiments in regression, classification and class discovery
demonstrate the performance of the proposed models using both synthetic data
and real-world time series data from astrophysics. Our methods are particularly
useful when the time series are sparsely and non-synchronously sampled.
"
"  In this paper, we present a novel and general framework called {\it Maximum
Entropy Discrimination Markov Networks} (MaxEnDNet), which integrates the
max-margin structured learning and Bayesian-style estimation and combines and
extends their merits. Major innovations of this model include: 1) It
generalizes the extant Markov network prediction rule based on a point
estimator of weights to a Bayesian-style estimator that integrates over a
learned distribution of the weights. 2) It extends the conventional max-entropy
discrimination learning of classification rule to a new structural max-entropy
discrimination paradigm of learning the distribution of Markov networks. 3) It
subsumes the well-known and powerful Maximum Margin Markov network (M$^3$N) as
a special case, and leads to a model similar to an $L_1$-regularized M$^3$N
that is simultaneously primal and dual sparse, or other types of Markov network
by plugging in different prior distributions of the weights. 4) It offers a
simple inference algorithm that combines existing variational inference and
convex-optimization based M$^3$N solvers as subroutines. 5) It offers a
PAC-Bayesian style generalization bound. This work represents the first
successful attempt to combine Bayesian-style learning (based on generative
models) with structured maximum margin learning (based on a discriminative
model), and outperforms a wide array of competing methods for structured
input/output learning on both synthetic and real data sets.
"
"  State Space Models (SSM) is a MATLAB 7.0 software toolbox for doing time
series analysis by state space methods. The software features fully interactive
construction and combination of models, with support for univariate and
multivariate models, complex time-varying (dynamic) models, non-Gaussian
models, and various standard models such as ARIMA and structural time-series
models. The software includes standard functions for Kalman filtering and
smoothing, simulation smoothing, likelihood evaluation, parameter estimation,
signal extraction and forecasting, with incorporation of exact initialization
for filters and smoothers, and support for missing observations and multiple
time series input with common analysis structure. The software also includes
implementations of TRAMO model selection and Hillmer-Tiao decomposition for
ARIMA models. The software will provide a general toolbox for doing time series
analysis on the MATLAB platform, allowing users to take advantage of its
readily available graph plotting and general matrix computation capabilities.
"
"  It is now well known that decentralised optimisation can be formulated as a
potential game, and game-theoretical learning algorithms can be used to find an
optimum. One of the most common learning techniques in game theory is
fictitious play. However fictitious play is founded on an implicit assumption
that opponents' strategies are stationary. We present a novel variation of
fictitious play that allows the use of a more realistic model of opponent
strategy. It uses a heuristic approach, from the online streaming data
literature, to adaptively update the weights assigned to recently observed
actions. We compare the results of the proposed algorithm with those of
stochastic and geometric fictitious play in a simple strategic form game, a
vehicle target assignment game and a disaster management problem. In all the
tests the rate of convergence of the proposed algorithm was similar or better
than the variations of fictitious play we compared it with. The new algorithm
therefore improves the performance of game-theoretical learning in
decentralised optimisation.
"
"  As an example of the recently-introduced concept of rate of innovation,
signals that are linear combinations of a finite number of Diracs per unit time
can be acquired by linear filtering followed by uniform sampling. However, in
reality, samples are rarely noiseless. In this paper, we introduce a novel
stochastic algorithm to reconstruct a signal with finite rate of innovation
from its noisy samples. Even though variants of this problem has been
approached previously, satisfactory solutions are only available for certain
classes of sampling kernels, for example kernels which satisfy the Strang-Fix
condition. In this paper, we consider the infinite-support Gaussian kernel,
which does not satisfy the Strang-Fix condition. Other classes of kernels can
be employed. Our algorithm is based on Gibbs sampling, a Markov chain Monte
Carlo (MCMC) method. Extensive numerical simulations demonstrate the accuracy
and robustness of our algorithm.
"
"  A Bayesian treatment of latent directed graph structure for non-iid data is
provided where each child datum is sampled with a directed conditional
dependence on a single unknown parent datum. The latent graph structure is
assumed to lie in the family of directed out-tree graphs which leads to
efficient Bayesian inference. The latent likelihood of the data and its
gradients are computable in closed form via Tutte's directed matrix tree
theorem using determinants and inverses of the out-Laplacian. This novel
likelihood subsumes iid likelihood, is exchangeable and yields efficient
unsupervised and semi-supervised learning algorithms. In addition to handling
taxonomy and phylogenetic datasets the out-tree assumption performs
surprisingly well as a semi-parametric density estimator on standard iid
datasets. Experiments with unsupervised and semisupervised learning are shown
on various UCI and taxonomy datasets.
"
"  We develop and analyze stochastic optimization algorithms for problems in
which the expected loss is strongly convex, and the optimum is (approximately)
sparse. Previous approaches are able to exploit only one of these two
structures, yielding an $\order(\pdim/T)$ convergence rate for strongly convex
objectives in $\pdim$ dimensions, and an $\order(\sqrt{(\spindex \log
\pdim)/T})$ convergence rate when the optimum is $\spindex$-sparse. Our
algorithm is based on successively solving a series of $\ell_1$-regularized
optimization problems using Nesterov's dual averaging algorithm. We establish
that the error of our solution after $T$ iterations is at most
$\order((\spindex \log\pdim)/T)$, with natural extensions to approximate
sparsity. Our results apply to locally Lipschitz losses including the logistic,
exponential, hinge and least-squares losses. By recourse to statistical minimax
results, we show that our convergence rates are optimal up to multiplicative
constant factors. The effectiveness of our approach is also confirmed in
numerical simulations, in which we compare to several baselines on a
least-squares regression problem.
"
"  Transfer entropy, an information-theoretic measure of time-directed
information transfer between joint processes, has steadily gained popularity in
the analysis of complex stochastic dynamics in diverse fields, including the
neurosciences, ecology, climatology and econometrics. We show that for a broad
class of predictive models, the log-likelihood ratio test statistic for the
null hypothesis of zero transfer entropy is a consistent estimator for the
transfer entropy itself. For finite Markov chains, furthermore, no explicit
model is required. In the general case, an asymptotic chi-squared distribution
is established for the transfer entropy estimator. The result generalises the
equivalence in the Gaussian case of transfer entropy and Granger causality, a
statistical notion of causal influence based on prediction via vector
autoregression, and establishes a fundamental connection between directed
information transfer and causality in the Wiener-Granger sense.
"
"  This paper poses a few fundamental questions regarding the attributes of the
volume profile of a Limit Order Books stochastic structure by taking into
consideration aspects of intraday and interday statistical features, the impact
of different exchange features and the impact of market participants in
different asset sectors. This paper aims to address the following questions:
  1. Is there statistical evidence that heavy-tailed sub-exponential volume
profiles occur at different levels of the Limit Order Book on the bid and ask
and if so does this happen on intra or interday time scales ?
  2.In futures exchanges, are heavy tail features exchange (CBOT, CME, EUREX,
SGX and COMEX) or asset class (government bonds, equities and precious metals)
dependent and do they happen on ultra-high (<1sec) or mid-range (1sec -10min)
high frequency data?
  3.Does the presence of stochastic heavy-tailed volume profile features evolve
in a manner that would inform or be indicative of market participant behaviors,
such as high frequency algorithmic trading, quote stuffing and price discovery
intra-daily?
  4. Is there statistical evidence for a need to consider dynamic behavior of
the parameters of models for Limit Order Book volume profiles on an intra-daily
time scale ?
  Progress on aspects of each question is obtained via statistically rigorous
results to verify the empirical findings for an unprecedentedly large set of
futures market LOB data. The data comprises several exchanges, several futures
asset classes and all trading days of 2010, using market depth (Type II) order
book data to 5 levels on the bid and ask.
"
"  This is a review article for Encyclopedia of Complexity and System Science,
to be published by Springer http://refworks.springer.com/complexity/. The paper
reviews statistical models for money, wealth, and income distributions
developed in the econophysics literature since late 1990s.
"
"  Our purpose is to model the dependence between two random variables, taking
into account a priori knowledge on these variables. For example, in many
applications (oceanography, finance...), there exists an order relation between
the two variables; when one takes high values, the other cannot take low
values, but the contrary is possible. The dependence for the high values of the
two variables is, therefore, not symmetric.
  However a minimal dependence also exists: low values of one variable are
associated with low values of the other variable. The dependence can also be
extreme for the maxima or the minima of the two variables. In this paper, we
construct step by step asymmetric copulas with asymptotic minimal dependence,
and with or without asymptotic maximal dependence, using mixture variables to
get at first asymmetric dependence and then minimal dependence. We fit these
models to a real dataset of sea states and compare them using Likelihood Ratio
Tests when they are nested, and BIC- criterion (Bayesian Information criterion)
otherwise.
"
"  Support vector machines have attracted much attention in theoretical and in
applied statistics. Main topics of recent interest are consistency, learning
rates and robustness. In this article, it is shown that support vector machines
are qualitatively robust. Since support vector machines can be represented by a
functional on the set of all probability measures, qualitative robustness is
proven by showing that this functional is continuous with respect to the
topology generated by weak convergence of probability measures. Combined with
the existence and uniqueness of support vector machines, our results show that
support vector machines are the solutions of a well-posed mathematical problem
in Hadamard's sense.
"
"  A Hilbert space embedding for probability measures has recently been
proposed, wherein any probability measure is represented as a mean element in a
reproducing kernel Hilbert space (RKHS). Such an embedding has found
applications in homogeneity testing, independence testing, dimensionality
reduction, etc., with the requirement that the reproducing kernel is
characteristic, i.e., the embedding is injective.
  In this paper, we generalize this embedding to finite signed Borel measures,
wherein any finite signed Borel measure is represented as a mean element in an
RKHS. We show that the proposed embedding is injective if and only if the
kernel is universal. This therefore, provides a novel characterization of
universal kernels, which are proposed in the context of achieving the Bayes
risk by kernel-based classification/regression algorithms. By exploiting this
relation between universality and the embedding of finite signed Borel measures
into an RKHS, we establish the relation between universal and characteristic
kernels.
"
"  When a model may be fitted separately to each individual statistical unit,
inspection of the point estimates may help the statistician to understand
between-individual variability and to identify possible relationships. However,
some information will be lost in such an approach because estimation
uncertainty is disregarded. We present a comparative method for exploratory
repeated-measures analysis to complement the point estimates that was motivated
by and is demonstrated by analysis of data from the CADET II breast-cancer
screening study. The approach helped to flag up some unusual reader behavior,
to assess differences in performance, and to identify potential random-effects
models for further analysis.
"
"  The spatial clustering of points from two or more classes (or species) has
important implications in many fields and may cause the spatial patterns of
segregation and association, which are two major types of spatial interaction
between the classes. The null patterns we consider are random labeling (RL) and
complete spatial randomness (CSR) of points from two or more classes, which is
called CSR independence. The segregation and association patterns can be
studied using a nearest neighbor contingency table (NNCT) which is constructed
using the frequencies of nearest neighbor (NN) types in a contingency table.
Among NNCT-tests Pielou's test is liberal the null pattern but Dixon's test has
the desired significance level under the RL pattern. We propose three new
multivariate clustering tests based on NNCTs. We compare the finite sample
performance of these new tests with Pielou's and Dixon's tests and Cuzick &
Edward's k-NN tests in terms of empirical size under the null cases and
empirical power under various segregation and association alternatives and
provide guidelines for using the tests in practice. We demonstrate that the
newly proposed NNCT-tests perform relatively well compared to their competitors
and illustrate the tests using three example data sets. Furthermore, we compare
the NNCT-tests with the second-order methods using these examples.
"
"  This paper describes a new approach, based on linear programming, for
computing nonnegative matrix factorizations (NMFs). The key idea is a
data-driven model for the factorization where the most salient features in the
data are used to express the remaining features. More precisely, given a data
matrix X, the algorithm identifies a matrix C such that X approximately equals
CX and some linear constraints. The constraints are chosen to ensure that the
matrix C selects features; these features can then be used to find a low-rank
NMF of X. A theoretical analysis demonstrates that this approach has guarantees
similar to those of the recent NMF algorithm of Arora et al. (2012). In
contrast with this earlier work, the proposed method extends to more general
noise models and leads to efficient, scalable algorithms. Experiments with
synthetic and real datasets provide evidence that the new approach is also
superior in practice. An optimized C++ implementation can factor a
multigigabyte matrix in a matter of minutes.
"
"  The extension of the classical Bayesian penalized spline method to inference
on vector-valued functions is considered, with an emphasis on characterizing
the suitability of the method for general application.We show that the standard
quadratic penalty is exactly analogous to the energy of a stretched string,
with the penalty parameter corresponding to its tension. This physical analogy
motivates a discussion of resolution independence, which we define as the
convergence of a computational function estimate to arbitrary accuracy with
increasing resolution.The multidimensional context makes direct application of
standard procedures for choosing the penalty parameter difficult, and a new
method is proposed and compared to the established generalized cross-validation
(GCV) and Akaike information criterion (AIC) functions.Our Bayesian method for
choosing this parameter is derived by introducing a scal e independence
criterion to ensure that simultaneously scaling the function samples and their
variances does not significantly change the posterior parameter distribution.
Due to the possibility of an exact polynomial fit, numerical issues prevent the
use of this prior, and a solution is presented based on adding a st ring
zero-point energy. This makes more complicated approaches recently propose d in
the literature unnecessary, and eliminates the requirement for sensitivity
analysis when the function deviates from the above mentioned polynomial. An
important class of problems which can be analyzed by this method are stochastic
numerical integrators, which are considered as an example problem. This work
represents the first extension of penalized spline methods to inference on
multidimensional numerical integrators reported in the literature. Several
numerical calculations illustrate the above points and address practical
application issues.
"
"  We propose computationally efficient encoders and decoders for lossy
compression using a Sparse Regression Code. The codebook is defined by a design
matrix and codewords are structured linear combinations of columns of this
matrix. The proposed encoding algorithm sequentially chooses columns of the
design matrix to successively approximate the source sequence. It is shown to
achieve the optimal distortion-rate function for i.i.d Gaussian sources under
the squared-error distortion criterion. For a given rate, the parameters of the
design matrix can be varied to trade off distortion performance with encoding
complexity. An example of such a trade-off as a function of the block length n
is the following. With computational resource (space or time) per source sample
of O((n/\log n)^2), for a fixed distortion-level above the Gaussian
distortion-rate function, the probability of excess distortion decays
exponentially in n. The Sparse Regression Code is robust in the following
sense: for any ergodic source, the proposed encoder achieves the optimal
distortion-rate function of an i.i.d Gaussian source with the same variance.
Simulations show that the encoder has good empirical performance, especially at
low and moderate rates.
"
"  Collaborative filtering is a useful technique for exploiting the preference
patterns of a group of users to predict the utility of items for the active
user. In general, the performance of collaborative filtering depends on the
number of rated examples given by the active user. The more the number of rated
examples given by the active user, the more accurate the predicted ratings will
be. Active learning provides an effective way to acquire the most informative
rated examples from active users. Previous work on active learning for
collaborative filtering only considers the expected loss function based on the
estimated model, which can be misleading when the estimated model is
inaccurate. This paper takes one step further by taking into account of the
posterior distribution of the estimated model, which results in more robust
active learning algorithm. Empirical studies with datasets of movie ratings
show that when the number of ratings from the active user is restricted to be
small, active learning methods only based on the estimated model don't perform
well while the active learning method using the model distribution achieves
substantially better performance.
"
"  Role mining tackles the problem of finding a role-based access control (RBAC)
configuration, given an access-control matrix assigning users to access
permissions as input. Most role mining approaches work by constructing a large
set of candidate roles and use a greedy selection strategy to iteratively pick
a small subset such that the differences between the resulting RBAC
configuration and the access control matrix are minimized. In this paper, we
advocate an alternative approach that recasts role mining as an inference
problem rather than a lossy compression problem. Instead of using combinatorial
algorithms to minimize the number of roles needed to represent the
access-control matrix, we derive probabilistic models to learn the RBAC
configuration that most likely underlies the given matrix.
  Our models are generative in that they reflect the way that permissions are
assigned to users in a given RBAC configuration. We additionally model how
user-permission assignments that conflict with an RBAC configuration emerge and
we investigate the influence of constraints on role hierarchies and on the
number of assignments. In experiments with access-control matrices from
real-world enterprises, we compare our proposed models with other role mining
methods. Our results show that our probabilistic models infer roles that
generalize well to new system users for a wide variety of data, while other
models' generalization abilities depend on the dataset given.
"
"  This report concerns the problem of dimensionality reduction through
information geometric methods on statistical manifolds. While there has been
considerable work recently presented regarding dimensionality reduction for the
purposes of learning tasks such as classification, clustering, and
visualization, these methods have focused primarily on Riemannian manifolds in
Euclidean space. While sufficient for many applications, there are many
high-dimensional signals which have no straightforward and meaningful Euclidean
representation. In these cases, signals may be more appropriately represented
as a realization of some distribution lying on a statistical manifold, or a
manifold of probability density functions (PDFs). We present a framework for
dimensionality reduction that uses information geometry for both statistical
manifold reconstruction as well as dimensionality reduction in the data domain.
"
"  Objectives: Text categorization has been used in biomedical informatics for
identifying documents containing relevant topics of interest. We developed a
simple method that uses a chi-square-based scoring function to determine the
likelihood of MEDLINE citations containing genetic relevant topic. Methods: Our
procedure requires construction of a genetic and a nongenetic domain document
corpus. We used MeSH descriptors assigned to MEDLINE citations for this
categorization task. We compared frequencies of MeSH descriptors between two
corpora applying chi-square test. A MeSH descriptor was considered to be a
positive indicator if its relative observed frequency in the genetic domain
corpus was greater than its relative observed frequency in the nongenetic
domain corpus. The output of the proposed method is a list of scores for all
the citations, with the highest score given to those citations containing MeSH
descriptors typical for the genetic domain. Results: Validation was done on a
set of 734 manually annotated MEDLINE citations. It achieved predictive
accuracy of 0.87 with 0.69 recall and 0.64 precision. We evaluated the method
by comparing it to three machine learning algorithms (support vector machines,
decision trees, na\""ive Bayes). Although the differences were not statistically
significantly different, results showed that our chi-square scoring performs as
good as compared machine learning algorithms. Conclusions: We suggest that the
chi-square scoring is an effective solution to help categorize MEDLINE
citations. The algorithm is implemented in the BITOLA literature-based
discovery support system as a preprocessor for gene symbol disambiguation
process.
"
"  Since its inception, the modus operandi of multi-task learning (MTL) has been
to minimize the task-wise mean of the empirical risks. We introduce a
generalized loss-compositional paradigm for MTL that includes a spectrum of
formulations as a subfamily. One endpoint of this spectrum is minimax MTL: a
new MTL formulation that minimizes the maximum of the tasks' empirical risks.
Via a certain relaxation of minimax MTL, we obtain a continuum of MTL
formulations spanning minimax MTL and classical MTL. The full paradigm itself
is loss-compositional, operating on the vector of empirical risks. It
incorporates minimax MTL, its relaxations, and many new MTL formulations as
special cases. We show theoretically that minimax MTL tends to avoid worst case
outcomes on newly drawn test tasks in the learning to learn (LTL) test setting.
The results of several MTL formulations on synthetic and real problems in the
MTL and LTL test settings are encouraging.
"
"  Bayesian inferences in high energy physics often use uniform prior
distributions for parameters about which little or no information is available
before data are collected. The resulting posterior distributions are therefore
sensitive to the choice of parametrization for the problem and may even be
improper if this choice is not carefully considered. Here we describe an
extensively tested methodology, known as reference analysis, which allows one
to construct parametrization-invariant priors that embody the notion of minimal
informativeness in a mathematically well-defined sense. We apply this
methodology to general cross section measurements and show that it yields
sensible results. A recent measurement of the single top quark cross section
illustrates the relevant techniques in a realistic situation.
"
"  Recent methods for estimating sparse undirected graphs for real-valued data
in high dimensional problems rely heavily on the assumption of normality. We
show how to use a semiparametric Gaussian copula--or ""nonparanormal""--for high
dimensional inference. Just as additive models extend linear models by
replacing linear functions with a set of one-dimensional smooth functions, the
nonparanormal extends the normal by transforming the variables by smooth
functions. We derive a method for estimating the nonparanormal, study the
method's theoretical properties, and show that it works well in many examples.
"
"  In distributed learning, the goal is to perform a learning task over data
distributed across multiple nodes with minimal (expensive) communication. Prior
work (Daume III et al., 2012) proposes a general model that bounds the
communication required for learning classifiers while allowing for $\eps$
training error on linearly separable data adversarially distributed across
nodes.
  In this work, we develop key improvements and extensions to this basic model.
Our first result is a two-party multiplicative-weight-update based protocol
that uses $O(d^2 \log{1/\eps})$ words of communication to classify distributed
data in arbitrary dimension $d$, $\eps$-optimally. This readily extends to
classification over $k$ nodes with $O(kd^2 \log{1/\eps})$ words of
communication. Our proposed protocol is simple to implement and is considerably
more efficient than baselines compared, as demonstrated by our empirical
results.
  In addition, we illustrate general algorithm design paradigms for doing
efficient learning over distributed data. We show how to solve
fixed-dimensional and high dimensional linear programming efficiently in a
distributed setting where constraints may be distributed across nodes. Since
many learning problems can be viewed as convex optimization problems where
constraints are generated by individual points, this models many typical
distributed learning scenarios. Our techniques make use of a novel connection
from multipass streaming, as well as adapting the multiplicative-weight-update
framework more generally to a distributed setting. As a consequence, our
methods extend to the wide range of problems solvable using these techniques.
"
"  Risk-limiting post-election audits limit the chance of certifying an
electoral outcome if the outcome is not what a full hand count would show.
Building on previous work, we report on pilot risk-limiting audits in four
elections during 2008 in three California counties: one during the February
2008 Primary Election in Marin County and three during the November 2008
General Elections in Marin, Santa Cruz and Yolo Counties. We explain what makes
an audit risk-limiting and how existing and proposed laws fall short. We
discuss the differences among our four pilot audits. We identify challenges to
practical, efficient risk-limiting audits and conclude that current approaches
are too complex to be used routinely on a large scale. One important logistical
bottleneck is the difficulty of exporting data from commercial election
management systems in a format amenable to audit calculations. Finally, we
propose a bare-bones risk-limiting audit that is less efficient than these
pilot audits, but avoids many practical problems.
"
"  Direct marketers use target models in order to minimize the spreading loss of
sales efforts. The application of target models has become more widespread with
the increasing range of sales efforts. Target models are relevant for offline
marketers sending printed mails as well as for online marketers who have to
avoid intensity. However business has retained its evaluation since the late
1960s. Marketing decision-makers still prefer managerial performance measures
of the economic benefit of a target model. Such benefit measures have merits
but are unfavorable in other respects: They constrain leadership by stretched
targets since they do not tell us how good a model could be. And they require a
predisposed decision regarding cut-offs. Since this is based on earlier
optimizations it virtually means sticking to traditions. Hence it is
recommended also to use cut-off invariant and potential oriented performance
measures for the model evaluation. This has three advantages: sustaining
stretched targets, identifying improvement potential and sup-porting an
automated evaluation of many different models. This article introduces a
concrete po-tential measure and shows how to calculate it. It is especially
recommended for direct marketing businesses churning out many specific target
models at short intervals.
"
"  This paper introduces Bayesian supervised and unsupervised segmentation
algorithms aimed at oceanic segmentation of SAR images. The data term,
\emph{i.e}., the density of the observed backscattered signal given the region,
is modeled by a finite mixture of Gamma densities with a given predefined
number of components. To estimate the parameters of the class conditional
densities, a new expectation maximization algorithm was developed. The prior is
a multi-level logistic Markov random field enforcing local continuity in a
statistical sense. The smoothness parameter controlling the degree of
homogeneity imposed on the scene is automatically estimated, by computing the
evidence with loopy belief propagation; the classical coding and least squares
fit methods are also considered. The maximum a posteriori segmentation is
computed efficiently by means of recent graph-cut techniques, namely the
$\alpha$-Expansion algorithm that extends the methodology to an optional number
of classes. The effectiveness of the proposed approaches is illustrated with
simulated images and real ERS and Envisat scenes containing oil spills.
"
"  Collaborative recommendation is an information-filtering technique that
attempts to present information items (movies, music, books, news, images, Web
pages, etc.) that are likely of interest to the Internet user. Traditionally,
collaborative systems deal with situations with two types of variables, users
and items. In its most common form, the problem is framed as trying to estimate
ratings for items that have not yet been consumed by a user. Despite
wide-ranging literature, little is known about the statistical properties of
recommendation systems. In fact, no clear probabilistic model even exists
allowing us to precisely describe the mathematical forces driving collaborative
filtering. To provide an initial contribution to this, we propose to set out a
general sequential stochastic model for collaborative recommendation and
analyze its asymptotic performance as the number of users grows. We offer an
in-depth analysis of the so-called cosine-type nearest neighbor collaborative
method, which is one of the most widely used algorithms in collaborative
filtering. We establish consistency of the procedure under mild assumptions on
the model. Rates of convergence and examples are also provided.
"
"  In Bayesian statistics, many problems can be expressed as the evaluation of
the expectation of a quantity of interest with respect to the posterior
distribution. Standard Monte Carlo method is often not applicable because the
encountered posterior distributions cannot be sampled directly. In this case,
the most popular strategies are the importance sampling method, Markov chain
Monte Carlo, and annealing. In this paper, we introduce a new scheme for
Bayesian inference, called Asymptotically Independent Markov Sampling (AIMS),
which is based on the above methods. We derive important ergodic properties of
AIMS. In particular, it is shown that, under certain conditions, the AIMS
algorithm produces a uniformly ergodic Markov chain. The choice of the free
parameters of the algorithm is discussed and recommendations are provided for
this choice, both theoretically and heuristically based. The efficiency of AIMS
is demonstrated with three numerical examples, which include both multi-modal
and higher-dimensional target posterior distributions.
"
"  Supervised linear feature extraction can be achieved by fitting a reduced
rank multivariate model. This paper studies rank penalized and rank constrained
vector generalized linear models. From the perspective of thresholding rules,
we build a framework for fitting singular value penalized models and use it for
feature extraction. Through solving the rank constraint form of the problem, we
propose progressive feature space reduction for fast computation in high
dimensions with little performance loss. A novel projective cross-validation is
proposed for parameter tuning in such nonconvex setups. Real data applications
are given to show the power of the methodology in supervised dimension
reduction and feature extraction.
"
"  This paper revisits the problem of analyzing multiple ratings given by
different judges. Different from previous work that focuses on distilling the
true labels from noisy crowdsourcing ratings, we emphasize gaining diagnostic
insights into our in-house well-trained judges. We generalize the well-known
DawidSkene model (Dawid & Skene, 1979) to a spectrum of probabilistic models
under the same ""TrueLabel + Confusion"" paradigm, and show that our proposed
hierarchical Bayesian model, called HybridConfusion, consistently outperforms
DawidSkene on both synthetic and real-world data sets.
"
"  Rejoinder to ""Latent variable graphical model selection via convex
optimization"" by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky
[arXiv:1008.1290].
"
"  As an increasing number of genome-wide association studies reveal the
limitations of attempting to explain phenotypic heritability by single genetic
loci, there is growing interest for associating complex phenotypes with sets of
genetic loci. While several methods for multi-locus mapping have been proposed,
it is often unclear how to relate the detected loci to the growing knowledge
about gene pathways and networks. The few methods that take biological pathways
or networks into account are either restricted to investigating a limited
number of predetermined sets of loci, or do not scale to genome-wide settings.
  We present SConES, a new efficient method to discover sets of genetic loci
that are maximally associated with a phenotype, while being connected in an
underlying network. Our approach is based on a minimum cut reformulation of the
problem of selecting features under sparsity and connectivity constraints that
can be solved exactly and rapidly.
  SConES outperforms state-of-the-art competitors in terms of runtime, scales
to hundreds of thousands of genetic loci, and exhibits higher power in
detecting causal SNPs in simulation studies than existing methods. On flowering
time phenotypes and genotypes from Arabidopsis thaliana, SConES detects loci
that enable accurate phenotype prediction and that are supported by the
literature.
  Matlab code for SConES is available at
http://webdav.tuebingen.mpg.de/u/karsten/Forschung/scones/
"
"  Building models, or maps, of robot environments is a highly active research
area; however, most existing techniques construct unstructured maps and assume
static environments. In this paper, we present an algorithm for learning object
models of non-stationary objects found in office-type environments. Our
algorithm exploits the fact that many objects found in office environments look
alike (e.g., chairs, recycling bins). It does so through a two-level
hierarchical representation, which links individual objects with generic shape
templates of object classes. We derive an approximate EM algorithm for learning
shape parameters at both levels of the hierarchy, using local occupancy grid
maps for representing shape. Additionally, we develop a Bayesian model
selection algorithm that enables the robot to estimate the total number of
objects and object templates in the environment. Experimental results using a
real robot equipped with a laser range finder indicate that our approach
performs well at learning object-based maps of simple office environments. The
approach outperforms a previously developed non-hierarchical algorithm that
models objects but lacks class templates.
"
"  I find a topological arrangement of assets traded in a phonographic market
which has associated a meaningful economic taxonomy. I continue using the
Minimal Spanning Tree and the Life-time Of Correlations between assets, but now
outside the stock markets. This is the first attempt to use these methods on
phonographic market where we have artists instead of stocks. The value of an
artist is defined by record sales. The graph is obtained starting from the
matrix of correlations coefficient computed between the world's most popular 30
artists by considering the synchronous time evolution of the difference of the
logarithm of weekly record sales. This method provides the hierarchical
structure of phonographic market and information on which music genre is
meaningful according to customers.
"
"  Ambient concentrations of many pollutants are associated with emissions due
to human activity, such as road transport and other combustion sources. In this
paper we consider air pollution as a multi--level phenomenon within a Bayesian
hierarchical model. We examine different scales of variation in pollution
concentrations ranging from large scale transboundary effects to more localised
effects which are directly related to human activity. Specifically, in the
first stage of the model, we isolate underlying patterns in pollution
concentrations due to global factors such as underlying climate and topography,
which are modelled together with spatial structure. At this stage measurements
from monitoring sites located within rural areas are used which, as far as
possible, are chosen to reflect background concentrations. Having isolated
these global effects, in the second stage we assess the effects of human
activity on pollution in urban areas. The proposed model was applied to
concentrations of nitrogen dioxide measured throughout the EU for which
significant increases are found to be associated with human activity in urban
areas. The approach proposed here provides valuable information that could be
used in performing health impact assessments and to inform policy.
"
"  We present a new Bayesian approach to model-robust linear regression that
leads to uncertainty estimates with the same robustness properties as the
Huber--White sandwich estimator. The sandwich estimator is known to provide
asymptotically correct frequentist inference, even when standard modeling
assumptions such as linearity and homoscedasticity in the data-generating
mechanism are violated. Our derivation provides a compelling Bayesian
justification for using this simple and popular tool, and it also clarifies
what is being estimated when the data-generating mechanism is not linear. We
demonstrate the applicability of our approach using a simulation study and
health care cost data from an evaluation of the Washington State Basic Health
Plan.
"
"  This paper considers cooperative spectrum sensing algorithms for Cognitive
Radios which focus on reducing the number of samples to make a reliable
detection. We develop an energy efficient detector with low detection delay
using decentralized sequential hypothesis testing. Our algorithm at the
Cognitive Radios employs an asynchronous transmission scheme which takes into
account the noise at the fusion center. We start with a distributed algorithm,
DualSPRT, in which Cognitive Radios sequentially collect the observations, make
local decisions using SPRT (Sequential Probability Ratio Test) and send them to
the fusion center. The fusion center sequentially processes these received
local decisions corrupted by noise, using an SPRT-like procedure to arrive at a
final decision. We theoretically analyse its probability of error and average
detection delay. We also asymptotically study its performance. Even though
DualSPRT performs asymptotically well, a modification at the fusion node
provides more control over the design of the algorithm parameters which then
performs better at the usual operating probabilities of error in Cognitive
Radio systems. We also analyse the modified algorithm theoretically. Later we
modify these algorithms to handle uncertainties in SNR and fading.
"
"  Spatial designs for monitoring stream networks, especially ephemeral systems,
are typically non-standard, `sparse' and can be very complex, reflecting the
complexity of the ecosystem being monitored, the scale of the population, and
the competing multiple monitoring objectives. The main purpose of this paper is
to present a review of approaches to spatial design to enable informed
decisions to be made about developing practical and optimal spatial designs for
future monitoring of streams.
"
"  Numerous statistics have been proposed for the measure of offensive ability
in major league baseball. While some of these measures may offer moderate
predictive power in certain situations, it is unclear which simple offensive
metrics are the most reliable or consistent. We address this issue with a
Bayesian hierarchical model for variable selection to capture which offensive
metrics are most predictive within players across time. Our sophisticated
methodology allows for full estimation of the posterior distributions for our
parameters and automatically adjusts for multiple testing, providing a distinct
advantage over alternative approaches. We implement our model on a set of 50
different offensive metrics and discuss our results in the context of
comparison to other variable selection techniques. We find that 33/50 metrics
demonstrate signal. However, these metrics are highly correlated with one
another and related to traditional notions of performance (e.g., plate
discipline, power, and ability to make contact).
"
"  When training data is sparse, more domain knowledge must be incorporated into
the learning algorithm in order to reduce the effective size of the hypothesis
space. This paper builds on previous work in which knowledge about qualitative
monotonicities was formally represented and incorporated into learning
algorithms (e.g., Clark & Matwin's work with the CN2 rule learning algorithm).
We show how to interpret knowledge of qualitative influences, and in particular
of monotonicities, as constraints on probability distributions, and to
incorporate this knowledge into Bayesian network learning algorithms. We show
that this yields improved accuracy, particularly with very small training sets
(e.g. less than 10 examples).
"
"  Markov jump processes and continuous time Bayesian networks are important
classes of continuous time dynamical systems. In this paper, we tackle the
problem of inferring unobserved paths in these models by introducing a fast
auxiliary variable Gibbs sampler. Our approach is based on the idea of
uniformization, and sets up a Markov chain over paths by sampling a finite set
of virtual jump times and then running a standard hidden Markov model forward
filtering-backward sampling algorithm over states at the set of extant and
virtual jump times. We demonstrate significant computational benefits over a
state-of-the-art Gibbs sampler on a number of continuous time Bayesian
networks.
"
"  Mapping human genetic variation is fundamentally interesting in fields such
as anthropology and forensic inference. At the same time patterns of genetic
diversity confound efforts to determine the genetic basis of complex disease.
Due to technological advances it is now possible to measure hundreds of
thousands of genetic variants per individual across the genome. Principal
component analysis (PCA) is routinely used to summarize the genetic similarity
between subjects. The eigenvectors are interpreted as dimensions of ancestry.
We build on this idea using a spectral graph approach. In the process we draw
on connections between multidimensional scaling and spectral kernel methods.
Our approach, based on a spectral embedding derived from the normalized
Laplacian of a graph, can produce more meaningful delineation of ancestry than
by using PCA. The method is stable to outliers and can more easily incorporate
different similarity measures of genetic data than PCA. We illustrate a new
algorithm for genetic clustering and association analysis on a large,
genetically heterogeneous sample.
"
"  Portfolio allocation with gross-exposure constraint is an effective method to
increase the efficiency and stability of selected portfolios among a vast pool
of assets, as demonstrated in Fan et al (2008). The required high-dimensional
volatility matrix can be estimated by using high frequency financial data. This
enables us to better adapt to the local volatilities and local correlations
among vast number of assets and to increase significantly the sample size for
estimating the volatility matrix. This paper studies the volatility matrix
estimation using high-dimensional high-frequency data from the perspective of
portfolio selection. Specifically, we propose the use of ""pairwise-refresh
time"" and ""all-refresh time"" methods proposed by Barndorff-Nielsen et al (2008)
for estimation of vast covariance matrix and compare their merits in the
portfolio selection. We also establish the concentration inequalities of the
estimates, which guarantee desirable properties of the estimated volatility
matrix in vast asset allocation with gross exposure constraints. Extensive
numerical studies are made via carefully designed simulations. Comparing with
the methods based on low frequency daily data, our methods can capture the most
recent trend of the time varying volatility and correlation, hence provide more
accurate guidance for the portfolio allocation in the next time period. The
advantage of using high-frequency data is significant in our simulation and
empirical studies, which consist of 50 simulated assets and 30 constituent
stocks of Dow Jones Industrial Average index.
"
"  There is growing body of learning problems for which it is natural to
organize the parameters into matrix, so as to appropriately regularize the
parameters under some matrix norm (in order to impose some more sophisticated
prior knowledge). This work describes and analyzes a systematic method for
constructing such matrix-based, regularization methods. In particular, we focus
on how the underlying statistical properties of a given problem can help us
decide which regularization function is appropriate.
  Our methodology is based on the known duality fact: that a function is
strongly convex with respect to some norm if and only if its conjugate function
is strongly smooth with respect to the dual norm. This result has already been
found to be a key component in deriving and analyzing several learning
algorithms. We demonstrate the potential of this framework by deriving novel
generalization and regret bounds for multi-task learning, multi-class learning,
and kernel learning.
"
"  Rejoinder of ``Statistical analysis of an archeological find''
[arXiv:0804.0079]
"
"  This paper introduces a popular dimension reduction method, sliced inverse
regression (SIR), into multivariate statistical process monitoring. Provides an
extension of SIR for the single-index model by adopting the idea from partial
least squares (PLS). Our partial sliced inverse regression (PSIR) method has
the merit of incorporating information from both predictors (x) and responses
(y), and it has capability of handling large, nonlinear, or ""n<p"" dataset. Two
statistics with their corresponding distributions and control limits are given
based on the X-space decomposition of PSIR for the purpose of fault detection
in process monitoring. Simulations showed PSIR outperformed over PLS and SIR
for both linear and nonlinear model.
"
"  Based on expert opinions, informative prior elicitation for the common
Weibull lifetime distribution usually presents some difficulties since it
requires to elicit a two-dimensional joint prior. We consider here a
reliability framework where the available expert information states directly in
terms of prior predictive values (lifetimes) and not parameter values, which
are less intuitive. The novelty of our procedure is to weigh the expert
information by the size m of a virtual sample yielding a similar information,
the prior being seen as a reference posterior. Thus, the prior calibration by
the Bayesian analyst, who has to moderate the subjective information with
respect to the data information, is made simple. A main result is the full
tractability of the prior under mild conditions, despite the conjugation issues
encountered with the Weibull distribution. Besides, m is a practical focus
point for discussion between analysts and experts, and a helpful parameter for
leading sensitivity studies and reducing the potential imbalance in posterior
selection between Bayesian Weibull models, which can be due to favoring
arbitrarily a prior. The calibration of m is discussed and a real example is
treated along the paper.
"
"  Discovering the latent structure from many observed variables is an important
yet challenging learning task. Existing approaches for discovering latent
structures often require the unknown number of hidden states as an input. In
this paper, we propose a quartet based approach which is \emph{agnostic} to
this number. The key contribution is a novel rank characterization of the
tensor associated with the marginal distribution of a quartet. This
characterization allows us to design a \emph{nuclear norm} based test for
resolving quartet relations. We then use the quartet test as a subroutine in a
divide-and-conquer algorithm for recovering the latent tree structure. Under
mild conditions, the algorithm is consistent and its error probability decays
exponentially with increasing sample size. We demonstrate that the proposed
approach compares favorably to alternatives. In a real world stock dataset, it
also discovers meaningful groupings of variables, and produces a model that
fits the data better.
"
"  The instability in the selection of models is a major concern with data sets
containing a large number of covariates. This paper deals with variable
selection methodology in the case of high-dimensional problems where the
response variable can be right censored. We focuse on new stable variable
selection methods based on bootstrap for two methodologies: the Cox
proportional hazard model and survival trees. As far as the Cox model is
concerned, we investigate the bootstrapping applied to two variable selection
techniques: the stepwise algorithm based on the AIC criterion and the
L1-penalization of Lasso. Regarding survival trees, we review two
methodologies: the bootstrap node-level stabilization and random survival
forests. We apply these different approaches to two real data sets. We compare
the methods on the prediction error rate based on the Harrell concordance index
and the relevance of the interpretation of the corresponding selected models.
The aim is to find a compromise between a good prediction performance and ease
to interpretation for clinicians. Results suggest that in the case of a small
number of individuals, a bootstrapping adapted to L1-penalization in the Cox
model or a bootstrap node-level stabilization in survival trees give a good
alternative to the random survival forest methodology, known to give the
smallest prediction error rate but difficult to interprete by
non-statisticians. In a clinical perspective, the complementarity between the
methods based on the Cox model and those based on survival trees would permit
to built reliable models easy to interprete by the clinician.
"
"  The global sensitivity analysis method, used to quantify the influence of
uncertain input variables on the response variability of a numerical model, is
applicable to deterministic computer code (for which the same set of input
variables gives always the same output value). This paper proposes a global
sensitivity analysis methodology for stochastic computer code (having a
variability induced by some uncontrollable variables). The framework of the
joint modeling of the mean and dispersion of heteroscedastic data is used. To
deal with the complexity of computer experiment outputs, non parametric joint
models (based on Generalized Additive Models and Gaussian processes) are
discussed. The relevance of these new models is analyzed in terms of the
obtained variance-based sensitivity indices with two case studies. Results show
that the joint modeling approach leads accurate sensitivity index estimations
even when clear heteroscedasticity is present.
"
"  A common way of studying the relationship between neural activity and
behavior is through the analysis of neuronal spike trains that are recorded
using one or more electrodes implanted in the brain. Each spike train typically
contains spikes generated by multiple neurons. A natural question that arises
is ""what is the number of neurons $\nu$ generating the spike train?""; This
article proposes a method-of-moments technique for estimating $\nu$. This
technique estimates the noise nonparametrically using data from the silent
region of the spike train and it applies to isolated spikes with a possibly
small, but nonnegligible, presence of overlapping spikes. Conditions are
established in which the resulting estimator for $\nu$ is shown to be strongly
consistent. To gauge its finite sample performance, the technique is applied to
simulated spike trains as well as to actual neuronal spike train data.
"
"  In radar systems, tracking targets in low signal-to-noise ratio (SNR)
environments is a very important task. There are some algorithms designed for
multitarget tracking. Their performances, however, are not satisfactory in low
SNR environments. Track-before-detect (TBD) algorithms have been developed as a
class of improved methods for tracking in low SNR environments. However,
multitarget TBD is still an open issue. In this paper, multitarget TBD
measurements are modeled, and a highly efficient filter in the framework of
finite set statistics (FISST) is designed. Then, the probability hypothesis
density (PHD) filter is applied to multitarget TBD. Indeed, to solve the
problem of the target and noise not being separated correctly when the SNR is
low, a shrinkage-PHD filter is derived, and the optimal parameter for shrinkage
operation is obtained by certain optimization procedures. Through simulation
results, it is shown that our method can track targets with high accuracy by
taking advantage of shrinkage operations.
"
"  Given a multivariate data set, sparse principal component analysis (SPCA)
aims to extract several linear combinations of the variables that together
explain the variance in the data as much as possible, while controlling the
number of nonzero loadings in these combinations. In this paper we consider 8
different optimization formulations for computing a single sparse loading
vector; these are obtained by combining the following factors: we employ two
norms for measuring variance (L2, L1) and two sparsity-inducing norms (L0, L1),
which are used in two different ways (constraint, penalty). Three of our
formulations, notably the one with L0 constraint and L1 variance, have not been
considered in the literature. We give a unifying reformulation which we propose
to solve via a natural alternating maximization (AM) method. We show the the AM
method is nontrivially equivalent to GPower (Journ\'{e}e et al; JMLR
11:517--553, 2010) for all our formulations. Besides this, we provide 24
efficient parallel SPCA implementations: 3 codes (multi-core, GPU and cluster)
for each of the 8 problems. Parallelism in the methods is aimed at i) speeding
up computations (our GPU code can be 100 times faster than an efficient serial
code written in C++), ii) obtaining solutions explaining more variance and iii)
dealing with big data problems (our cluster code is able to solve a 357 GB
problem in about a minute).
"
"  High-dimensional data models, often with low sample size, abound in many
interdisciplinary studies, genomics and large biological systems being most
noteworthy. The conventional assumption of multinormality or linearity of
regression may not be plausible for such models which are likely to be
statistically complex due to a large number of parameters as well as various
underlying restraints. As such, parametric approaches may not be very
effective. Anything beyond parametrics, albeit, having increased scope and
robustness perspectives, may generally be baffled by the low sample size and
hence unable to give reasonable margins of errors. Kendall's tau statistic is
exploited in this context with emphasis on dimensional rather than sample size
asymptotics. The Chen--Stein theorem has been thoroughly appraised in this
study. Applications of these findings in some microarray data models are
illustrated.
"
"  Recently, considerable interest has focused on variable selection methods in
regression situations where the number of predictors, $p$, is large relative to
the number of observations, $n$. Two commonly applied variable selection
approaches are the Lasso, which computes highly shrunk regression coefficients,
and Forward Selection, which uses no shrinkage. We propose a new approach,
""Forward-Lasso Adaptive SHrinkage"" (FLASH), which includes the Lasso and
Forward Selection as special cases, and can be used in both the linear
regression and the Generalized Linear Model domains. As with the Lasso and
Forward Selection, FLASH iteratively adds one variable to the model in a
hierarchical fashion but, unlike these methods, at each step adjusts the level
of shrinkage so as to optimize the selection of the next variable. We first
present FLASH in the linear regression setting and show that it can be fitted
using a variant of the computationally efficient LARS algorithm. Then, we
extend FLASH to the GLM domain and demonstrate, through numerous simulations
and real world data sets, as well as some theoretical analysis, that FLASH
generally outperforms many competing approaches.
"
"  We introduce Dimple, a fully open-source API for probabilistic modeling.
Dimple allows the user to specify probabilistic models in the form of graphical
models, Bayesian networks, or factor graphs, and performs inference (by
automatically deriving an inference engine from a variety of algorithms) on the
model. Dimple also serves as a compiler for GP5, a hardware accelerator for
inference.
"
"  Reference prices have long been studied in applied economics and business
research. One of the classic formulations of the reference price is in terms of
an iterative function of past prices. There are a number of limitations of such
a formulation, however. Such limitations include burdensome computational time
to estimate parameters, an inability to truly account for customer
heterogeneity, and an estimation procedure that implies a misspecified model.
Managerial recommendations based on inferences from such a model can be quite
misleading. We mathematically reformulate the reference price by developing a
closed-form expansion that addresses the aforementioned issues, enabling one to
elicit truly meaningful managerial advice from the model. We estimate our model
on a real world data set to illustrate the efficacy of our approach. Our work
is not only useful from a modeling perspective, but also has important
behavioral and managerial implications, which modelers and non-modelers alike
would find useful.
"
"  This paper addresses the problem of predicting a wind farm's power generation
when no or few statistical data is available. The study is based on a
time-series wind speed model and on a simple dynamic model of a DFIG wind
turbine including cut-off and cut-in behaviours. The wind turbine is modeled as
a stochastic hybrid system with three operation modes. Numerical results,
obtained using Monte-Carlo simulations, provide the annual distribution of a
wind farm's active power generation. For different numbers of wind turbines, we
compare the numerical results obtained using the dynamic model with those
obtained considering the wind turbine's steady-state power curve. Simulations
show that the wind turbine's dynamics do not need to be considered for
analyzing the annual distribution of a wind farm generation.
"
"  This article presents certain recent methodologies and some new results for
the statistical analysis of probability distributions on manifolds. An
important example considered in some detail here is the 2-D shape space of
k-ads, comprising all configurations of $k$ planar landmarks ($k>2$)-modulo
translation, scaling and rotation.
"
"  Motivated by vision tasks such as robust face and object recognition, we
consider the following general problem: given a collection of low-dimensional
linear subspaces in a high-dimensional ambient (image) space, and a query point
(image), efficiently determine the nearest subspace to the query in $\ell^1$
distance. In contrast to the naive exhaustive search which entails large-scale
linear programs, we show that the computational burden can be cut down
significantly by a simple two-stage algorithm: (1) projecting the query and
data-base subspaces into lower-dimensional space by random Cauchy matrix, and
solving small-scale distance evaluations (linear programs) in the projection
space to locate candidate nearest; (2) with few candidates upon independent
repetition of (1), getting back to the high-dimensional space and performing
exhaustive search. To preserve the identity of the nearest subspace with
nontrivial probability, the projection dimension typically is low-order
polynomial of the subspace dimension multiplied by logarithm of number of the
subspaces (Theorem 2.1). The reduced dimensionality and hence complexity
renders the proposed algorithm particularly relevant to vision application such
as robust face and object instance recognition that we investigate empirically.
"
"  We study the problem of estimating high-dimensional regression models
regularized by a structured sparsity-inducing penalty that encodes prior
structural information on either the input or output variables. We consider two
widely adopted types of penalties of this kind as motivating examples: (1) the
general overlapping-group-lasso penalty, generalized from the group-lasso
penalty; and (2) the graph-guided-fused-lasso penalty, generalized from the
fused-lasso penalty. For both types of penalties, due to their nonseparability
and nonsmoothness, developing an efficient optimization method remains a
challenging problem. In this paper we propose a general optimization approach,
the smoothing proximal gradient (SPG) method, which can solve structured sparse
regression problems with any smooth convex loss under a wide spectrum of
structured sparsity-inducing penalties. Our approach combines a smoothing
technique with an effective proximal gradient method. It achieves a convergence
rate significantly faster than the standard first-order methods, subgradient
methods, and is much more scalable than the most widely used interior-point
methods. The efficiency and scalability of our method are demonstrated on both
simulation experiments and real genetic data sets.
"
"  Let $(X,Y)$ be a random variable consisting of an observed feature vector
$X\in \mathcal{X}$ and an unobserved class label $Y\in \{1,2,...,L\}$ with
unknown joint distribution. In addition, let $\mathcal{D}$ be a training data
set consisting of $n$ completely observed independent copies of $(X,Y)$. Usual
classification procedures provide point predictors (classifiers)
$\widehat{Y}(X,\mathcal{D})$ of $Y$ or estimate the conditional distribution of
$Y$ given $X$. In order to quantify the certainty of classifying $X$ we propose
to construct for each $\theta =1,2,...,L$ a p-value
$\pi_{\theta}(X,\mathcal{D})$ for the null hypothesis that $Y=\theta$, treating
$Y$ temporarily as a fixed parameter. In other words, the point predictor
$\widehat{Y}(X,\mathcal{D})$ is replaced with a prediction region for $Y$ with
a certain confidence. We argue that (i) this approach is advantageous over
traditional approaches and (ii) any reasonable classifier can be modified to
yield nonparametric p-values. We discuss issues such as optimality, single use
and multiple use validity, as well as computational and graphical aspects.
"
"  This paper studies the deviations of the regret in a stochastic multi-armed
bandit problem. When the total number of plays n is known beforehand by the
agent, Audibert et al. (2009) exhibit a policy such that with probability at
least 1-1/n, the regret of the policy is of order log(n). They have also shown
that such a property is not shared by the popular ucb1 policy of Auer et al.
(2002). This work first answers an open question: it extends this negative
result to any anytime policy. The second contribution of this paper is to
design anytime robust policies for specific multi-armed bandit problems in
which some restrictions are put on the set of possible distributions of the
different arms.
"
"  Statistical modeling of presence-only data has attracted much recent
attention in the ecological literature, leading to a proliferation of methods,
including the inhomogeneous Poisson process (IPP) model, maximum entropy
(Maxent) modeling of species distributions and logistic regression models.
Several recent articles have shown the close relationships between these
methods. We explain why the IPP intensity function is a more natural object of
inference in presence-only studies than occurrence probability (which is only
defined with reference to quadrat size), and why presence-only data only allows
estimation of relative, and not absolute intensity of species occurrence. All
three of the above techniques amount to parametric density estimation under the
same exponential family model (in the case of the IPP, the fitted density is
multiplied by the number of presence records to obtain a fitted intensity). We
show that IPP and Maxent give the exact same estimate for this density, but
logistic regression in general yields a different estimate in finite samples.
When the model is misspecified - as it practically always is - logistic
regression and the IPP may have substantially different asymptotic limits with
large data sets. We propose ``infinitely weighted logistic regression,'' which
is exactly equivalent to the IPP in finite samples. Consequently, many
already-implemented methods extending logistic regression can also extend the
Maxent and IPP models in directly analogous ways using this technique.
"
"  The analysis of data arising from environmental health studies which collect
a large number of measures of exposure can benefit from using latent variable
models to summarize exposure information. However, difficulties with estimation
of model parameters may arise since existing fitting procedures for linear
latent variable models require correctly specified residual variance structures
for unbiased estimation of regression parameters quantifying the association
between (latent) exposure and health outcomes. We propose an estimating
equations approach for latent exposure models with longitudinal health outcomes
which is robust to misspecification of the outcome variance. We show that
compared to maximum likelihood, the loss of efficiency of the proposed method
is relatively small when the model is correctly specified. The proposed
equations formalize the ad-hoc regression on factor scores procedure, and
generalize regression calibration. We propose two weighting schemes for the
equations, and compare their efficiency. We apply this method to a study of the
effects of in-utero lead exposure on child development.
"
"  In biomedical research the use of discrete scales which describe
characteristics of individuals are widely applied for the evaluation of
clinical conditions. However, the number of classes (partitions) used in a
discrete scale has never been mathematically evaluated against the accuracy of
a scale to predict the true cases. This work, using as accuracy markers the
sensitivity and specificity, revealed that the number of classes of a discrete
scale affects its estimating ability of correctly classifying the true
diseased. In particular, it was proved that the sensitivity of scales is a
non-decreasing function of the number of their classes. This result has
particular interest in clinical research providing a methodology for developing
more accurate tools for disease diagnosis.
"
"  We consider multivariate two-sample tests of means, where the location shift
between the two populations is expected to be related to a known graph
structure. An important application of such tests is the detection of
differentially expressed genes between two patient populations, as shifts in
expression levels are expected to be coherent with the structure of graphs
reflecting gene properties such as biological process, molecular function,
regulation, or metabolism. For a fixed graph of interest, we demonstrate that
accounting for graph structure can yield more powerful tests under the
assumption of smooth distribution shift on the graph. We also investigate the
identification of non-homogeneous subgraphs of a given large graph, which poses
both computational and multiple testing problems. The relevance and benefits of
the proposed approach are illustrated on synthetic data and on breast cancer
gene expression data analyzed in context of KEGG pathways.
"
"  One of the most important tasks in image processing problem and machine
vision is object recognition, and the success of many proposed methods relies
on a suitable choice of algorithm for the segmentation of an image. This paper
focuses on how to apply texture operators based on the concept of fractal
dimension and cooccurence matrix, to the problem of object recognition and a
new method based on fractal dimension is introduced. Several images, in which
the result of the segmentation can be shown, are used to illustrate the use of
each method and a comparative study of each operator is made.
"
"  For many large undirected models that arise in real-world applications, exact
maximumlikelihood training is intractable, because it requires computing
marginal distributions of the model. Conditional training is even more
difficult, because the partition function depends not only on the parameters,
but also on the observed input, requiring repeated inference over each training
example. An appealing idea for such models is to independently train a local
undirected classifier over each clique, afterwards combining the learned
weights into a single global model. In this paper, we show that this piecewise
method can be justified as minimizing a new family of upper bounds on the log
partition function. On three natural-language data sets, piecewise training is
more accurate than pseudolikelihood, and often performs comparably to global
training using belief propagation.
"
"  A trend in compressed sensing (CS) is to exploit structure for improved
reconstruction performance. In the basic CS model, exploiting the clustering
structure among nonzero elements in the solution vector has drawn much
attention, and many algorithms have been proposed. However, few algorithms
explicitly consider correlation within a cluster. Meanwhile, in the multiple
measurement vector (MMV) model correlation among multiple solution vectors is
largely ignored. Although several recently developed algorithms consider the
exploitation of the correlation, these algorithms need to know a priori the
correlation structure, thus limiting their effectiveness in practical problems.
  Recently, we developed a sparse Bayesian learning (SBL) algorithm, namely
T-SBL, and its variants, which adaptively learn the correlation structure and
exploit such correlation information to significantly improve reconstruction
performance. Here we establish their connections to other popular algorithms,
such as the group Lasso, iterative reweighted $\ell_1$ and $\ell_2$ algorithms,
and algorithms for time-varying sparsity. We also provide strategies to improve
these existing algorithms.
"
"  Images obtained with coherent illumination, as is the case of sonar,
ultrasound-B, laser and Synthetic Aperture Radar -- SAR, are affected by
speckle noise which reduces the ability to extract information from the data.
Specialized techniques are required to deal with such imagery, which has been
modeled by the G0 distribution and under which regions with different degrees
of roughness and mean brightness can be characterized by two parameters; a
third parameter, the number of looks, is related to the overall signal-to-noise
ratio. Assessing distances between samples is an important step in image
analysis; they provide grounds of the separability and, therefore, of the
performance of classification procedures. This work derives and compares eight
stochastic distances and assesses the performance of hypothesis tests that
employ them and maximum likelihood estimation. We conclude that tests based on
the triangular distance have the closest empirical size to the theoretical one,
while those based on the arithmetic-geometric distances have the best power.
Since the power of tests based on the triangular distance is close to optimum,
we conclude that the safest choice is using this distance for hypothesis
testing, even when compared with classical distances as Kullback-Leibler and
Bhattacharyya.
"
"  This paper descibes a new method of calculating the mean duration and mean
age of onset of a chronic disease from incidence and mortality rates. It is
based on an ordinary differential equation resulting from a simple compartment
model. Applicability of the method is demonstrated in data about dementia in
Germany.
"
"  Modern statistical applications involving large data sets have focused
attention on statistical methodologies which are both efficient computationally
and able to deal with the screening of large numbers of different candidate
models. Here we consider computationally efficient variational Bayes approaches
to inference in high-dimensional heteroscedastic linear regression, where both
the mean and variance are described in terms of linear functions of the
predictors and where the number of predictors can be larger than the sample
size. We derive a closed form variational lower bound on the log marginal
likelihood useful for model selection, and propose a novel fast greedy search
algorithm on the model space which makes use of one step optimization updates
to the variational lower bound in the current model for screening large numbers
of candidate predictor variables for inclusion/exclusion in a computationally
thrifty way. We show that the model search strategy we suggest is related to
widely used orthogonal matching pursuit algorithms for model search but yields
a framework for potentially extending these algorithms to more complex models.
The methodology is applied in simulations and in two real examples involving
prediction for food constituents using NIR technology and prediction of disease
progression in diabetes.
"
"  Inspired by a growing interest in analyzing network data, we study the
problem of node classification on graphs, focusing on approaches based on
kernel machines. Conventionally, kernel machines are linear classifiers in the
implicit feature space. We argue that linear classification in the feature
space of kernels commonly used for graphs is often not enough to produce good
results. When this is the case, one naturally considers nonlinear classifiers
in the feature space. We show that repeating this process produces something we
call ""deep kernel machines."" We provide some examples where deep kernel
machines can make a big difference in classification performance, and point out
some connections to various recent literature on deep architectures in
artificial intelligence and machine learning.
"
"  Selecting important features in non-linear or kernel spaces is a difficult
challenge in both classification and regression problems. When many of the
features are irrelevant, kernel methods such as the support vector machine and
kernel ridge regression can sometimes perform poorly. We propose weighting the
features within a kernel with a sparse set of weights that are estimated in
conjunction with the original classification or regression problem. The
iterative algorithm, KNIFE, alternates between finding the coefficients of the
original problem and finding the feature weights through kernel linearization.
In addition, a slight modification of KNIFE yields an efficient algorithm for
finding feature regularization paths, or the paths of each feature's weight.
Simulation results demonstrate the utility of KNIFE for both kernel regression
and support vector machines with a variety of kernels. Feature path
realizations also reveal important non-linear correlations among features that
prove useful in determining a subset of significant variables. Results on vowel
recognition data, Parkinson's disease data, and microarray data are also given.
"
"  This paper is devoted to regret lower bounds in the classical model of
stochastic multi-armed bandit. A well-known result of Lai and Robbins, which
has then been extended by Burnetas and Katehakis, has established the presence
of a logarithmic bound for all consistent policies. We relax the notion of
consistence, and exhibit a generalisation of the logarithmic bound. We also
show the non existence of logarithmic bound in the general case of Hannan
consistency. To get these results, we study variants of popular Upper
Confidence Bounds (ucb) policies. As a by-product, we prove that it is
impossible to design an adaptive policy that would select the best of two
algorithms by taking advantage of the properties of the environment.
"
"  Many statistical $M$-estimators are based on convex optimization problems
formed by the combination of a data-dependent loss function with a norm-based
regularizer. We analyze the convergence rates of projected gradient and
composite gradient methods for solving such problems, working within a
high-dimensional framework that allows the data dimension $\pdim$ to grow with
(and possibly exceed) the sample size $\numobs$. This high-dimensional
structure precludes the usual global assumptions---namely, strong convexity and
smoothness conditions---that underlie much of classical optimization analysis.
We define appropriately restricted versions of these conditions, and show that
they are satisfied with high probability for various statistical models. Under
these conditions, our theory guarantees that projected gradient descent has a
globally geometric rate of convergence up to the \emph{statistical precision}
of the model, meaning the typical distance between the true unknown parameter
$\theta^*$ and an optimal solution $\hat{\theta}$. This result is substantially
sharper than previous convergence results, which yielded sublinear convergence,
or linear convergence only up to the noise level. Our analysis applies to a
wide range of $M$-estimators and statistical models, including sparse linear
regression using Lasso ($\ell_1$-regularized regression); group Lasso for block
sparsity; log-linear models with regularization; low-rank matrix recovery using
nuclear norm regularization; and matrix decomposition. Overall, our analysis
reveals interesting connections between statistical precision and computational
efficiency in high-dimensional estimation.
"
"  Gaussian processes (GP) are attractive building blocks for many probabilistic
models. Their drawbacks, however, are the rapidly increasing inference time and
memory requirement alongside increasing data. The problem can be alleviated
with compactly supported (CS) covariance functions, which produce sparse
covariance matrices that are fast in computations and cheap to store. CS
functions have previously been used in GP regression but here the focus is in a
classification problem. This brings new challenges since the posterior
inference has to be done approximately. We utilize the expectation propagation
algorithm and show how its standard implementation has to be modified to obtain
computational benefits from the sparse covariance matrices. We study four CS
covariance functions and show that they may lead to substantial speed up in the
inference time compared to globally supported functions.
"
"  The FARIMA models, which have long-range-dependence (LRD), are widely used in
many areas. Through deriving a precise characterisation of the spectrum,
autocovariance function, and variance time function, we show that this family
is very atypical among LRD processes, being extremely close to the fractional
Gaussian noise in a precise sense. Furthermore, we show that this closeness
property is not robust to additive noise. We argue that the use of FARIMA, and
more generally fractionally differenced time series, should be reassessed in
some contexts, in particular when convergence rate under rescaling is important
and noise is expected.
"
"  Deregulation of energy markets, penetration of renewables, advanced metering
capabilities, and the urge for situational awareness, all call for system-wide
power system state estimation (PSSE). Implementing a centralized estimator
though is practically infeasible due to the complexity scale of an
interconnection, the communication bottleneck in real-time monitoring, regional
disclosure policies, and reliability issues. In this context, distributed PSSE
methods are treated here under a unified and systematic framework. A novel
algorithm is developed based on the alternating direction method of
multipliers. It leverages existing PSSE solvers, respects privacy policies,
exhibits low communication load, and its convergence to the centralized
estimates is guaranteed even in the absence of local observability. Beyond the
conventional least-squares based PSSE, the decentralized framework accommodates
a robust state estimator. By exploiting interesting links to the compressive
sampling advances, the latter jointly estimates the state and identifies
corrupted measurements. The novel algorithms are numerically evaluated using
the IEEE 14-, 118-bus, and a 4,200-bus benchmarks. Simulations demonstrate that
the attainable accuracy can be reached within a few inter-area exchanges, while
largest residual tests are outperformed.
"
"  An important monitoring task for power systems is accurate estimation of the
system operation state. Under the nonlinear AC power flow model, the state
estimation (SE) problem is inherently nonconvex giving rise to many local
optima. In addition to nonconvexity, SE is challenged by data integrity and
cyber-security issues. Unfortunately, existing robust (R-) SE schemes employed
routinely in practice rely on iterative solvers, which are sensitive to
initialization and cannot ensure global optimality. A novel R-SE approach is
formulated here by capitalizing on the sparsity of an overcomplete outlier
vector model. Observability and identifiability issues of this model are
investigated, and neat links are established between R-SE and error control
coding. The \emph{convex} semidefinite relaxation (SDR) technique is further
pursued to render the nonconvex R-SE problem efficiently solvable. The
resultant algorithm markedly outperforms existing iterative alternatives, as
corroborated through numerical tests on the standard IEEE 30-bus system.
"
"  Accurate representation of the physical layer is required for analysis and
simulation of multi-hop networking in sensor, ad hoc, and mesh networks. This
paper investigates, models, and analyzes the correlations that exist in shadow
fading between links in multi-hop networks. Radio links that are geographically
proximate often experience similar environmental shadowing effects and thus
have correlated fading. We describe a measurement procedure and campaign to
measure a large number of multi-hop networks in an ensemble of environments.
The measurements show statistically significant correlations among shadowing
experienced on different links in the network, with correlation coefficients up
to 0.33. We propose a statistical model for the shadowing correlation between
link pairs which shows strong agreement with the measurements, and we compare
the new model with an existing shadowing correlation model of Gudmundson
(1991). Finally, we analyze multi-hop paths in three and four node networks
using both correlated and independent shadowing models and show that
independent shadowing models can underestimate the probability of route failure
by a factor of two or greater.
"
"  Community detection has been one of the central problems in network studies
and directed network is particularly challenging due to asymmetry among its
links. In this paper, we found that incorporating the direction of links
reveals new perspectives on communities regarding to two different roles,
source and terminal, that a node plays in each community. Intriguingly, such
communities appear to be connected with unique spectral property of the graph
Laplacian of the adjacency matrix and we exploit this connection by using
regularized SVD methods. We propose harvesting algorithms, coupled with
regularized SVDs, that are linearly scalable for efficient identification of
communities in huge directed networks. The proposed algorithm shows great
performance and scalability on benchmark networks in simulations and
successfully recovers communities in real network applications.
"
"  Stochastic networks are a plausible representation of the relational
information among entities in dynamic systems such as living cells or social
communities. While there is a rich literature in estimating a static or
temporally invariant network from observation data, little has been done toward
estimating time-varying networks from time series of entity attributes. In this
paper we present two new machine learning methods for estimating time-varying
networks, which both build on a temporally smoothed $l_1$-regularized logistic
regression formalism that can be cast as a standard convex-optimization problem
and solved efficiently using generic solvers scalable to large networks. We
report promising results on recovering simulated time-varying networks. For
real data sets, we reverse engineer the latent sequence of temporally rewiring
political networks between Senators from the US Senate voting records and the
latent evolving regulatory networks underlying 588 genes across the life cycle
of Drosophila melanogaster from the microarray time course.
"
"  Genome-wide association studies have become increasingly common due to
advances in technology and have permitted the identification of differences in
single nucleotide polymorphism (SNP) alleles that are associated with diseases.
However, while typical GWAS analysis techniques treat markers individually,
complex diseases are unlikely to have a single causative gene. There is thus a
pressing need for multi-SNP analysis methods that can reveal system-level
differences in cases and controls. Here, we present a novel multi-SNP GWAS
analysis method called Pathways of Distinction Analysis (PoDA). The method uses
GWAS data and known pathway-gene and gene-SNP associations to identify pathways
that permit, ideally, the distinction of cases from controls. The technique is
based upon the hypothesis that if a pathway is related to disease risk, cases
will appear more similar to other cases than to controls for the SNPs
associated with that pathway. By systematically applying the method to all
pathways of potential interest, we can identify those for which the hypothesis
holds true, i.e., pathways containing SNPs for which the samples exhibit
greater within-class similarity than across classes. Importantly, PoDA improves
on existing single-SNP and SNP-set enrichment analyses in that it does not
require the SNPs in a pathway to exhibit independent main effects. This permits
PoDA to reveal pathways in which epistatic interactions drives risk. In this
paper, we detail the PoDA method and apply it to two GWA studies: one of breast
cancer, and the other of liver cancer. The results obtained strongly suggest
that there exist pathway-wide genomic differences that contribute to disease
susceptibility. PoDA thus provides an analytical tool that is complementary to
existing techniques and has the power to enrich our understanding of disease
genomics at the systems-level.
"
"  Rescaled spike and slab models are a new Bayesian variable selection method
for linear regression models. In high dimensional orthogonal settings such
models have been shown to possess optimal model selection properties. We review
background theory and discuss applications of rescaled spike and slab models to
prediction problems involving orthogonal polynomials. We first consider global
smoothing and discuss potential weaknesses. Some of these deficiencies are
remedied by using local regression. The local regression approach relies on an
intimate connection between local weighted regression and weighted generalized
ridge regression. An important implication is that one can trace the effective
degrees of freedom of a curve as a way to visualize and classify curvature.
Several motivating examples are presented.
"
"  In this paper, we propose a simple, versatile model for learning the
structure and parameters of multivariate distributions from a data set.
Learning a Markov network from a given data set is not a simple problem,
because Markov networks rigorously represent Markov properties, and this rigor
imposes complex constraints on the design of the networks. Our proposed model
removes these constraints, acquiring important aspects from the information
geometry. The proposed parameter- and structure-learning algorithms are simple
to execute as they are based solely on local computation at each node.
Experiments demonstrate that our algorithms work appropriately.
"
"  This paper proposes to use a rather new modelling approach in the realm of
solar radiation forecasting. In this work, two forecasting models:
Autoregressive Moving Average (ARMA) and Neural Network (NN) models are
combined to form a model committee. The Bayesian inference is used to affect a
probability to each model in the committee. Hence, each model's predictions are
weighted by their respective probability. The models are fitted to one year of
hourly Global Horizontal Irradiance (GHI) measurements. Another year (the test
set) is used for making genuine one hour ahead (h+1) out-of-sample forecast
comparisons. The proposed approach is benchmarked against the persistence
model. The very first results show an improvement brought by this approach.
"
"  Structure learning of Gaussian graphical models is an extensively studied
problem in the classical multivariate setting where the sample size n is larger
than the number of random variables p, as well as in the more challenging
setting when p>>n. However, analogous approaches for learning the structure of
graphical models with mixed discrete and continuous variables when p>>n remain
largely unexplored. Here we describe a statistical learning procedure for this
problem based on limited-order correlations and assess its performance with
synthetic and real data.
"
"  We present simple and computationally efficient nonparametric estimators of
R\'enyi entropy and mutual information based on an i.i.d. sample drawn from an
unknown, absolutely continuous distribution over $\R^d$. The estimators are
calculated as the sum of $p$-th powers of the Euclidean lengths of the edges of
the `generalized nearest-neighbor' graph of the sample and the empirical copula
of the sample respectively. For the first time, we prove the almost sure
consistency of these estimators and upper bounds on their rates of convergence,
the latter of which under the assumption that the density underlying the sample
is Lipschitz continuous. Experiments demonstrate their usefulness in
independent subspace analysis.
"
"  In many statistical signal processing applications, the estimation of
nuisance parameters and parameters of interest is strongly linked to the
resulting performance. Generally, these applications deal with complex data.
This paper focuses on covariance matrix estimation problems in non-Gaussian
environments and particularly, the M-estimators in the context of elliptical
distributions. Firstly, this paper extends to the complex case the results of
Tyler in [1]. More precisely, the asymptotic distribution of these estimators
as well as the asymptotic distribution of any homogeneous function of degree 0
of the M-estimates are derived. On the other hand, we show the improvement of
such results on two applications: DOA (directions of arrival) estimation using
the MUSIC (MUltiple SIgnal Classification) algorithm and adaptive radar
detection based on the ANMF (Adaptive Normalized Matched Filter) test.
"
"  Consider a two-by-two factorial experiment with more than 1 replicate.
Suppose that we have uncertain prior information that the two-factor
interaction is zero. We describe new simultaneous frequentist confidence
intervals for the 4 population cell means, with simultaneous confidence
coefficient 1-alpha, that utilize this prior information in the following
sense. These simultaneous confidence intervals define a cube with expected
volume that (a) is relatively small when the two-factor interaction is zero and
(b) has maximum value that is not too large. Also, these intervals coincide
with the standard simultaneous confidence intervals obtained by Tukey's method,
with simultaneous confidence coefficient 1-alpha, when the data strongly
contradict the prior information that the two-factor interaction is zero. We
illustrate the application of these new simultaneous confidence intervals to a
real data set.
"
"  We propose a novel Bayesian optimisation procedure for outlier detection in
the Capital Asset Pricing Model. We use a parametric product partition model to
robustly estimate the systematic risk of an asset. We assume that the returns
follow independent normal distributions and we impose a partition structure on
the parameters of interest. The partition structure imposed on the parameters
induces a corresponding clustering of the returns. We identify via an
optimisation procedure the partition that best separates standard observations
from the atypical ones. The methodology is illustrated with reference to a real
data set, for which we also provide a microeconomic interpretation of the
detected outliers.
"
"  We consider the problem of estimating neural activity from measurements of
the magnetic fields recorded by magnetoencephalography. We exploit the temporal
structure of the problem and model the neural current as a collection of
evolving current dipoles, which appear and disappear, but whose locations are
constant throughout their lifetime. This fully reflects the physiological
interpretation of the model. In order to conduct inference under this proposed
model, it was necessary to develop an algorithm based around state-of-the-art
sequential Monte Carlo methods employing carefully designed importance
distributions. Previous work employed a bootstrap filter and an artificial
dynamic structure where dipoles performed a random walk in space, yielding
nonphysical artefacts in the reconstructions; such artefacts are not observed
when using the proposed model. The algorithm is validated with simulated data,
in which it provided an average localisation error which is approximately half
that of the bootstrap filter. An application to complex real data derived from
a somatosensory experiment is presented. Assessment of model fit via marginal
likelihood showed a clear preference for the proposed model and the associated
reconstructions show better localisation.
"
"  We perform a finite sample analysis of the detection levels for sparse
principal components of a high-dimensional covariance matrix. Our minimax
optimal test is based on a sparse eigenvalue statistic. Alas, computing this
test is known to be NP-complete in general, and we describe a computationally
efficient alternative test using convex relaxations. Our relaxation is also
proved to detect sparse principal components at near optimal detection levels,
and it performs well on simulated datasets. Moreover, using polynomial time
reductions from theoretical computer science, we bring significant evidence
that our results cannot be improved, thus revealing an inherent trade off
between statistical and computational performance.
"
"  We develop a novel advanced Particle Markov chain Monte Carlo algorithm that
is capable of sampling from the posterior distribution of non-linear state
space models for both the unobserved latent states and the unknown model
parameters. We apply this novel methodology to five population growth models,
including models with strong and weak Allee effects, and test if it can
efficiently sample from the complex likelihood surface that is often associated
with these models. Utilising real and also synthetically generated data sets we
examine the extent to which observation noise and process error may frustrate
efforts to choose between these models. Our novel algorithm involves an
Adaptive Metropolis proposal combined with an SIR Particle MCMC algorithm
(AdPMCMC). We show that the AdPMCMC algorithm samples complex, high-dimensional
spaces efficiently, and is therefore superior to standard Gibbs or Metropolis
Hastings algorithms that are known to converge very slowly when applied to the
non-linear state space ecological models considered in this paper.
Additionally, we show how the AdPMCMC algorithm can be used to recursively
estimate the Bayesian Cram\'er-Rao Lower Bound of Tichavsk\'y (1998). We derive
expressions for these Cram\'er-Rao Bounds and estimate them for the models
considered. Our results demonstrate a number of important features of common
population growth models, most notably their multi-modal posterior surfaces and
dependence between the static and dynamic parameters. We conclude by sampling
from the posterior distribution of each of the models, and use Bayes factors to
highlight how observation noise significantly diminishes our ability to select
among some of the models, particularly those that are designed to reproduce an
Allee effect.
"
"  Kalman filtering and smoothing algorithms are used in many areas, including
tracking and navigation, medical applications, and financial trend filtering.
One of the basic assumptions required to apply the Kalman smoothing framework
is that error covariance matrices are known and given. In this paper, we study
a general class of inference problems where covariance matrices can depend
functionally on unknown parameters. In the Kalman framework, this allows
modeling situations where covariance matrices may depend functionally on the
state sequence being estimated. We present an extended formulation and
generalized Gauss-Newton (GGN) algorithm for inference in this context. When
applied to dynamic systems inference, we show the algorithm can be implemented
to preserve the computational efficiency of the classic Kalman smoother. The
new approach is illustrated with a synthetic numerical example.
"
"  Uncertainty quantification (UQ) techniques are frequently used to ascertain
output variability in systems with parametric uncertainty. Traditional
algorithms for UQ are either system-agnostic and slow (such as Monte Carlo) or
fast with stringent assumptions on smoothness (such as polynomial chaos and
Quasi-Monte Carlo). In this work, we develop a fast UQ approach for hybrid
dynamical systems by extending the polynomial chaos methodology to these
systems. To capture discontinuities, we use a wavelet-based Wiener-Haar
expansion. We develop a boundary layer approach to propagate uncertainty
through separable reset conditions. We also introduce a transport theory based
approach for propagating uncertainty through hybrid dynamical systems. Here the
expansion yields a set of hyperbolic equations that are solved by integrating
along characteristics. The solution of the partial differential equation along
the characteristics allows one to quantify uncertainty in hybrid or switching
dynamical systems. The above methods are demonstrated on example problems.
"
"  Although there is a rich literature on methods for allowing the variance in a
univariate regression model to vary with predictors, time and other factors,
relatively little has been done in the multivariate case. Our focus is on
developing a class of nonparametric covariance regression models, which allow
an unknown p x p covariance matrix to change flexibly with predictors. The
proposed modeling framework induces a prior on a collection of covariance
matrices indexed by predictors through priors for predictor-dependent loadings
matrices in a factor model. In particular, the predictor-dependent loadings are
characterized as a sparse combination of a collection of unknown dictionary
functions (e.g, Gaussian process random functions). The induced covariance is
then a regularized quadratic function of these dictionary elements. Our
proposed framework leads to a highly-flexible, but computationally tractable
formulation with simple conjugate posterior updates that can readily handle
missing data. Theoretical properties are discussed and the methods are
illustrated through simulations studies and an application to the Google Flu
Trends data.
"
"  This paper considers the sparse eigenvalue problem, which is to extract
dominant (largest) sparse eigenvectors with at most $k$ non-zero components. We
propose a simple yet effective solution called truncated power method that can
approximately solve the underlying nonconvex optimization problem. A strong
sparse recovery result is proved for the truncated power method, and this
theory is our key motivation for developing the new algorithm. The proposed
method is tested on applications such as sparse principal component analysis
and the densest $k$-subgraph problem. Extensive experiments on several
synthetic and real-world large scale datasets demonstrate the competitive
empirical performance of our method.
"
"  Traditionally, multitask learning (MTL) assumes that all the tasks are
related. This can lead to negative transfer when tasks are indeed incoherent.
Recently, a number of approaches have been proposed that alleviate this problem
by discovering the underlying task clusters or relationships. However, they are
limited to modeling these relationships at the task level, which may be
restrictive in some applications. In this paper, we propose a novel MTL
formulation that captures task relationships at the feature-level. Depending on
the interactions among tasks and features, the proposed method construct
different task clusters for different features, without even the need of
pre-specifying the number of clusters. Computationally, the proposed
formulation is strongly convex, and can be efficiently solved by accelerated
proximal methods. Experiments are performed on a number of synthetic and
real-world data sets. Under various degrees of task relationships, the accuracy
of the proposed method is consistently among the best. Moreover, the
feature-specific task clusters obtained agree with the known/plausible task
structures of the data.
"
"  Sparsity-constrained optimization has wide applicability in machine learning,
statistics, and signal processing problems such as feature selection and
compressive Sensing. A vast body of work has studied the sparsity-constrained
optimization from theoretical, algorithmic, and application aspects in the
context of sparse estimation in linear models where the fidelity of the
estimate is measured by the squared error. In contrast, relatively less effort
has been made in the study of sparsity-constrained optimization in cases where
nonlinear models are involved or the cost function is not quadratic. In this
paper we propose a greedy algorithm, Gradient Support Pursuit (GraSP), to
approximate sparse minima of cost functions of arbitrary form. Should a cost
function have a Stable Restricted Hessian (SRH) or a Stable Restricted
Linearization (SRL), both of which are introduced in this paper, our algorithm
is guaranteed to produce a sparse vector within a bounded distance from the
true sparse optimum. Our approach generalizes known results for quadratic cost
functions that arise in sparse linear regression and Compressive Sensing. We
also evaluate the performance of GraSP through numerical simulations on
synthetic data, where the algorithm is employed for sparse logistic regression
with and without $\ell_2$-regularization.
"
"  We introduce block-tree graphs as a framework for deriving efficient
algorithms on graphical models. We define block-tree graphs as a
tree-structured graph where each node is a cluster of nodes such that the
clusters in the graph are disjoint. This differs from junction-trees, where two
clusters connected by an edge always have at least one common node. When
compared to junction-trees, we show that constructing block-tree graphs is
faster, and finding optimal block-tree graphs has a much smaller search space.
Applying our block-tree graph framework to graphical models, we show that, for
some graphs, e.g., grid graphs, using block-tree graphs for inference is
computationally more efficient than using junction-trees. For graphical models
with boundary conditions, the block-tree graph framework transforms the
boundary valued problem into an initial value problem. For Gaussian graphical
models, the block-tree graph framework leads to a linear state-space
representation. Since exact inference in graphical models can be
computationally intractable, we propose to use spanning block-trees to derive
approximate inference algorithms. Experimental results show the improved
performance in using spanning block-trees versus using spanning trees for
approximate estimation over Gaussian graphical models.
"
"  Components of biological systems interact with each other in order to carry
out vital cell functions. Such information can be used to improve estimation
and inference, and to obtain better insights into the underlying cellular
mechanisms. Discovering regulatory interactions among genes is therefore an
important problem in systems biology. Whole-genome expression data over time
provides an opportunity to determine how the expression levels of genes are
affected by changes in transcription levels of other genes, and can therefore
be used to discover regulatory interactions among genes.
  In this paper, we propose a novel penalization method, called truncating
lasso, for estimation of causal relationships from time-course gene expression
data. The proposed penalty can correctly determine the order of the underlying
time series, and improves the performance of the lasso-type estimators.
Moreover, the resulting estimate provides information on the time lag between
activation of transcription factors and their effects on regulated genes. We
provide an efficient algorithm for estimation of model parameters, and show
that the proposed method can consistently discover causal relationships in the
large $p$, small $n$ setting. The performance of the proposed model is
evaluated favorably in simulated, as well as real, data examples. The proposed
truncating lasso method is implemented in the R-package grangerTlasso and is
available at http://www.stat.lsa.umich.edu/~shojaie.
"
"  In real supervised learning scenarios, it is not uncommon that the training
and test sample follow different probability distributions, thus rendering the
necessity to correct the sampling bias. Focusing on a particular covariate
shift problem, we derive high probability confidence bounds for the kernel mean
matching (KMM) estimator, whose convergence rate turns out to depend on some
regularity measure of the regression function and also on some capacity measure
of the kernel. By comparing KMM with the natural plug-in estimator, we
establish the superiority of the former hence provide concrete
evidence/understanding to the effectiveness of KMM under covariate shift.
"
"  The restricted Boltzmann machine is a graphical model for binary random
variables. Based on a complete bipartite graph separating hidden and observed
variables, it is the binary analog to the factor analysis model. We study this
graphical model from the perspectives of algebraic statistics and tropical
geometry, starting with the observation that its Zariski closure is a Hadamard
power of the first secant variety of the Segre variety of projective lines. We
derive a dimension formula for the tropicalized model, and we use it to show
that the restricted Boltzmann machine is identifiable in many cases. Our
methods include coding theory and geometry of linear threshold functions.
"
"  In order to improve forecasts, a decisionmaker often combines probabilities
given by various sources, such as human experts and machine learning
classifiers. When few training data are available, aggregation can be improved
by incorporating prior knowledge about the event being forecasted and about
salient properties of the experts. To this end, we develop a generative
Bayesian aggregation model for probabilistic classi cation. The model includes
an event-specific prior, measures of individual experts' bias, calibration,
accuracy, and a measure of dependence betweeen experts. Rather than require
absolute measures, we show that aggregation may be expressed in terms of
relative accuracy between experts. The model results in a weighted logarithmic
opinion pool (LogOps) that satis es consistency criteria such as the external
Bayesian property. We derive analytic solutions for independent and for
exchangeable experts. Empirical tests demonstrate the model's use, comparing
its accuracy with other aggregation methods.
"
"  Iterative information processing, either based on heuristics or analytical
frameworks, has been shown to be a very powerful tool for the design of
efficient, yet feasible, wireless receiver architectures. Within this context,
algorithms performing message-passing on a probabilistic graph, such as the
sum-product (SP) and variational message passing (VMP) algorithms, have become
increasingly popular.
  In this contribution, we apply a combined VMP-SP message-passing technique to
the design of receivers for MIMO-ODFM systems. The message-passing equations of
the combined scheme can be obtained from the equations of the stationary points
of a constrained region-based free energy approximation. When applied to a
MIMO-OFDM probabilistic model, we obtain a generic receiver architecture
performing iterative channel weight and noise precision estimation,
equalization and data decoding. We show that this generic scheme can be
particularized to a variety of different receiver structures, ranging from
high-performance iterative structures to low complexity receivers. This allows
for a flexible design of the signal processing specially tailored for the
requirements of each specific application. The numerical assessment of our
solutions, based on Monte Carlo simulations, corroborates the high performance
of the proposed algorithms and their superiority to heuristic approaches.
"
"  In many organisms the expression levels of each gene are controlled by the
activation levels of known ""Transcription Factors"" (TF). A problem of
considerable interest is that of estimating the ""Transcription Regulation
Networks"" (TRN) relating the TFs and genes. While the expression levels of
genes can be observed, the activation levels of the corresponding TFs are
usually unknown, greatly increasing the difficulty of the problem. Based on
previous experimental work, it is often the case that partial information about
the TRN is available. For example, certain TFs may be known to regulate a given
gene or in other cases a connection may be predicted with a certain
probability. In general, the biology of the problem indicates there will be
very few connections between TFs and genes. Several methods have been proposed
for estimating TRNs. However, they all suffer from problems such as unrealistic
assumptions about prior knowledge of the network structure or computational
limitations. We propose a new approach that can directly utilize prior
information about the network structure in conjunction with observed gene
expression data to estimate the TRN. Our approach uses $L_1$ penalties on the
network to ensure a sparse structure. This has the advantage of being
computationally efficient as well as making many fewer assumptions about the
network structure. We use our methodology to construct the TRN for E. coli and
show that the estimate is biologically sensible and compares favorably with
previous estimates.
"
"  The popular cubic smoothing spline estimate of a regression function arises
as the minimizer of the penalized sum of squares $\sum_j(Y_j - {\mu}(t_j))^2 +
{\lambda}\int_a^b [{\mu}""(t)]^2 dt$, where the data are $t_j,Y_j$, $j=1,...,
n$. The minimization is taken over an infinite-dimensional function space, the
space of all functions with square integrable second derivatives. But the
calculations can be carried out in a finite-dimensional space. The reduction
from minimizing over an infinite dimensional space to minimizing over a finite
dimensional space occurs for more general objective functions: the data may be
related to the function ${\mu}$ in another way, the sum of squares may be
replaced by a more suitable expression, or the penalty, $\int_a^b [{\mu}""(t)]^2
dt$, might take a different form. This paper reviews the Reproducing Kernel
Hilbert Space structure that provides a finite-dimensional solution for a
general minimization problem. Particular attention is paid to penalties based
on linear differential operators. In this case, one can sometimes easily
calculate the minimizer explicitly, using Green's functions.
"
"  General regression and classification models are constructed as linear
combinations of simple rules derived from the data. Each rule consists of a
conjunction of a small number of simple statements concerning the values of
individual input variables. These rule ensembles are shown to produce
predictive accuracy comparable to the best methods. However, their principal
advantage lies in interpretation. Because of its simple form, each rule is easy
to understand, as is its influence on individual predictions, selected subsets
of predictions, or globally over the entire space of joint input variable
values. Similarly, the degree of relevance of the respective input variables
can be assessed globally, locally in different regions of the input space, or
at individual prediction points. Techniques are presented for automatically
identifying those variables that are involved in interactions with other
variables, the strength and degree of those interactions, as well as the
identities of the other variables with which they interact. Graphical
representations are used to visualize both main and interaction effects.
"
"  We study a new parametric approach for particular hidden stochastic models
such as the Stochastic Volatility model. This method is based on contrast
minimization and deconvolution. After proving consistency and asymptotic
normality of the estimation leading to asymptotic confidence intervals, we
provide a thorough numerical study, which compares most of the classical
methods that are used in practice (Quasi Maximum Likelihood estimator,
Simulated Expectation Maximization Likelihood estimator and Bayesian
estimators). We prove that our estimator clearly outperforms the Maximum
Likelihood Estimator in term of computing time, but also most of the other
methods. We also show that this contrast method is the most robust with respect
to non Gaussianity of the error and also does not need any tuning parameter.
"
"  We introduce Gaussian Process Topic Models (GPTMs), a new family of topic
models which can leverage a kernel among documents while extracting correlated
topics. GPTMs can be considered a systematic generalization of the Correlated
Topic Models (CTMs) using ideas from Gaussian Process (GP) based embedding.
Since GPTMs work with both a topic covariance matrix and a document kernel
matrix, learning GPTMs involves a novel component-solving a suitable Sylvester
equation capturing both topic and document dependencies. The efficacy of GPTMs
is demonstrated with experiments evaluating the quality of both topic modeling
and embedding.
"
"  This paper proposes a new randomized strategy for adaptive MCMC using
Bayesian optimization. This approach applies to non-differentiable objective
functions and trades off exploration and exploitation to reduce the number of
potentially costly objective function evaluations. We demonstrate the strategy
in the complex setting of sampling from constrained, discrete and densely
connected probabilistic graphical models where, for each variation of the
problem, one needs to adjust the parameters of the proposal mechanism
automatically to ensure efficient mixing of the Markov chains.
"
"  Understanding the structure and evolution of web-based user-object bipartite
networks is an important task since they play a fundamental role in online
information filtering. In this paper, we focus on investigating the patterns of
online users' behavior and the effect on recommendation process. Empirical
analysis on the e-commercial systems show that users have significant taste
diversity and their interests for niche items highly overlap. Additionally,
recommendation process are investigated on both the real networks and the
reshuffled networks in which real users' behavior patterns can be gradually
destroyed. Our results shows that the performance of personalized
recommendation methods is strongly related to the real network structure.
Detail study on each item shows that recommendation accuracy for hot items is
almost maximum and quite robust to the reshuffling process. However, niche
items cannot be accurately recommended after removing users' behavior patterns.
Our work also is meaningful in practical sense since it reveals an effective
direction to improve the accuracy and the robustness of the existing
recommender systems.
"
"  Breiman (2001) proposed to statisticians awareness of two cultures: 1.
Parametric modeling culture, pioneered by R.A.Fisher and Jerzy Neyman; 2.
Algorithmic predictive culture, pioneered by machine learning research.
  Parzen (2001), as a part of discussing Breiman (2001), proposed that
researchers be aware of many cultures, including the focus of our research: 3.
Nonparametric, quantile based, information theoretic modeling. We provide a
unification of many statistical methods for traditional small data sets and
emerging big data sets in terms of comparison density, copula density, measure
of dependence, correlation, information, new measures (called LP score
comoments) that apply to long tailed distributions with out finite second order
moments. A very important goal is to unify methods for discrete and continuous
random variables. Our research extends these methods to modern high dimensional
data modeling.
"
"  Defining the energy function as the negative logarithm of the density, we
explore the energy landscape of a distribution via the tree of sublevel sets of
its energy. This tree represents the hierarchy among the connected components
of the sublevel sets. We propose ways to annotate the tree so that it provides
information on both topological and statistical aspects of the distribution,
such as the local energy minima (local modes), their local domains and volumes,
and the barriers between them. We develop a computational method to estimate
the tree and reconstruct the energy landscape from Monte Carlo samples
simulated at a wide energy range of a distribution. This method can be applied
to any arbitrary distribution on a space with defined connectedness. We test
the method on multimodal distributions and posterior distributions to show that
our estimated trees are accurate compared to theoretical values. When used to
perform Bayesian inference of DNA sequence segmentation, this approach reveals
much more information than the standard approach based on marginal posterior
distributions.
"
"  We consider a group of Bayesian agents who try to estimate a state of the
world $\theta$ through interaction on a social network. Each agent $v$
initially receives a private measurement of $\theta$: a number $S_v$ picked
from a Gaussian distribution with mean $\theta$ and standard deviation one.
Then, in each discrete time iteration, each reveals its estimate of $\theta$ to
its neighbors, and, observing its neighbors' actions, updates its belief using
Bayes' Law.
  This process aggregates information efficiently, in the sense that all the
agents converge to the belief that they would have, had they access to all the
private measurements. We show that this process is computationally efficient,
so that each agent's calculation can be easily carried out. We also show that
on any graph the process converges after at most $2N \cdot D$ steps, where $N$
is the number of agents and $D$ is the diameter of the network. Finally, we
show that on trees and on distance transitive-graphs the process converges
after $D$ steps, and that it preserves privacy, so that agents learn very
little about the private signal of most other agents, despite the efficient
aggregation of information. Our results extend those in an unpublished
manuscript of the first and last authors.
"
"  This paper describes a new algorithm for hyperspectral image unmixing. Most
of the unmixing algorithms proposed in the literature do not take into account
the possible spatial correlations between the pixels. In this work, a Bayesian
model is introduced to exploit these correlations. The image to be unmixed is
assumed to be partitioned into regions (or classes) where the statistical
properties of the abundance coefficients are homogeneous. A Markov random field
is then proposed to model the spatial dependency of the pixels within any
class. Conditionally upon a given class, each pixel is modeled by using the
classical linear mixing model with additive white Gaussian noise. This strategy
is investigated the well known linear mixing model. For this model, the
posterior distributions of the unknown parameters and hyperparameters allow
ones to infer the parameters of interest. These parameters include the
abundances for each pixel, the means and variances of the abundances for each
class, as well as a classification map indicating the classes of all pixels in
the image. To overcome the complexity of the posterior distribution of
interest, we consider Markov chain Monte Carlo methods that generate samples
distributed according to the posterior of interest. The generated samples are
then used for parameter and hyperparameter estimation. The accuracy of the
proposed algorithms is illustrated on synthetic and real data.
"
"  We propose a new yet natural algorithm for learning the graph structure of
general discrete graphical models (a.k.a. Markov random fields) from samples.
Our algorithm finds the neighborhood of a node by sequentially adding nodes
that produce the largest reduction in empirical conditional entropy; it is
greedy in the sense that the choice of addition is based only on the reduction
achieved at that iteration. Its sequential nature gives it a lower
computational complexity as compared to other existing comparison-based
techniques, all of which involve exhaustive searches over every node set of a
certain size. Our main result characterizes the sample complexity of this
procedure, as a function of node degrees, graph size and girth in factor-graph
representation. We subsequently specialize this result to the case of Ising
models, where we provide a simple transparent characterization of sample
complexity as a function of model and graph parameters.
  For tree graphs, our algorithm is the same as the classical Chow-Liu
algorithm, and in that sense can be considered the extension of the same to
graphs with cycles.
"
"  Missing data is a recurrent issue in epidemiology where the infection process
may be partially observed. Approximate Bayesian Computation, an alternative to
data imputation methods such as Markov Chain Monte Carlo integration, is
proposed for making inference in epidemiological models. It is a
likelihood-free method that relies exclusively on numerical simulations. ABC
consists in computing a distance between simulated and observed summary
statistics and weighting the simulations according to this distance. We propose
an original extension of ABC to path-valued summary statistics, corresponding
to the cumulated number of detections as a function of time. For a standard
compartmental model with Suceptible, Infectious and Recovered individuals
(SIR), we show that the posterior distributions obtained with ABC and MCMC are
similar. In a refined SIR model well-suited to the HIV contact-tracing data in
Cuba, we perform a comparison between ABC with full and binned detection times.
For the Cuban data, we evaluate the efficiency of the detection system and
predict the evolution of the HIV-AIDS disease. In particular, the percentage of
undetected infectious individuals is found to be of the order of 40%.
"
"  Nous montrons comment il est possible d'utiliser l'algorithme d'auto
organisation de Kohonen pour traiter des donn\'ees avec valeurs manquantes et
estimer ces derni\`eres. Apr\`es un rappel m\'ethodologique, nous illustrons
notre propos \`a partir de trois applications \`a des donn\'ees r\'eelles.
  -----
  We show how it is possible to use the Kohonen self-organizing algorithm to
deal with data which contain missing values and to estimate them. After a
methodological recall, we illustrate our purpose from three real databases
applications.
"
"  Genetic association study is an essential step to discover genetic factors
that are associated with a complex trait of interest. In this paper we present
a novel generalized quasi-likelihood score (GQLS) test that is suitable for a
study with either a quantitative trait or a binary trait. We use a logistic
regression model to link the phenotypic value of the trait to the distribution
of allelic frequencies. In our model, the allele frequencies are treated as a
response and the trait is treated as a covariate that allows us to leave the
distribution of the trait values unspecified. Simulation studies indicate that
our method is generally more powerful in comparison with the family-based
association test (FBAT) and controls the type I error at the desired levels. We
apply our method to analyze data on Holstein cattle for an estimated breeding
value phenotype, and to analyze data from the Collaborative Study of the
Genetics of Alcoholism for alcohol dependence. The results show a good portion
of significant SNPs and regions consistent with previous reports in the
literature, and also reveal new significant SNPs and regions that are
associated with the complex trait of interest.
"
"  We show an efficient algorithm for the following problem: Given uniformly
random points from an arbitrary n-dimensional simplex, estimate the simplex.
The size of the sample and the number of arithmetic operations of our algorithm
are polynomial in n. This answers a question of Frieze, Jerrum and Kannan
[FJK]. Our result can also be interpreted as efficiently learning the
intersection of n+1 half-spaces in R^n in the model where the intersection is
bounded and we are given polynomially many uniform samples from it. Our proof
uses the local search technique from Independent Component Analysis (ICA), also
used by [FJK]. Unlike these previous algorithms, which were based on analyzing
the fourth moment, ours is based on the third moment.
  We also show a direct connection between the problem of learning a simplex
and ICA: a simple randomized reduction to ICA from the problem of learning a
simplex. The connection is based on a known representation of the uniform
measure on a simplex. Similar representations lead to a reduction from the
problem of learning an affine transformation of an n-dimensional l_p ball to
ICA.
"
"  Relational learning can be used to augment one data source with other
correlated sources of information, to improve predictive accuracy. We frame a
large class of relational learning problems as matrix factorization problems,
and propose a hierarchical Bayesian model. Training our Bayesian model using
random-walk Metropolis-Hastings is impractically slow, and so we develop a
block Metropolis-Hastings sampler which uses the gradient and Hessian of the
likelihood to dynamically tune the proposal. We demonstrate that a predictive
model of brain response to stimuli can be improved by augmenting it with side
information about the stimuli.
"
"  Perhaps surprisingly, it is possible to predict how long an algorithm will
take to run on a previously unseen input, using machine learning techniques to
build a model of the algorithm's runtime as a function of problem-specific
instance features. Such models have important applications to algorithm
analysis, portfolio-based algorithm selection, and the automatic configuration
of parameterized algorithms. Over the past decade, a wide variety of techniques
have been studied for building such models. Here, we describe extensions and
improvements of existing models, new families of models, and -- perhaps most
importantly -- a much more thorough treatment of algorithm parameters as model
inputs. We also comprehensively describe new and existing features for
predicting algorithm runtime for propositional satisfiability (SAT), travelling
salesperson (TSP) and mixed integer programming (MIP) problems. We evaluate
these innovations through the largest empirical analysis of its kind, comparing
to a wide range of runtime modelling techniques from the literature. Our
experiments consider 11 algorithms and 35 instance distributions; they also
span a very wide range of SAT, MIP, and TSP instances, with the least
structured having been generated uniformly at random and the most structured
having emerged from real industrial applications. Overall, we demonstrate that
our new models yield substantially better runtime predictions than previous
approaches in terms of their generalization to new problem instances, to new
algorithms from a parameterized space, and to both simultaneously.
"
"  A raga is a melodic structure with fixed notes and a set of rules
characterizing a certain mood endorsed through performance. By a vadi swar is
meant that note which plays the most significant role in expressing the raga. A
samvadi swar similarly is the second most significant note. However, the
determination of their significance has an element of subjectivity and hence we
are motivated to find some truths through an objective analysis. The paper
proposes a probabilistic method of note detection and demonstrates how the
relative frequency (relative number of occurrences of the pitch) of the more
important notes stabilize far more quickly than that of others. In addition, a
count for distinct transitory and similar looking non-transitory (fundamental)
frequency movements (but possibly embedding distinct emotions!) between the
notes is also taken depicting the varnalankars or musical ornaments decorating
the notes and note sequences as rendered by the artist. They reflect certain
structural properties of the ragas. Several case studies are presented.
"
"  Many applications involve agents sharing a resource, such as networks or
services. When agents are honest, the system functions well and there is a net
profit. Unfortunately, some agents may be malicious, but it may be hard to
detect them. We consider the intrusion response problem of how to permanently
blacklist agents, in order to maximise expected profit. This is not trivial, as
blacklisting may erroneously expel honest agents. Conversely, while we gain
information by allowing an agent to remain, we may incur a cost due to
malicious behaviour. We present an efficient algorithm (HIPER) for making
near-optimal decisions for this problem. Additionally, we derive three
algorithms by reducing the problem to a Markov decision process (MDP).
Theoretically, we show that HIPER is near-optimal. Experimentally, its
performance is close to that of the full MDP solution, when the (stronger)
requirements of the latter are met.
"
"  Batting average is one of the principle performance measures for an
individual baseball player. It is natural to statistically model this as a
binomial-variable proportion, with a given (observed) number of qualifying
attempts (called ``at-bats''), an observed number of successes (``hits'')
distributed according to the binomial distribution, and with a true (but
unknown) value of $p_i$ that represents the player's latent ability. This is a
common data structure in many statistical applications; and so the
methodological study here has implications for such a range of applications. We
look at batting records for each Major League player over the course of a
single season (2005). The primary focus is on using only the batting records
from an earlier part of the season (e.g., the first 3 months) in order to
estimate the batter's latent ability, $p_i$, and consequently, also to predict
their batting-average performance for the remainder of the season. Since we are
using a season that has already concluded, we can then validate our estimation
performance by comparing the estimated values to the actual values for the
remainder of the season. The prediction methods to be investigated are
motivated from empirical Bayes and hierarchical Bayes interpretations. A newly
proposed nonparametric empirical Bayes procedure performs particularly well in
the basic analysis of the full data set, though less well with analyses
involving more homogeneous subsets of the data. In those more homogeneous
situations better performance is obtained from appropriate versions of more
familiar methods. In all situations the poorest performing choice is the
na\""{{\i}}ve predictor which directly uses the current average to predict the
future average.
"
"  On the basis of dynamical principles we derive the Logistic Equation (LE),
widely employed (among multiple applications) in the simulation of population
growth, and demonstrate that scale-invariance and a mean-value constraint are
sufficient and necessary conditions for obtaining it. We also generalize the LE
to multi-component systems and show that the above dynamical mechanisms
underlie large number of scale-free processes. Examples are presented regarding
city-populations, diffusion in complex networks, and popularity of
technological products, all of them obeying the multi-component logistic
equation in an either stochastic or deterministic way. So as to assess the
predictability-power of our present formalism, we advance a prediction,
regarding the next 60 months, for the number of users of the three main web
browsers (Explorer, Firefox and Chrome) popularly referred as ""Browser Wars"".
"
"  The matter density is an important knowledge for today cosmology as many
phenomena are linked to matter fluctuations. However, this density is not
directly available, but estimated through lensing maps or galaxy surveys. In
this article, we focus on galaxy surveys which are incomplete and noisy
observations of the galaxy density. Incomplete, as part of the sky is
unobserved or unreliable. Noisy as they are count maps degraded by Poisson
noise. Using a data augmentation method, we propose a two-step method for
recovering the density map, one step for inferring missing data and one for
estimating of the density. The results show that the missing areas are
efficiently inferred and the statistical properties of the maps are very well
preserved.
"
"  In this paper, we present a technique for parameterizing Leslie transition
matrices from simple age and sex population counts, using an implementation of
""Wood's Method"" [wood]; these matrices can forecast population by age and sex
(the ""cohort component"" method) using simple matrix multiplication and a
starting population. Our approach improves on previous methods for creating
Leslie matrices in two respects: it eliminates the need to calculate input
demographic rates from ""raw"" data, and our new format for the Leslie matrix
more elegantly reveals the population's demographic components of change
(fertility, mortality, and migration). The paper is organized around three main
themes. First, we describe the underlying algorithm, ""Wood's Method,"" which
uses quadratic optimization to fit a transition matrix to age and sex
population counts. Second, we use demographic theory to create constraint sets
that make the algorithm useable for human populations. Finally, we use the
method to forecast 3,120 US counties and show that it holds promise for
automating cohort-component forecasts. This paper describes the first published
successful application of Wood's method to human populations; it also points to
more general promise of constrained optimization techniques in demographic
modeling.
"
"  Mass spectrometry (MS) is an important technique for chemical profiling which
calculates for a sample a high dimensional histogram-like spectrum. A crucial
step of MS data processing is the peak picking which selects peaks containing
information about molecules with high concentrations which are of interest in
an MS investigation. We present a new procedure of the peak picking based on a
sparse coding algorithm. Given a set of spectra of different classes, i.e. with
different positions and heights of the peaks, this procedure can extract peaks
by means of unsupervised learning. Instead of an $l_1$-regularization penalty
term used in the original sparse coding algorithm we propose using an
elastic-net penalty term for better regularization. The evaluation is done by
means of simulation. We show that for a large region of parameters the proposed
peak picking method based on the sparse coding features outperforms a mean
spectrum-based method. Moreover, we demonstrate the procedure applying it to
two real-life datasets.
"
"  Landscape classification of the well-known biodiversity hotspot, Western
Ghats (mountains), on the west coast of India, is an important part of a
world-wide program of monitoring biodiversity. To this end, a massive
vegetation data set, consisting of 51,834 4-variate observations has been
clustered into different landscapes by Nagendra and Gadgil [Current Sci. 75
(1998) 264--271]. But a study of such importance may be affected by
nonuniqueness of cluster analysis and the lack of methods for quantifying
uncertainty of the clusterings obtained. Motivated by this applied problem of
much scientific importance, we propose a new methodology for obtaining the
global, as well as the local modes of the posterior distribution of clustering,
along with the desired credible and ""highest posterior density"" regions in a
nonparametric Bayesian framework. To meet the need of an appropriate metric for
computing the distance between any two clusterings, we adopt and provide a much
simpler, but accurate modification of the metric proposed in [In Felicitation
Volume in Honour of Prof. B. K. Kale (2009) MacMillan]. A very fast and
efficient Bayesian methodology, based on [Sankhy\={a} Ser. B 70 (2008)
133--155], has been utilized to solve the computational problems associated
with the massive data and to obtain samples from the posterior distribution of
clustering on which our proposed methods of summarization are illustrated.
"
"  There has been an explosion of interest in statistical models for analyzing
network data, and considerable interest in the class of exponential random
graph (ERG) models, especially in connection with difficulties in computing
maximum likelihood estimates. The issues associated with these difficulties
relate to the broader structure of discrete exponential families. This paper
re-examines the issues in two parts. First we consider the closure of
$k$-dimensional exponential families of distribution with discrete base measure
and polyhedral convex support $\mathrm{P}$. We show that the normal fan of
$\mathrm{P}$ is a geometric object that plays a fundamental role in deriving
the statistical and geometric properties of the corresponding extended
exponential families. We discuss its relevance to maximum likelihood
estimation, both from a theoretical and computational standpoint. Second, we
apply our results to the analysis of ERG models. In particular, by means of a
detailed example, we provide some characterization of the properties of ERG
models, and, in particular, of certain behaviors of ERG models known as
degeneracy.
"
"  We consider an optimal investment and consumption problem for a Black-Scholes
financial market with stochastic volatility and unknown stock appreciation
rate. The volatility parameter is driven by an external economic factor modeled
as a diffusion process of Ornstein-Uhlenbeck type with unknown drift. We use
the dynamical programming approach and find an optimal financial strategy which
depends on the drift parameter. To estimate the drift coefficient we observe
the economic factor $Y$ in an interval $[0,T_0]$ for fixed $T_0>0$, and use
sequential estimation. We show, that the consumption and investment strategy
calculated through this sequential procedure is $\delta$-optimal.
"
"  In this paper, we consider the problem of estimating multiple graphical
models simultaneously using the fused lasso penalty, which encourages adjacent
graphs to share similar structures. A motivating example is the analysis of
brain networks of Alzheimer's disease using neuroimaging data. Specifically, we
may wish to estimate a brain network for the normal controls (NC), a brain
network for the patients with mild cognitive impairment (MCI), and a brain
network for Alzheimer's patients (AD). We expect the two brain networks for NC
and MCI to share common structures but not to be identical to each other;
similarly for the two brain networks for MCI and AD. The proposed formulation
can be solved using a second-order method. Our key technical contribution is to
establish the necessary and sufficient condition for the graphs to be
decomposable. Based on this key property, a simple screening rule is presented,
which decomposes the large graphs into small subgraphs and allows an efficient
estimation of multiple independent (small) subgraphs, dramatically reducing the
computational cost. We perform experiments on both synthetic and real data; our
results demonstrate the effectiveness and efficiency of the proposed approach.
"
"  We propose Dirichlet Process mixtures of Generalized Linear Models (DP-GLM),
a new method of nonparametric regression that accommodates continuous and
categorical inputs, and responses that can be modeled by a generalized linear
model. We prove conditions for the asymptotic unbiasedness of the DP-GLM
regression mean function estimate. We also give examples for when those
conditions hold, including models for compactly supported continuous
distributions and a model with continuous covariates and categorical response.
We empirically analyze the properties of the DP-GLM and why it provides better
results than existing Dirichlet process mixture regression models. We evaluate
DP-GLM on several data sets, comparing it to modern methods of nonparametric
regression like CART, Bayesian trees and Gaussian processes. Compared to
existing techniques, the DP-GLM provides a single model (and corresponding
inference algorithms) that performs well in many regression settings.
"
"  We model anomaly and change in data by embedding the data in an ultrametric
space. Taking our initial data as cross-tabulation counts (or other input data
formats), Correspondence Analysis allows us to endow the information space with
a Euclidean metric. We then model anomaly or change by an induced ultrametric.
The induced ultrametric that we are particularly interested in takes a
sequential - e.g. temporal - ordering of the data into account. We apply this
work to the flow of narrative expressed in the film script of the Casablanca
movie; and to the evolution between 1988 and 2004 of the Colombian social
conflict and violence.
"
"  Estimating the coefficients of a noisy polynomial phase signal is important
in fields including radar, biology and radio communications. One approach
attempts to perform polynomial regression on the phase of the signal. This is
complicated by the fact that the phase is wrapped modulo 2\pi and must be
unwrapped before regression can be performed. In this paper we consider an
estimator that performs phase unwrapping in a least squares manner. We describe
the asymptotic properties of this estimator, showing that it is strongly
consistent and asymptotically normally distributed.
"
"  Power-law distributions contain precious information about a large variety of
processes in geoscience and elsewhere. Although there are sound theoretical
grounds for these distributions, the empirical evidence in favor of power laws
has been traditionally weak. Recently, Clauset et al. have proposed a
systematic method to find over which range (if any) a certain distribution
behaves as a power law. However, their method has been found to fail, in the
sense that true (simulated) power-law tails are not recognized as such in some
instances, and then the power-law hypothesis is rejected. Moreover, the method
does not work well when extended to power-law distributions with an upper
truncation. We explain in detail a similar but alternative procedure, valid for
truncated as well as for non-truncated power-law distributions, based in
maximum likelihood estimation, the Kolmogorov-Smirnov goodness-of-fit test, and
Monte Carlo simulations. An overview of the main concepts as well as a recipe
for their practical implementation is provided. The performance of our method
is put to test on several empirical data which were previously analyzed with
less systematic approaches. The databases presented here include the half-lives
of the radionuclides, the seismic moment of earthquakes in the whole world and
in Southern California, a proxy for the energy dissipated by tropical cyclones
elsewhere, the area burned by forest fires in Italy, and the waiting times
calculated over different spatial subdivisions of Southern California. We find
the functioning of the method very satisfactory.
"
"  This work proposes a way to align statistical modeling with decision making.
We provide a method that propagates the uncertainty in predictive modeling to
the uncertainty in operational cost, where operational cost is the amount spent
by the practitioner in solving the problem. The method allows us to explore the
range of operational costs associated with the set of reasonable statistical
models, so as to provide a useful way for practitioners to understand
uncertainty. To do this, the operational cost is cast as a regularization term
in a learning algorithm's objective function, allowing either an optimistic or
pessimistic view of possible costs, depending on the regularization parameter.
From another perspective, if we have prior knowledge about the operational
cost, for instance that it should be low, this knowledge can help to restrict
the hypothesis space, and can help with generalization. We provide a
theoretical generalization bound for this scenario. We also show that learning
with operational costs is related to robust optimization.
"
"  This technical report is the union of two contributions to the discussion of
the Read Paper ""Riemann manifold Langevin and Hamiltonian Monte Carlo methods""
by B. Calderhead and M. Girolami, presented in front of the Royal Statistical
Society on October 13th 2010 and to appear in the Journal of the Royal
Statistical Society Series B. The first comment establishes a parallel and
possible interactions with Adaptive Monte Carlo methods. The second comment
exposes a detailed study of Riemannian Manifold Hamiltonian Monte Carlo (RMHMC)
for a weakly identifiable model presenting a strong ridge in its geometry.
"
"  In comparative proteomics studies, LC-MS/MS data is generally quantified
using one or both of two measures: the spectral count, derived from the
identification of MS/MS spectra, or some measure of ion abundance derived from
the LC-MS data. Here we contrast the performance of these measures and show
that ion abundance is the more sensitive. We also examine how the conclusions
of a comparative analysis are influenced by the manner in which the LC-MS/MS
data is `rolled up' to the protein level, and show that divergent conclusions
obtained using different rollups can be informative. Our analysis is based on
two publicly available reference data sets, BIATECH-54 and CPTAC, which were
developed for the purpose of assessing methods used in label-free differential
proteomic studies. We find that the use of the ion abundance measure reveals
properties of both data sets not readily apparent using the spectral count.
"
"  High angular resolution diffusion imaging data is the observed characteristic
function for the local diffusion of water molecules in tissue. This data is
used to infer structural information in brain imaging. Nonparametric scalar
measures are proposed to summarize such data, and to locally characterize
spatial features of the diffusion probability density function (PDF), relying
on the geometry of the characteristic function. Summary statistics are defined
so that their distributions are, to first-order, both independent of nuisance
parameters and also analytically tractable. The dominant direction of the
diffusion at a spatial location (voxel) is determined, and a new set of axes
are introduced in Fourier space. Variation quantified in these axes determines
the local spatial properties of the diffusion density. Nonparametric hypothesis
tests for determining whether the diffusion is unimodal, isotropic or
multi-modal are proposed. More subtle characteristics of white-matter
microstructure, such as the degree of anisotropy of the PDF and symmetry
compared with a variety of asymmetric PDF alternatives, may be ascertained
directly in the Fourier domain without parametric assumptions on the form of
the diffusion PDF. We simulate a set of diffusion processes and characterize
their local properties using the newly introduced summaries. We show how
complex white-matter structures across multiple voxels exhibit clear
ellipsoidal and asymmetric structure in simulation, and assess the performance
of the statistics in clinically-acquired magnetic resonance imaging data.
"
"  Standard compressive sensing results state that to exactly recover an s
sparse signal in R^p, one requires O(s. log(p)) measurements. While this bound
is extremely useful in practice, often real world signals are not only sparse,
but also exhibit structure in the sparsity pattern. We focus on
group-structured patterns in this paper. Under this model, groups of signal
coefficients are active (or inactive) together. The groups are predefined, but
the particular set of groups that are active (i.e., in the signal support) must
be learned from measurements. We show that exploiting knowledge of groups can
further reduce the number of measurements required for exact signal recovery,
and derive universal bounds for the number of measurements needed. The bound is
universal in the sense that it only depends on the number of groups under
consideration, and not the particulars of the groups (e.g., compositions,
sizes, extents, overlaps, etc.). Experiments show that our result holds for a
variety of overlapping group configurations.
"
"  In this paper some new experimental results about the statistical
characterization of the non-line-of-sight (NLOS) bias affecting time-of-arrival
(TOA) estimation in ultrawideband (UWB) wireless localization systems are
illustrated. Then, these results are exploited to assess the performance of
various maximum-likelihood (ML) based algorithms for joint TOA localization and
NLOS bias mitigation. Our numerical results evidence that the accuracy of all
the considered algorithms is appreciably influenced by the LOS/NLOS conditions
of the propagation environment.
"
"  We propose a specialized string kernel for small bio-molecules, peptides and
pseudo-sequences of binding interfaces. The kernel incorporates
physico-chemical properties of amino acids and elegantly generalize eight
kernels, such as the Oligo, the Weighted Degree, the Blended Spectrum, and the
Radial Basis Function. We provide a low complexity dynamic programming
algorithm for the exact computation of the kernel and a linear time algorithm
for it's approximation. Combined with kernel ridge regression and SupCK, a
novel binding pocket kernel, the proposed kernel yields biologically relevant
and good prediction accuracy on the PepX database. For the first time, a
machine learning predictor is capable of accurately predicting the binding
affinity of any peptide to any protein. The method was also applied to both
single-target and pan-specific Major Histocompatibility Complex class II
benchmark datasets and three Quantitative Structure Affinity Model benchmark
datasets.
  On all benchmarks, our method significantly (p-value < 0.057) outperforms the
current state-of-the-art methods at predicting peptide-protein binding
affinities. The proposed approach is flexible and can be applied to predict any
quantitative biological activity. The method should be of value to a large
segment of the research community with the potential to accelerate
peptide-based drug and vaccine development.
"
"  We present a novel factor analysis method that can be applied to the
discovery of common factors shared among trajectories in multivariate time
series data. These factors satisfy a precedence-ordering property: certain
factors are recruited only after some other factors are activated.
Precedence-ordering arise in applications where variables are activated in a
specific order, which is unknown. The proposed method is based on a linear
model that accounts for each factor's inherent delays and relative order. We
present an algorithm to fit the model in an unsupervised manner using
techniques from convex and non-convex optimization that enforce sparsity of the
factor scores and consistent precedence-order of the factor loadings. We
illustrate the Order-Preserving Factor Analysis (OPFA) method for the problem
of extracting precedence-ordered factors from a longitudinal (time course)
study of gene expression data.
"
"  Visual perception is a challenging problem in part due to illumination
variations. A possible solution is to first estimate an illumination invariant
representation before using it for recognition. The object albedo and surface
normals are examples of such representations. In this paper, we introduce a
multilayer generative model where the latent variables include the albedo,
surface normals, and the light source. Combining Deep Belief Nets with the
Lambertian reflectance assumption, our model can learn good priors over the
albedo from 2D images. Illumination variations can be explained by changing
only the lighting latent variable in our model. By transferring learned
knowledge from similar objects, albedo and surface normals estimation from a
single image is possible in our model. Experiments demonstrate that our model
is able to generalize as well as improve over standard baselines in one-shot
face recognition.
"
"  We present a new algorithm for exactly solving decision making problems
represented as influence diagrams. We do not require the usual assumptions of
no forgetting and regularity; this allows us to solve problems with
simultaneous decisions and limited information. The algorithm is empirically
shown to outperform a state-of-the-art algorithm on randomly generated problems
of up to 150 variables and $10^{64}$ solutions. We show that the problem is
NP-hard even if the underlying graph structure of the problem has small
treewidth and the variables take on a bounded number of states, but that a
fully polynomial time approximation scheme exists for these cases. Moreover, we
show that the bound on the number of states is a necessary condition for any
efficient approximation scheme.
"
"  While loopy belief propagation (LBP) performs reasonably well for inference
in some Gaussian graphical models with cycles, its performance is
unsatisfactory for many others. In particular for some models LBP does not
converge, and in general when it does converge, the computed variances are
incorrect (except for cycle-free graphs for which belief propagation (BP) is
non-iterative and exact). In this paper we propose {\em feedback message
passing} (FMP), a message-passing algorithm that makes use of a special set of
vertices (called a {\em feedback vertex set} or {\em FVS}) whose removal
results in a cycle-free graph. In FMP, standard BP is employed several times on
the cycle-free subgraph excluding the FVS while a special message-passing
scheme is used for the nodes in the FVS. The computational complexity of exact
inference is $O(k^2n)$, where $k$ is the number of feedback nodes, and $n$ is
the total number of nodes. When the size of the FVS is very large, FMP is
intractable. Hence we propose {\em approximate FMP}, where a pseudo-FVS is used
instead of an FVS, and where inference in the non-cycle-free graph obtained by
removing the pseudo-FVS is carried out approximately using LBP. We show that,
when approximate FMP converges, it yields exact means and variances on the
pseudo-FVS and exact means throughout the remainder of the graph. We also
provide theoretical results on the convergence and accuracy of approximate FMP.
In particular, we prove error bounds on variance computation. Based on these
theoretical results, we design efficient algorithms to select a pseudo-FVS of
bounded size. The choice of the pseudo-FVS allows us to explicitly trade off
between efficiency and accuracy. Experimental results show that using a
pseudo-FVS of size no larger than $\log(n)$, this procedure converges much more
often, more quickly, and provides more accurate results than LBP on the entire
graph.
"
"  A variety of resting state neuroimaging data tend to exhibit fractal behavior
where its power spectrum follows power-law scaling. Resting state functional
connectivity is significantly influenced by fractal behavior which may not
directly originate from neuronal population activities of the brain. To
describe the fractal behavior, we adopted the fractionally integrated process
(FIP) model instead of the fractional Gaussian noise (FGN) since the FIP model
covers more general aspects of fractality than the FGN model. We also introduce
a novel concept called the nonfractal connectivity which is defined as the
correlation of short memory independent of fractal behavior, and compared it
with the fractal connectivity which is an asymptotic wavelet correlation. We
propose several wavelet-based estimators of fractal connectivity and nonfractal
connectivity for a multivariate fractionally integrated noise (mFIN). The
performance of these estimators was evaluated through simulation studies and
the analyses of resting state functional MRI data of the rat brain.
"
"  Principal Component Analysis (PCA) is a commonly used tool for dimension
reduction in analyzing high dimensional data; Multilinear Principal Component
Analysis (MPCA) has the potential to serve the similar function for analyzing
tensor structure data. MPCA and other tensor decomposition methods have been
proved effective to reduce the dimensions for both real data analyses and
simulation studies (Ye, 2005; Lu, Plataniotis and Venetsanopoulos, 2008; Kolda
and Bader, 2009; Li, Kim and Altman, 2010). In this paper, we investigate
MPCA's statistical properties and provide explanations for its advantages.
Conventional PCA, vectorizing the tensor data, may lead to inefficient and
unstable prediction due to its extremely large dimensionality. On the other
hand, MPCA, trying to preserve the data structure, searches for low-dimensional
multilinear projections and decreases the dimensionality efficiently. The
asymptotic theories for order-two MPCA, including asymptotic distributions for
principal components, associated projections and the explained variance, are
developed. Finally, MPCA is shown to improve conventional PCA on analyzing the
{\sf Olivetti Faces} data set, by constructing more module oriented basis in
reconstructing the test faces.
"
"  In the paradigm of multi-task learning, mul- tiple related prediction tasks
are learned jointly, sharing information across the tasks. We propose a
framework for multi-task learn- ing that enables one to selectively share the
information across the tasks. We assume that each task parameter vector is a
linear combi- nation of a finite number of underlying basis tasks. The
coefficients of the linear combina- tion are sparse in nature and the overlap
in the sparsity patterns of two tasks controls the amount of sharing across
these. Our model is based on on the assumption that task pa- rameters within a
group lie in a low dimen- sional subspace but allows the tasks in differ- ent
groups to overlap with each other in one or more bases. Experimental results on
four datasets show that our approach outperforms competing methods.
"
"  We construct a framework for studying clustering algorithms, which includes
two key ideas: persistence and functoriality. The first encodes the idea that
the output of a clustering scheme should carry a multiresolution structure, the
second the idea that one should be able to compare the results of clustering
algorithms as one varies the data set, for example by adding points or by
applying functions to it. We show that within this framework, one can prove a
theorem analogous to one of J. Kleinberg, in which one obtains an existence and
uniqueness theorem instead of a non-existence result. We explore further
properties of this unique scheme, stability and convergence are established.
"
"  Time series analysis of fossil biodiversity of marine invertebrates in the
Paleobiology Database (PBDB) shows a significant periodicity at approximately
63 My, in agreement with previous analyses based on the Sepkoski database. I
discuss how this result did not appear in a previous analysis of the PBDB. The
existence of the 63 My periodicity, despite very different treatment of
systematic error in both PBDB and Sepkoski databases strongly argues for
consideration of its reality in the fossil record. Cross-spectral analysis of
the two datasets finds that a 62 My periodicity coincides in phase by 1.6 My,
equivalent to better than the errors in either measurement. Consequently, the
two data sets not only contain the same strong periodicity, but its peaks and
valleys closely correspond in time. Two other spectral peaks appear in the PBDB
analysis, but appear to be artifacts associated with detrending and with the
increased interval length. Sampling-standardization procedures implemented by
the PBDB collaboration suggest that the signal is not an artifact of sampling
bias. Further work should focus on finding the cause of the 62 My periodicity.
"
"  Rejoinder of ""Treelets--An adaptive multi-scale basis for spare unordered
data"" [arXiv:0707.0481]
"
"  LAGE is a systematic framework developed in Java. The motivation of LAGE is
to provide a scalable and parallel solution to reconstruct Gene Regulatory
Networks (GRNs) from continuous gene expression data for very large amount of
genes. The basic idea of our framework is motivated by the philosophy of
divideand-conquer. Specifically, LAGE recursively partitions genes into
multiple overlapping communities with much smaller sizes, learns
intra-community GRNs respectively before merge them altogether. Besides, the
complete information of overlapping communities serves as the byproduct, which
could be used to mine meaningful functional modules in biological networks.
"
"  We consider the problem of classification using similarity/distance functions
over data. Specifically, we propose a framework for defining the goodness of a
(dis)similarity function with respect to a given learning task and propose
algorithms that have guaranteed generalization properties when working with
such good functions. Our framework unifies and generalizes the frameworks
proposed by [Balcan-Blum ICML 2006] and [Wang et al ICML 2007]. An attractive
feature of our framework is its adaptability to data - we do not promote a
fixed notion of goodness but rather let data dictate it. We show, by giving
theoretical guarantees that the goodness criterion best suited to a problem can
itself be learned which makes our approach applicable to a variety of domains
and problems. We propose a landmarking-based approach to obtaining a classifier
from such learned goodness criteria. We then provide a novel diversity based
heuristic to perform task-driven selection of landmark points instead of random
selection. We demonstrate the effectiveness of our goodness criteria learning
method as well as the landmark selection heuristic on a variety of
similarity-based learning datasets and benchmark UCI datasets on which our
method consistently outperforms existing approaches by a significant margin.
"
"  Color-Magnitude Diagrams (CMDs) are plots that compare the magnitudes
(luminosities) of stars in different wavelengths of light (colors). High
nonlinear correlations among the mass, color, and surface temperature of newly
formed stars induce a long narrow curved point cloud in a CMD known as the main
sequence. Aging stars form new CMD groups of red giants and white dwarfs. The
physical processes that govern this evolution can be described with
mathematical models and explored using complex computer models. These
calculations are designed to predict the plotted magnitudes as a function of
parameters of scientific interest, such as stellar age, mass, and metallicity.
Here, we describe how we use the computer models as a component of a complex
likelihood function in a Bayesian analysis that requires sophisticated
computing, corrects for contamination of the data by field stars, accounts for
complications caused by unresolved binary-star systems, and aims to compare
competing physics-based computer models of stellar evolution.
"
"  In this note we show that the locally stationary wavelet process can be
decomposed into a sum of signals, each of which following a moving average
process with time-varying parameters. We then show that such moving average
processes are equivalent to state space models with stochastic design
components. Using a simple simulation step, we propose a heuristic method of
estimating the above state space models and then we apply the methodology to
foreign exchange rates data.
"
"  We study a regression characterization for the quadratic estimator of weak
lensing, developed by Hu and Okamoto (2001,2002), for cosmic microwave
background observations. This characterization motivates a modification of the
quadratic estimator by an adaptive Wiener filter which uses the robust Bayesian
techniques described in Strawderman (1971) and Berger (1980). This technique
requires the user to propose a fiducial model for the spectral density of the
unknown lensing potential but the resulting estimator is developed to be robust
to misspecification of this model. The role of the fiducial spectral density is
to give the estimator superior statistical performance in a ""neighborhood of
the fiducial model"" while controlling the statistical errors when the fiducial
spectral density is drastically wrong. Our estimate also highlights some
advantages provided by a Bayesian analysis of the quadratic estimator.
"
"  We introduce a proximal version of dual coordinate ascent method. We
demonstrate how the derived algorithmic framework can be used for numerous
regularized loss minimization problems, including $\ell_1$ regularization and
structured output SVM. The convergence rates we obtain match, and sometimes
improve, state-of-the-art results.
"
"  We consider a regularized least squares problem, with regularization by
structured sparsity-inducing norms, which extend the usual $\ell_1$ and the
group lasso penalty, by allowing the subsets to overlap. Such regularizations
lead to nonsmooth problems that are difficult to optimize, and we propose in
this paper a suitable version of an accelerated proximal method to solve them.
We prove convergence of a nested procedure, obtained composing an accelerated
proximal method with an inner algorithm for computing the proximity operator.
By exploiting the geometrical properties of the penalty, we devise a new active
set strategy, thanks to which the inner iteration is relatively fast, thus
guaranteeing good computational performances of the overall algorithm. Our
approach allows to deal with high dimensional problems without pre-processing
for dimensionality reduction, leading to better computational and prediction
performances with respect to the state-of-the art methods, as shown empirically
both on toy and real data.
"
"  Semi-Markov processes (SMPs) provide a rich framework for many real-world
problems. However, due to difficulty implementing practical solutions they are
rarely used with their full capability. The theory of SMPs is quite mature but
was mainly developed at a time when computational resources were not widely
available. With the exception of some of the simplest cases, solutions to SMPs
are inherently numerical, and SMPs have been underutilized by practitioners
because of difficulty implementing the theory in applications. This paper
demonstrates the theory and computational methods needed to implement SMP
models in practical settings. Methods are illustrated with an application
modeling the movement of coronary patients in a hospital. Our aim is to allow
practitioners to use richer SMP models without being burdened with the rigorous
mathematical theory.
"
"  We describe a simple and efficient procedure for approximating the L\'evy
measure of a $\text{Gamma}(\alpha,1)$ random variable. We use this
approximation to derive a finite sum-representation that converges almost
surely to Ferguson's representation of the Dirichlet process based on arrivals
of a homogeneous Poisson process. We compare the efficiency of our
approximation to several other well known approximations of the Dirichlet
process and demonstrate a substantial improvement.
"
"  This article concerns the dimension reduction in regression for large data
set. We introduce a new method based on the sliced inverse regression approach,
called cluster-based regularized sliced inverse regression. Our method not only
keeps the merit of considering both response and predictors' information, but
also enhances the capability of handling highly correlated variables. It is
justified under certain linearity conditions. An empirical application on a
macroeconomic data set shows that our method has outperformed the dynamic
factor model and other shrinkage methods.
"
"  The LASSO is a widely used statistical methodology for simultaneous
estimation and variable selection. In the last years, many authors analyzed
this technique from a theoretical and applied point of view. We introduce and
study the adaptive LASSO problem for discretely observed ergodic diffusion
processes. We prove oracle properties also deriving the asymptotic distribution
of the LASSO estimator. Our theoretical framework is based on the random field
approach and it applied to more general families of regular statistical
experiments in the sense of Ibragimov-Hasminskii (1981). Furthermore, we
perform a simulation and real data analysis to provide some evidence on the
applicability of this method.
"
"  We study the distribution of hard-, soft-, and adaptive soft-thresholding
estimators within a linear regression model where the number of parameters k
can depend on sample size n and may diverge with n. In addition to the case of
known error-variance, we define and study versions of the estimators when the
error-variance is unknown. We derive the finite-sample distribution of each
estimator and study its behavior in the large-sample limit, also investigating
the effects of having to estimate the variance when the degrees of freedom n-k
does not tend to infinity or tends to infinity very slowly. Our analysis
encompasses both the case where the estimators are tuned to perform consistent
model selection and the case where the estimators are tuned to perform
conservative model selection. Furthermore, we discuss consistency, uniform
consistency and derive the uniform convergence rate under either type of
tuning.
"
"  The problem addressed in this article is the bias to income and expenditure
elasticities estimated on pseudo-panel data caused by measurement error and
unobserved heterogeneity. We gauge empirically these biases by comparing
cross-sectional, pseudo-panel and true panel data from both Polish and American
expenditure surveys. Our results suggest that unobserved heterogeneity imparts
a downward bias to cross-section estimates of income elasticities of at-home
food expenditures and an upward bias to estimates of income elasticities of
away-from-home food expenditures. ""Within"" and first-difference estimators
suffer less bias, but only if the effects of measurement error are accounted
for with instrumental variables.
"
"  We consider the problem of sparse variable selection in nonparametric
additive models, with the prior knowledge of the structure among the covariates
to encourage those variables within a group to be selected jointly. Previous
works either study the group sparsity in the parametric setting (e.g., group
lasso), or address the problem in the non-parametric setting without exploiting
the structural information (e.g., sparse additive models). In this paper, we
present a new method, called group sparse additive models (GroupSpAM), which
can handle group sparsity in additive models. We generalize the l1/l2 norm to
Hilbert spaces as the sparsity-inducing penalty in GroupSpAM. Moreover, we
derive a novel thresholding condition for identifying the functional sparsity
at the group level, and propose an efficient block coordinate descent algorithm
for constructing the estimate. We demonstrate by simulation that GroupSpAM
substantially outperforms the competing methods in terms of support recovery
and prediction accuracy in additive models, and also conduct a comparative
experiment on a real breast cancer dataset.
"
"  While learning the maximum likelihood value of parameters of an undirected
graphical model is hard, modelling the posterior distribution over parameters
given data is harder. Yet, undirected models are ubiquitous in computer vision
and text modelling (e.g. conditional random fields). But where Bayesian
approaches for directed models have been very successful, a proper Bayesian
treatment of undirected models in still in its infant stages. We propose a new
method for approximating the posterior of the parameters given data based on
the Laplace approximation. This approximation requires the computation of the
covariance matrix over features which we compute using the linear response
approximation based in turn on loopy belief propagation. We develop the theory
for conditional and 'unconditional' random fields with or without hidden
variables. In the conditional setting we introduce a new variant of bagging
suitable for structured domains. Here we run the loopy max-product algorithm on
a 'super-graph' composed of graphs for individual models sampled from the
posterior and connected by constraints. Experiments on real world data validate
the proposed methods.
"
"  Based on a data set obtained in a dental longitudinal study, conducted in
Flanders (Belgium), the joint time to caries distribution of permanent first
molars was modeled as a function of covariates. This involves an analysis of
multivariate continuous doubly-interval-censored data since: (i) the emergence
time of a tooth and the time it experiences caries were recorded yearly, and
(ii) events on teeth of the same child are dependent. To model the joint
distribution of the emergence times and the times to caries, we propose a
dependent Bayesian semiparametric model. A major feature of the proposed
approach is that survival curves can be estimated without imposing assumptions
such as proportional hazards, additive hazards, proportional odds or
accelerated failure time.
"
"  The spatio-temporal analysis of residential fires could allow decision makers
to plan effective resource allocations in fire management according to fire
clustering levels in space and time. In this study, we provide guidelines for
the use of various methods in detecting the differences in clustering patterns
of fire and non-fire (i.e., background residential) locations and how these
patterns change over time. As a preliminary analysis step, various exploratory
data analysis methods, such as, intensity plots (i.e., kernel density
estimates) are used. Moreover, the use of Diggle's D-function (a second order
analysis technique) is proposed for detecting the clustering of residential
fire locations (if any) and whether there is additional clustering (or
regularity) in the locations of the fires compared to background residential
pattern. A test for trend over time (in years, months, and weeks) of the fire
location patterns are provided with a space-time interaction analysis by
spatio-temporal K-function. Residential fire data from \c{C}ankaya Municipality
of Ankara, Turkey is used as an illustrative example. The presented methodology
is also applicable to residential fire data from similar urban settings.
"
"  In many situations where the interest lies in identifying clusters one might
expect that not all available variables carry information about these groups.
Furthermore, data quality (e.g. outliers or missing entries) might present a
serious and sometimes hard-to-assess problem for large and complex datasets. In
this paper we show that a small proportion of atypical observations might have
serious adverse effects on the solutions found by the sparse clustering
algorithm of Witten and Tibshirani (2010). We propose a robustification of
their sparse K-means algorithm based on the trimmed K-means algorithm of
Cuesta-Albertos et al. (1997) Our proposal is also able to handle datasets with
missing values. We illustrate the use of our method on microarray data for
cancer patients where we are able to identify strong biological clusters with a
much reduced number of genes. Our simulation studies show that, when there are
outliers in the data, our robust sparse K-means algorithm performs better than
other competing methods both in terms of the selection of features and also the
identified clusters. This robust sparse K-means algorithm is implemented in the
R package RSKC which is publicly available from the CRAN repository.
"
"  We study sparse principal components analysis in high dimensions, where $p$
(the number of variables) can be much larger than $n$ (the number of
observations), and analyze the problem of estimating the subspace spanned by
the principal eigenvectors of the population covariance matrix. We introduce
two complementary notions of $\ell_q$ subspace sparsity: row sparsity and
column sparsity. We prove nonasymptotic lower and upper bounds on the minimax
subspace estimation error for $0\leq q\leq1$. The bounds are optimal for row
sparse subspaces and nearly optimal for column sparse subspaces, they apply to
general classes of covariance matrices, and they show that $\ell_q$ constrained
estimates can achieve optimal minimax rates without restrictive spiked
covariance conditions. Interestingly, the form of the rates matches known
results for sparse regression when the effective noise variance is defined
appropriately. Our proof employs a novel variational $\sin\Theta$ theorem that
may be useful in other regularized spectral estimation problems.
"
"  We present a novel method for identifying a biochemical reaction network
based on multiple sets of estimated reaction rates in the corresponding
reaction rate equations arriving from various (possibly different) experiments.
The current method, unlike some of the graphical approaches proposed in the
literature, uses the values of the experimental measurements only relative to
the geometry of the biochemical reactions under the assumption that the
underlying reaction network is the same for all the experiments.
  The proposed approach utilizes algebraic statistical methods in order to
parametrize the set of possible reactions so as to identify the most likely
network structure, and is easily scalable to very complicated biochemical
systems involving a large number of species and reactions. The method is
illustrated with a numerical example of a hypothetical network arising form a
""mass transfer""-type model.
"
"  Various statistical methods important for genetic analysis are considered and
developed. Namely, we concentrate on the multifactor dimensionality reduction,
logic regression, random forests and stochastic gradient boosting. These
methods and their new modifications, e.g., the MDR method with ""independent
rule"", are used to study the risk of complex diseases such as cardiovascular
ones. The roles of certain combinations of single nucleotide polymorphisms and
external risk factors are examined. To perform the data analysis concerning the
ischemic heart disease and myocardial infarction the supercomputer SKIF
""Chebyshev"" of the Lomonosov Moscow State University was employed.
"
"  OpenGM is a C++ template library for defining discrete graphical models and
performing inference on these models, using a wide range of state-of-the-art
algorithms. No restrictions are imposed on the factor graph to allow for
higher-order factors and arbitrary neighborhood structures. Large models with
repetitive structure are handled efficiently because (i) functions that occur
repeatedly need to be stored only once, and (ii) distinct functions can be
implemented differently, using different encodings alongside each other in the
same model. Several parametric functions (e.g. metrics), sparse and dense value
tables are provided and so is an interface for custom C++ code. Algorithms are
separated by design from the representation of graphical models and are easily
exchangeable. OpenGM, its algorithms, HDF5 file format and command line tools
are modular and extendible.
"
"  bnlearn is an R package which includes several algorithms for learning the
structure of Bayesian networks with either discrete or continuous variables.
Both constraint-based and score-based algorithms are implemented, and can use
the functionality provided by the snow package to improve their performance via
parallel computing. Several network scores and conditional independence
algorithms are available for both the learning algorithms and independent use.
Advanced plotting options are provided by the Rgraphviz package.
"
"  This paper studies parallelization schemes for stochastic Vector Quantization
algorithms in order to obtain time speed-ups using distributed resources. We
show that the most intuitive parallelization scheme does not lead to better
performances than the sequential algorithm. Another distributed scheme is
therefore introduced which obtains the expected speed-ups. Then, it is improved
to fit implementation on distributed architectures where communications are
slow and inter-machines synchronization too costly. The schemes are tested with
simulated distributed architectures and, for the last one, with Microsoft
Windows Azure platform obtaining speed-ups up to 32 Virtual Machines.
"
"  We study the distribution of the adaptive LASSO estimator (Zou (2006)) in
finite samples as well as in the large-sample limit. The large-sample
distributions are derived both for the case where the adaptive LASSO estimator
is tuned to perform conservative model selection as well as for the case where
the tuning results in consistent model selection. We show that the
finite-sample as well as the large-sample distributions are typically highly
non-normal, regardless of the choice of the tuning parameter. The uniform
convergence rate is also obtained, and is shown to be slower than $n^{-1/2}$ in
case the estimator is tuned to perform consistent model selection. In
particular, these results question the statistical relevance of the `oracle'
property of the adaptive LASSO estimator established in Zou (2006). Moreover,
we also provide an impossibility result regarding the estimation of the
distribution function of the adaptive LASSO estimator.The theoretical results,
which are obtained for a regression model with orthogonal design, are
complemented by a Monte Carlo study using non-orthogonal regressors.
"
"  The objective of change-point detection is to discover abrupt property
changes lying behind time-series data. In this paper, we present a novel
statistical change-point detection algorithm based on non-parametric divergence
estimation between time-series samples from two retrospective segments. Our
method uses the relative Pearson divergence as a divergence measure, and it is
accurately and efficiently estimated by a method of direct density-ratio
estimation. Through experiments on artificial and real-world datasets including
human-activity sensing, speech, and Twitter messages, we demonstrate the
usefulness of the proposed method.
"
"  Chargaff's second parity rule (CSPR) asserts that the frequencies of short
polynucleotide chains are the same as those of the complementary reversed
chains. Up to now, this hypothesis has only been observed empirically and there
is currently no explanation for its presence in DNA strands. Here we argue that
CSPR is a probabilistic consequence of the reverse complementarity between
paired strands, because the Gibbs distribution associated with the chemical
energy between the bonds satisfies CSPR. We develop a statistical test to study
the validity of CSPR under the Gibbsian assumption and we apply it to a large
set of bacterial genomes taken from the GenBank repository.
"
"  Under the Basel II standards, the Operational Risk (OpRisk) advanced
measurement approach is not prescriptive regarding the class of statistical
model utilised to undertake capital estimation. It has however become well
accepted to utlise a Loss Distributional Approach (LDA) paradigm to model the
individual OpRisk loss process corresponding to the Basel II Business
line/event type. In this paper we derive a novel class of doubly stochastic
alpha-stable family LDA models. These models provide the ability to capture the
heavy tailed loss process typical of OpRisk whilst also providing analytic
expressions for the compound process annual loss density and distributions as
well as the aggregated compound process annual loss models. In particular we
develop models of the annual loss process in two scenarios. The first scenario
considers the loss process with a stochastic intensity parameter, resulting in
an inhomogeneous compound Poisson processes annually. The resulting arrival
process of losses under such a model will have independent counts over
increments within the year. The second scenario considers discretization of the
annual loss process into monthly increments with dependent time increments as
captured by a Binomial process with a stochastic probability of success
changing annually. Each of these models will be coupled under an LDA framework
with heavy-tailed severity models comprised of $\alpha$-stable severities for
the loss amounts per loss event. In this paper we will derive analytic results
for the annual loss distribution density and distribution under each of these
models and study their properties.
"
"  A measure-preserving dynamical system can be approximated by a Markov shift
with a bistochastic matrix. This leads to using empirical stochastic matrices
to measure and estimate properties of stirring protocols. Specifically, the
second largest eigenvalue can be used to statistically decide if a stirring
protocol is weak-mixing, ergodic, or nonergodic. Such hypothesis tests require
appropriate probability distributions. In this paper, we propose using Monte
Carlo empirical probability distributions from unistochastic matrices to
establish critical values. These unistochastic matrices arise from randomly
constructed Householder matrices.
"
"  A database of objects discovered in houses in the Roman city of Pompeii
provides a unique view of ordinary life in an ancient city. Experts have used
this collection to study the structure of Roman households, exploring the
distribution and variability of tasks in architectural spaces, but such
approaches are necessarily affected by modern cultural assumptions. In this
study we present a data-driven approach to household archeology, treating it as
an unsupervised labeling problem. This approach scales to large data sets and
provides a more objective complement to human interpretation.
"
"  We introduce a method for constructing skills capable of solving tasks drawn
from a distribution of parameterized reinforcement learning problems. The
method draws example tasks from a distribution of interest and uses the
corresponding learned policies to estimate the topology of the
lower-dimensional piecewise-smooth manifold on which the skill policies lie.
This manifold models how policy parameters change as task parameters vary. The
method identifies the number of charts that compose the manifold and then
applies non-linear regression in each chart to construct a parameterized skill
by predicting policy parameters from task parameters. We evaluate our method on
an underactuated simulated robotic arm tasked with learning to accurately throw
darts at a parameterized target location.
"
"  We propose a novel probabilistic method for detection of objects in noisy
images. The method uses results from percolation and random graph theories. We
present an algorithm that allows to detect objects of unknown shapes in the
presence of random noise. The algorithm has linear complexity and exponential
accuracy and is appropriate for real-time systems. We prove results on
consistency and algorithmic complexity of our procedure.
"
"  We consider the estimation of the policy gradient in partially observable
Markov decision processes (POMDP) with a special class of structured policies
that are finite-state controllers. We show that the gradient estimation can be
done in the Actor-Critic framework, by making the critic compute a ""value""
function that does not depend on the states of POMDP. This function is the
conditional mean of the true value function that depends on the states. We show
that the critic can be implemented using temporal difference (TD) methods with
linear function approximations, and the analytical results on TD and
Actor-Critic can be transfered to this case. Although Actor-Critic algorithms
have been used extensively in Markov decision processes (MDP), up to now they
have not been proposed for POMDP as an alternative to the earlier proposal
GPOMDP algorithm, an actor-only method. Furthermore, we show that the same idea
applies to semi-Markov problems with a subset of finite-state controllers.
"
"  Although there are many methods for functional data analysis (FDA), little
emphasis is put on characterizing variability among volatilities of individual
functions. In particular, certain individuals exhibit erratic swings in their
trajectory while other individuals have more stable trajectories. There is
evidence of such volatility heterogeneity in blood pressure trajectories during
pregnancy, for example, and reason to suspect that volatility is a biologically
important feature. Most FDA models implicitly assume similar or identical
smoothness of the individual functions, and hence can lead to misleading
inferences on volatility and an inadequate representation of the functions. We
propose a novel class of FDA models characterized using hierarchical stochastic
differential equations. We model the derivatives of a mean function and
deviation functions using Gaussian processes, while also allowing covariate
dependence including on the volatilities of the deviation functions. Following
a Bayesian approach to inference, a Markov chain Monte Carlo algorithm is used
for posterior computation. The methods are tested on simulated data and applied
to blood pressure trajectories during pregnancy.
"
"  This paper presents a general stochastic model developed for a class of
cooperative wireless relay networks, in which imperfect knowledge of the
channel state information at the destination node is assumed. The framework
incorporates multiple relay nodes operating under general known non-linear
processing functions. When a non-linear relay function is considered, the
likelihood function is generally intractable resulting in the maximum
likelihood and the maximum a posteriori detectors not admitting closed form
solutions. We illustrate our methodology to overcome this intractability under
the example of a popular optimal non-linear relay function choice and
demonstrate how our algorithms are capable of solving the previously
intractable detection problem. Overcoming this intractability involves
development of specialised Bayesian models. We develop three novel algorithms
to perform detection for this Bayesian model, these include a Markov chain
Monte Carlo Approximate Bayesian Computation (MCMC-ABC) approach; an Auxiliary
Variable MCMC (MCMC-AV) approach; and a Suboptimal Exhaustive Search Zero
Forcing (SES-ZF) approach. Finally, numerical examples comparing the symbol
error rate (SER) performance versus signal to noise ratio (SNR) of the three
detection algorithms are studied in simulated examples.
"
"  This paper focuses on obtaining clustering information about a distribution
from its i.i.d. samples. We develop theoretical results to understand and use
clustering information contained in the eigenvectors of data adjacency matrices
based on a radial kernel function with a sufficiently fast tail decay. In
particular, we provide population analyses to gain insights into which
eigenvectors should be used and when the clustering information for the
distribution can be recovered from the sample. We learn that a fixed number of
top eigenvectors might at the same time contain redundant clustering
information and miss relevant clustering information. We use this insight to
design the data spectroscopic clustering (DaSpec) algorithm that utilizes
properly selected eigenvectors to determine the number of clusters
automatically and to group the data accordingly. Our findings extend the
intuitions underlying existing spectral techniques such as spectral clustering
and Kernel Principal Components Analysis, and provide new understanding into
their usability and modes of failure. Simulation studies and experiments on
real-world data are conducted to show the potential of our algorithm. In
particular, DaSpec is found to handle unbalanced groups and recover clusters of
different shapes better than the competing methods.
"
"  We describe work in progress by a collaboration of astronomers and
statisticians developing a suite of Bayesian data analysis tools for extrasolar
planet (exoplanet) detection, planetary orbit estimation, and adaptive
scheduling of observations. Our work addresses analysis of stellar reflex
motion data, where a planet is detected by observing the ""wobble"" of its host
star as it responds to the gravitational tug of the orbiting planet. Newtonian
mechanics specifies an analytical model for the resulting time series, but it
is strongly nonlinear, yielding complex, multimodal likelihood functions; it is
even more complex when multiple planets are present. The parameter spaces range
in size from few-dimensional to dozens of dimensions, depending on the number
of planets in the system, and the type of motion measured (line-of-sight
velocity, or position on the sky). Since orbits are periodic, Bayesian
generalizations of periodogram methods facilitate the analysis. This relies on
the model being linearly separable, enabling partial analytical
marginalization, reducing the dimension of the parameter space. Subsequent
analysis uses adaptive Markov chain Monte Carlo methods and adaptive importance
sampling to perform the integrals required for both inference (planet detection
and orbit measurement), and information-maximizing sequential design (for
adaptive scheduling of observations). We present an overview of our current
techniques and highlight directions being explored by ongoing research.
"
"  We consider the classic problem of estimating T, the total number of species
in a population, from repeated counts in a simple random sample. We look first
at the Chao-Lee estimator: we initially show that such estimator can be
obtained by reconciling two estimators of the unobserved probability, and then
develop a sequence of improvements culminating in a Dirichlet prior Bayesian
reinterpretation of the estimation problem. By means of this, we obtain
simultaneous estimates of T, of the normalized interspecies variance $\gamma^2$
and of the parameter $\lambda$ of the prior. Several simulations show that our
estimation method is more flexible than several known methods we used as
comparison; the only limitation, apparently shared by all other methods, seems
to be that it cannot deal with the rare cases in which $\gamma^2 >1$
"
"  In this paper, we consider the Integrated Completed Likelihood (ICL) as a
useful criterion for estimating the number of changes in the underlying
distribution of data in problems where detecting the precise location of these
changes is the main goal. The exact computation of the ICL requires O(Kn2)
operations (with K the number of segments and n the number of data-points)
which is prohibitive in many practical situations with large sequences of data.
We describe a framework to estimate the ICL with O(Kn) complexity. Our approach
is general in the sense that it can accommodate any given model distribution.
We checked the run-time and validity of our approach on simulated data and
demonstrate its good performance when analyzing real Next-Generation Sequencing
(NGS) data using a negative binomial model.
"
"  Prediction markets show considerable promise for developing flexible
mechanisms for machine learning. Here, machine learning markets for
multivariate systems are defined, and a utility-based framework is established
for their analysis. This differs from the usual approach of defining static
betting functions. It is shown that such markets can implement model
combination methods used in machine learning, such as product of expert and
mixture of expert approaches as equilibrium pricing models, by varying agent
utility functions. They can also implement models composed of local potentials,
and message passing methods. Prediction markets also allow for more flexible
combinations, by combining multiple different utility functions. Conversely,
the market mechanisms implement inference in the relevant probabilistic models.
This means that market mechanism can be utilized for implementing parallelized
model building and inference for probabilistic modelling.
"
"  Constraint-based (CB) learning is a formalism for learning a causal network
with a database D by performing a series of conditional-independence tests to
infer structural information. This paper considers a new test of independence
that combines ideas from Bayesian learning, Bayesian network inference, and
classical hypothesis testing to produce a more reliable and robust test. The
new test can be calculated in the same asymptotic time and space required for
the standard tests such as the chi-squared test, but it allows the
specification of a prior distribution over parameters and can be used when the
database is incomplete. We prove that the test is correct, and we demonstrate
empirically that, when used with a CB causal discovery algorithm with
noninformative priors, it recovers structural features more reliably and it
produces networks with smaller KL-Divergence, especially as the number of nodes
increases or the number of records decreases. Another benefit is the dramatic
reduction in the probability that a CB algorithm will stall during the search,
providing a remedy for an annoying problem plaguing CB learning when the
database is small.
"
"  Probe-level microarray data are usually stored in matrices, where the row and
column correspond to array and probe, respectively. Scientists routinely
summarize each array by a single index as the expression level of each probe
set (gene). We examine the adequacy of a unidimensional summary for
characterizing the data matrix of each probe set. To do so, we propose a
low-rank matrix model for the probe-level intensities, and develop a useful
framework for testing the adequacy of unidimensionality against targeted
alternatives. This is an interesting statistical problem where inference has to
be made based on one data matrix whose entries are not i.i.d. We analyze the
asymptotic properties of the proposed test statistics, and use Monte Carlo
simulations to assess their small sample performance. Applications of the
proposed tests to GeneChip data show that evidence against a unidimensional
model is often indicative of practically relevant features of a probe set.
"
"  This paper introduces a new method for semi-supervised learning on high
dimensional nonlinear manifolds, which includes a phase of unsupervised basis
learning and a phase of supervised function learning. The learned bases provide
a set of anchor points to form a local coordinate system, such that each data
point $x$ on the manifold can be locally approximated by a linear combination
of its nearby anchor points, with the linear weights offering a
local-coordinate coding of $x$. We show that a high dimensional nonlinear
function can be approximated by a global linear function with respect to this
coding scheme, and the approximation quality is ensured by the locality of such
coding. The method turns a difficult nonlinear learning problem into a simple
global linear learning problem, which overcomes some drawbacks of traditional
local learning methods. The work also gives a theoretical justification to the
empirical success of some biologically-inspired models using sparse coding of
sensory data, since a local coding scheme must be sufficiently sparse. However,
sparsity does not always satisfy locality conditions, and can thus possibly
lead to suboptimal results. The properties and performances of the method are
empirically verified on synthetic data, handwritten digit classification, and
object recognition tasks.
"
"  Clustering analysis is one of the most widely used statistical tools in many
emerging areas such as microarray data analysis. For microarray and other
high-dimensional data, the presence of many noise variables may mask underlying
clustering structures. Hence removing noise variables via variable selection is
necessary. For simultaneous variable selection and parameter estimation,
existing penalized likelihood approaches in model-based clustering analysis all
assume a common diagonal covariance matrix across clusters, which however may
not hold in practice. To analyze high-dimensional data, particularly those with
relatively low sample sizes, this article introduces a novel approach that
shrinks the variances together with means, in a more general situation with
cluster-specific (diagonal) covariance matrices. Furthermore, selection of
grouped variables via inclusion or exclusion of a group of variables altogether
is permitted by a specific form of penalty, which facilitates incorporating
subject-matter knowledge, such as gene functions in clustering microarray
samples for disease subtype discovery. For implementation, EM algorithms are
derived for parameter estimation, in which the M-steps clearly demonstrate the
effects of shrinkage and thresholding. Numerical examples, including an
application to acute leukemia subtype discovery with microarray gene expression
data, are provided to demonstrate the utility and advantage of the proposed
method.
"
"  We investigate the issue of model selection and the use of the nonconformity
(strangeness) measure in batch learning. Using the nonconformity measure we
propose a new training algorithm that helps avoid the need for Cross-Validation
or Leave-One-Out model selection strategies. We provide a new generalisation
error bound using the notion of nonconformity to upper bound the loss of each
test example and show that our proposed approach is comparable to standard
model selection methods, but with theoretical guarantees of success and faster
convergence. We demonstrate our novel model selection technique using the
Support Vector Machine.
"
"  Biological data objects often have both of the following features: (i) they
are functions rather than single numbers or vectors, and (ii) they are
correlated due to phylogenetic relationships. In this paper we give a flexible
statistical model for such data, by combining assumptions from phylogenetics
with Gaussian processes. We describe its use as a nonparametric Bayesian prior
distribution, both for prediction (placing posterior distributions on ancestral
functions) and model selection (comparing rates of evolution across a
phylogeny, or identifying the most likely phylogenies consistent with the
observed data). Our work is integrative, extending the popular phylogenetic
Brownian Motion and Ornstein-Uhlenbeck models to functional data and Bayesian
inference, and extending Gaussian Process regression to phylogenies. We provide
a brief illustration of the application of our method.
"
"  Neurons perform computations, and convey the results of those computations
through the statistical structure of their output spike trains. Here we present
a practical method, grounded in the information-theoretic analysis of
prediction, for inferring a minimal representation of that structure and for
characterizing its complexity. Starting from spike trains, our approach finds
their causal state models (CSMs), the minimal hidden Markov models or
stochastic automata capable of generating statistically identical time series.
We then use these CSMs to objectively quantify both the generalizable structure
and the idiosyncratic randomness of the spike train. Specifically, we show that
the expected algorithmic information content (the information needed to
describe the spike train exactly) can be split into three parts describing (1)
the time-invariant structure (complexity) of the minimal spike-generating
process, which describes the spike train statistically; (2) the randomness
(internal entropy rate) of the minimal spike-generating process; and (3) a
residual pure noise term not described by the minimal spike-generating process.
We use CSMs to approximate each of these quantities. The CSMs are inferred
nonparametrically from the data, making only mild regularity assumptions, via
the causal state splitting reconstruction algorithm. The methods presented here
complement more traditional spike train analyses by describing not only spiking
probability and spike train entropy, but also the complexity of a spike train's
structure. We demonstrate our approach using both simulated spike trains and
experimental data recorded in rat barrel cortex during vibrissa stimulation.
"
"  In this paper we propose a computationally efficient algorithm for on-line
variable selection in multivariate regression problems involving high
dimensional data streams. The algorithm recursively extracts all the latent
factors of a partial least squares solution and selects the most important
variables for each factor. This is achieved by means of only one sparse
singular value decomposition which can be efficiently updated on-line and in an
adaptive fashion. Simulation results based on artificial data streams
demonstrate that the algorithm is able to select important variables in dynamic
settings where the correlation structure among the observed streams is governed
by a few hidden components and the importance of each variable changes over
time. We also report on an application of our algorithm to a multivariate
version of the ""enhanced index tracking"" problem using financial data streams.
The application consists of performing on-line asset allocation with the
objective of overperforming two benchmark indices simultaneously.
"
"  Gaussian graphical models are semi-algebraic subsets of the cone of positive
definite covariance matrices. Submatrices with low rank correspond to
generalizations of conditional independence constraints on collections of
random variables. We give a precise graph-theoretic characterization of when
submatrices of the covariance matrix have small rank for a general class of
mixed graphs that includes directed acyclic and undirected graphs as special
cases. Our new trek separation criterion generalizes the familiar
$d$-separation criterion. Proofs are based on the trek rule, the resulting
matrix factorizations and classical theorems of algebraic combinatorics on the
expansions of determinants of path polynomials.
"
"  Music genre classification is an essential tool for music information
retrieval systems and it has been finding critical applications in various
media platforms. Two important problems of the automatic music genre
classification are feature extraction and classifier design. This paper
investigates inter-genre similarity modelling (IGS) to improve the performance
of automatic music genre classification. Inter-genre similarity information is
extracted over the mis-classified feature population. Once the inter-genre
similarity is modelled, elimination of the inter-genre similarity reduces the
inter-genre confusion and improves the identification rates. Inter-genre
similarity modelling is further improved with iterative IGS modelling(IIGS) and
score modelling for IGS elimination(SMIGS). Experimental results with promising
classification improvements are provided.
"
"  Our research focuses on analysing human activities according to a known
behaviorist scenario, in case of noisy and high dimensional collected data. The
data come from the monitoring of patients with dementia diseases by wearable
cameras. We define a structural model of video recordings based on a Hidden
Markov Model. New spatio-temporal features, color features and localization
features are proposed as observations. First results in recognition of
activities are promising.
"
"  This is a discussion of paper ""Treelets--An adaptive multi-scale basis for
sparse unordered data"" [arXiv:0707.0481] by Ann B. Lee, Boaz Nadler and Larry
Wasserman. In this paper the authors defined a new type of dimension reduction
algorithm, namely, the treelet algorithm. The treelet method has the merit of
being completely data driven, and its decomposition is easier to interpret as
compared to PCR. It is suitable in some certain situations, but it also has its
own limitations. I will discuss both the strength and the weakness of this
method when applied to microarray data analysis.
"
"  We describe a method for inferring linear causal relations among
multi-dimensional variables. The idea is to use an asymmetry between the
distributions of cause and effect that occurs if both the covariance matrix of
the cause and the structure matrix mapping cause to the effect are
independently chosen. The method works for both stochastic and deterministic
causal relations, provided that the dimensionality is sufficiently high (in
some experiments, 5 was enough). It is applicable to Gaussian as well as
non-Gaussian data.
"
"  We establish a general formula for the distribution of the score in table
tennis. We use this formula to derive the probability distribution (and hence
the expectation and variance) of the number of rallies necessary to achieve any
given score. We use these findings to investigate the dependence of these
quantities on the different parameters involved (number of points needed to win
a set, number of consecutive serves, etc.), with particular focus on the rule
change imposed in 2001 by the International Table Tennis Federation (ITTF).
Finally we briefly indicate how our results can lead to more efficient
estimation techniques of individual players' abilities.
"
"  This Paper outlines study behaviour of rotating shaft with high speed under
thermal effects. The method of obtaining the frequency response functions of a
rotor system with study whirl effect in this revision the raw data obtained
from the experimental results (using Smart Office program) are curve-fitted by
theoretical data regenerated from some of the experimental data and simulating
it using finite element (ANSYS 12). (FE) models using the Eigen analysis
capability were used to simulate the vibration. The results were compared with
experimental data show analysis data with acceptable accuracy and performance.
The rotating effect causes un-symmetry in the system matrices, resulting in
complexity in decoupling the mathematical models of the system for the purpose
of modal analysis. Different method is therefore required, which can handle
general system matrices rather than symmetrical matrices, which is normal for
passive structures. Mathematical model of the system from the test data can be
assembled. The frequency response functions are extracted, Campbell diagram are
draw and simulated. (FE) is used to carry out such as simulation since it has
good capability for Eigen analysis and also good graphical facility.
  Keywords: Thermal effects, Modelling, Campbell diagram, Whirl, Rotor
dynamics.
"
"  Functional data analysis involves data described by regular functions rather
than by a finite number of real valued variables. While some robust data
analysis methods can be applied directly to the very high dimensional vectors
obtained from a fine grid sampling of functional data, all methods benefit from
a prior simplification of the functions that reduces the redundancy induced by
the regularity. In this paper we propose to use a clustering approach that
targets variables rather than individual to design a piecewise constant
representation of a set of functions. The contiguity constraint induced by the
functional nature of the variables allows a polynomial complexity algorithm to
give the optimal solution.
"
"  Accurate forecasting of zero coupon bond yields for a continuum of maturities
is paramount to bond portfolio management and derivative security pricing. Yet
a universal model for yield curve forecasting has been elusive, and prior
attempts often resulted in a trade-off between goodness of fit and consistency
with economic theory. To address this, herein we propose a novel formulation
which connects the dynamic factor model (DFM) framework with concepts from
functional data analysis: a DFM with functional factor loading curves. This
results in a model capable of forecasting functional time series. Further, in
the yield curve context we show that the model retains economic interpretation.
Model estimation is achieved through an expectation-maximization algorithm,
where the time series parameters and factor loading curves are simultaneously
estimated in a single step. Efficient computing is implemented and a
data-driven smoothing parameter is nicely incorporated. We show that our model
performs very well on forecasting actual yield data compared with existing
approaches, especially in regard to profit-based assessment for an innovative
trading exercise. We further illustrate the viability of our model to
applications outside of yield forecasting.
"
"  This paper addresses the problem of recovering the spatial market potential
of a retail product from spatially distributed sales data. In order to tackle
the problem in a general way, the concept of spatial potential is introduced.
The potential is concurrently measured at different spatial locations and the
measurements are analyzed in order to recover the spatial potential. The
measuring instruments used to collect the data interact with each other, that
is, the measurement at a given spatial location is affected by the concurrent
measurements at other locations. An approach based on a novel geostatistical
model is developed. In particular, the model is able to handle both the
measuring instrument interaction and the missing data. A model estimation
procedure based on the expectation-maximization algorithm is provided as well
as standard inferential tools. The model is applied to the estimation of the
spatial market potential of a newspaper for the city of Bergamo, Italy. The
estimated spatial market potential is eventually analyzed in order to identify
the areas with the highest potential, to identify the areas where it is
profitable to open additional newsstands and to evaluate the newspaper total
market volume of the city.
"
"  Differential privacy is a framework for privately releasing summaries of a
database. Previous work has focused mainly on methods for which the output is a
finite dimensional vector, or an element of some discrete set. We develop
methods for releasing functions while preserving differential privacy.
Specifically, we show that adding an appropriate Gaussian process to the
function of interest yields differential privacy. When the functions lie in the
same RKHS as the Gaussian process, then the correct noise level is established
by measuring the ""sensitivity"" of the function in the RKHS norm. As examples we
consider kernel density estimation, kernel support vector machines, and
functions in reproducing kernel Hilbert spaces.
"
"  We propose a non-standard subsampling procedure to make formal statistical
inference about the business cycle, one of the most important unobserved
feature characterising fluctuations of economic growth. We show that some
characteristics of business cycle can be modelled in a non-parametric way by
discrete spectrum of the Almost Periodically Correlated (APC) time series. On
the basis of estimated characteristics of this spectrum business cycle is
extracted by filtering. As an illustration we characterise the man properties
of business cycles in industrial production index for Polish economy.
"
"  As the penetration of intermittent energy sources grows substantially, loads
will be required to play an increasingly important role in compensating the
fast time-scale fluctuations in generated power. Recent numerical modeling of
thermostatically controlled loads (TCLs) has demonstrated that such load
following is feasible, but analytical models that satisfactorily quantify the
aggregate power consumption of a group of TCLs are desired to enable controller
design. We develop such a model for the aggregate power response of a
homogeneous population of TCLs to uniform variation of all TCL setpoints. A
linearized model of the response is derived, and a linear quadratic regulator
(LQR) has been designed. Using the TCL setpoint as the control input, the LQR
enables aggregate power to track reference signals that exhibit step, ramp and
sinusoidal variations. Although much of the work assumes a homogeneous
population of TCLs with deterministic dynamics, we also propose a method for
probing the dynamics of systems where load characteristics are not well known.
"
"  We study the mixtures of factorizing probability distributions represented as
visible marginal distributions in stochastic layered networks. We take the
perspective of kernel transitions of distributions, which gives a unified
picture of distributed representations arising from Deep Belief Networks (DBN)
and other networks without lateral connections. We describe combinatorial and
geometric properties of the set of kernels and products of kernels realizable
by DBNs as the network parameters vary. We describe explicit classes of
probability distributions, including exponential families, that can be learned
by DBNs. We use these submodels to bound the maximal and the expected
Kullback-Leibler approximation errors of DBNs from above depending on the
number of hidden layers and units that they contain.
"
"  We study the problem of learning a latent tree graphical model where samples
are available only from a subset of variables. We propose two consistent and
computationally efficient algorithms for learning minimal latent trees, that
is, trees without any redundant hidden nodes. Unlike many existing methods, the
observed nodes (or variables) are not constrained to be leaf nodes. Our first
algorithm, recursive grouping, builds the latent tree recursively by
identifying sibling groups using so-called information distances. One of the
main contributions of this work is our second algorithm, which we refer to as
CLGrouping. CLGrouping starts with a pre-processing procedure in which a tree
over the observed variables is constructed. This global step groups the
observed nodes that are likely to be close to each other in the true latent
tree, thereby guiding subsequent recursive grouping (or equivalent procedures)
on much smaller subsets of variables. This results in more accurate and
efficient learning of latent trees. We also present regularized versions of our
algorithms that learn latent tree approximations of arbitrary distributions. We
compare the proposed algorithms to other methods by performing extensive
numerical experiments on various latent tree graphical models such as hidden
Markov models and star graphs. In addition, we demonstrate the applicability of
our methods on real-world datasets by modeling the dependency structure of
monthly stock returns in the S&P index and of the words in the 20 newsgroups
dataset.
"
"  Bivariate linear mixed models are useful when analyzing longitudinal data of
two associated markers. In this paper, we present a bivariate linear mixed
model including random effects or first-order auto-regressive process and
independent measurement error for both markers. Codes and tricks to fit these
models using SAS Proc MIXED are provided. Limitations of this program are
discussed and an example in the field of HIV infection is shown. Despite some
limitations, SAS Proc MIXED is a useful tool that may be easily extendable to
multivariate response in longitudinal studies.
"
"  Since the early days of digital communication, Hidden Markov Models (HMMs)
have now been routinely used in speech recognition, processing of natural
languages, images, and in bioinformatics. An HMM $(X_i,Y_i)_{i\ge 1}$ assumes
observations $X_1,X_2,...$ to be conditionally independent given an
""explanotary"" Markov process $Y_1,Y_2,...$, which itself is not observed;
moreover, the conditional distribution of $X_i$ depends solely on $Y_i$.
Central to the theory and applications of HMM is the Viterbi algorithm to find
{\em a maximum a posteriori} estimate $q_{1:n}=(q_1,q_2,...,q_n)$ of $Y_{1:n}$
given the observed data $x_{1:n}$. Maximum {\em a posteriori} paths are also
called Viterbi paths or alignments. Recently, attempts have been made to study
the behavior of Viterbi alignments of HMMs with two hidden states when $n$
tends to infinity. It has indeed been shown that in some special cases a
well-defined limiting Viterbi alignment exists. While innovative, these
attempts have relied on rather strong assumptions. This work proves the
existence of infinite Viterbi alignments for virtually any HMM with two hidden
states.
"
"  GRavitational lEnsing Accuracy Testing 2010 (GREAT10) is a public image
analysis challenge aimed at the development of algorithms to analyze
astronomical images. Specifically, the challenge is to measure varying image
distortions in the presence of a variable convolution kernel, pixelization and
noise. This is the second in a series of challenges set to the astronomy,
computer science and statistics communities, providing a structured environment
in which methods can be improved and tested in preparation for planned
astronomical surveys. GREAT10 extends upon previous work by introducing
variable fields into the challenge. The ""Galaxy Challenge"" involves the precise
measurement of galaxy shape distortions, quantified locally by two parameters
called shear, in the presence of a known convolution kernel. Crucially, the
convolution kernel and the simulated gravitational lensing shape distortion
both now vary as a function of position within the images, as is the case for
real data. In addition, we introduce the ""Star Challenge"" that concerns the
reconstruction of a variable convolution kernel, similar to that in a typical
astronomical observation. This document details the GREAT10 Challenge for
potential participants. Continually updated information is also available from
http://www.greatchallenges.info.
"
"  Motivated by multi-task machine learning with Banach spaces, we propose the
notion of vector-valued reproducing kernel Banach spaces (RKBS). Basic
properties of the spaces and the associated reproducing kernels are
investigated. We also present feature map constructions and several concrete
examples of vector-valued RKBS. The theory is then applied to multi-task
machine learning. Especially, the representer theorem and characterization
equations for the minimizer of regularized learning schemes in vector-valued
RKBS are established.
"
"  This paper is devoted to the problem of sampling Gaussian fields in high
dimension. Solutions exist for two specific structures of inverse covariance :
sparse and circulant. The proposed approach is valid in a more general case and
especially as it emerges in inverse problems. It relies on a
perturbation-optimization principle: adequate stochastic perturbation of a
criterion and optimization of the perturbed criterion. It is shown that the
criterion minimizer is a sample of the target density. The motivation in
inverse problems is related to general (non-convolutive) linear observation
models and their resolution in a Bayesian framework implemented through
sampling algorithms when existing samplers are not feasible. It finds a direct
application in myopic and/or unsupervised inversion as well as in some
non-Gaussian inversion. An illustration focused on hyperparameter estimation
for super-resolution problems assesses the effectiveness of the proposed
approach.
"
"  Model-based Bayesian Reinforcement Learning (BRL) allows a found
formalization of the problem of acting optimally while facing an unknown
environment, i.e., avoiding the exploration-exploitation dilemma. However,
algorithms explicitly addressing BRL suffer from such a combinatorial explosion
that a large body of work relies on heuristic algorithms. This paper introduces
BOLT, a simple and (almost) deterministic heuristic algorithm for BRL which is
optimistic about the transition function. We analyze BOLT's sample complexity,
and show that under certain parameters, the algorithm is near-optimal in the
Bayesian sense with high probability. Then, experimental results highlight the
key differences of this method compared to previous work.
"
"  Monte Carlo methods are often necessary for the implementation of optimal
Bayesian estimators. A fundamental technique that can be used to generate
samples from virtually any target probability distribution is the so-called
rejection sampling method, which generates candidate samples from a proposal
distribution and then accepts them or not by testing the ratio of the target
and proposal densities. The class of adaptive rejection sampling (ARS)
algorithms is particularly interesting because they can achieve high acceptance
rates. However, the standard ARS method can only be used with log-concave
target densities. For this reason, many generalizations have been proposed.
  In this work, we investigate two different adaptive schemes that can be used
to draw exactly from a large family of univariate probability density functions
(pdf's), not necessarily log-concave, possibly multimodal and with tails of
arbitrary concavity. These techniques are adaptive in the sense that every time
a candidate sample is rejected, the acceptance rate is improved. The two
proposed algorithms can work properly when the target pdf is multimodal, with
first and second derivatives analytically intractable, and when the tails are
log-convex in a infinite domain. Therefore, they can be applied in a number of
scenarios in which the other generalizations of the standard ARS fail. Two
illustrative numerical examples are shown.
"
"  Unsupervised classification methods learn a discriminative classifier from
unlabeled data, which has been proven to be an effective way of simultaneously
clustering the data and training a classifier from the data. Various
unsupervised classification methods obtain appealing results by the classifiers
learned in an unsupervised manner. However, existing methods do not consider
the misclassification error of the unsupervised classifiers except unsupervised
SVM, so the performance of the unsupervised classifiers is not fully evaluated.
In this work, we study the misclassification error of two popular classifiers,
i.e. the nearest neighbor classifier (NN) and the plug-in classifier, in the
setting of unsupervised classification.
"
"  We consider the problem of learning the structure of undirected graphical
models with bounded treewidth, within the maximum likelihood framework. This is
an NP-hard problem and most approaches consider local search techniques. In
this paper, we pose it as a combinatorial optimization problem, which is then
relaxed to a convex optimization problem that involves searching over the
forest and hyperforest polytopes with special structures, independently. A
supergradient method is used to solve the dual problem, with a run-time
complexity of $O(k^3 n^{k+2} \log n)$ for each iteration, where $n$ is the
number of variables and $k$ is a bound on the treewidth. We compare our
approach to state-of-the-art methods on synthetic datasets and classical
benchmarks, showing the gains of the novel convex approach.
"
"  We consider the reconstruction problem in compressed sensing in which the
observations are recorded in a finite number of bits. They may thus contain
quantization errors (from being rounded to the nearest representable value) and
saturation errors (from being outside the range of representable values). Our
formulation has an objective of weighted $\ell_2$-$\ell_1$ type, along with
constraints that account explicitly for quantization and saturation errors, and
is solved with an augmented Lagrangian method. We prove a consistency result
for the recovered solution, stronger than those that have appeared to date in
the literature, showing in particular that asymptotic consistency can be
obtained without oversampling. We present extensive computational comparisons
with formulations proposed previously, and variants thereof.
"
"  The observations in many applications consist of counts of discrete events,
such as photons hitting a detector, which cannot be effectively modeled using
an additive bounded or Gaussian noise model, and instead require a Poisson
noise model. As a result, accurate reconstruction of a spatially or temporally
distributed phenomenon (f*) from Poisson data (y) cannot be effectively
accomplished by minimizing a conventional penalized least-squares objective
function. The problem addressed in this paper is the estimation of f* from y in
an inverse problem setting, where (a) the number of unknowns may potentially be
larger than the number of observations and (b) f* admits a sparse
approximation. The optimization formulation considered in this paper uses a
penalized negative Poisson log-likelihood objective function with nonnegativity
constraints (since Poisson intensities are naturally nonnegative). In
particular, the proposed approach incorporates key ideas of using separable
quadratic approximations to the objective function at each iteration and
penalization terms related to l1 norms of coefficient vectors, total variation
seminorms, and partition-based multiscale estimation methods.
"
"  This paper examines the problem of estimating the parameters of a bandlimited
signal from samples corrupted by random jitter (timing noise) and additive iid
Gaussian noise, where the signal lies in the span of a finite basis. For the
presented classical estimation problem, the Cramer-Rao lower bound (CRB) is
computed, and an Expectation-Maximization (EM) algorithm approximating the
maximum likelihood (ML) estimator is developed. Simulations are performed to
study the convergence properties of the EM algorithm and compare the
performance both against the CRB and a basic linear estimator. These
simulations demonstrate that by post-processing the jittered samples with the
proposed EM algorithm, greater jitter can be tolerated, potentially reducing
on-chip ADC power consumption substantially.
"
"  We propose an algorithm for simultaneously detecting and locating
changepoints in a time series, and a framework for predicting the distribution
of the next point in the series. The kernel of the algorithm is a system of
equations that computes, for each index i, the probability that the last (most
recent) change point occurred at i. We evaluate this algorithm by applying it
to the change point detection problem and comparing it to the generalized
likelihood ratio (GLR) algorithm. We find that our algorithm is as good as GLR,
or better, over a wide range of scenarios, and that the advantage increases as
the signal-to-noise ratio decreases.
"
"  We demonstrate that a number of sociology models for social network dynamics
can be viewed as continuous time Bayesian networks (CTBNs). A sampling-based
approximate inference method for CTBNs can be used as the basis of an
expectation-maximization procedure that achieves better accuracy in estimating
the parameters of the model than the standard method of moments
algorithmfromthe sociology literature. We extend the existing social network
models to allow for indirect and asynchronous observations of the links. A
Markov chain Monte Carlo sampling algorithm for this new model permits
estimation and inference. We provide results on both a synthetic network (for
verification) and real social network data.
"
"  Dimension reduction and variable selection are performed routinely in
case-control studies, but the literature on the theoretical aspects of the
resulting estimates is scarce. We bring our contribution to this literature by
studying estimators obtained via L1 penalized likelihood optimization. We show
that the optimizers of the L1 penalized retrospective likelihood coincide with
the optimizers of the L1 penalized prospective likelihood. This extends the
results of Prentice and Pyke (1979), obtained for non-regularized likelihoods.
We establish both the sup-norm consistency of the odds ratio, after model
selection, and the consistency of subset selection of our estimators. The
novelty of our theoretical results consists in the study of these properties
under the case-control sampling scheme. Our results hold for selection
performed over a large collection of candidate variables, with cardinality
allowed to depend and be greater than the sample size. We complement our
theoretical results with a novel approach of determining data driven tuning
parameters, based on the bisection method. The resulting procedure offers
significant computational savings when compared with grid search based methods.
All our numerical experiments support strongly our theoretical findings.
"
"  Boosting combines weak classifiers to form highly accurate predictors.
Although the case of binary classification is well understood, in the
multiclass setting, the ""correct"" requirements on the weak classifier, or the
notion of the most efficient boosting algorithms are missing. In this paper, we
create a broad and general framework, within which we make precise and identify
the optimal requirements on the weak-classifier, as well as design the most
effective, in a certain sense, boosting algorithms that assume such
requirements.
"
"  Regression curves for studying trait relationships are developed herein. The
adaptive evolution model is considered an Ornstein-Uhlenbeck system whose
parameters are estimated by a novel engagement of generalized least-squares and
optimization. Our algorithm is implemented to ecological data.
"
"  The main hurdle in the realization of dynamic spectrum access (DSA) systems
from physical layer perspective is the reliable sensing of low power licensed
users. One such scenario shows up in the unlicensed use of TV bands where the
TV Band Devices (TVBDs) are required to sense extremely low power wireless
microphones (WMs). The lack of technical standard among various wireless
manufacturers and the resemblance of certain WM signals to narrow-band
interference signals, such as spurious emissions, further aggravate the
problem. Due to these uncertainties, it is extremely difficult to abstract the
features of WM signals and hence develop robust sensing algorithms. To partly
counter these challenges, we develop a two-stage sub-space algorithm that
detects multiple narrow-band analog frequency-modulated signals generated by
WMs. The performance of the algorithm is verified by using experimentally
captured low power WM signals with received power ranging from -100 to -105
dBm. The problem of differentiating between the WM and other narrow-band
signals is left as a future work.
"
"  Obtaining compact and discriminative features is one of the major challenges
in many of the real-world image classification tasks such as face verification
and object recognition. One possible approach is to represent input image on
the basis of high-level features that carry semantic meaning which humans can
understand. In this paper, a model coined deep attribute network (DAN) is
proposed to address this issue. For an input image, the model outputs the
attributes of the input image without performing any classification. The
efficacy of the proposed model is evaluated on unconstrained face verification
and real-world object recognition tasks using the LFW and the a-PASCAL
datasets. We demonstrate the potential of deep learning for attribute-based
classification by showing comparable results with existing state-of-the-art
results. Once properly trained, the DAN is fast and does away with calculating
low-level features which are maybe unreliable and computationally expensive.
"
"  It is widely recognized nowadays that complex diseases are caused by, amongst
the others, multiple genetic factors. The recent advent of genome-wide
association study (GWA) has triggered a wave of research aimed at discovering
genetic factors underlying common complex diseases. While the number of
reported susceptible genetic variants is increasing steadily, the application
of such findings into diseases prognosis for the general population is still
unclear, and there are doubts about whether the size of the contribution by
such factors is significant. In this respect, some recent simulation-based
studies have shed more light to the prospect of genetic tests. In this report,
we discuss several aspects of simulation-based studies: their parameters, their
assumptions, and the information they provide.
"
"  Dose-finding studies are frequently conducted to evaluate the effect of
different doses or concentration levels of a compound on a response of
interest. Applications include the investigation of a new medicinal drug, a
herbicide or fertilizer, a molecular entity, an environmental toxin, or an
industrial chemical. In pharmaceutical drug development, dose-finding studies
are of critical importance because of regulatory requirements that marketed
doses are safe and provide clinically relevant efficacy. Motivated by a
dose-finding study in moderate persistent asthma, we propose response-adaptive
designs addressing two major challenges in dose-finding studies: uncertainty
about the dose-response models and large variability in parameter estimates. To
allocate new cohorts of patients in an ongoing study, we use optimal designs
that are robust under model uncertainty. In addition, we use a Bayesian
shrinkage approach to stabilize the parameter estimates over the successive
interim analyses used in the adaptations. This approach allows us to calculate
updated parameter estimates and model probabilities that can then be used to
calculate the optimal design for subsequent cohorts. The resulting designs are
hence robust with respect to model misspecification and additionally can
efficiently adapt to the information accrued in an ongoing study. We focus on
adaptive designs for estimating the minimum effective dose, although
alternative optimality criteria or mixtures thereof could be used, enabling the
design to address multiple objectives.
"
"  Previous algorithms for constructing regression tree models for longitudinal
and multiresponse data have mostly followed the CART approach. Consequently,
they inherit the same selection biases and computational difficulties as CART.
We propose an alternative, based on the GUIDE approach, that treats each
longitudinal data series as a curve and uses chi-squared tests of the residual
curve patterns to select a variable to split each node of the tree. Besides
being unbiased, the method is applicable to data with fixed and random time
points and with missing values in the response or predictor variables.
Simulation results comparing its mean squared prediction error with that of
MVPART are given, as well as examples comparing it with standard linear mixed
effects and generalized estimating equation models. Conditions for asymptotic
consistency of regression tree function estimates are also given.
"
"  In high-throughput genomics, large-scale designed experiments are becoming
common, and analysis approaches based on highly multivariate regression and
anova concepts are key tools. Shrinkage models of one form or another can
provide comprehensive approaches to the problems of simultaneous inference that
involve implicit multiple comparisons over the many, many parameters
representing effects of design factors and covariates. We use such approaches
here in a study of cardiovascular genomics. The primary experimental context
concerns a carefully designed, and rich, gene expression study focused on
gene-environment interactions, with the goals of identifying genes implicated
in connection with disease states and known risk factors, and in generating
expression signatures as proxies for such risk factors. A coupled exploratory
analysis investigates cross-species extrapolation of gene expression
signatures--how these mouse-model signatures translate to humans. The latter
involves exploration of sparse latent factor analysis of human observational
data and of how it relates to projected risk signatures derived in the animal
models. The study also highlights a range of applied statistical and genomic
data analysis issues, including model specification, computational questions
and model-based correction of experimental artifacts in DNA microarray data.
"
"  Woodall and Montgomery [35] in a discussion paper, state that multivariate
process control is one of the most rapidly developing sections of statistical
process control. Nowadays, in industry, there are many situations in which the
simultaneous monitoring or control, of two or more related quality - process
characteristics is necessary. Process monitoring problems in which several
related variables are of interest are collectively known as Multivariate
Statistical Process Control (MSPC).This article has three parts. In the first
part, we discuss in brief the basic procedures for the implementation of
multivariate statistical process control via control charting. In the second
part we present the most useful procedures for interpreting the out-of-control
variable when a control charting procedure gives an out-of-control signal in a
multivariate process. Finally, in the third part, we present applications of
multivariate statistical process control in the area of industrial process
control, informatics, and business.
"
"  In this work we show that, using the eigen-decomposition of the adjacency
matrix, we can consistently estimate latent positions for random dot product
graphs provided the latent positions are i.i.d. from some distribution. If
class labels are observed for a number of vertices tending to infinity, then we
show that the remaining vertices can be classified with error converging to
Bayes optimal using the $k$-nearest-neighbors classification rule. We evaluate
the proposed methods on simulated data and a graph derived from Wikipedia.
"
"  In standard passive imitation learning, the goal is to learn a target policy
by passively observing full execution trajectories of it. Unfortunately,
generating such trajectories can require substantial expert effort and be
impractical in some cases. In this paper, we consider active imitation learning
with the goal of reducing this effort by querying the expert about the desired
action at individual states, which are selected based on answers to past
queries and the learner's interactions with an environment simulator. We
introduce a new approach based on reducing active imitation learning to i.i.d.
active learning, which can leverage progress in the i.i.d. setting. Our first
contribution, is to analyze reductions for both non-stationary and stationary
policies, showing that the label complexity (number of queries) of active
imitation learning can be substantially less than passive learning. Our second
contribution, is to introduce a practical algorithm inspired by the reductions,
which is shown to be highly effective in four test domains compared to a number
of alternatives.
"
"  This paper describes a novel approach based on ""proportional imputation"" when
identical units produced in a batch have random but independent installation
and failure times. The current problem is motivated by a real life industrial
production-delivery supply chain where identical units are shipped after
production to a third party warehouse and then sold at a future date for
possible installation. Due to practical limitations, at any given time point,
the exact installation as well as the failure times are known for only those
units which have failed within that time frame after the installation. Hence,
in-house reliability engineers are presented with a very limited, as well as
partial, data to estimate different model parameters related to installation
and failure distributions. In reality, other units in the batch are generally
not utilized due to lack of proper statistical methodology, leading to gross
misspecification. In this paper we have introduced a likelihood based
parametric and computationally efficient solution to overcome this problem.
"
"  For obtaining causal inferences that are objective, and therefore have the
best chance of revealing scientific truths, carefully designed and executed
randomized experiments are generally considered to be the gold standard.
Observational studies, in contrast, are generally fraught with problems that
compromise any claim for objectivity of the resulting causal inferences. The
thesis here is that observational studies have to be carefully designed to
approximate randomized experiments, in particular, without examining any final
outcome data. Often a candidate data set will have to be rejected as inadequate
because of lack of data on key covariates, or because of lack of overlap in the
distributions of key covariates between treatment and control groups, often
revealed by careful propensity score analyses. Sometimes the template for the
approximating randomized experiment will have to be altered, and the use of
principal stratification can be helpful in doing this. These issues are
discussed and illustrated using the framework of potential outcomes to define
causal effects, which greatly clarifies critical issues.
"
"  Nonparametric mixture models based on the Dirichlet process are an elegant
alternative to finite models when the number of underlying components is
unknown, but inference in such models can be slow. Existing attempts to
parallelize inference in such models have relied on introducing approximations,
which can lead to inaccuracies in the posterior estimate. In this paper, we
describe auxiliary variable representations for the Dirichlet process and the
hierarchical Dirichlet process that allow us to sample from the true posterior
in a distributed manner. We show that our approach allows scalable inference
without the deterioration in estimate quality that accompanies existing
methods.
"
"  Assessment of learning in higher education is a critical concern to policy
makers, educators, parents, and students. And, doing so appropriately is likely
to require including constructed response tests in the assessment system. We
examined whether scoring costs and other concerns with using open-end measures
on a large scale (e.g., turnaround time and inter-reader consistency) could be
addressed by machine grading the answers. Analyses with 1359 students from 14
colleges found that two human readers agreed highly with each other in the
scores they assigned to the answers to three types of open-ended questions.
These reader assigned scores also agreed highly with those assigned by a
computer. The correlations of the machine-assigned scores with SAT scores,
college grades, and other measures were comparable to the correlations of these
variables with the hand-assigned scores. Machine scoring did not widen
differences in mean scores between racial/ethnic or gender groups. Our findings
demonstrated that machine scoring can facilitate the use of open-ended
questions in large-scale testing programs by providing a fast, accurate, and
economical way to grade responses.
"
"  1. A recent unpublished manuscript whose conclusions were widely circulated
in the electronic media (Zinman, 2009) claimed that Oregon 2007 payday loan
(PL) rate-limiting regulations (hereafter, ""Cap"") have hurt borrowers.
  2. The report's main conclusion, phrased in cause-and-effect language in the
abstract - ""...restricting access caused deterioration in the overall financial
condition of the Oregon households..."" - relies on a single, small-sample
survey funded by the payday-lending industry (PLI). The survey is fraught with
methodological flaws.
  3. Moreover, survey results do not support the claim that Oregon borrowers
fared worse than Washington borrowers, on any variable that can be plausibly
attributed to the Cap.
  4. In fact, Oregon respondents fared better than Washington respondents on
two key variables: on-time bill payment rate and avoiding phone-line
disconnects. On all other relevant variables they fared similarly to Washington
respondents. In short, the reported claim is baseless.
"
"  Ranking objects is a simple and natural procedure for organizing data. It is
often performed by assigning a quality score to each object according to its
relevance to the problem at hand. Ranking is widely used for object selection,
when resources are limited and it is necessary to select a subset of most
relevant objects for further processing. In real world situations, the object's
scores are often calculated from noisy measurements, casting doubt on the
ranking reliability. We introduce an analytical method for assessing the
influence of noise levels on the ranking reliability. We use two similarity
measures for reliability evaluation, Top-K-List overlap and Kendall's tau
measure, and show that the former is much more sensitive to noise than the
latter. We apply our method to gene selection in a series of microarray
experiments of several cancer types. The results indicate that the reliability
of the lists obtained from these experiments is very poor, and that experiment
sizes which are necessary for attaining reasonably stable Top-K-Lists are much
larger than those currently available. Simulations support our analytical
results.
"
"  In a recent study by Ginther et al., the probability of receiving a U.S.
National Institutes of Health (NIH) RO1 award was related to the applicant's
race/ethnicity. The results indicate black/African-American applicants were 10%
less likely than white peers to receive an award, after controlling for
background and qualifications. It has generated a widespread debate regarding
the unfairness of the NIH grant review process and its correction. In this
paper, the work by Ginther et al. was augmented by pairing analysis,
axiomatically-individualized productivity and normalized funding success
measurement. Although there are racial differences in R01 grant success rates,
normalized figures of merit for funding success explain the discrepancy. The
suggested ""leverage points for policy intervention"" are in question and require
deeper and more thorough investigations. Further adjustments in policies to
remove racial disparity should be made more systematically for equal
opportunity, rather than being limited to the NIH review process.
"
"  This paper compares the Maximum-likelihood method and Bayesian method for
finite element model updating. The Maximum-likelihood method was implemented
using genetic algorithm while the Bayesian method was implemented using the
Markov Chain Monte Carlo. These methods were tested on a simple beam and an
unsymmetrical H-shaped structure. The results show that the Bayesian method
gave updated finite element models that predicted more accurate modal
properties than the updated finite element models obtained through the use of
the Maximum-likelihood method. Furthermore, both these methods were found to
require the same levels of computational loads.
"
"  Mass spectrometry-based proteomics has become the tool of choice for
identifying and quantifying the proteome of an organism. Though recent years
have seen a tremendous improvement in instrument performance and the
computational tools used, significant challenges remain, and there are many
opportunities for statisticians to make important contributions. In the most
widely used ""bottom-up"" approach to proteomics, complex mixtures of proteins
are first subjected to enzymatic cleavage, the resulting peptide products are
separated based on chemical or physical properties and analyzed using a mass
spectrometer. The two fundamental challenges in the analysis of bottom-up
MS-based proteomics are as follows: (1) Identifying the proteins that are
present in a sample, and (2) Quantifying the abundance levels of the identified
proteins. Both of these challenges require knowledge of the biological and
technological context that gives rise to observed data, as well as the
application of sound statistical principles for estimation and inference. We
present an overview of bottom-up proteomics and outline the key statistical
issues that arise in protein identification and quantification.
"
"  Often we wish to predict a large number of variables that depend on each
other as well as on other observed variables. Structured prediction methods are
essentially a combination of classification and graphical modeling, combining
the ability of graphical models to compactly model multivariate data with the
ability of classification methods to perform prediction using large sets of
input features. This tutorial describes conditional random fields, a popular
probabilistic method for structured prediction. CRFs have seen wide application
in natural language processing, computer vision, and bioinformatics. We
describe methods for inference and parameter estimation for CRFs, including
practical issues for implementing large scale CRFs. We do not assume previous
knowledge of graphical modeling, so this tutorial is intended to be useful to
practitioners in a wide variety of fields.
"
"  We propose graph kernels based on subgraph matchings, i.e.
structure-preserving bijections between subgraphs. While recently proposed
kernels based on common subgraphs (Wale et al., 2008; Shervashidze et al.,
2009) in general can not be applied to attributed graphs, our approach allows
to rate mappings of subgraphs by a flexible scoring scheme comparing vertex and
edge attributes by kernels. We show that subgraph matching kernels generalize
several known kernels. To compute the kernel we propose a graph-theoretical
algorithm inspired by a classical relation between common subgraphs of two
graphs and cliques in their product graph observed by Levi (1973). Encouraging
experimental results on a classification task of real-world graphs are
presented.
"
"  We consider a joint processing of $n$ independent sparse regression problems.
Each is based on a sample $(y_{i1},x_{i1})...,(y_{im},x_{im})$ of $m$ \iid
observations from $y_{i1}=x_{i1}\t\beta_i+\eps_{i1}$, $y_{i1}\in \R$, $x_{i
1}\in\R^p$, $i=1,...,n$, and $\eps_{i1}\dist N(0,\sig^2)$, say. $p$ is large
enough so that the empirical risk minimizer is not consistent. We consider
three possible extensions of the lasso estimator to deal with this problem, the
lassoes, the group lasso and the RING lasso, each utilizing a different
assumption how these problems are related. For each estimator we give a
Bayesian interpretation, and we present both persistency analysis and
non-asymptotic error bounds based on restricted eigenvalue - type assumptions.
"
"  We consider principal component analysis for contaminated data-set in the
high dimensional regime, where the dimensionality of each observation is
comparable or even more than the number of observations. We propose a
deterministic high-dimensional robust PCA algorithm which inherits all
theoretical properties of its randomized counterpart, i.e., it is tractable,
robust to contaminated points, easily kernelizable, asymptotic consistent and
achieves maximal robustness -- a breakdown point of 50%. More importantly, the
proposed method exhibits significantly better computational efficiency, which
makes it suitable for large-scale real applications.
"
"  Portfolio balancing requires estimates of covariance between asset returns.
Returns data have histories which greatly vary in length, since assets begin
public trading at different times. This can lead to a huge amount of missing
data--too much for the conventional imputation-based approach. Fortunately, a
well-known factorization of the MVN likelihood under the prevailing historical
missingness pattern leads to a simple algorithm of OLS regressions that is much
more reliable. When there are more assets than returns, however, OLS becomes
unstable. Gramacy, et al. (2008), showed how classical shrinkage regression may
be used instead, thus extending the state of the art to much bigger asset
collections, with further accuracy and interpretation advantages. In this
paper, we detail a fully Bayesian hierarchical formulation that extends the
framework further by allowing for heavy-tailed errors, relaxing the historical
missingness assumption, and accounting for estimation risk. We illustrate how
this approach compares favorably to the classical one using synthetic data and
an investment exercise with real returns. An accompanying R package is on CRAN.
"
"  Gaussian Markov random fields (GMRFs) are useful in a broad range of
applications. In this paper we tackle the problem of learning a sparse GMRF in
a high-dimensional space. Our approach uses the l1-norm as a regularization on
the inverse covariance matrix. We utilize a novel projected gradient method,
which is faster than previous methods in practice and equal to the best
performing of these in asymptotic complexity. We also extend the l1-regularized
objective to the problem of sparsifying entire blocks within the inverse
covariance matrix. Our methods generalize fairly easily to this case, while
other methods do not. We demonstrate that our extensions give better
generalization performance on two real domains--biological network analysis and
a 2D-shape modeling image task.
"
"  Dimensional reduction of high dimensional data can be achieved by keeping
only the relevant eigenmodes after principal component analysis. However,
differentiating relevant eigenmodes from the random noise eigenmodes is
problematic. A new method based on the random matrix theory and a statistical
goodness-of-fit test is proposed in this paper. It is validated by numerical
simulations and applied to real-time magnetic resonance cardiac cine images.
"
"  Solar tomography has progressed rapidly in recent years thanks to the
development of robust algorithms and the availability of more powerful
computers. It can today provide crucial insights in solving issues related to
the line-of-sight integration present in the data of solar imagers and
coronagraphs. However, there remain challenges such as the increase of the
available volume of data, the handling of the temporal evolution of the
observed structures, and the heterogeneity of the data in multi-spacecraft
studies.
  We present a generic software package that can perform fast tomographic
inversions that scales linearly with the number of measurements, linearly with
the length of the reconstruction cube (and not the number of voxels) and
linearly with the number of cores and can use data from different sources and
with a variety of physical models: TomograPy
(http://nbarbey.github.com/TomograPy/), an open-source software freely
available on the Python Package Index. For performance, TomograPy uses a
parallelized-projection algorithm. It relies on the World Coordinate System
standard to manage various data sources. A variety of inversion algorithms are
provided to perform the tomographic-map estimation. A test suite is provided
along with the code to ensure software quality. Since it makes use of the
Siddon algorithm it is restricted to rectangular parallelepiped voxels but the
spherical geometry of the corona can be handled through proper use of priors.
  We describe the main features of the code and show three practical examples
of multi-spacecraft tomographic inversions using STEREO/EUVI and STEREO/COR1
data. Static and smoothly varying temporal evolution models are presented.
"
"  We identify three properties of the standard oncology phase I trial design or
3 + 3 design. We show that the standard design implicitly uses isotonic
regression to estimate a maximum tolerated dose. We next illustrate the
relationship between the standard design and a Bayesian design proposed by Ji
et al. (2007). A slight modification to this Bayesian design, under a
particular model specification, would assign treatments in a manner identical
to the standard design. We finally present calculations revealing the behavior
of the standard design in a worst case scenario and compare its behavior with
other 3 + 3-like designs.
"
"  The kernel method is a potential approach to analyzing structured data such
as sequences, trees, and graphs; however, unordered trees have not been
investigated extensively. Kimura et al. (2011) proposed a kernel function for
unordered trees on the basis of their subpaths, which are vertical
substructures of trees responsible for hierarchical information in them. Their
kernel exhibits practically good performance in terms of accuracy and speed;
however, linear-time computation is not guaranteed theoretically, unlike the
case of the other unordered tree kernel proposed by Vishwanathan and Smola
(2003). In this paper, we propose a theoretically guaranteed linear-time kernel
computation algorithm that is practically fast, and we present an efficient
prediction algorithm whose running time depends only on the size of the input
tree. Experimental results show that the proposed algorithms are quite
efficient in practice.
"
"  Exponential models of distributions are widely used in machine learning for
classiffication and modelling. It is well known that they can be interpreted as
maximum entropy models under empirical expectation constraints. In this work,
we argue that for classiffication tasks, mutual information is a more suitable
information theoretic measure to be optimized. We show how the principle of
minimum mutual information generalizes that of maximum entropy, and provides a
comprehensive framework for building discriminative classiffiers. A game
theoretic interpretation of our approach is then given, and several
generalization bounds provided. We present iterative algorithms for solving the
minimum information problem and its convex dual, and demonstrate their
performance on various classiffication tasks. The results show that minimum
information classiffiers outperform the corresponding maximum entropy models.
"
"  A time-dependent model for the energy of a flaring solar active region is
presented based on a stochastic jump-transition model (Wheatland and Glukhov
1998; Wheatland 2008; Wheatland 2009). The magnetic free energy of the model
active region varies in time due to a prescribed (deterministic) rate of energy
input and prescribed (random) flare jumps downwards in energy. The model has
been shown to reproduce observed flare statistics, for specific
time-independent choices for the energy input and flare transition rates.
However, many solar active regions exhibit time variation in flare
productivity, as exemplified by NOAA active region AR 11029 (Wheatland 2010).
In this case a time-dependent model is needed. Time variation is incorporated
for two cases: 1. a step change in the rates of flare jumps; and 2. a step
change in the rate of energy supply to the system. Analytic arguments are
presented describing the qualitative behavior of the system in the two cases.
In each case the system adjusts by shifting to a new stationary state over a
relaxation time which is estimated analytically. The new model retains
flare-like event statistics. In each case the frequency-energy distribution is
a power law for flare energies less than a time-dependent rollover set by the
largest energy the system is likely to attain at a given time. For Case 1, the
model exhibits a double exponential waiting-time distribution, corresponding to
flaring at a constant mean rate during two intervals (before and after the step
change), if the average energy of the system is large. For Case 2 the
waiting-time distribution is a simple exponential, again provided the average
energy of the system is large. Monte Carlo simulations of Case~1 are presented
which confirm the analytic estimates. The simulation results provide a
qualitative model for observed flare statistics in active region AR 11029.
"
"  We analyze general model selection procedures using penalized empirical loss
minimization under computational constraints. While classical model selection
approaches do not consider computational aspects of performing model selection,
we argue that any practical model selection procedure must not only trade off
estimation and approximation error, but also the computational effort required
to compute empirical minimizers for different function classes. We provide a
framework for analyzing such problems, and we give algorithms for model
selection under a computational budget. These algorithms satisfy oracle
inequalities that show that the risk of the selected model is not much worse
than if we had devoted all of our omputational budget to the optimal function
class.
"
"  Representing distributions over permutations can be a daunting task due to
the fact that the number of permutations of $n$ objects scales factorially in
$n$. One recent way that has been used to reduce storage complexity has been to
exploit probabilistic independence, but as we argue, full independence
assumptions impose strong sparsity constraints on distributions and are
unsuitable for modeling rankings. We identify a novel class of independence
structures, called \emph{riffled independence}, encompassing a more expressive
family of distributions while retaining many of the properties necessary for
performing efficient inference and reducing sample complexity. In riffled
independence, one draws two permutations independently, then performs the
\emph{riffle shuffle}, common in card games, to combine the two permutations to
form a single permutation. Within the context of ranking, riffled independence
corresponds to ranking disjoint sets of objects independently, then
interleaving those rankings. In this paper, we provide a formal introduction to
riffled independence and present algorithms for using riffled independence
within Fourier-theoretic frameworks which have been explored by a number of
recent papers. Additionally, we propose an automated method for discovering
sets of items which are riffle independent from a training set of rankings. We
show that our clustering-like algorithms can be used to discover meaningful
latent coalitions from real preference ranking datasets and to learn the
structure of hierarchically decomposable models based on riffled independence.
"
"  Understanding the adaptation process of plants to drought stress is essential
in improving management practices, breeding strategies as well as engineering
viable crops for a sustainable agriculture in the coming decades.
Hyper-spectral imaging provides a particularly promising approach to gain such
understanding since it allows to discover non-destructively spectral
characteristics of plants governed primarily by scattering and absorption
characteristics of the leaf internal structure and biochemical constituents.
Several drought stress indices have been derived using hyper-spectral imaging.
However, they are typically based on few hyper-spectral images only, rely on
interpretations of experts, and consider few wavelengths only. In this study,
we present the first data-driven approach to discovering spectral drought
stress indices, treating it as an unsupervised labeling problem at massive
scale. To make use of short range dependencies of spectral wavelengths, we
develop an online variational Bayes algorithm for latent Dirichlet allocation
with convolved Dirichlet regularizer. This approach scales to massive datasets
and, hence, provides a more objective complement to plant physiological
practices. The spectral topics found conform to plant physiological knowledge
and can be computed in a fraction of the time compared to existing LDA
approaches.
"
"  We present surrogate regret bounds for arbitrary surrogate losses in the
context of binary classification with label-dependent costs. Such bounds relate
a classifier's risk, assessed with respect to a surrogate loss, to its
cost-sensitive classification risk. Two approaches to surrogate regret bounds
are developed. The first is a direct generalization of Bartlett et al. [2006],
who focus on margin-based losses and cost-insensitive classification, while the
second adopts the framework of Steinwart [2007] based on calibration functions.
Nontrivial surrogate regret bounds are shown to exist precisely when the
surrogate loss satisfies a ""calibration"" condition that is easily verified for
many common losses. We apply this theory to the class of uneven margin losses,
and characterize when these losses are properly calibrated. The uneven hinge,
squared error, exponential, and sigmoid losses are then treated in detail.
"
"  Modern data acquisition based on high-throughput technology is often facing
the problem of missing data. Algorithms commonly used in the analysis of such
large-scale data often depend on a complete set. Missing value imputation
offers a solution to this problem. However, the majority of available
imputation methods are restricted to one type of variable only: continuous or
categorical. For mixed-type data the different types are usually handled
separately. Therefore, these methods ignore possible relations between variable
types. We propose a nonparametric method which can cope with different types of
variables simultaneously. We compare several state of the art methods for the
imputation of missing values. We propose and evaluate an iterative imputation
method (missForest) based on a random forest. By averaging over many unpruned
classification or regression trees random forest intrinsically constitutes a
multiple imputation scheme. Using the built-in out-of-bag error estimates of
random forest we are able to estimate the imputation error without the need of
a test set. Evaluation is performed on multiple data sets coming from a diverse
selection of biological fields with artificially introduced missing values
ranging from 10% to 30%. We show that missForest can successfully handle
missing values, particularly in data sets including different types of
variables. In our comparative study missForest outperforms other methods of
imputation especially in data settings where complex interactions and nonlinear
relations are suspected. The out-of-bag imputation error estimates of
missForest prove to be adequate in all settings. Additionally, missForest
exhibits attractive computational efficiency and can cope with high-dimensional
data.
"
"  We speed up marginal inference by ignoring factors that do not significantly
contribute to overall accuracy. In order to pick a suitable subset of factors
to ignore, we propose three schemes: minimizing the number of model factors
under a bound on the KL divergence between pruned and full models; minimizing
the KL divergence under a bound on factor count; and minimizing the weighted
sum of KL divergence and factor count. All three problems are solved using an
approximation of the KL divergence than can be calculated in terms of marginals
computed on a simple seed graph. Applied to synthetic image denoising and to
three different types of NLP parsing models, this technique performs marginal
inference up to 11 times faster than loopy BP, with graph sizes reduced up to
98%-at comparable error in marginals and parsing accuracy. We also show that
minimizing the weighted sum of divergence and size is substantially faster than
minimizing either of the other objectives based on the approximation to
divergence presented here.
"
"  We consider the problem of portfolio selection within the classical Markowitz
mean-variance framework, reformulated as a constrained least-squares regression
problem. We propose to add to the objective function a penalty proportional to
the sum of the absolute values of the portfolio weights. This penalty
regularizes (stabilizes) the optimization problem, encourages sparse portfolios
(i.e. portfolios with only few active positions), and allows to account for
transaction costs. Our approach recovers as special cases the
no-short-positions portfolios, but does allow for short positions in limited
number. We implement this methodology on two benchmark data sets constructed by
Fama and French. Using only a modest amount of training data, we construct
portfolios whose out-of-sample performance, as measured by Sharpe ratio, is
consistently and significantly better than that of the naive evenly-weighted
portfolio which constitutes, as shown in recent literature, a very tough
benchmark.
"
"  To find efficient screening methods for high dimensional linear regression
models, this paper studies the relationship between model fitting and screening
performance. Under a sparsity assumption, we show that a subset that includes
the true submodel always yields smaller residual sum of squares (i.e., has
better model fitting) than all that do not in a general asymptotic setting.
This indicates that, for screening important variables, we could follow a
""better fitting, better screening"" rule, i.e., pick a ""better"" subset that has
better model fitting. To seek such a better subset, we consider the
optimization problem associated with best subset regression. An EM algorithm,
called orthogonalizing subset screening, and its accelerating version are
proposed for searching for the best subset. Although the two algorithms cannot
guarantee that a subset they yield is the best, their monotonicity property
makes the subset have better model fitting than initial subsets generated by
popular screening methods, and thus the subset can have better screening
performance asymptotically. Simulation results show that our methods are very
competitive in high dimensional variable screening even for finite sample
sizes.
"
"  In this paper, we propose a new algorithm for exploratory projection pursuit.
The basis of the algorithm is the insight that previous approaches used fairly
narrow definitions of interestingness / non interestingness. We argue that
allowing these definitions to depend on the problem / data at hand is a more
natural approach in an exploratory technique. This also allows our technique
much greater applicability than the approaches extant in the literature.
Complementing this insight, we propose a class of projection indices based on
the spatial distribution function that can make use of such information.
  Finally, with the help of real datasets, we demonstrate how a range of
multivariate exploratory tasks can be addressed with our algorithm. The
examples further demonstrate that the proposed indices are quite capable of
focussing on the interesting structure in the data, even when this structure is
otherwise hard to detect or arises from very subtle patterns.
"
"  For the problems of low-rank matrix completion, the efficiency of the
widely-used nuclear norm technique may be challenged under many circumstances,
especially when certain basis coefficients are fixed, for example, the low-rank
correlation matrix completion in various fields such as the financial market
and the low-rank density matrix completion from the quantum state tomography.
To seek a solution of high recovery quality beyond the reach of the nuclear
norm, in this paper, we propose a rank-corrected procedure using a nuclear
semi-norm to generate a new estimator. For this new estimator, we establish a
non-asymptotic recovery error bound. More importantly, we quantify the
reduction of the recovery error bound for this rank-corrected procedure.
Compared with the one obtained for the nuclear norm penalized least squares
estimator, this reduction can be substantial (around 50%). We also provide
necessary and sufficient conditions for rank consistency in the sense of Bach
(2008). Very interestingly, these conditions are highly related to the concept
of constraint nondegeneracy in matrix optimization. As a byproduct, our results
provide a theoretical foundation for the majorized penalty method of Gao and
Sun (2010) and Gao (2010) for structured low-rank matrix optimization problems.
Extensive numerical experiments demonstrate that our proposed rank-corrected
procedure can simultaneously achieve a high recovery accuracy and capture the
low-rank structure.
"
"  This chapter defines a new concept and framework for constructing fusion
rules for evidences. This framework is based on a referee function, which does
a decisional arbitrament conditionally to basic decisions provided by the
several sources of information. A simple sampling method is derived from this
framework. The purpose of this sampling approach is to avoid the combinatorics
which are inherent to the definition of fusion rules of evidences. This
definition of the fusion rule by the means of a sampling process makes possible
the construction of several rules on the basis of an algorithmic implementation
of the referee function, instead of a mathematical formulation. Incidentally,
it is a versatile and intuitive way for defining rules. The framework is
implemented for various well known evidence rules. On the basis of this
framework, new rules for combining evidences are proposed, which takes into
account a consensual evaluation of the sources of information.
"
"  Multi-modal data collections, such as corpora of paired images and text
snippets, require analysis methods beyond single-view component and topic
models. For continuous observations the current dominant approach is based on
extensions of canonical correlation analysis, factorizing the variation into
components shared by the different modalities and those private to each of
them. For count data, multiple variants of topic models attempting to tie the
modalities together have been presented. All of these, however, lack the
ability to learn components private to one modality, and consequently will try
to force dependencies even between minimally correlating modalities. In this
work we combine the two approaches by presenting a novel HDP-based topic model
that automatically learns both shared and private topics. The model is shown to
be especially useful for querying the contents of one domain given samples of
the other.
"
"  This review outlines concepts of mathematical statistics, elements of
probability theory, hypothesis tests and point estimation for use in the
analysis of modern astronomical data. Least squares, maximum likelihood, and
Bayesian approaches to statistical inference are treated. Resampling methods,
particularly the bootstrap, provide valuable procedures when distributions
functions of statistics are not known. Several approaches to model selection
and good- ness of fit are considered. Applied statistics relevant to
astronomical research are briefly discussed: nonparametric methods for use when
little is known about the behavior of the astronomical populations or
processes; data smoothing with kernel density estimation and nonparametric
regression; unsupervised clustering and supervised classification procedures
for multivariate problems; survival analysis for astronomical datasets with
nondetections; time- and frequency-domain times series analysis for light
curves; and spatial statistics to interpret the spatial distributions of points
in low dimensions. Two types of resources are presented: about 40 recommended
texts and monographs in various fields of statistics, and the public domain R
software system for statistical analysis. Together with its \sim 3500 (and
growing) add-on CRAN packages, R implements a vast range of statistical
procedures in a coherent high-level language with advanced graphics.
"
"  Relational data representations have become an increasingly important topic
due to the recent proliferation of network datasets (e.g., social, biological,
information networks) and a corresponding increase in the application of
statistical relational learning (SRL) algorithms to these domains. In this
article, we examine a range of representation issues for graph-based relational
data. Since the choice of relational data representation for the nodes, links,
and features can dramatically affect the capabilities of SRL algorithms, we
survey approaches and opportunities for relational representation
transformation designed to improve the performance of these algorithms. This
leads us to introduce an intuitive taxonomy for data representation
transformations in relational domains that incorporates link transformation and
node transformation as symmetric representation tasks. In particular, the
transformation tasks for both nodes and links include (i) predicting their
existence, (ii) predicting their label or type, (iii) estimating their weight
or importance, and (iv) systematically constructing their relevant features. We
motivate our taxonomy through detailed examples and use it to survey and
compare competing approaches for each of these tasks. We also discuss general
conditions for transforming links, nodes, and features. Finally, we highlight
challenges that remain to be addressed.
"
"  We propose an approach for approximating the partition function which is
based on two steps: (1) computing the partition function of a simplified model
which is obtained by deleting model edges, and (2) rectifying the result by
applying an edge-by-edge correction. The approach leads to an intuitive
framework in which one can trade-off the quality of an approximation with the
complexity of computing it. It also includes the Bethe free energy
approximation as a degenerate case. We develop the approach theoretically in
this paper and provide a number of empirical results that reveal its practical
utility.
"
"  This paper proposes a novel uncertainty quantification framework for
computationally demanding systems characterized by a large vector of
non-Gaussian uncertainties. It combines state-of-the-art techniques in advanced
Monte Carlo sampling with Bayesian formulations. The key departure from
existing works is the use of inexpensive, approximate computational models in a
rigorous manner. Such models can readily be derived by coarsening the
discretization size in the solution of the governing PDEs, increasing the time
step when integration of ODEs is performed, using fewer iterations if a
non-linear solver is employed or making use of lower order models. It is shown
that even in cases where the inexact models provide very poor approximations of
the exact response, statistics of the latter can be quantified accurately with
significant reductions in the computational effort. Multiple approximate models
can be used and rigorous confidence bounds of the estimates produced are
provided at all stages.
"
"  We propose a tree regularization framework, which enables many tree models to
perform feature selection efficiently. The key idea of the regularization
framework is to penalize selecting a new feature for splitting when its gain
(e.g. information gain) is similar to the features used in previous splits. The
regularization framework is applied on random forest and boosted trees here,
and can be easily applied to other tree models. Experimental studies show that
the regularized trees can select high-quality feature subsets with regard to
both strong and weak classifiers. Because tree models can naturally deal with
categorical and numerical variables, missing values, different scales between
variables, interactions and nonlinearities etc., the tree regularization
framework provides an effective and efficient feature selection solution for
many practical problems.
"
"  We study the problem of estimating a temporally varying coefficient and
varying structure (VCVS) graphical model underlying nonstationary time series
data, such as social states of interacting individuals or microarray expression
profiles of gene networks, as opposed to i.i.d. data from an invariant model
widely considered in current literature of structural estimation. In
particular, we consider the scenario in which the model evolves in a piece-wise
constant fashion. We propose a procedure that minimizes the so-called TESLA
loss (i.e., temporally smoothed L1 regularized regression), which allows
jointly estimating the partition boundaries of the VCVS model and the
coefficient of the sparse precision matrix on each block of the partition. A
highly scalable proximal gradient method is proposed to solve the resultant
convex optimization problem; and the conditions for sparsistent estimation and
the convergence rate of both the partition boundaries and the network structure
are established for the first time for such estimators.
"
"  Pooling specimens, a well-accepted sampling strategy in biomedical research,
can be applied to reduce the cost of studying biomarkers. Even if the cost of a
single assay is not a major restriction in evaluating biomarkers, pooling can
be a powerful design that increases the efficiency of estimation based on data
that is censored due to an instrument's lower limit of detection (LLOD).
However, there are situations when the pooling design strongly aggravates the
detection limit problem. To combine the benefits of pooled assays and
individual assays, hybrid designs that involve taking a sample of both pooled
and individual specimens have been proposed. We examine the efficiency of these
hybrid designs in estimating parameters of two systems subject to a LLOD: (1)
normally distributed biomarker with normally distributed measurement error and
pooling error; (2) Gamma distributed biomarker with double exponentially
distributed measurement error and pooling error. Three-assay design and
two-assay design with replicates are applied to estimate the measurement and
pooling error. The Maximum likelihood method is used to estimate the
parameters. We found that the simple one-pool design, where all assays but one
are random individuals and a single pooled assay includes the remaining
specimens, under plausible conditions, is very efficient and can be recommended
for practical use.
"
"  There is an increasing body of evidence suggesting that exact nearest
neighbour search in high-dimensional spaces is affected by the curse of
dimensionality at a fundamental level. Does it necessarily mean that the same
is true for k nearest neighbours based learning algorithms such as the k-NN
classifier? We analyse this question at a number of levels and show that the
answer is different at each of them. As our first main observation, we show the
consistency of a k approximate nearest neighbour classifier. However, the
performance of the classifier in very high dimensions is provably unstable. As
our second main observation, we point out that the existing model for
statistical learning is oblivious of dimension of the domain and so every
learning problem admits a universally consistent deterministic reduction to the
one-dimensional case by means of a Borel isomorphism.
"
"  In this paper, we propose a novel policy iteration method, called dynamic
policy programming (DPP), to estimate the optimal policy in the
infinite-horizon Markov decision processes. We prove the finite-iteration and
asymptotic l\infty-norm performance-loss bounds for DPP in the presence of
approximation/estimation error. The bounds are expressed in terms of the
l\infty-norm of the average accumulated error as opposed to the l\infty-norm of
the error in the case of the standard approximate value iteration (AVI) and the
approximate policy iteration (API). This suggests that DPP can achieve a better
performance than AVI and API since it averages out the simulation noise caused
by Monte-Carlo sampling throughout the learning process. We examine this
theoretical results numerically by com- paring the performance of the
approximate variants of DPP with existing reinforcement learning (RL) methods
on different problem domains. Our results show that, in all cases, DPP-based
algorithms outperform other RL methods by a wide margin.
"
"  Dealing with uncertainty in Bayesian Network structures using maximum a
posteriori (MAP) estimation or Bayesian Model Averaging (BMA) is often
intractable due to the superexponential number of possible directed, acyclic
graphs. When the prior is decomposable, two classes of graphs where efficient
learning can take place are tree structures, and fixed-orderings with limited
in-degree. We show how MAP estimates and BMA for selectively conditioned
forests (SCF), a combination of these two classes, can be computed efficiently
for ordered sets of variables. We apply SCFs to temporal data to learn Dynamic
Bayesian Networks having an intra-timestep forest and inter-timestep limited
in-degree structure, improving model accuracy over DBNs without the combination
of structures. We also apply SCFs to Bayes Net classification to learn
selective forest augmented Naive Bayes classifiers. We argue that the built-in
feature selection of selective augmented Bayes classifiers makes them
preferable to similar non-selective classifiers based on empirical evidence.
"
"  The outcome of a functional genomics pipeline is usually a partial list of
genomic features, ranked by their relevance in modelling biological phenotype
in terms of a classification or regression model. Due to resampling protocols
or just within a meta-analysis comparison, instead of one list it is often the
case that sets of alternative feature lists (possibly of different lengths) are
obtained. Here we introduce a method, based on the algebraic theory of
symmetric groups, for studying the variability between lists (""list stability"")
in the case of lists of unequal length. We provide algorithms evaluating
stability for lists embedded in the full feature set or just limited to the
features occurring in the partial lists. The method is demonstrated first on
synthetic data in a gene filtering task and then for finding gene profiles on a
recent prostate cancer dataset.
"
"  We consider the problem of algorithmically recommending items to users on a
Yahoo! front page module. Our approach is based on a novel multilevel
hierarchical model that we refer to as a User Profile Model with Graphical
Lasso (UPG). The UPG provides a personalized recommendation to users by
simultaneously incorporating both user covariates and historical user
interactions with items in a model based way. In fact, we build a per-item
regression model based on a rich set of user covariates and estimate individual
user affinity to items by introducing a latent random vector for each user. The
vector random effects are assumed to be drawn from a prior with a precision
matrix that measures residual partial associations among items. To ensure
better estimates of a precision matrix in high-dimensions, the matrix elements
are constrained through a Lasso penalty. Our model is fitted through a
penalized-quasi likelihood procedure coupled with a scalable EM algorithm. We
employ several computational strategies like multi-threading, conjugate
gradients and heavily exploit problem structure to scale our computations in
the E-step. For the M-step we take recourse to a scalable variant of the
Graphical Lasso algorithm for covariance selection. Through extensive
experiments on a new data set obtained from Yahoo! front page and a benchmark
data set from a movie recommender application, we show that our UPG model
significantly improves performance compared to several state-of-the-art methods
in the literature, especially those based on a bilinear random effects model
(BIRE). In particular, we show that the gains of UPG are significant compared
to BIRE when the number of users is large and the number of items to select
from is small. For large item sets and relatively small user sets the results
of UPG and BIRE are comparable. The UPG leads to faster model building and
produces outputs which are interpretable.
"
"  Canonical Correlation Analysis (CCA) is a classical tool for finding
correlations among the components of two random vectors. In recent years, CCA
has been widely applied to the analysis of genomic data, where it is common for
researchers to perform multiple assays on a single set of patient samples.
Recent work has proposed sparse variants of CCA to address the high
dimensionality of such data. However, classical and sparse CCA are based on
linear models, and are thus limited in their ability to find general
correlations. In this paper, we present two approaches to high-dimensional
nonparametric CCA, building on recent developments in high-dimensional
nonparametric regression. We present estimation procedures for both approaches,
and analyze their theoretical properties in the high-dimensional setting. We
demonstrate the effectiveness of these procedures in discovering nonlinear
correlations via extensive simulations, as well as through experiments with
genomic data.
"
"  We propose a solution to the image deconvolution problem where the
convolution kernel or point spread function (PSF) is assumed to be only
partially known. Small perturbations generated from the model are exploited to
produce a few principal components explaining the PSF uncertainty in a high
dimensional space. Unlike recent developments on blind deconvolution of natural
images, we assume the image is sparse in the pixel basis, a natural sparsity
arising in magnetic resonance force microscopy (MRFM). Our approach adopts a
Bayesian Metropolis-within-Gibbs sampling framework. The performance of our
Bayesian semi-blind algorithm for sparse images is superior to previously
proposed semi-blind algorithms such as the alternating minimization (AM)
algorithm and blind algorithms developed for natural images. We illustrate our
myopic algorithm on real MRFM tobacco virus data.
"
"  Santer et al (2008) (S08) compared climate models and observations in the
tropical troposphere and reported that ""there is no longer a serious
discrepancy between modeled and observed trends in tropical lapse rates."" They
found no statistically significant differences between modeled (ensemble mean)
trends and observed trends at the T2LT and T2 layers, and they found no
significant difference between observed and modeled surface-minus-troposphere
lapse rates. However they only used data over the 1979-1999 period. Using the
S08 methodology on up-to-date data, we find a statistically significant
discrepancy between observations and models with respect to trends in the UAH
data, as well as lapse rate trends comparing either RSS or UAH to the HADCRUT3v
land-ocean surface trend.
"
"  The concept of a common modulated oscillation spanning multiple time series
is formalized, a method for the recovery of such a signal from potentially
noisy observations is proposed, and the time-varying bias properties of the
recovery method are derived. The method, an extension of wavelet ridge analysis
to the multivariate case, identifies the common oscillation by seeking, at each
point in time, a frequency for which a bandpassed version of the signal obtains
a local maximum in power. The lowest-order bias is shown to involve a quantity,
termed the instantaneous curvature, which measures the strength of local
quadratic modulation of the signal after demodulation by the common oscillation
frequency. The bias can be made to be small if the analysis filter, or wavelet,
can be chosen such that the signal's instantaneous curvature changes little
over the filter time scale. An application is presented to the detection of
vortex motions in a set of freely-drifting oceanographic instruments tracking
the ocean currents.
"
"  We congratulate Lee, Nadler and Wasserman (henceforth LNW) on a very
interesting paper on new methodology and supporting theory [arXiv:0707.0481].
Treelets seem to tackle two important problems of modern data analysis at once.
For datasets with many variables, treelets give powerful predictions even if
variables are highly correlated and redundant. Maybe more importantly,
interpretation of the results is intuitive. Useful insights about relevant
groups of variables can be gained. Our comments and questions include: (i)
Could the success of treelets be replicated by a combination of hierarchical
clustering and PCA? (ii) When choosing a suitable basis, treelets seem to be
largely an unsupervised method. Could the results be even more interpretable
and powerful if treelets would take into account some supervised response
variable? (iii) Interpretability of the result hinges on the sparsity of the
final basis. Do we expect that the selected groups of variables will always be
sufficiently small to be amenable for interpretation?
"
"  The purpose of sufficient dimension reduction (SDR) is to find the
low-dimensional subspace of input features that is sufficient for predicting
output values. In this paper, we propose a novel distribution-free SDR method
called sufficient component analysis (SCA), which is computationally more
efficient than existing methods. In our method, a solution is computed by
iteratively performing dependence estimation and maximization: Dependence
estimation is analytically carried out by recently-proposed least-squares
mutual information (LSMI), and dependence maximization is also analytically
carried out by utilizing the Epanechnikov kernel. Through large-scale
experiments on real-world image classification and audio tagging problems, the
proposed method is shown to compare favorably with existing dimension reduction
approaches.
"
"  In this paper, we study the risk bounds for samples independently drawn from
an infinitely divisible (ID) distribution. In particular, based on a martingale
method, we develop two deviation inequalities for a sequence of random
variables of an ID distribution with zero Gaussian component. By applying the
deviation inequalities, we obtain the risk bounds based on the covering number
for the ID distribution. Finally, we analyze the asymptotic convergence of the
risk bound derived from one of the two deviation inequalities and show that the
convergence rate of the bound is faster than the result for the generic i.i.d.
empirical process (Mendelson, 2003).
"
"  Given a limited number of entries from the superposition of a low-rank matrix
plus the product of a known fat compression matrix times a sparse matrix,
recovery of the low-rank and sparse components is a fundamental task subsuming
compressed sensing, matrix completion, and principal components pursuit. This
paper develops algorithms for distributed sparsity-regularized rank
minimization over networks, when the nuclear- and $\ell_1$-norm are used as
surrogates to the rank and nonzero entry counts of the sought matrices,
respectively. While nuclear-norm minimization has well-documented merits when
centralized processing is viable, non-separability of the singular-value sum
challenges its distributed minimization. To overcome this limitation, an
alternative characterization of the nuclear norm is adopted which leads to a
separable, yet non-convex cost minimized via the alternating-direction method
of multipliers. The novel distributed iterations entail reduced-complexity
per-node tasks, and affordable message passing among single-hop neighbors.
Interestingly, upon convergence the distributed (non-convex) estimator provably
attains the global optimum of its centralized counterpart, regardless of
initialization. Several application domains are outlined to highlight the
generality and impact of the proposed framework. These include unveiling
traffic anomalies in backbone networks, predicting networkwide path latencies,
and mapping the RF ambiance using wireless cognitive radios. Simulations with
synthetic and real network data corroborate the convergence of the novel
distributed algorithm, and its centralized performance guarantees.
"
"  One of the popular dynamics on complex networks is the epidemic spreading. An
epidemic model describes how infections spread throughout a network. Among the
compartmental models used to describe epidemics, the
Susceptible-Infected-Susceptible (SIS) model has been widely used. In the SIS
model, each node can be susceptible, become infected with a given infection
rate, and become again susceptible with a given curing rate. In this paper, we
add a new compartment to the classic SIS model to account for human response to
epidemic spread. Each individual can be infected, susceptible, or alert.
Susceptible individuals can become alert with an alerting rate if infected
individuals exist in their neighborhood. An individual in the alert state is
less probable to become infected than an individual in the susceptible state;
due to a newly adopted cautious behavior. The problem is formulated as a
continuous-time Markov process on a general static graph and then modeled into
a set of ordinary differential equations using mean field approximation method
and the corresponding Kolmogorov forward equations. The model is then studied
using results from algebraic graph theory and center manifold theorem. We
analytically show that our model exhibits two distinct thresholds in the
dynamics of epidemic spread. Below the first threshold, infection dies out
exponentially. Beyond the second threshold, infection persists in the steady
state. Between the two thresholds, the infection spreads at the first stage but
then dies out asymptotically as the result of increased alertness in the
network. Finally, simulations are provided to support our findings. Our results
suggest that alertness can be considered as a strategy of controlling the
epidemics which propose multiple potential areas of applications, from
infectious diseases mitigations to malware impact reduction.
"
"  Due to the catastrophic consequences of tsunamis, early warnings need to be
issued quickly in order to mitigate the hazard. Additionally, there is a need
to represent the uncertainty in the predictions of tsunami characteristics
corresponding to the uncertain trigger features (e.g. either position, shape
and speed of a landslide, or sea floor deformation associated with an
earthquake). Unfortunately, computer models are expensive to run. This leads to
significant delays in predictions and makes the uncertainty quantification
impractical. Statistical emulators run almost instantaneously and may represent
well the outputs of the computer model. In this paper, we use the Outer Product
Emulator to build a fast statistical surrogate of a landslide-generated tsunami
computer model. This Bayesian framework enables us to build the emulator by
combining prior knowledge of the computer model properties with a few carefully
chosen model evaluations. The good performance of the emulator is validated
using the Leave-One-Out method.
"
"  We provide a systematic study of the problem of finding the source of a rumor
in a network. We model rumor spreading in a network with a variant of the
popular SIR model and then construct an estimator for the rumor source. This
estimator is based upon a novel topological quantity which we term
\textbf{rumor centrality}. We establish that this is an ML estimator for a
class of graphs. We find the following surprising threshold phenomenon: on
trees which grow faster than a line, the estimator always has non-trivial
detection probability, whereas on trees that grow like a line, the detection
probability will go to 0 as the network grows. Simulations performed on
synthetic networks such as the popular small-world and scale-free networks, and
on real networks such as an internet AS network and the U.S. electric power
grid network, show that the estimator either finds the source exactly or within
a few hops of the true source across different network topologies. We compare
rumor centrality to another common network centrality notion known as distance
centrality. We prove that on trees, the rumor center and distance center are
equivalent, but on general networks, they may differ. Indeed, simulations show
that rumor centrality outperforms distance centrality in finding rumor sources
in networks which are not tree-like.
"
"  We present two alternative ways to apply PAC-Bayesian analysis to sequences
of dependent random variables. The first is based on a new lemma that enables
to bound expectations of convex functions of certain dependent random variables
by expectations of the same functions of independent Bernoulli random
variables. This lemma provides an alternative tool to Hoeffding-Azuma
inequality to bound concentration of martingale values. Our second approach is
based on integration of Hoeffding-Azuma inequality with PAC-Bayesian analysis.
We also introduce a way to apply PAC-Bayesian analysis in situation of limited
feedback. We combine the new tools to derive PAC-Bayesian generalization and
regret bounds for the multiarmed bandit problem. Although our regret bound is
not yet as tight as state-of-the-art regret bounds based on other
well-established techniques, our results significantly expand the range of
potential applications of PAC-Bayesian analysis and introduce a new analysis
tool to reinforcement learning and many other fields, where martingales and
limited feedback are encountered.
"
"  We propose a new method for estimating the intrinsic dimension of a dataset
by applying the principle of regularized maximum likelihood to the distances
between close neighbors. We propose a regularization scheme which is motivated
by divergence minimization principles. We derive the estimator by a Poisson
process approximation, argue about its convergence properties and apply it to a
number of simulated and real datasets. We also show it has the best overall
performance compared with two other intrinsic dimension estimators.
"
"  Three classes of stochastic networks and their performance measures are
considered. These performance measures are defined as the expected value of
some random variables and cannot normally be obtained analytically as functions
of network parameters in a closed form. We give similar representations for the
random variables to provide a useful way of analytical study of these functions
and their gradients. The representations are used to obtain sufficient
conditions for the gradient estimates to be unbiased. The conditions are rather
general and usually met in simulation study of the stochastic networks.
Applications of the results are discussed and some practical algorithms of
calculating unbiased estimates of the gradients are also presented.
"
"  Gaussian processes (GPs) provide a nonparametric representation of functions.
However, classical GP inference suffers from high computational cost and it is
difficult to design nonstationary GP priors in practice. In this paper, we
propose a sparse Gaussian process model, EigenGP, based on the Karhunen-Loeve
(KL) expansion of a GP prior. We use the Nystrom approximation to obtain data
dependent eigenfunctions and select these eigenfunctions by evidence
maximization. This selection reduces the number of eigenfunctions in our model
and provides a nonstationary covariance function. To handle nonlinear
likelihoods, we develop an efficient expectation propagation (EP) inference
algorithm, and couple it with expectation maximization for eigenfunction
selection. Because the eigenfunctions of a Gaussian kernel are associated with
clusters of samples - including both the labeled and unlabeled - selecting
relevant eigenfunctions enables EigenGP to conduct semi-supervised learning.
Our experimental results demonstrate improved predictive performance of EigenGP
over alternative state-of-the-art sparse GP and semisupervised learning methods
for regression, classification, and semisupervised classification.
"
"  This paper introduces a new approach to solve sensor management problems.
Classically sensor management problems can be well formalized as
Partially-Observed Markov Decision Processes (POMPD). The original approach
developped here consists in deriving the optimal parameterized policy based on
a stochastic gradient estimation. We assume in this work that it is possible to
learn the optimal policy off-line (in simulation) using models of the
environement and of the sensor(s). The learned policy can then be used to
manage the sensor(s). In order to approximate the gradient in a stochastic
context, we introduce a new method to approximate the gradient, based on
Infinitesimal Perturbation Approximation (IPA). The effectiveness of this
general framework is illustrated by the managing of an Electronically Scanned
Array Radar. First simulations results are finally proposed.
"
"  The problem of linear modulation classification using likelihood based
methods is considered. Asymptotic properties of most commonly used classifiers
in the literature are derived. These classifiers are based on hybrid likelihood
ratio test (HLRT) and average likelihood ratio test (ALRT), respectively. Both
a single-sensor setting and a multi-sensor setting that uses a distributed
decision fusion approach are analyzed. For a modulation classification system
using a single sensor, it is shown that HLRT achieves asymptotically vanishing
probability of error (Pe) whereas the same result cannot be proven for ALRT. In
a multi-sensor setting using soft decision fusion, conditions are derived under
which Pe vanishes asymptotically. Furthermore, the asymptotic analysis of the
fusion rule that assumes independent sensor decisions is carried out.
"
"  We propose two nonlinear Kalman smoothers that rely on Student's t
distributions. The T-Robust smoother finds the maximum a posteriori likelihood
(MAP) solution for Gaussian process noise and Student's t observation noise,
and is extremely robust against outliers, outperforming the recently proposed
l1-Laplace smoother in extreme situations (e.g. 50% or more outliers). The
second estimator, which we call the T-Trend smoother, is able to follow sudden
changes in the process model, and is derived as a MAP solver for a model with
Student's t-process noise and Gaussian observation noise. We design specialized
methods to solve both problems which exploit the special structure of the
Student's t-distribution, and provide a convergence theory. Both smoothers can
be implemented with only minor modifications to an existing L2 smoother
implementation. Numerical results for linear and nonlinear models illustrating
both robust and fast tracking applications are presented.
"
"  Background and Objective: The survival-agreement plot was proposed and
improved to assess the reliability of a quantitative measure. We propose the
use of survival analysis as an alternative non-parametric approach for
comparison of ordinal qualitative data.
  Study Design and Setting: Two case studies were presented. The first one is
related to a randomized, double blind, placebo-controlled clinical trial to
investigate the safety and efficacy of silymarin/metionin for chronic hepatitis
C. The second one is a prospective study to identify gustatory alterations due
to chorda tympani nerve involvement in patients with chronic otitis media
without prior surgery.
  Results: No significant difference was detected between the two treatments
related to the chronic hepatitis C (p > 0.5). On the other hand, a significant
association was observed between the healthy side and the affected side of the
face of patients with chronic otitis media related to gustatory alterations (p
< 0.05).
  Conclusion: The proposed method can serve as an alternative procedure to
statistical test for comparison of samples from ordinal qualitative variables.
This approach has the advantage of being more familiar to clinical researchers.
"
"  A predictive model of terrorist activity is developed by examining the daily
number of terrorist attacks in Indonesia from 1994 through 2007. The dynamic
model employs a shot noise process to explain the self-exciting nature of the
terrorist activities. This estimates the probability of future attacks as a
function of the times since the past attacks. In addition, the excess of
nonattack days coupled with the presence of multiple coordinated attacks on the
same day compelled the use of hurdle models to jointly model the probability of
an attack day and corresponding number of attacks. A power law distribution
with a shot noise driven parameter best modeled the number of attacks on an
attack day. Interpretation of the model parameters is discussed and predictive
performance of the models is evaluated.
"
"  Precipitation is a complex physical process that varies in space and time.
Predictions and interpolations at unobserved times and/or locations help to
solve important problems in many areas. In this paper, we present a
hierarchical Bayesian model for spatio-temporal data and apply it to obtain
short term predictions of rainfall. The model incorporates physical knowledge
about the underlying processes that determine rainfall, such as advection,
diffusion and convection. It is based on a temporal autoregressive convolution
with spatially colored and temporally white innovations. By linking the
advection parameter of the convolution kernel to an external wind vector, the
model is temporally nonstationary. Further, it allows for nonseparable and
anisotropic covariance structures. With the help of the Voronoi tessellation,
we construct a natural parametrization, that is, space as well as time
resolution consistent, for data lying on irregular grid points. In the
application, the statistical model combines forecasts of three other
meteorological variables obtained from a numerical weather prediction model
with past precipitation observations. The model is then used to predict
three-hourly precipitation over 24 hours. It performs better than a separable,
stationary and isotropic version, and it performs comparably to a deterministic
numerical weather prediction model for precipitation and has the advantage that
it quantifies prediction uncertainty.
"
"  We present methods for online linear optimization that take advantage of
benign (as opposed to worst-case) sequences. Specifically if the sequence
encountered by the learner is described well by a known ""predictable process"",
the algorithms presented enjoy tighter bounds as compared to the typical worst
case bounds. Additionally, the methods achieve the usual worst-case regret
bounds if the sequence is not benign. Our approach can be seen as a way of
adding prior knowledge about the sequence within the paradigm of online
learning. The setting is shown to encompass partial and side information.
Variance and path-length bounds can be seen as particular examples of online
learning with simple predictable sequences.
  We further extend our methods and results to include competing with a set of
possible predictable processes (models), that is ""learning"" the predictable
process itself concurrently with using it to obtain better regret guarantees.
We show that such model selection is possible under various assumptions on the
available feedback. Our results suggest a promising direction of further
research with potential applications to stock market and time series
prediction.
"
"  Actors in realistic social networks play not one but a number of diverse
roles depending on whom they interact with, and a large number of such
role-specific interactions collectively determine social communities and their
organizations. Methods for analyzing social networks should capture these
multi-faceted role-specific interactions, and, more interestingly, discover the
latent organization or hierarchy of social communities. We propose a
hierarchical Mixed Membership Stochastic Blockmodel to model the generation of
hierarchies in social communities, selective membership of actors to subsets of
these communities, and the resultant networks due to within- and
cross-community interactions. Furthermore, to automatically discover these
latent structures from social networks, we develop a Gibbs sampling algorithm
for our model. We conduct extensive validation of our model using synthetic
networks, and demonstrate the utility of our model in real-world datasets such
as predator-prey networks and citation networks.
"
"  In this paper, we present new results on using orthogonal matching pursuit
(OMP), to solve the sparse approximation problem over redundant dictionaries
for complex cases (i.e., complex measurement vector, complex dictionary and
complex additive white Gaussian noise (CAWGN)). A sufficient condition that OMP
can recover the optimal representation of an exactly sparse signal in the
complex cases is proposed both in noiseless and bound Gaussian noise settings.
Similar to exact recovery condition (ERC) results in real cases, we extend them
to complex case and derivate the corresponding ERC in the paper. It leverages
this theory to show that OMP succeed for k-sparse signal from a class of
complex dictionary. Besides, an application with geometrical theory of
diffraction (GTD) model is presented for complex cases. Finally, simulation
experiments illustrate the validity of the theoretical analysis.
"
"  Information diffusion and virus propagation are fundamental processes taking
place in networks. While it is often possible to directly observe when nodes
become infected with a virus or adopt the information, observing individual
transmissions (i.e., who infects whom, or who influences whom) is typically
very difficult. Furthermore, in many applications, the underlying network over
which the diffusions and propagations spread is actually unobserved. We tackle
these challenges by developing a method for tracing paths of diffusion and
influence through networks and inferring the networks over which contagions
propagate. Given the times when nodes adopt pieces of information or become
infected, we identify the optimal network that best explains the observed
infection times. Since the optimization problem is NP-hard to solve exactly, we
develop an efficient approximation algorithm that scales to large datasets and
finds provably near-optimal networks.
  We demonstrate the effectiveness of our approach by tracing information
diffusion in a set of 170 million blogs and news articles over a one year
period to infer how information flows through the online media space. We find
that the diffusion network of news for the top 1,000 media sites and blogs
tends to have a core-periphery structure with a small set of core media sites
that diffuse information to the rest of the Web. These sites tend to have
stable circles of influence with more general news media sites acting as
connectors between them.
"
"  We propose and illustrate a hierarchical Bayesian approach for matching
statistical records observed on different occasions. We show how this model can
be profitably adopted both in record linkage problems and in capture--recapture
setups, where the size of a finite population is the real object of interest.
There are at least two important differences between the proposed model-based
approach and the current practice in record linkage. First, the statistical
model is built up on the actually observed categorical variables and no
reduction (to 0--1 comparisons) of the available information takes place.
Second, the hierarchical structure of the model allows a two-way propagation of
the uncertainty between the parameter estimation step and the matching
procedure so that no plug-in estimates are used and the correct uncertainty is
accounted for both in estimating the population size and in performing the
record linkage. We illustrate and motivate our proposal through a real data
example and simulations.
"
"  Current methods for population mean estimation from data collected by
Respondent Driven Sampling (RDS) are based on the Horvitz-Thompson estimator
together with a set of assumptions on the sampling model under which the
inclusion probabilities can be determined from the information contained in the
data. In this paper, we argue that such set of assumptions are too simplistic
to be realistic and that under realistic sampling models, the situation is far
more complicated. Specifically, we study a realistic RDS sampling model that is
motivated by a real world RDS dataset. We show that, for this model, the
inclusion probabilities, which are necessary for the application of the
Horvitz-Thompson estimator, can not be determined by the information in the
sample alone. An implication is that, unless additional information about the
underlying population network is obtained, it is hopeless to conceive of a
general theory of population mean estimation from current RDS data.
"
"  In this paper, we extend Meek's conjecture (Meek 1997) from directed and
acyclic graphs to chain graphs, and prove that the extended conjecture is true.
Specifically, we prove that if a chain graph H is an independence map of the
independence model induced by another chain graph G, then (i) G can be
transformed into H by a sequence of directed and undirected edge additions and
feasible splits and mergings, and (ii) after each operation in the sequence H
remains an independence map of the independence model induced by G. Our result
has the same important consequence for learning chain graphs from data as the
proof of Meek's conjecture in (Chickering 2002) had for learning Bayesian
networks from data: It makes it possible to develop efficient and
asymptotically correct learning algorithms under mild assumptions.
"
"  We study the problem of estimating the ridges of a density function. Ridge
estimation is an extension of mode finding and is useful for understanding the
structure of a density. It can also be used to find hidden structure in point
cloud data. We show that, under mild regularity conditions, the ridges of the
kernel density estimator consistently estimate the ridges of the true density.
When the data are noisy measurements of a manifold, we show that the ridges are
close and topologically similar to the hidden manifold. To find the estimated
ridges in practice, we adapt the modified mean-shift algorithm proposed by
Ozertem and Erdogmus [J. Mach. Learn. Res. 12 (2011) 1249-1286]. Some numerical
experiments verify that the algorithm is accurate.
"
"  Hierarchical beta process has found interesting applications in recent years.
In this paper we present a modified hierarchical beta process prior with
applications to hierarchical modeling of multiple data sources. The novel use
of the prior over a hierarchical factor model allows factors to be shared
across different sources. We derive a slice sampler for this model, enabling
tractable inference even when the likelihood and the prior over parameters are
non-conjugate. This allows the application of the model in much wider contexts
without restrictions. We present two different data generative models a linear
GaussianGaussian model for real valued data and a linear Poisson-gamma model
for count data. Encouraging transfer learning results are shown for two real
world applications text modeling and content based image retrieval.
"
"  Due to the dynamic nature of biological systems, biological networks
underlying temporal process such as the development of {\it Drosophila
melanogaster} can exhibit significant topological changes to facilitate dynamic
regulatory functions. Thus it is essential to develop methodologies that
capture the temporal evolution of networks, which make it possible to study the
driving forces underlying dynamic rewiring of gene regulation circuity, and to
predict future network structures. Using a new machine learning method called
Tesla, which builds on a novel temporal logistic regression technique, we
report the first successful genome-wide reverse-engineering of the latent
sequence of temporally rewiring gene networks over more than 4000 genes during
the life cycle of \textit{Drosophila melanogaster}, given longitudinal gene
expression measurements and even when a single snapshot of such measurement
resulted from each (time-specific) network is available. Our methods offer the
first glimpse of time-specific snapshots and temporal evolution patterns of
gene networks in a living organism during its full developmental course. The
recovered networks with this unprecedented resolution chart the onset and
duration of many gene interactions which are missed by typical static network
analysis, and are suggestive of a wide array of other temporal behaviors of the
gene network over time not noticed before.
"
"  Parameter estimation in astrophysics often requires the use of complex
physical models. In this paper we study the problem of estimating the
parameters that describe star formation history (SFH) in galaxies. Here,
high-dimensional spectral data from galaxies are appropriately modeled as
linear combinations of physical components, called simple stellar populations
(SSPs), plus some nonlinear distortions. Theoretical data for each SSP is
produced for a fixed parameter vector via computer modeling. Though the
parameters that define each SSP are continuous, optimizing the signal model
over a large set of SSPs on a fine parameter grid is computationally infeasible
and inefficient. The goal of this study is to estimate the set of parameters
that describes the SFH of each galaxy. These target parameters, such as the
average ages and chemical compositions of the galaxy's stellar populations, are
derived from the SSP parameters and the component weights in the signal model.
Here, we introduce a principled approach of choosing a small basis of SSP
prototypes for SFH parameter estimation. The basic idea is to quantize the
vector space and effective support of the model components. In addition to
greater computational efficiency, we achieve better estimates of the SFH target
parameters. In simulations, our proposed quantization method obtains a
substantial improvement in estimating the target parameters over the common
method of employing a parameter grid. Sparse coding techniques are not
appropriate for this problem without proper constraints, while constrained
sparse coding methods perform poorly for parameter estimation because their
objective is signal reconstruction, not estimation of the target parameters.
"
"  Computational Intelligence (CI) is a sub-branch of Artificial Intelligence
paradigm focusing on the study of adaptive mechanisms to enable or facilitate
intelligent behavior in complex and changing environments. There are several
paradigms of CI [like artificial neural networks, evolutionary computations,
swarm intelligence, artificial immune systems, fuzzy systems and many others],
each of these has its origins in biological systems [biological neural systems,
natural Darwinian evolution, social behavior, immune system, interactions of
organisms with their environment]. Most of those paradigms evolved into
separate machine learning (ML) techniques, where probabilistic methods are used
complementary with CI techniques in order to effectively combine elements of
learning, adaptation, evolution and Fuzzy logic to create heuristic algorithms
that are, in some sense, intelligent. The current trend is to develop consensus
techniques, since no single machine learning algorithms is superior to others
in all possible situations. In order to overcome this problem several
meta-approaches were proposed in ML focusing on the integration of results from
different methods into single prediction. We discuss here the Landau theory for
the nonlinear equation that can describe the adaptive integration of
information acquired from an ensemble of independent learning agents. The
influence of each individual agent on other learners is described similarly to
the social impact theory. The final decision outcome for the consensus system
is calculated using majority rule in the stationary limit, yet the minority
solutions can survive inside the majority population as the complex
intermittent clusters of opposite opinion.
"
"  Motivation: High-throughput sequencing enables expression analysis at the
level of individual transcripts. The analysis of transcriptome expression
levels and differential expression estimation requires a probabilistic approach
to properly account for ambiguity caused by shared exons and finite read
sampling as well as the intrinsic biological variance of transcript expression.
  Results: We present BitSeq (Bayesian Inference of Transcripts from Sequencing
data), a Bayesian approach for estimation of transcript expression level from
RNA-seq experiments. Inferred relative expression is represented by Markov
chain Monte Carlo (MCMC) samples from the posterior probability distribution of
a generative model of the read data. We propose a novel method for differential
expression analysis across replicates which propagates uncertainty from the
sample-level model while modelling biological variance using an
expression-level-dependent prior. We demonstrate the advantages of our method
using simulated data as well as an RNA-seq dataset with technical and
biological replication for both studied conditions.
  Availability: The implementation of the transcriptome expression estimation
and differential expression analysis, BitSeq, has been written in C++.
"
"  Gaussian Process (GP) regression models typically assume that residuals are
Gaussian and have the same variance for all observations. However, applications
with input-dependent noise (heteroscedastic residuals) frequently arise in
practice, as do applications in which the residuals do not have a Gaussian
distribution. In this paper, we propose a GP Regression model with a latent
variable that serves as an additional unobserved covariate for the regression.
This model (which we call GPLC) allows for heteroscedasticity since it allows
the function to have a changing partial derivative with respect to this
unobserved covariate. With a suitable covariance function, our GPLC model can
handle (a) Gaussian residuals with input-dependent variance, or (b)
non-Gaussian residuals with input-dependent variance, or (c) Gaussian residuals
with constant variance. We compare our model, using synthetic datasets, with a
model proposed by Goldberg, Williams and Bishop (1998), which we refer to as
GPLV, which only deals with case (a), as well as a standard GP model which can
handle only case (c). Markov Chain Monte Carlo methods are developed for both
modelsl. Experiments show that when the data is heteroscedastic, both GPLC and
GPLV give better results (smaller mean squared error and negative
log-probability density) than standard GP regression. In addition, when the
residual are Gaussian, our GPLC model is generally nearly as good as GPLV,
while when the residuals are non-Gaussian, our GPLC model is better than GPLV.
"
"  Dual decomposition provides a tractable framework for designing algorithms
for finding the most probable (MAP) configuration in graphical models. However,
for many real-world inference problems, the typical decomposition has a large
integrality gap, due to frustrated cycles. One way to tighten the relaxation is
to introduce additional constraints that explicitly enforce cycle consistency.
Earlier work showed that cluster-pursuit algorithms, which iteratively
introduce cycle and other higherorder consistency constraints, allows one to
exactly solve many hard inference problems. However, these algorithms
explicitly enumerate a candidate set of clusters, limiting them to triplets or
other short cycles. We solve the search problem for cycle constraints, giving a
nearly linear time algorithm for finding the most frustrated cycle of arbitrary
length. We show how to use this search algorithm together with the dual
decomposition framework and clusterpursuit. The new algorithm exactly solves
MAP inference problems arising from relational classification and stereo
vision.
"
"  This paper presents a study of the body orientation of domestic cattle on
free pastures in several European states, based on Google satellite
photographs. In sum, 232 herds with 3412 individuals were evaluated. Two
independent groups participated in our study and came to the same conclusion
that, in contradiction to the recent findings of other researchers, no
alignment of the animals and of their herds along geomagnetic field lines could
be found. Several possible reasons for this discrepancy should be taken into
account: poor quality of Google satellite photographs, difficulties in
determining the body axis, selection of herds or animals within herds, lack of
blinding in the evaluation, possible subconscious bias, and, most importantly,
high sensitivity of the calculated main directions of the Rayleigh vectors to
some kind of bias or to some overlooked or ignored confounder. This factor
could easily have led to an unsubstantiated positive conclusion about the
existence of magnetoreception.
"
"  The rapid explosion in retail data calls for more effective and efficient
discovery of association rules to develop relevant business strategies and
rules.Unlike online shopping sites, most brick and mortar retail shops are
located in geographically and demographically diverse areas. This diversity
presents a new challenge to the classical association rule model which assumes
a homogenous group of customers behaving differently. The focus of this paper
is centered on the discovery of association rules that were hidden as a result
of a geographical and demographically diverse data. We will introduce a novel
measure which incorporates the entropy measure with modified weighting for the
detection of association rules not detected by the standard measures due to
Simpson's paradox. The proposed measure is evaluated using a real-word case
study involving a major retailer of fashion good in the context of traditional
brick and mortar setting.
"
"  This paper presents Natural Evolution Strategies (NES), a recent family of
algorithms that constitute a more principled approach to black-box optimization
than established evolutionary algorithms. NES maintains a parameterized
distribution on the set of solution candidates, and the natural gradient is
used to update the distribution's parameters in the direction of higher
expected fitness. We introduce a collection of techniques that address issues
of convergence, robustness, sample complexity, computational complexity and
sensitivity to hyperparameters. This paper explores a number of implementations
of the NES family, ranging from general-purpose multi-variate normal
distributions to heavy-tailed and separable distributions tailored towards
global optimization and search in high dimensional spaces, respectively.
Experimental results show best published performance on various standard
benchmarks, as well as competitive performance on others.
"
"  We study the problem of estimating, in the sense of optimal transport
metrics, a measure which is assumed supported on a manifold embedded in a
Hilbert space. By establishing a precise connection between optimal transport
metrics, optimal quantization, and learning theory, we derive new probabilistic
bounds for the performance of a classic algorithm in unsupervised learning
(k-means), when used to produce a probability measure derived from the data. In
the course of the analysis, we arrive at new lower bounds, as well as
probabilistic upper bounds on the convergence rate of the empirical law of
large numbers, which, unlike existing bounds, are applicable to a wide class of
measures.
"
"  High dimensional data analysis is known to be as a challenging problem. In
this article, we give a theoretical analysis of high dimensional classification
of Gaussian data which relies on a geometrical analysis of the error measure.
It links a problem of classification with a problem of nonparametric
regression. We give an algorithm designed for high dimensional data which
appears straightforward in the light of our theoretical work, together with the
thresholding estimation theory. We finally attempt to give a general treatment
of the problem that can be extended to frameworks other than gaussian.
"
"  The Bernoulli Factory is an algorithm that takes as input a series of i.i.d.
Bernoulli random variables with an unknown but fixed success probability $p$,
and outputs a corresponding series of Bernoulli random variables with success
probability $f(p)$, where the function $f$ is known and defined on the interval
$[0,1]$. While several practical uses of the method have been proposed in Monte
Carlo applications, these require an implementation framework that is flexible,
general and efficient. We present such a framework for functions that are
either strictly linear, concave, or convex on the unit interval using a series
of envelope functions defined through a cascade, and show that this method not
only greatly reduces the number of input bits needed in practice compared to
other currently proposed solutions for more specific problems, and is easy to
specify for simple forms, but can easily be coupled to asymptotically efficient
methods to allow for theoretically strong results.
"
"  We extend to the beta-divergence (Itakura-Saito) case beta =0, the
comparative bi-stochaticization analyses-previously conducted (arXiv:1208.3428)
for the (Kullback-Leibler) beta=1 and (squared-Euclidean) beta = 2 cases -of
the 3,107 - county 1995-2000 U. S. migration network. A heuristic, ""greedy""
algorithm is devised. While the largest 25,329 entries of the 735,531 non-zero
entries of the bi-stochasticized table - in the beta=1 case - are required to
complete the widely-applied two-stage (double-standardization and
strong-component hierarchical clustering) procedure, 105,363 of the 735,531 are
needed (reflective of greater uniformity of entries) in the beta=0 instance.
The North Carolina counties of Mecklenburg (Charlotte) and Wake (Raleigh) are
considerably relatively more cosmopolitan in the beta=0 study. The Colorado
county of El Paso (Colorado Springs) replaces the Florida Atlantic county of
Brevard (the ""Space Coast"") as the most cosmopolitan, with Brevard becoming the
second-most. Honolulu County splinters away from the other four (still-grouped)
Hawaiian counties, becoming the fifth most cosmopolitan county nation-wide. The
five counties of Rhode Island remain intact as a regional entity, but the eight
counties of Connecticut fragment, leaving only five counties clustered.
"
"  We use confirmatory factor analysis to derive a unifying measure of
comparison of scientists based on bibliometric measurements, by utilizing the
h-index, some similar h-type indices as well as other common measures of
scientific performance. We use a real data example from nine well-known
departments of statistics to demonstrate our approach and argue that our
combined measure results in a better overall evaluation of a researchers'
scientific work.
"
"  Given a query on the PASCAL database maintained by the INIST, we design user
interfaces to visualize and browse two types of graphs extracted from
abstracts: 1) the graph of all associations between authors (co-author graph),
2) the graph of strong associations between authors and terms automatically
extracted from abstracts and grouped using linguistic variations. We adapt for
this purpose the TermWatch system that comprises a term extractor, a relation
identifier which yields the terminological network and a clustering module. The
results are output on two interfaces: a graphic one mapping the clusters in a
2D space and a terminological hypertext network allowing the user to
interactively explore results and return to source texts.
"
"  This paper describes several new algorithms for estimating the parameters of
a periodic bandlimited signal from samples corrupted by jitter (timing noise)
and additive noise. Both classical (non-random) and Bayesian formulations are
considered: an Expectation-Maximization (EM) algorithm is developed to compute
the maximum likelihood (ML) estimator for the classical estimation framework,
and two Gibbs samplers are proposed to approximate the Bayes least squares
(BLS) estimate for parameters independently distributed according to a uniform
prior. Simulations are performed to demonstrate the significant performance
improvement achievable using these algorithms as compared to linear estimators.
The ML estimator is also compared to the Cramer-Rao lower bound to determine
the range of jitter for which the estimator is approximately efficient. These
simulations provide evidence that the nonlinear algorithms derived here can
tolerate 1.4-2 times more jitter than linear estimators, reducing on-chip ADC
power consumption by 50-75 percent.
"
"  Most methods for decision-theoretic online learning are based on the Hedge
algorithm, which takes a parameter called the learning rate. In most previous
analyses the learning rate was carefully tuned to obtain optimal worst-case
performance, leading to suboptimal performance on easy instances, for example
when there exists an action that is significantly better than all others. We
propose a new way of setting the learning rate, which adapts to the difficulty
of the learning problem: in the worst case our procedure still guarantees
optimal performance, but on easy instances it achieves much smaller regret. In
particular, our adaptive method achieves constant regret in a probabilistic
setting, when there exists an action that on average obtains strictly smaller
loss than all other actions. We also provide a simulation study comparing our
approach to existing methods.
"
"  We consider two variables that are related to each other by an invertible
function. While it has previously been shown that the dependence structure of
the noise can provide hints to determine which of the two variables is the
cause, we presently show that even in the deterministic (noise-free) case,
there are asymmetries that can be exploited for causal inference. Our method is
based on the idea that if the function and the probability density of the cause
are chosen independently, then the distribution of the effect will, in a
certain sense, depend on the function. We provide a theoretical analysis of
this method, showing that it also works in the low noise regime, and link it to
information geometry. We report strong empirical results on various real-world
data sets from different domains.
"
"  We describe a new variational lower-bound on the minimum energy configuration
of a planar binary Markov Random Field (MRF). Our method is based on adding
auxiliary nodes to every face of a planar embedding of the graph in order to
capture the effect of unary potentials. A ground state of the resulting
approximation can be computed efficiently by reduction to minimum-weight
perfect matching. We show that optimization of variational parameters achieves
the same lower-bound as dual-decomposition into the set of all cycles of the
original graph. We demonstrate that our variational optimization converges
quickly and provides high-quality solutions to hard combinatorial problems
10-100x faster than competing algorithms that optimize the same bound.
"
"  Outlying curves often occur in functional or longitudinal datasets, and can
be very influential on parameter estimators and very hard to detect visually.
In this article we introduce estimators of the mean and the principal
components that are resistant to, and then can be used for detection of,
outlying sample trajectories. The estimators are based on reduced-rank t-models
and are specifically aimed at sparse and irregularly sampled functional data.
The outlier-resistance properties of the estimators and their relative
efficiency for noncontaminated data are studied theoretically and by
simulation. Applications to the analysis of Internet traffic data and glycated
hemoglobin levels in diabetic children are presented.
"
"  Discussion of ""A statistical analysis of multiple temperature proxies: Are
reconstructions of surface temperatures over the last 1000 years reliable?"" by
B.B. McShane and A.J. Wyner [arXiv:1104.4002]
"
"  In a context where most published articles are devoted to the development of
""new methods"", comparison studies are generally appreciated by readers but
surprisingly given poor consideration by many scientific journals. In
connection with recent articles on over-optimism and epistemology published in
Bioinformatics, this letter stresses the importance of neutral comparison
studies for the objective evaluation of existing methods and the establishment
of standards by drawing parallels with clinical research.
"
"  Communication networks have evolved from specialized, research and tactical
transmission systems to large-scale and highly complex interconnections of
intelligent devices, increasingly becoming more commercial, consumer-oriented,
and heterogeneous. Propelled by emergent social networking services and
high-definition streaming platforms, network traffic has grown explosively
thanks to the advances in processing speed and storage capacity of
state-of-the-art communication technologies. As ""netizens"" demand a seamless
networking experience that entails not only higher speeds, but also resilience
and robustness to failures and malicious cyber-attacks, ample opportunities for
signal processing (SP) research arise. The vision is for ubiquitous smart
network devices to enable data-driven statistical learning algorithms for
distributed, robust, and online network operation and management, adaptable to
the dynamically-evolving network landscape with minimal need for human
intervention. The present paper aims at delineating the analytical background
and the relevance of SP tools to dynamic network monitoring, introducing the SP
readership to the concept of dynamic network cartography -- a framework to
construct maps of the dynamic network state in an efficient and scalable manner
tailored to large-scale heterogeneous networks.
"
"  A debate exists over whether tropical troposphere temperature trends in
climate models are inconsistent with observations (Karl et al. 2006, IPCC
(2007), Douglass et al 2007, Santer et al 2008). Most recently, Santer et al
(2008, herein S08) asserted that the Douglass et al statistical methodology was
flawed and that a correct methodology showed there is no statistically
significant difference between the model ensemble mean trend and either RSS or
UAH satellite observations. However this result was based on data ending in
1999. Using data up to the end of 2007 (as available to S08) or to the end of
2008 and applying exactly the same methodology as S08 results in a
statistically significant difference between the ensemble mean trend and UAH
observations and approaching statistical significance for the RSS T2 data. The
claim by S08 to have achieved a ""partial resolution"" of the discrepancy between
observations and the model ensemble mean trend is unwarranted.
"
"  This paper is a survey paper on stochastic epidemic models. A simple
stochastic epidemic model is defined and exact and asymptotic model properties
(relying on a large community) are presented. The purpose of modelling is
illustrated by studying effects of vaccination and also in terms of inference
procedures for important parameters, such as the basic reproduction number and
the critical vaccination coverage. Several generalizations towards realism,
e.g. multitype and household epidemic models, are also presented, as is a model
for endemic diseases.
"
"  Determining the magnitude and location of neural sources within the brain
that are responsible for generating magnetoencephalography (MEG) signals
measured on the surface of the head is a challenging problem in functional
neuroimaging. The number of potential sources within the brain exceeds by an
order of magnitude the number of recording sites. As a consequence, the
estimates for the magnitude and location of the neural sources will be
ill-conditioned because of the underdetermined nature of the problem. One
well-known technique designed to address this imbalance is the minimum norm
estimator (MNE). This approach imposes an $L^2$ regularization constraint that
serves to stabilize and condition the source parameter estimates. However,
these classes of regularizer are static in time and do not consider the
temporal constraints inherent to the biophysics of the MEG experiment. In this
paper we propose a dynamic state-space model that accounts for both spatial and
temporal correlations within and across candidate intracortical sources. In our
model, the observation model is derived from the steady-state solution to
Maxwell's equations while the latent model representing neural dynamics is
given by a random walk process.
"
"  We present a graph-based variational algorithm for multiclass classification
of high-dimensional data, motivated by total variation techniques. The energy
functional is based on a diffuse interface model with a periodic potential. We
augment the model by introducing an alternative measure of smoothness that
preserves symmetry among the class labels. Through this modification of the
standard Laplacian, we construct an efficient multiclass method that allows for
sharp transitions between classes. The experimental results demonstrate that
our approach is competitive with the state of the art among other graph-based
algorithms.
"
"  We consider the problem of multivariate density estimation when the unknown
density is assumed to follow a particular form of dimensionality reduction, a
noisy independent factor analysis (IFA) model. In this model the data are
generated by a number of latent independent components having unknown
distributions and are observed in Gaussian noise. We do not assume that either
the number of components or the matrix mixing the components are known. We show
that the densities of this form can be estimated with a fast rate. Using the
mirror averaging aggregation algorithm, we construct a density estimator which
achieves a nearly parametric rate log^(1/4)n/sqrt(n), independent of the
dimensionality of the data, as the sample size $n$ tends to infinity. This
estimator is adaptive to the number of components, their distributions and the
mixing matrix. We then apply this density estimator to construct nonparametric
plug-in classifiers and show that they achieve the best obtainable rate of the
excess Bayes risk, to within a logarithmic factor independent of the dimension
of the data. Applications of this classifier to simulated data sets and to real
data from a remote sensing experiment show promising results.
"
"  Transition points are common in physiological processes. However the
transition between normothermia and hypothermia during haemorrhagic shock has
rarely been systematically quantified from intensive time series data. We
estimated the critical transition point (CTP) and provided confidence intervals
for core body temperature response to acute severe haemorrhage in a conscious
rat model. Estimates were obtained by traditional piecewise linear regression
(broken stick model) and compared to those from the more novel bent cable
regression. Bent cable regression relaxes the assumption of an abrupt point
transition, and thus allows the capture of a potentially gradual transition
phase; the broken stick is a special case of the bent cable model. We
calculated two types of confidence intervals, assuming either independent or
autoregressive structure for the residuals. In spite of the severity of the
haemorrhage, median temperature change was minor (0.8 C; IQR 0.57-1.31 C) and
only four of 38 rats were clinically hypothermic (core temperature < 35 C).
However, a transition could be estimated for 23 rats. Bent cable fits were
superior when the transition appeared to be gradual rather than abrupt. In all
cases, assuming independence gave incorrect uncertainty estimates of CTP. For
15 animals, neither model could be fitted because of irregular temperature
profiles that did not conform to the assumption of a single transition.
Arbitrary imposition of broken stick fits on a gradual transition profile and
assuming independent rather than autocorrelated error may result in misleading
estimates of CTP. Identification of the onset of irreversible shock will
require further quantification of appropriate time-dependent physiological
variables and their behaviour during haemorrhage.
"
"  We consider the problem of inferring the interactions between a set of N
binary variables from the knowledge of their frequencies and pairwise
correlations. The inference framework is based on the Hopfield model, a special
case of the Ising model where the interaction matrix is defined through a set
of patterns in the variable space, and is of rank much smaller than N. We show
that Maximum Lik elihood inference is deeply related to Principal Component
Analysis when the amp litude of the pattern components, xi, is negligible
compared to N^1/2. Using techniques from statistical mechanics, we calculate
the corrections to the patterns to the first order in xi/N^1/2. We stress that
it is important to generalize the Hopfield model and include both attractive
and repulsive patterns, to correctly infer networks with sparse and strong
interactions. We present a simple geometrical criterion to decide how many
attractive and repulsive patterns should be considered as a function of the
sampling noise. We moreover discuss how many sampled configurations are
required for a good inference, as a function of the system size, N and of the
amplitude, xi. The inference approach is illustrated on synthetic and
biological data.
"
"  We present a robust multiple manifolds structure learning (RMMSL) scheme to
robustly estimate data structures under the multiple low intrinsic dimensional
manifolds assumption. In the local learning stage, RMMSL efficiently estimates
local tangent space by weighted low-rank matrix factorization. In the global
learning stage, we propose a robust manifold clustering method based on local
structure learning results. The proposed clustering method is designed to get
the flattest manifolds clusters by introducing a novel curved-level similarity
function. Our approach is evaluated and compared to state-of-the-art methods on
synthetic data, handwritten digit images, human motion capture data and
motorbike videos. We demonstrate the effectiveness of the proposed approach,
which yields higher clustering accuracy, and produces promising results for
challenging tasks of human motion segmentation and motion flow learning from
videos.
"
"  We empirically investigate the best trade-off between sparse and
uniformly-weighted multiple kernel learning (MKL) using the elastic-net
regularization on real and simulated datasets. We find that the best trade-off
parameter depends not only on the sparsity of the true kernel-weight spectrum
but also on the linear dependence among kernels and the number of samples.
"
"  A basic assumption of statistical learning theory is that train and test data
are drawn from the same underlying distribution. Unfortunately, this assumption
doesn't hold in many applications. Instead, ample labeled data might exist in a
particular `source' domain while inference is needed in another, `target'
domain. Domain adaptation methods leverage labeled data from both domains to
improve classification on unseen data in the target domain. In this work we
survey domain transfer learning methods for various application domains with
focus on recent work in Computer Vision.
"
"  We show that the class of strongly connected graphical models with treewidth
at most k can be properly efficiently PAC-learnt with respect to the
Kullback-Leibler Divergence. Previous approaches to this problem, such as those
of Chow ([1]), and Ho gen ([7]) have shown that this class is PAC-learnable by
reducing it to a combinatorial optimization problem. However, for k > 1, this
problem is NP-complete ([15]), and so unless P=NP, these approaches will take
exponential amounts of time. Our approach differs significantly from these, in
that it first attempts to find approximate conditional independencies by
solving (polynomially many) submodular optimization problems, and then using a
dynamic programming formulation to combine the approximate conditional
independence information to derive a graphical model with underlying graph of
the tree-width specified. This gives us an efficient (polynomial time in the
number of random variables) PAC-learning algorithm which requires only
polynomial number of samples of the true distribution, and only polynomial
running time.
"
"  We present a new joint longitudinal and survival model aimed at estimating
the association between the risk of an event and the change in and history of a
biomarker that is repeatedly measured over time. We use cubic B-splines models
for the longitudinal component that lend themselves to straight-forward
formulations of the slope and integral of the trajectory of the biomarker. The
model is applied to data collected in a long term follow-up study of HIV
infected infants in Uganda. Estimation is carried out using MCMC methods. We
also explore using the deviance information criteria, the conditional
predictive ordinate and ROC curves for model selection and evaluation.
"
"  In the theory of compressed sensing (CS), the sparsity ||x||_0 of the unknown
signal x\in\R^p is commonly assumed to be a known parameter. However, it is
typically unknown in practice. Due to the fact that many aspects of CS depend
on knowing ||x||_0, it is important to estimate this parameter in a data-driven
way. A second practical concern is that ||x||_0 is a highly unstable function
of x. In particular, for real signals with entries not exactly equal to 0, the
value ||x||_0=p is not a useful description of the effective number of
coordinates. In this paper, we propose to estimate a stable measure of sparsity
s(x):=||x||_1^2/||x||_2^2, which is a sharp lower bound on ||x||_0. Our
estimation procedure uses only a small number of linear measurements, does not
rely on any sparsity assumptions, and requires very little computation. A
confidence interval for s(x) is provided, and its width is shown to have no
dependence on the signal dimension p. Moreover, this result extends naturally
to the matrix recovery setting, where a soft version of matrix rank can be
estimated with analogous guarantees. Finally, we show that the use of
randomized measurements is essential to estimating s(x). This is accomplished
by proving that the minimax risk for estimating s(x) with deterministic
measurements is large when n<<p.
"
"  We consider the problem of learning a forest of nonlinear decision rules with
general loss functions. The standard methods employ boosted decision trees such
as Adaboost for exponential loss and Friedman's gradient boosting for general
loss. In contrast to these traditional boosting algorithms that treat a tree
learner as a black box, the method we propose directly learns decision forests
via fully-corrective regularized greedy search using the underlying forest
structure. Our method achieves higher accuracy and smaller models than gradient
boosting (and Adaboost with exponential loss) on many datasets.
"
"  Parameter estimation in Markov random fields (MRFs) is a difficult task, in
which inference over the network is run in the inner loop of a gradient descent
procedure. Replacing exact inference with approximate methods such as loopy
belief propagation (LBP) can suffer from poor convergence. In this paper, we
provide a different approach for combining MRF learning and Bethe
approximation. We consider the dual of maximum likelihood Markov network
learning - maximizing entropy with moment matching constraints - and then
approximate both the objective and the constraints in the resulting
optimization problem. Unlike previous work along these lines (Teh & Welling,
2003), our formulation allows parameter sharing between features in a general
log-linear model, parameter regularization and conditional training. We show
that piecewise training (Sutton & McCallum, 2005) is a very restricted special
case of this formulation. We study two optimization strategies: one based on a
single convex approximation and one that uses repeated convex approximations.
We show results on several real-world networks that demonstrate that these
algorithms can significantly outperform learning with loopy and piecewise. Our
results also provide a framework for analyzing the trade-offs of different
relaxations of the entropy objective and of the constraints.
"
"  A predictor variable or dose that is measured with substantial error may
possess an error-free milestone, such that it is known with negligible error
whether the value of the variable is to the left or right of the milestone.
Such a milestone provides a basis for estimating a linear relationship between
the true but unknown value of the error-free predictor and an outcome, because
the milestone creates a strong and valid instrumental variable. The inferences
are nonparametric and robust, and in the simplest cases, they are exact and
distribution free. We also consider multiple milestones for a single predictor
and milestones for several predictors whose partial slopes are estimated
simultaneously. Examples are drawn from the Wisconsin Longitudinal Study, in
which a BA degree acts as a milestone for sixteen years of education, and the
binary indicator of military service acts as a milestone for years of service.
"
"  One of the challenges with functional data is incorporating spatial
structure, or local correlation, into the analysis. This structure is inherent
in the output from an increasing number of biomedical technologies, and a
functional linear model is often used to estimate the relationship between the
predictor functions and scalar responses. Common approaches to the ill-posed
problem of estimating a coefficient function typically involve two stages:
regularization and estimation. Regularization is usually done via dimension
reduction, projecting onto a predefined span of basis functions or a reduced
set of eigenvectors (principal components). In contrast, we present a unified
approach that directly incorporates spatial structure into the estimation
process by exploiting the joint eigenproperties of the predictors and a linear
penalty operator. In this sense, the components in the regression are
`partially empirical' and the framework is provided by the generalized singular
value decomposition (GSVD). The GSVD clarifies the penalized estimation process
and informs the choice of penalty by making explicit the joint influence of the
penalty and predictors on the bias, variance, and performance of the estimated
coefficient function. Laboratory spectroscopy data and simulations are used to
illustrate the concepts.
"
"  We comment on a recent approach to spatial stochastic inversion, which
centers on a concept known as ""anchors"" and conducts nonparametric estimation
of the likelihood of the anchors (along with other model parameters) with
respect to data obtained from field processes. The approach is called ""anchored
inversion"" or, less desirably, ""method of anchored distribution"". Conceptual
and technical observations are made regarding the development, interpretation,
and use of this approach. We also point out that this approach has been
superseded by new developments.
"
"  We present the Procrustes measure, a novel measure based on Procrustes
rotation that enables quantitative comparison of the output of manifold-based
embedding algorithms (such as LLE (Roweis and Saul, 2000) and Isomap (Tenenbaum
et al, 2000)). The measure also serves as a natural tool when choosing
dimension-reduction parameters. We also present two novel dimension-reduction
techniques that attempt to minimize the suggested measure, and compare the
results of these techniques to the results of existing algorithms. Finally, we
suggest a simple iterative method that can be used to improve the output of
existing algorithms.
"
"  In this paper we extend temporal difference policy evaluation algorithms to
performance criteria that include the variance of the cumulative reward. Such
criteria are useful for risk management, and are important in domains such as
finance and process control. We propose both TD(0) and LSTD(lambda) variants
with linear function approximation, prove their convergence, and demonstrate
their utility in a 4-dimensional continuous state space problem.
"
"  Understanding the spatiotemporal distribution of people within a city is
crucial to many planning applications. Obtaining data to create required
knowledge, currently involves costly survey methods. At the same time
ubiquitous mobile sensors from personal GPS devices to mobile phones are
collecting massive amounts of data on urban systems. The locations,
communications, and activities of millions of people are recorded and stored by
new information technologies. This work utilizes novel dynamic data, generated
by mobile phone users, to measure spatiotemporal changes in population. In the
process, we identify the relationship between land use and dynamic population
over the course of a typical week. A machine learning classification algorithm
is used to identify clusters of locations with similar zoned uses and mobile
phone activity patterns. It is shown that the mobile phone data is capable of
delivering useful information on actual land use that supplements zoning
regulations.
"
"  Confidence intervals based on penalized maximum likelihood estimators such as
the LASSO, adaptive LASSO, and hard-thresholding are analyzed. In the
known-variance case, the finite-sample coverage properties of such intervals
are determined and it is shown that symmetric intervals are the shortest. The
length of the shortest intervals based on the hard-thresholding estimator is
larger than the length of the shortest interval based on the adaptive LASSO,
which is larger than the length of the shortest interval based on the LASSO,
which in turn is larger than the standard interval based on the maximum
likelihood estimator. In the case where the penalized estimators are tuned to
possess the `sparsity property', the intervals based on these estimators are
larger than the standard interval by an order of magnitude. Furthermore, a
simple asymptotic confidence interval construction in the `sparse' case, that
also applies to the smoothly clipped absolute deviation estimator, is
discussed. The results for the known-variance case are shown to carry over to
the unknown-variance case in an appropriate asymptotic sense.
"
"  This paper suggests a learning-theoretic perspective on how synaptic
plasticity benefits global brain functioning. We introduce a model, the
selectron, that (i) arises as the fast time constant limit of leaky
integrate-and-fire neurons equipped with spiking timing dependent plasticity
(STDP) and (ii) is amenable to theoretical analysis. We show that the selectron
encodes reward estimates into spikes and that an error bound on spikes is
controlled by a spiking margin and the sum of synaptic weights. Moreover, the
efficacy of spikes (their usefulness to other reward maximizing selectrons)
also depends on total synaptic strength. Finally, based on our analysis, we
propose a regularized version of STDP, and show the regularization improves the
robustness of neuronal learning when faced with multiple stimuli.
"
"  As compared to load demand, frequent wind energy intermittencies produce
large short-term (sub 1-hr to 3-hr) deficits (and surpluses) in the energy
supply. These intermittent deficits pose systemic and structural risks that
will likely lead to energy deficits that have significant reliability
implications for energy system operators and consumers. This work provides a
toolset to help policy makers quantify these first-order risks. The thinking
methodology / framework shows that increasing wind energy penetration
significantly increases the risk of loss in California. In addition, the work
presents holistic risk tables as a general innovation to help decision makers
quickly grasp the full impact of risk.
"
"  Suppose that multiple experts (or learning algorithms) provide us with
alternative Bayesian network (BN) structures over a domain, and that we are
interested in combining them into a single consensus BN structure.
Specifically, we are interested in that the consensus BN structure only
represents independences all the given BN structures agree upon and that it has
as few parameters associated as possible. In this paper, we prove that there
may exist several non-equivalent consensus BN structures and that finding one
of them is NP-hard. Thus, we decide to resort to heuristics to find an
approximated consensus BN structure. In this paper, we consider the heuristic
proposed in
\citep{MatzkevichandAbramson1992,MatzkevichandAbramson1993a,MatzkevichandAbramson1993b}.
This heuristic builds upon two algorithms, called Methods A and B, for
efficiently deriving the minimal directed independence map of a BN structure
relative to a given node ordering. Methods A and B are claimed to be correct
although no proof is provided (a proof is just sketched). In this paper, we
show that Methods A and B are not correct and propose a correction of them.
"
"  The complex wave representation (CWR) converts unsigned 2D distance
transforms into their corresponding wave functions. Here, the distance
transform S(X) appears as the phase of the wave function
\phi(X)---specifically, \phi(X)=exp(iS(X)/\tau where \tau is a free parameter.
In this work, we prove a novel result using the higher-order stationary phase
approximation: we show convergence of the normalized power spectrum (squared
magnitude of the Fourier transform) of the wave function to the density
function of the distance transform gradients as the free parameter \tau-->0. In
colloquial terms, spatial frequencies are gradient histogram bins. Since the
distance transform gradients have only orientation information (as their
magnitudes are identically equal to one almost everywhere), as \tau-->0, the 2D
Fourier transform values mainly lie on the unit circle in the spatial frequency
domain. The proof of the result involves standard integration techniques and
requires proper ordering of limits. Our mathematical relation indicates that
the CWR of distance transforms is an intriguing, new representation.
"
"  With inspiration from Random Forests (RF) in the context of classification, a
new clustering ensemble method---Cluster Forests (CF) is proposed.
Geometrically, CF randomly probes a high-dimensional data cloud to obtain ""good
local clusterings"" and then aggregates via spectral clustering to obtain
cluster assignments for the whole dataset. The search for good local
clusterings is guided by a cluster quality measure kappa. CF progressively
improves each local clustering in a fashion that resembles the tree growth in
RF. Empirical studies on several real-world datasets under two different
performance metrics show that CF compares favorably to its competitors.
Theoretical analysis reveals that the kappa measure makes it possible to grow
the local clustering in a desirable way---it is ""noise-resistant"". A
closed-form expression is obtained for the mis-clustering rate of spectral
clustering under a perturbation model, which yields new insights into some
aspects of spectral clustering.
"
"  Producing overlapping schemes is a major issue in clustering. Recent proposed
overlapping methods relies on the search of an optimal covering and are based
on different metrics, such as Euclidean distance and I-Divergence, used to
measure closeness between observations. In this paper, we propose the use of
another measure for overlapping clustering based on a kernel similarity metric
.We also estimate the number of overlapped clusters using the Gram matrix.
Experiments on both Iris and EachMovie datasets show the correctness of the
estimation of number of clusters and show that measure based on kernel
similarity metric improves the precision, recall and f-measure in overlapping
clustering.
"
"  According to current research in bibliometrics, percentiles (or percentile
rank classes) are the most suitable method for normalising the citation counts
of individual publications in terms of the subject area, the document type and
the publication year. Up to now, bibliometric research has concerned itself
primarily with the calculation of percentiles. This study suggests how
percentiles can be analysed meaningfully for an evaluation study. Publication
sets from four universities are compared with each other to provide sample
data. These suggestions take into account on the one hand the distribution of
percentiles over the publications in the sets (here: universities) and on the
other hand concentrate on the range of publications with the highest citation
impact - that is, the range which is usually of most interest in the evaluation
of scientific performance.
"
"  We study the problem of selecting a subset of k random variables from a large
set, in order to obtain the best linear prediction of another variable of
interest. This problem can be viewed in the context of both feature selection
and sparse approximation. We analyze the performance of widely used greedy
heuristics, using insights from the maximization of submodular functions and
spectral analysis. We introduce the submodularity ratio as a key quantity to
help understand why greedy algorithms perform well even when the variables are
highly correlated. Using our techniques, we obtain the strongest known
approximation guarantees for this problem, both in terms of the submodularity
ratio and the smallest k-sparse eigenvalue of the covariance matrix. We further
demonstrate the wide applicability of our techniques by analyzing greedy
algorithms for the dictionary selection problem, and significantly improve the
previously known guarantees. Our theoretical analysis is complemented by
experiments on real-world and synthetic data sets; the experiments show that
the submodularity ratio is a stronger predictor of the performance of greedy
algorithms than other spectral parameters.
"
"  Global marketing managers are interested in understanding the speed of the
new product diffusion process and how the speed has changed in our ever more
technologically advanced and global marketplace. Understanding the process
allows firms to forecast the expected rate of return on their new products and
develop effective marketing strategies. The most recent major study on this
topic [Marketing Science 21 (2002) 97--114] investigated new product diffusions
in the United States. We expand upon that study in three important ways. (1)
Van den Bulte notes that a similar study is needed in the international
context, especially in developing countries. Our study covers four new product
diffusions across 31 developed and developing nations from 1980--2004. Our
sample accounts for about 80% of the global economic output and 60% of the
global population, allowing us to examine more general phenomena. (2) His model
contains the implicit assumption that the diffusion speed parameter is constant
throughout the diffusion life cycle of a product. Recognizing the likely
effects on the speed parameter of recent changes in the marketplace, we model
the parameter as a semiparametric function, allowing it the flexibility to
change over time. (3) We perform a variable selection to determine that the
number of internet users and the consumer price index are strongly associated
with the speed of diffusion.
"
"  We present a multi-task learning approach to jointly estimate the means of
multiple independent data sets. The proposed multi-task averaging (MTA)
algorithm results in a convex combination of the single-task maximum likelihood
estimates. We derive the optimal minimum risk estimator and the minimax
estimator, and show that these estimators can be efficiently estimated.
Simulations and real data experiments demonstrate that MTA estimators often
outperform both single-task and James-Stein estimators.
"
"  Discussion of ""A statistical analysis of multiple temperature proxies: Are
reconstructions of surface temperatures over the last 1000 years reliable?"" by
B.B. McShane and A.J. Wyner [arXiv:1104.4002]
"
"  In this work we perform a meta-analysis of neuroimaging data, consisting of
locations of peak activations identified in 162 separate studies on emotion.
Neuroimaging meta-analyses are typically performed using kernel-based methods.
However, these methods require the width of the kernel to be set a priori and
to be constant across the brain. To address these issues, we propose a fully
Bayesian nonparametric binary regression method to perform neuroimaging
meta-analyses. In our method, each location (or voxel) has a probability of
being a peak activation, and the corresponding probability function is based on
a spatially adaptive Gaussian Markov random field (GMRF). We also include
parameters in the model to robustify the procedure against miscoding of the
voxel response. Posterior inference is implemented using efficient MCMC
algorithms extended from those introduced in Holmes and Held [Bayesian Anal. 1
(2006) 145--168]. Our method allows the probability function to be locally
adaptive with respect to the covariates, that is, to be smooth in one region of
the covariate space and wiggly or even discontinuous in another. Posterior
miscoding probabilities for each of the identified voxels can also be obtained,
identifying voxels that may have been falsely classified as being activated.
Simulation studies and application to the emotion neuroimaging data indicate
that our method is superior to standard kernel-based methods.
"
"  We develop a fully Bayesian, computationally efficient framework for
incorporating model uncertainty into Type II Tobit models and apply this to the
investigation of the determinants of Foreign Direct Investment (FDI). While
direct evaluation of modelprobabilities is intractable in this setting, we show
that by using conditional Bayes Factors, which nest model moves inside a Gibbs
sampler, we are able to incorporate model uncertainty in a straight-forward
fashion. We conclude with a study of global FDI flows between 1988-2000.
"
"  The local linear embedding algorithm (LLE) is a non-linear dimension-reducing
technique, widely used due to its computational simplicity and intuitive
approach. LLE first linearly reconstructs each input point from its nearest
neighbors and then preserves these neighborhood relations in the
low-dimensional embedding. We show that the reconstruction weights computed by
LLE capture the high-dimensional structure of the neighborhoods, and not the
low-dimensional manifold structure. Consequently, the weight vectors are highly
sensitive to noise. Moreover, this causes LLE to converge to a linear
projection of the input, as opposed to its non-linear embedding goal. To
overcome both of these problems, we propose to compute the weight vectors using
a low-dimensional neighborhood representation. We prove theoretically that this
straightforward and computationally simple modification of LLE reduces LLE's
sensitivity to noise. This modification also removes the need for
regularization when the number of neighbors is larger than the dimension of the
input. We present numerical examples demonstrating both the perturbation and
linear projection problems, and the improved outputs using the low-dimensional
neighborhood representation.
"
"  We study the convergence rate of stochastic optimization of exact (NP-hard)
objectives, for which only biased estimates of the gradient are available. We
motivate this problem in the context of learning the structure and parameters
of Ising models. We first provide a convergence-rate analysis of deterministic
errors for forward-backward splitting (FBS). We then extend our analysis to
biased stochastic errors, by first characterizing a family of samplers and
providing a high probability bound that allows understanding not only FBS, but
also proximal gradient (PG) methods. We derive some interesting conclusions:
FBS requires only a logarithmically increasing number of random samples in
order to converge (although at a very low rate); the required number of random
samples is the same for the deterministic and the biased stochastic setting for
FBS and basic PG; accelerated PG is not guaranteed to converge in the biased
stochastic setting.
"
"  Bayesian reinforcement learning (BRL) encodes prior knowledge of the world in
a model and represents uncertainty in model parameters by maintaining a
probability distribution over them. This paper presents Monte Carlo BRL
(MC-BRL), a simple and general approach to BRL. MC-BRL samples a priori a
finite set of hypotheses for the model parameter values and forms a discrete
partially observable Markov decision process (POMDP) whose state space is a
cross product of the state space for the reinforcement learning task and the
sampled model parameter space. The POMDP does not require conjugate
distributions for belief representation, as earlier works do, and can be solved
relatively easily with point-based approximation algorithms. MC-BRL naturally
handles both fully and partially observable worlds. Theoretical and
experimental results show that the discrete POMDP approximates the underlying
BRL task well with guaranteed performance.
"
"  We present a data dependent generalization bound for a large class of
regularized algorithms which implement structured sparsity constraints. The
bound can be applied to standard squared-norm regularization, the Lasso, the
group Lasso, some versions of the group Lasso with overlapping groups, multiple
kernel learning and other regularization schemes. In all these cases
competitive results are obtained. A novel feature of our bound is that it can
be applied in an infinite dimensional setting such as the Lasso in a separable
Hilbert space or multiple kernel learning with a countable number of kernels.
"
"  Let X be a data matrix of rank \rho, whose rows represent n points in
d-dimensional space. The linear support vector machine constructs a hyperplane
separator that maximizes the 1-norm soft margin. We develop a new oblivious
dimension reduction technique which is precomputed and can be applied to any
input matrix X. We prove that, with high probability, the margin and minimum
enclosing ball in the feature space are preserved to within \epsilon-relative
error, ensuring comparable generalization as in the original space in the case
of classification. For regression, we show that the margin is preserved to
\epsilon-relative error with high probability. We present extensive experiments
with real and synthetic data to support our theory.
"
"  The learning curve expresses the error rate of a predictive modeling
procedure as a function of the sample size of the training dataset. It
typically is a decreasing, convex function with a positive limiting value. An
estimate of the learning curve can be used to assess whether a modeling
procedure should be expected to become substantially more accurate if
additional training data become available. This article proposes a new
procedure for estimating learning curves using imputation. We focus on
classification, although the idea is applicable to other predictive modeling
settings. Simulation studies indicate that the learning curve can be estimated
with useful accuracy for a roughly four-fold increase in the size of the
training set relative to the available data, and that the proposed imputation
approach outperforms an alternative estimation approach based on parameterizing
the learning curve. We illustrate the method with an application that predicts
the risk of disease progression for people with chronic lymphocytic leukemia.
"
"  This paper considers the robust and efficient implementation of Gaussian
process regression with a Student-t observation model. The challenge with the
Student-t model is the analytically intractable inference which is why several
approximative methods have been proposed. The expectation propagation (EP) has
been found to be a very accurate method in many empirical studies but the
convergence of the EP is known to be problematic with models containing
non-log-concave site functions such as the Student-t distribution. In this
paper we illustrate the situations where the standard EP fails to converge and
review different modifications and alternative algorithms for improving the
convergence. We demonstrate that convergence problems may occur during the
type-II maximum a posteriori (MAP) estimation of the hyperparameters and show
that the standard EP may not converge in the MAP values in some difficult
cases. We present a robust implementation which relies primarily on parallel EP
updates and utilizes a moment-matching-based double-loop algorithm with
adaptively selected step size in difficult cases. The predictive performance of
the EP is compared to the Laplace, variational Bayes, and Markov chain Monte
Carlo approximations.
"
"  Alternating minimization represents a widely applicable and empirically
successful approach for finding low-rank matrices that best fit the given data.
For example, for the problem of low-rank matrix completion, this method is
believed to be one of the most accurate and efficient, and formed a major
component of the winning entry in the Netflix Challenge.
  In the alternating minimization approach, the low-rank target matrix is
written in a bi-linear form, i.e. $X = UV^\dag$; the algorithm then alternates
between finding the best $U$ and the best $V$. Typically, each alternating step
in isolation is convex and tractable. However the overall problem becomes
non-convex and there has been almost no theoretical understanding of when this
approach yields a good result.
  In this paper we present first theoretical analysis of the performance of
alternating minimization for matrix completion, and the related problem of
matrix sensing. For both these problems, celebrated recent results have shown
that they become well-posed and tractable once certain (now standard)
conditions are imposed on the problem. We show that alternating minimization
also succeeds under similar conditions. Moreover, compared to existing results,
our paper shows that alternating minimization guarantees faster (in particular,
geometric) convergence to the true matrix, while allowing a simpler analysis.
"
"  We propose to investigate test statistics for testing homogeneity in
reproducing kernel Hilbert spaces. Asymptotic null distributions under null
hypothesis are derived, and consistency against fixed and local alternatives is
assessed. Finally, experimental evidence of the performance of the proposed
approach on both artificial data and a speaker verification task is provided.
"
"  Recent technological advances have made it possible to simultaneously measure
multiple protein activities at the single cell level. With such data collected
under different stimulatory or inhibitory conditions, it is possible to infer
the causal relationships among proteins from single cell interventional data.
In this article we propose a Bayesian hierarchical modeling framework to infer
the signaling pathway based on the posterior distributions of parameters in the
model. Under this framework, we consider network sparsity and model the
existence of an association between two proteins both at the overall level
across all experiments and at each individual experimental level. This allows
us to infer the pairs of proteins that are associated with each other and their
causal relationships. We also explicitly consider both intrinsic noise and
measurement error. Markov chain Monte Carlo is implemented for statistical
inference. We demonstrate that this hierarchical modeling can effectively pool
information from different interventional experiments through simulation
studies and real data analysis.
"
"  We address the problem of general supervised learning when data can only be
accessed through an (indefinite) similarity function between data points.
Existing work on learning with indefinite kernels has concentrated solely on
binary/multi-class classification problems. We propose a model that is generic
enough to handle any supervised learning task and also subsumes the model
previously proposed for classification. We give a ""goodness"" criterion for
similarity functions w.r.t. a given supervised learning task and then adapt a
well-known landmarking technique to provide efficient algorithms for supervised
learning using ""good"" similarity functions. We demonstrate the effectiveness of
our model on three important super-vised learning problems: a) real-valued
regression, b) ordinal regression and c) ranking where we show that our method
guarantees bounded generalization error. Furthermore, for the case of
real-valued regression, we give a natural goodness definition that, when used
in conjunction with a recent result in sparse vector recovery, guarantees a
sparse predictor with bounded generalization error. Finally, we report results
of our learning algorithms on regression and ordinal regression tasks using
non-PSD similarity functions and demonstrate the effectiveness of our
algorithms, especially that of the sparse landmark selection algorithm that
achieves significantly higher accuracies than the baseline methods while
offering reduced computational costs.
"
"  Variables in many massive high-dimensional data sets are structured, arising
for example from measurements on a regular grid as in imaging and time series
or from spatial-temporal measurements as in climate studies. Classical
multivariate techniques ignore these structural relationships often resulting
in poor performance. We propose a generalization of the singular value
decomposition (SVD) and principal components analysis (PCA) that is appropriate
for massive data sets with structured variables or known two-way dependencies.
By finding the best low rank approximation of the data with respect to a
transposable quadratic norm, our decomposition, entitled the Generalized least
squares Matrix Decomposition (GMD), directly accounts for structural
relationships. As many variables in high-dimensional settings are often
irrelevant or noisy, we also regularize our matrix decomposition by adding
two-way penalties to encourage sparsity or smoothness. We develop fast
computational algorithms using our methods to perform generalized PCA (GPCA),
sparse GPCA, and functional GPCA on massive data sets. Through simulations and
a whole brain functional MRI example we demonstrate the utility of our
methodology for dimension reduction, signal recovery, and feature selection
with high-dimensional structured data.
"
"  A Systematic Evolution of Ligands by EXponential enrichment (SELEX)
experiment begins in round one with a random pool of oligonucleotides in
equilibrium solution with a target. Over a few rounds, oligonucleotides having
a high affinity for the target are selected. Data from a high throughput SELEX
experiment consists of lists of thousands of oligonucleotides sampled after
each round. Thus far, SELEX experiments have been very good at suggesting the
highest affinity oligonucleotide, but modeling lower affinity recognition site
variants has been difficult. Furthermore, an alignment step has always been
used prior to analyzing SELEX data. We present a novel model, based on a
biochemical parametrization of SELEX, which allows us to use data from all
rounds to estimate the affinities of the oligonucleotides. Most notably, our
model also aligns the oligonucleotides. We use our model to analyze a SELEX
experiment containing double stranded DNA oligonucleotides and the
transcription factor Bicoid as the target. Our SELEX model outperformed other
published methods for predicting putative binding sites for Bicoid as indicated
by the results of an in-vivo ChIP-chip experiment.
"
"  This article proposes a family of link functions for the multinomial response
model. The link family includes the multicategorical logistic link as one of
its members. Conditions for the local orthogonality of the link and the
regression parameters are given. It is shown that local orthogonality of the
parameters in a neighbourhood makes the link family location and scale
invariant. Confidence regions for jointly estimating the percentiles based on
the parametric family of link functions are also determined. A numerical
example based on a combination drug study is used to illustrate the proposed
parametric link family and the confidence regions for joint percentile
estimation.
"
"  The elastic net was introduced as a heuristic algorithm for combinatorial
optimisation and has been applied, among other problems, to biological
modelling. It has an energy function which trades off a fitness term against a
tension term. In the original formulation of the algorithm the tension term was
implicitly based on a first-order derivative. In this paper we generalise the
elastic net model to an arbitrary quadratic tension term, e.g. derived from a
discretised differential operator, and give an efficient learning algorithm. We
refer to these as generalised elastic nets (GENs). We give a theoretical
analysis of the tension term for 1D nets with periodic boundary conditions, and
show that the model is sensitive to the choice of finite difference scheme that
represents the discretised derivative. We illustrate some of these issues in
the context of cortical map models, by relating the choice of tension term to a
cortical interaction function. In particular, we prove that this interaction
takes the form of a Mexican hat for the original elastic net, and of
progressively more oscillatory Mexican hats for higher-order derivatives. The
results apply not only to generalised elastic nets but also to other methods
using discrete differential penalties, and are expected to be useful in other
areas, such as data analysis, computer graphics and optimisation problems.
"
"  For earthquake-resistant design, engineering seismologists employ
time-history analysis for nonlinear simulations. The nonstationary stochastic
method previously developed by Pousse et al. (2006) has been updated. This
method has the advantage of being both simple, fast and taking into account the
basic concepts of seismology (Brune's source, realistic time envelope function,
nonstationarity and ground-motion variability). Time-domain simulations are
derived from the signal spectrogram and depend on few ground-motion parameters:
Arias intensity, significant relative duration and central frequency. These
indicators are obtained from empirical attenuation equations that relate them
to the magnitude of the event, the source-receiver distance, and the site
conditions. We improve the nonstationary stochastic method by using new
functional forms (new surface rock dataset, analysis of both intra-event and
inter-event residuals, consideration of the scaling relations and VS30), by
assessing the central frequency with S-transform and by better considering the
stress drop variability.
"
"  We assume i.i.d. data sampled from a mixture distribution with K components
along fixed d-dimensional linear subspaces and an additional outlier component.
For p>0, we study the simultaneous recovery of the K fixed subspaces by
minimizing the l_p-averaged distances of the sampled data points from any K
subspaces. Under some conditions, we show that if $0<p\leq1$, then all
underlying subspaces can be precisely recovered by l_p minimization with
overwhelming probability. On the other hand, if K>1 and p>1, then the
underlying subspaces cannot be recovered or even nearly recovered by l_p
minimization. The results of this paper partially explain the successes and
failures of the basic approach of l_p energy minimization for modeling data by
multiple subspaces.
"
"  Popular music is a key cultural expression that has captured listeners'
attention for ages. Many of the structural regularities underlying musical
discourse are yet to be discovered and, accordingly, their historical evolution
remains formally unknown. Here we unveil a number of patterns and metrics
characterizing the generic usage of primary musical facets such as pitch,
timbre, and loudness in contemporary western popular music. Many of these
patterns and metrics have been consistently stable for a period of more than
fifty years, thus pointing towards a great degree of conventionalism.
Nonetheless, we prove important changes or trends related to the restriction of
pitch transitions, the homogenization of the timbral palette, and the growing
loudness levels. This suggests that our perception of the new would be rooted
on these changing characteristics. Hence, an old tune could perfectly sound
novel and fashionable, provided that it consisted of common harmonic
progressions, changed the instrumentation, and increased the average loudness.
"
"  The theoretical base of the research of occupational injuries is the idea of
the process as Markov chain of random variables. However the exact proof of
this position was not carried out whereas the experimental passing of the
hypothesis is connected always with the determined confidence limits and
consequently it gives the space for alternative assumptions. In this research
some databases of occupational injuries had been studied using spectral
analysis techniques and the presentation of the occupational injuries as the
temporal sequence of the cases (""telegraph wave"" process type). Databases had
corresponding chapters such as ""enterprise"" with number of employees about
7000, ""big enterprise"" (the number of employees about 35000), ""whole branch of
industry"", ""whole enterprises of industrial region"" receiving during 10 years
from different countries having distinguish system of the work organization
(Russia and Italy). The behaviour of spectra on principal is not changed when
vary the length of realization, resolution, smoothing, upper boundary
frequency, country and year of datas. All spectra showed that the occupational
injuries process has a not Markov, but deterministic polyharmonic behaviour.
This phenomenon is characterized by the spectral lines with two frequencies
corresponding 24 hours (circadian rythm have main amplitude) and 7 days
(amplitude is two times smaller) exactly. Harmonics corresponding one month
(biorythms) were not founded. The task of the futher investigations is to find
the psychophysiological and biomedical processes determined and having high
level of correlation with the occupational injuries process.
"
"  To better understand the interplay of censoring and sparsity we develop
finite sample properties of nonparametric Cox proportional hazard's model. Due
to high impact of sequencing data, carrying genetic information of each
individual, we work with over-parametrized problem and propose general class of
group penalties suitable for sparse structured variable selection and
estimation. Novel non-asymptotic sandwich bounds for the partial likelihood are
developed. We establish how they extend notion of local asymptotic normality
(LAN) of Le Cam's. Such non-asymptotic LAN principles are further extended to
high dimensional spaces where $p \gg n$. Finite sample prediction properties of
penalized estimator in non-parametric Cox proportional hazards model, under
suitable censoring conditions, agree with those of penalized estimator in
linear models.
"
"  We consider the problem of estimating the incidence of residential burglaries
that occur over a well-defined period of time within the 10 most populous
cities in North Carolina. Our analysis typifies some of the general issues that
arise in estimating and comparing local crime rates over time and for different
cities. Typically, the only information we have about crime incidence within
any particular city is what that city's police department tells us, and the
police can only count and describe the crimes that come to their attention. To
address this, our study combines information from police-based residential
burglary counts and the National Crime Victimization Survey to obtain interval
estimates of residential burglary incidence at the local level. We use those
estimates as a basis for commenting on the fragility of between-city and
over-time comparisons that often appear in both public discourse about crime
patterns.
"
"  It has recently been observed that certain extremely simple feature encoding
techniques are able to achieve state of the art performance on several standard
image classification benchmarks including deep belief networks, convolutional
nets, factored RBMs, mcRBMs, convolutional RBMs, sparse autoencoders and
several others. Moreover, these ""triangle"" or ""soft threshold"" encodings are
ex- tremely efficient to compute. Several intuitive arguments have been put
forward to explain this remarkable performance, yet no mathematical
justification has been offered.
  The main result of this report is to show that these features are realized as
an approximate solution to the a non-negative sparse coding problem. Using this
connection we describe several variants of the soft threshold features and
demonstrate their effectiveness on two image classification benchmark tasks.
"
"  The problem of minimizing a continuously differentiable convex function over
an intersection of closed convex sets is ubiquitous in applied mathematics. It
is particularly interesting when it is easy to project onto each separate set,
but nontrivial to project onto their intersection. Algorithms based on Newton's
method such as the interior point method are viable for small to medium-scale
problems. However, modern applications in statistics, engineering, and machine
learning are posing problems with potentially tens of thousands of parameters
or more. We revisit this convex programming problem and propose an algorithm
that scales well with dimensionality. Our proposal is an instance of a
sequential unconstrained minimization technique and revolves around three
ideas: the majorization-minimization (MM) principle, the classical penalty
method for constrained optimization, and quasi-Newton acceleration of
fixed-point algorithms. The performance of our distance majorization algorithms
is illustrated in several applications.
"
"  This manuscript studies statistical properties of linear classifiers obtained
through minimization of an unregularized convex risk over a finite sample.
Although the results are explicitly finite-dimensional, inputs may be passed
through feature maps; in this way, in addition to treating the consistency of
logistic regression, this analysis also handles boosting over a finite weak
learning class with, for instance, the exponential, logistic, and hinge losses.
In this finite-dimensional setting, it is still possible to fit arbitrary
decision boundaries: scaling the complexity of the weak learning class with the
sample size leads to the optimal classification risk almost surely.
"
"  The area under the ROC curve is widely used as a measure of performance of
classification rules. However, it has recently been shown that the measure is
fundamentally incoherent, in the sense that it treats the relative severities
of misclassifications differently when different classifiers are used. To
overcome this, Hand (2009) proposed the $H$ measure, which allows a given
researcher to fix the distribution of relative severities to a
classifier-independent setting on a given problem. This note extends the
discussion, and proposes a modified standard distribution for the $H$ measure,
which better matches the requirements of researchers, in particular those faced
with heavily unbalanced datasets, the $Beta(\pi_1+1,\pi_0+1)$ distribution.
[Preprint submitted at Pattern Recognition Letters]
"
"  To interpret differentially expressed genes or other discovered features,
researchers conduct hypothesis tests to determine which biological categories
such as those of the Gene Ontology (GO) are enriched in the sense of having
differential representation among the discovered features. We study application
of better estimators of the local false discovery rate (LFDR), a probability
that the biological category has equivalent representation among the
preselected features.
  We identified three promising estimators of the LFDR for detecting
differential representation: a semiparametric estimator (SPE), a normalized
maximum likelihood estimator (NMLE), and a maximum likelihood estimator (MLE).
We found that the MLE performs at least as well as the SPE for on the order of
100 of GO categories even when the ideal number of components in its underlying
mixture model is unknown. However, the MLE is unreliable when the number of GO
categories is small compared to the number of PMM components. Thus, if the
number of categories is on the order of 10, the SPE is a more reliable LFDR
estimator. The NMLE depends not only on the data but also on a specified value
of the prior probability of differential representation. It is therefore an
appropriate LFDR estimator only when the number of GO categories is too small
for application of the other methods.
  For enrichment detection, we recommend estimating the LFDR by the MLE given
at least a medium number (~100) of GO categories, by the SPE given a small
number of GO categories (~10), and by the NMLE given a very small number (~1)
of GO categories.
"
"  We consider the problem of online linear regression on individual sequences.
The goal in this paper is for the forecaster to output sequential predictions
which are, after $T$ time rounds, almost as good as the ones output by the best
linear predictor in a given $\ell^1$-ball in $\\R^d$. We consider both the
cases where the dimension~$d$ is small and large relative to the time horizon
$T$. We first present regret bounds with optimal dependencies on $d$, $T$, and
on the sizes $U$, $X$ and $Y$ of the $\ell^1$-ball, the input data and the
observations. The minimax regret is shown to exhibit a regime transition around
the point $d = \sqrt{T} U X / (2 Y)$. Furthermore, we present efficient
algorithms that are adaptive, \ie, that do not require the knowledge of $U$,
$X$, $Y$, and $T$, but still achieve nearly optimal regret bounds.
"
"  The identity of the famous place of La Mancha appearing at the Quijote is an
unknown with a history almost as long as that of the famous book by Miguel de
Cervantes. This work analyzes data obtained from a Geographic Information
System and compares the results with those of the previous works. Three
different variables with two possible values each are considered: time or space
data, 3 or 4 reference points, and the commonly used distances to the place of
La Mancha or a set of recently proposed ones. The village in the Campo de
Montiel which is closest to be the place of La Mancha happens to be Carrizosa
or Villanueva de los Infantes, depending on the configuration, with the latter
being the solution for the configuration in which the relative errors are the
smallest and the second candidate village is furthest from the first.
  -----
  La identidad del famoso lugar de la Mancha que aparece en El Quijote es una
inc\'ognita con una historia casi tan larga como la publicaci\'on de la famosa
obra de Miguel de Cervantes. Este trabajo analiza datos obtenidos mediante un
Sistema de Informaci\'on Geogr\'afica y compara los resultados con los de los
trabajos anteriores. Se consideran tres variables diferentes con dos posibles
valores cada una: datos temporales o espaciales, 3 \'o 4 puntos de referencia y
las distancias al lugar de La Mancha usadas habitualmente o unas recientemente
introducidas. La localidad del Campo de Montiel m\'as cercana a ser el lugar de
La Mancha resulta ser Carrizosa o Villanueva de los Infantes, dependiendo de la
configuraci\'on, siendo \'esta \'ultima la soluci\'on para la configuraci\'on
en que los errores relativos son los m\'as peque\~nos y la segunda localidad
candidata est\'a m\'as lejos de la primera.
"
"  Object Oriented Data Analysis is a new area in statistics that studies
populations of general data objects. In this article we consider populations of
tree-structured objects as our focus of interest. We develop improved analysis
tools for data lying in a binary tree space analogous to classical Principal
Component Analysis methods in Euclidean space. Our extensions of PCA are
analogs of one dimensional subspaces that best fit the data. Previous work was
based on the notion of tree-lines.
  In this paper, a generalization of the previous tree-line notion is proposed:
k-tree-lines. Previously proposed tree-lines are k-tree-lines where k=1. New
sub-cases of k-tree-lines studied in this work are the 2-tree-lines and
tree-curves, which explain much more variation per principal component than
tree-lines. The optimal principal component tree-lines were computable in
linear time. Because 2-tree-lines and tree-curves are more complex, they are
computationally more expensive, but yield improved data analysis results.
  We provide a comparative study of all these methods on a motivating data set
consisting of brain vessel structures of 98 subjects.
"
"  The problem of biclustering consists of the simultaneous clustering of rows
and columns of a matrix such that each of the submatrices induced by a pair of
row and column clusters is as uniform as possible. In this paper we approximate
the optimal biclustering by applying one-way clustering algorithms
independently on the rows and on the columns of the input matrix. We show that
such a solution yields a worst-case approximation ratio of 1+sqrt(2) under
L1-norm for 0-1 valued matrices, and of 2 under L2-norm for real valued
matrices.
"
"  Summary: MALDIquant is an R package providing a complete and modular analysis
pipeline for quantitative analysis of mass spectrometry data. MALDIquant is
specifically designed with application in clinical diagnostics in mind and
implements sophisticated routines for importing raw data, preprocessing,
non-linear peak alignment, and calibration. It also handles technical
replicates as well as spectra with unequal resolution.
  Availability: MALDIquant and its associated R packages readBrukerFlexData and
readMzXmlData are freely available from the R archive CRAN
(http://cran.r-project.org). The software is distributed under the GNU General
Public License (version 3 or later) and is accompanied by example files and
data. Additional documentation is available from
http://strimmerlab.org/software/maldiquant/.
"
"  This paper presents theory for Normalized Random Measures (NRMs), Normalized
Generalized Gammas (NGGs), a particular kind of NRM, and Dependent Hierarchical
NRMs which allow networks of dependent NRMs to be analysed. These have been
used, for instance, for time-dependent topic modelling. In this paper, we first
introduce some mathematical background of completely random measures (CRMs) and
their construction from Poisson processes, and then introduce NRMs and NGGs.
Slice sampling is also introduced for posterior inference. The dependency
operators in Poisson processes and for the corresponding CRMs and NRMs is then
introduced and Posterior inference for the NGG presented. Finally, we give
dependency and composition results when applying these operators to NRMs so
they can be used in a network with hierarchical and dependent relations.
"
"  Changepoints are abrupt variations in the generative parameters of a data
sequence. Online detection of changepoints is useful in modelling and
prediction of time series in application areas such as finance, biometrics, and
robotics. While frequentist methods have yielded online filtering and
prediction techniques, most Bayesian papers have focused on the retrospective
segmentation problem. Here we examine the case where the model parameters
before and after the changepoint are independent and we derive an online
algorithm for exact inference of the most recent changepoint. We compute the
probability distribution of the length of the current ``run,'' or time since
the last changepoint, using a simple message-passing algorithm. Our
implementation is highly modular so that the algorithm may be applied to a
variety of types of data. We illustrate this modularity by demonstrating the
algorithm on three different real-world data sets.
"
"  Machine learning algorithms frequently require careful tuning of model
hyperparameters, regularization terms, and optimization parameters.
Unfortunately, this tuning is often a ""black art"" that requires expert
experience, unwritten rules of thumb, or sometimes brute-force search. Much
more appealing is the idea of developing automatic approaches which can
optimize the performance of a given learning algorithm to the task at hand. In
this work, we consider the automatic tuning problem within the framework of
Bayesian optimization, in which a learning algorithm's generalization
performance is modeled as a sample from a Gaussian process (GP). The tractable
posterior distribution induced by the GP leads to efficient use of the
information gathered by previous experiments, enabling optimal choices about
what parameters to try next. Here we show how the effects of the Gaussian
process prior and the associated inference procedure can have a large impact on
the success or failure of Bayesian optimization. We show that thoughtful
choices can lead to results that exceed expert-level performance in tuning
machine learning algorithms. We also describe new algorithms that take into
account the variable cost (duration) of learning experiments and that can
leverage the presence of multiple cores for parallel experimentation. We show
that these proposed algorithms improve on previous automatic procedures and can
reach or surpass human expert-level optimization on a diverse set of
contemporary algorithms including latent Dirichlet allocation, structured SVMs
and convolutional neural networks.
"
"  The detection of change-points in heterogeneous sequences is a statistical
challenge with many applications in fields such as finance, signal analysis and
biology. A wide variety of literature exists for finding an ideal set of
change-points for characterizing the data. In this tutorial we elaborate on the
Hidden Markov Model (HMM) and present two different frameworks for applying HMM
to change-point models. Then we provide a summary of two procedures for
inference in change-point analysis, which are particular cases of the
forward-backward algorithm for HMMs, and discuss common implementation
problems. Lastly, we provide two examples of the HMM methods on available data
sets and we shortly discuss about the applications to current genomics studies.
The R code used in the examples is provided in the appendix.
"
"  Discussion of ""Latent variable graphical model selection via convex
optimization"" by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky
[arXiv:1008.1290].
"
"  Parametric inference for spatial max-stable processes is difficult since the
related likelihoods are unavailable. A composite likelihood approach based on
the bivariate distribution of block maxima has been recently proposed in the
literature. However modeling block maxima is a wasteful approach provided that
other information is available. Moreover an approach based on block, typically
annual, maxima is unable to take into account the fact that maxima occur or not
simultaneously. If time series of, say, daily data are available, then
estimation procedures based on exceedances of a high threshold could mitigate
such problems. In this paper we focus on two approaches for composing
likelihoods based on pairs of exceedances. The first one comes from the tail
approximation for bivariate distribution proposed by Ledford and Tawn (1996)
when both pairs of observations exceed the fixed threshold. The second one uses
the bivariate extension (Rootzen and Tajvidi, 2006) of the generalized Pareto
distribution which allows to model exceedances when at least one of the
components is over the threshold. The two approaches are compared through a
simulation study according to different degrees of spatial dependency. Results
show that both the strength of the spatial dependencies and the threshold
choice play a fundamental role in determining which is the best estimating
procedure.
"
"  There have been several studies of the genome-wide temporal transcriptional
program of viruses, based on microarray experiments, which are generally useful
in the construction of gene regulation network. It seems that biological
interpretations in these studies are directly based on the normalized data and
some crude statistics, which provide rough estimates of limited features of the
profile and may incur biases. This paper introduces a hierarchical Bayesian
shape restricted regression method for making inference on the time course
expression of virus genes. Estimates of many salient features of the expression
profile like onset time, inflection point, maximum value, time to maximum
value, area under curve, etc. can be obtained immediately by this method.
Applying this method to a baculovirus microarray time course expression data
set, we indicate that many biological questions can be formulated
quantitatively and we are able to offer insights into the baculovirus biology.
"
"  We study the average case performance of multi-task Gaussian process (GP)
regression as captured in the learning curve, i.e. the average Bayes error for
a chosen task versus the total number of examples $n$ for all tasks. For GP
covariances that are the product of an input-dependent covariance function and
a free-form inter-task covariance matrix, we show that accurate approximations
for the learning curve can be obtained for an arbitrary number of tasks $T$. We
use these to study the asymptotic learning behaviour for large $n$.
Surprisingly, multi-task learning can be asymptotically essentially useless, in
the sense that examples from other tasks help only when the degree of
inter-task correlation, $\rho$, is near its maximal value $\rho=1$. This effect
is most extreme for learning of smooth target functions as described by e.g.
squared exponential kernels. We also demonstrate that when learning many tasks,
the learning curves separate into an initial phase, where the Bayes error on
each task is reduced down to a plateau value by ""collective learning"" even
though most tasks have not seen examples, and a final decay that occurs once
the number of examples is proportional to the number of tasks.
"
"  We propose a simple kernel based nearest neighbor approach for handwritten
digit classification. The ""distance"" here is actually a kernel defining the
similarity between two images. We carefully study the effects of different
number of neighbors and weight schemes and report the results. With only a few
nearest neighbors (or most similar images) to vote, the test set error rate on
MNIST database could reach about 1.5%-2.0%, which is very close to many
advanced models.
"
"  Learning with hidden variables is a central challenge in probabilistic
graphical models that has important implications for many real-life problems.
The classical approach is using the Expectation Maximization (EM) algorithm.
This algorithm, however, can get trapped in local maxima. In this paper we
explore a new approach that is based on the Information Bottleneck principle.
In this approach, we view the learning problem as a tradeoff between two
information theoretic objectives. The first is to make the hidden variables
uninformative about the identity of specific instances. The second is to make
the hidden variables informative about the observed attributes. By exploring
different tradeoffs between these two objectives, we can gradually converge on
a high-scoring solution. As we show, the resulting, Information Bottleneck
Expectation Maximization (IB-EM) algorithm, manages to find solutions that are
superior to standard EM methods.
"
"  Undirected graphs can be used to describe matrix variate distributions. In
this paper, we develop new methods for estimating the graphical structures and
underlying parameters, namely, the row and column covariance and inverse
covariance matrices from the matrix variate data. Under sparsity conditions, we
show that one is able to recover the graphs and covariance matrices with a
single random matrix from the matrix variate normal distribution. Our method
extends, with suitable adaptation, to the general setting where replicates are
available. We establish consistency and obtain the rates of convergence in the
operator and the Frobenius norm. We show that having replicates will allow one
to estimate more complicated graphical structures and achieve faster rates of
convergence. We provide simulation evidence showing that we can recover
graphical structures as well as estimating the precision matrices, as predicted
by theory.
"
"  To investigate whether treating cancer patients with
erythropoiesis-stimulating agents (ESAs) would increase the mortality risk,
Bennett et al. [Journal of the American Medical Association 299 (2008)
914--924] conducted a meta-analysis with the data from 52 phase III trials
comparing ESAs with placebo or standard of care. With a standard parametric
random effects modeling approach, the study concluded that ESA administration
was significantly associated with increased average mortality risk. In this
article we present a simple nonparametric inference procedure for the
distribution of the random effects. We re-analyzed the ESA mortality data with
the new method. Our results about the center of the random effects distribution
were markedly different from those reported by Bennett et al. Moreover, our
procedure, which estimates the distribution of the random effects, as opposed
to just a simple population average, suggests that the ESA may be beneficial to
mortality for approximately a quarter of the study populations. This new
meta-analysis technique can be implemented with study-level summary statistics.
In contrast to existing methods for parametric random effects models, the
validity of our proposal does not require the number of studies involved to be
large. From the results of an extensive numerical study, we find that the new
procedure performs well even with moderate individual study sample sizes.
"
"  In this paper we are concerned with fully automatic and locally adaptive
estimation of functions in a ""signal + noise""-model where the regression
function may additionally be blurred by a linear operator, e.g. by a
convolution. To this end, we introduce a general class of statistical
multiresolution estimators and develop an algorithmic framework for computing
those. By this we mean estimators that are defined as solutions of convex
optimization problems with supremum-type constraints. We employ a combination
of the alternating direction method of multipliers with Dykstra's algorithm for
computing orthogonal projections onto intersections of convex sets and prove
numerical convergence. The capability of the proposed method is illustrated by
various examples from imaging and signal detection.
"
"  Recently-developed genotype imputation methods are a powerful tool for
detecting untyped genetic variants that affect disease susceptibility in
genetic association studies. However, existing imputation methods require
individual-level genotype data, whereas, in practice, it is often the case that
only summary data are available. For example, this may occur because, for
reasons of privacy or politics, only summary data are made available to the
research community at large; or because only summary data are collected, as in
DNA pooling experiments. In this article we introduce a new statistical method
that can accurately infer the frequencies of untyped genetic variants in these
settings, and indeed substantially improve frequency estimates at typed
variants in pooling experiments where observations are noisy. Our approach,
which predicts each allele frequency using a linear combination of observed
frequencies, is statistically straightforward, and related to a long history of
the use of linear methods for estimating missing values (e.g., Kriging). The
main statistical novelty is our approach to regularizing the covariance matrix
estimates, and the resulting linear predictors, which is based on methods from
population genetics. We find that, besides being both fast and
flexible---allowing new problems to be tackled that cannot be handled by
existing imputation approaches purpose-built for the genetic context---these
linear methods are also very accurate. Indeed, imputation accuracy using this
approach is similar to that obtained by state-of-the-art imputation methods
that use individual-level data, but at a fraction of the computational cost.
"
"  There have lately been several suggestions for parametrized distances on a
graph that generalize the shortest path distance and the commute time or
resistance distance. The need for developing such distances has risen from the
observation that the above-mentioned common distances in many situations fail
to take into account the global structure of the graph. In this article, we
develop the theory of one family of graph node distances, known as the
randomized shortest path dissimilarity, which has its foundation in statistical
physics. We show that the randomized shortest path dissimilarity can be easily
computed in closed form for all pairs of nodes of a graph. Moreover, we come up
with a new definition of a distance measure that we call the free energy
distance. The free energy distance can be seen as an upgrade of the randomized
shortest path dissimilarity as it defines a metric, in addition to which it
satisfies the graph-geodetic property. The derivation and computation of the
free energy distance are also straightforward. We then make a comparison
between a set of generalized distances that interpolate between the shortest
path distance and the commute time, or resistance distance. This comparison
focuses on the applicability of the distances in graph node clustering and
classification. The comparison, in general, shows that the parametrized
distances perform well in the tasks. In particular, we see that the results
obtained with the free energy distance are among the best in all the
experiments.
"
"  Purely data driven approaches for machine learning present difficulties when
data is scarce relative to the complexity of the model or when the model is
forced to extrapolate. On the other hand, purely mechanistic approaches need to
identify and specify all the interactions in the problem at hand (which may not
be feasible) and still leave the issue of how to parameterize the system. In
this paper, we present a hybrid approach using Gaussian processes and
differential equations to combine data driven modelling with a physical model
of the system. We show how different, physically-inspired, kernel functions can
be developed through sensible, simple, mechanistic assumptions about the
underlying system. The versatility of our approach is illustrated with three
case studies from motion capture, computational biology and geostatistics.
"
"  Coherence and phase synchronization between time series corresponding to
different spatial locations are usually interpreted as indicators of the
connectivity between locations. In neurophysiology, time series of electric
neuronal activity are essential for studying brain interconnectivity. Such
signals can either be invasively measured from depth electrodes, or computed
from very high time resolution, non-invasive, extracranial recordings of scalp
electric potential differences (EEG: electroencephalogram) and magnetic fields
(MEG: magnetoencephalogram) by means of a tomography such as sLORETA
(standardized low resolution brain electromagnetic tomography). There are two
problems in this case. First, in the usual situation of unknown cortical
geometry, the estimated signal at each brain location is a vector with three
components (i.e. a current density vector), which means that coherence and
phase synchronization must be generalized to pairs of multivariate time series.
Second, the inherent low spatial resolution of the EEG/MEG tomography
introduces artificially high zero-lag coherence and phase synchronization. In
this report, solutions to both problems are presented. Two additional
generalizations are briefly mentioned: (1) conditional coherence and phase
synchronization; and (2) non-stationary time-frequency analysis. Finally, a
non-parametric randomization method for connectivity significance testing is
outlined. The new connectivity measures proposed here can be applied to pairs
of univariate EEG/MEG signals, as is traditional in the published literature.
However, these calculations cannot be interpreted as connectivity, since it is
in general incorrect to associate an extracranial electrode or sensor to the
underlying cortex.
"
"  Reinforcement learning addresses the dilemma between exploration to find
profitable actions and exploitation to act according to the best observations
already made. Bandit problems are one such class of problems in stateless
environments that represent this explore/exploit situation. We propose a
learning algorithm for bandit problems based on fractional expectation of
rewards acquired. The algorithm is theoretically shown to converge on an
eta-optimal arm and achieve O(n) sample complexity. Experimental results show
the algorithm incurs substantially lower regrets than parameter-optimized
eta-greedy and SoftMax approaches and other low sample complexity
state-of-the-art techniques.
"
"  We consider the estimation of wealth inequality measures with their
confidence interval, based on survey data with interval censoring. We rely on a
Bayesian hierarchical model. It consists of a model where, due to survey
sampling and unit nonresponse, the summaries of the wealth distribution of
households are observed with error; a mixture of multivariate models for the
wealth components where groups correspond to portfolios of assets; and a prior
on the parameters. A Gibbs sampler is used for numerical purposes to do the
inference. We apply this strategy to the French 2004 Wealth Survey. In order to
alleviate the nonresponse, the amounts were systematically collected in the
form of brackets. Matched administrative data on the liability of the
respondents for wealth tax and response to overview questions are used to
better localize the wealth components. It implies nonrectangular
multidimensional censoring. The variance of the error term in the model for the
population inequality measures is obtained using linearization and taking into
account the complex sampling design and the various weight adjustments.
"
"  The paramount importance of replicating associations is well recognized in
the genome-wide associaton (GWA) research community, yet methods for assessing
replicability of associations are scarce. Published GWA studies often combine
separately the results of primary studies and of the follow-up studies.
Informally, reporting the two separate meta-analyses, that of the primary
studies and follow-up studies, gives a sense of the replicability of the
results. We suggest a formal empirical Bayes approach for discovering whether
results have been replicated across studies, in which we estimate the optimal
rejection region for discovering replicated results. We demonstrate, using
realistic simulations, that the average false discovery proportion of our
method remains small. We apply our method to six type two diabetes (T2D) GWA
studies. Out of 803 SNPs discovered to be associated with T2D using a typical
meta-analysis, we discovered 219 SNPs with replicated associations with T2D. We
recommend complementing a meta-analysis with a replicability analysis for GWA
studies.
"
"  Various methods to control the influence of a covariate on a response
variable are compared. In particular, ANOVA with or without homogeneity of
variances (HOV) of errors and Kruskal-Wallis (K-W) tests on covariate-adjusted
residuals and analysis of covariance (ANCOVA) are compared. Covariate-adjusted
residuals are obtained from the overall regression line fit to the entire data
set ignoring the treatment levels. It is demonstrated that the methods on
covariate-adjusted residuals are only appropriate when the regression lines are
parallel and means are equal for treatment factors. Empirical size and power
performance of the methods are compared by extensive Monte Carlo simulations.
We manipulated the conditions such as assumptions of normality and HOV, sample
size, and clustering of the covariates. Guidelines on which method to use for
various cases are also provided.
"
"  Advances in sensing technologies and the growth of the internet have resulted
in an explosion in the size of modern datasets, while storage and processing
power continue to lag behind. This motivates the need for algorithms that are
efficient, both in terms of the number of measurements needed and running time.
To combat the challenges associated with large datasets, we propose a general
framework for active hierarchical clustering that repeatedly runs an
off-the-shelf clustering algorithm on small subsets of the data and comes with
guarantees on performance, measurement complexity and runtime complexity. We
instantiate this framework with a simple spectral clustering algorithm and
provide concrete results on its performance, showing that, under some
assumptions, this algorithm recovers all clusters of size ?(log n) using O(n
log^2 n) similarities and runs in O(n log^3 n) time for a dataset of n objects.
Through extensive experimentation we also demonstrate that this framework is
practically alluring.
"
"  We show that the log-likelihood of several probabilistic graphical models is
Lipschitz continuous with respect to the lp-norm of the parameters. We discuss
several implications of Lipschitz parametrization. We present an upper bound of
the Kullback-Leibler divergence that allows understanding methods that penalize
the lp-norm of differences of parameters as the minimization of that upper
bound. The expected log-likelihood is lower bounded by the negative lp-norm,
which allows understanding the generalization ability of probabilistic models.
The exponential of the negative lp-norm is involved in the lower bound of the
Bayes error rate, which shows that it is reasonable to use parameters as
features in algorithms that rely on metric spaces (e.g. classification,
dimensionality reduction, clustering). Our results do not rely on specific
algorithms for learning the structure or parameters. We show preliminary
results for activity recognition and temporal segmentation.
"
"  This paper investigates the detection of information hidden by the Least
Significant Bit (LSB) matching scheme. In a theoretical context of known image
media parameters, two important results are presented. First, the use of
hypothesis testing theory allows us to design the Most Powerful (MP) test.
Second, a study of the MP test gives us the opportunity to analytically
calculate its statistical performance in order to warrant a given probability
of false-alarm. In practice when detecting LSB matching, the unknown image
parameters have to be estimated. Based on the local estimator used in the
Weighted Stego-image (WS) detector, a practical test is presented. A numerical
comparison with state-of-the-art detectors shows the good performance of the
proposed tests and highlights the relevance of the proposed methodology.
"
"  In this paper we propose a unified framework for structured prediction with
latent variables which includes hidden conditional random fields and latent
structured support vector machines as special cases. We describe a local
entropy approximation for this general formulation using duality, and derive an
efficient message passing algorithm that is guaranteed to converge. We
demonstrate its effectiveness in the tasks of image segmentation as well as 3D
indoor scene understanding from single images, showing that our approach is
superior to latent structured support vector machines and hidden conditional
random fields.
"
"  In this paper, we provide new complexity results for algorithms that learn
discrete-variable Bayesian networks from data. Our results apply whenever the
learning algorithm uses a scoring criterion that favors the simplest model able
to represent the generative distribution exactly. Our results therefore hold
whenever the learning algorithm uses a consistent scoring criterion and is
applied to a sufficiently large dataset. We show that identifying high-scoring
structures is hard, even when we are given an independence oracle, an inference
oracle, and/or an information oracle. Our negative results also apply to the
learning of discrete-variable Bayesian networks in which each node has at most
k parents, for all k > 3.
"
"  We define a copula process which describes the dependencies between
arbitrarily many random variables independently of their marginal
distributions. As an example, we develop a stochastic volatility model,
Gaussian Copula Process Volatility (GCPV), to predict the latent standard
deviations of a sequence of random variables. To make predictions we use
Bayesian inference, with the Laplace approximation, and with Markov chain Monte
Carlo as an alternative. We find both methods comparable. We also find our
model can outperform GARCH on simulated and financial data. And unlike GARCH,
GCPV can easily handle missing data, incorporate covariates other than time,
and model a rich class of covariance structures.
"
"  Following a review of metric, ultrametric and generalized ultrametric, we
review their application in data analysis. We show how they allow us to explore
both geometry and topology of information, starting with measured data. Some
themes are then developed based on the use of metric, ultrametric and
generalized ultrametric in logic. In particular we study approximation chains
in an ultrametric or generalized ultrametric context. Our aim in this work is
to extend the scope of data analysis by facilitating reasoning based on the
data analysis; and to show how quantitative and qualitative data analysis can
be incorporated into logic programming.
"
"  We consider the minimum error entropy (MEE) criterion and an empirical risk
minimization learning algorithm in a regression setting. A learning theory
approach is presented for this MEE algorithm and explicit error bounds are
provided in terms of the approximation ability and capacity of the involved
hypothesis space when the MEE scaling parameter is large. Novel asymptotic
analysis is conducted for the generalization error associated with Renyi's
entropy and a Parzen window function, to overcome technical difficulties arisen
from the essential differences between the classical least squares problems and
the MEE setting. A semi-norm and the involved symmetrized least squares error
are introduced, which is related to some ranking algorithms.
"
"  Drug-induced liver injury (DILI) is a major public health issue and of
serious concern for the pharmaceutical industry. Early detection of signs of a
drug's potential for DILI is vital for pharmaceutical companies' evaluation of
new drugs. A combination of extreme values of liver specific variables indicate
potential DILI (Hy's Law). We estimate the probability of severe DILI using the
Heffernan and Tawn (2004) conditional dependence model which arises naturally
in applications where a multidimensional random variable is extreme in at least
one component. We extend the current model by including the assumption of
stochastically ordered survival curves for different doses in a Phase 3 study.
"
"  This paper presents the application of a particle filter for data
assimilation in the context of puff-based dispersion models. Particle filters
provide estimates of the higher moments, and are well suited for strongly
nonlinear and/or non-Gaussian models. The Gaussian puff model SCIPUFF, is used
in predicting the chemical concentration field after a chemical incident. This
model is highly nonlinear and evolves with variable state dimension and, after
sufficient time, high dimensionality. While the particle filter formalism
naturally supports variable state dimensionality high dimensionality represents
a challenge in selecting an adequate number of particles, especially for the
Bootstrap version. We present an implementation of the Bootstrap particle
filter and compare its performance with the SCIPUFF predictions. Both the model
and the Particle Filter are evaluated on the Dipole Pride 26 experimental data.
Since there is no available ground truth, the data has been divided in two
sets: training and testing. We show that even with a modest number of
particles, the Bootstrap particle filter provides better estimates of the
concentration field compared with the process model, without excessive increase
in computational complexity.
"
"  In dynamic models of infectious disease transmission, typically various
mixing patterns are imposed on the so-called Who-Acquires-Infection-From-Whom
matrix (WAIFW). These imposed mixing patterns are based on prior knowledge of
age-related social mixing behavior rather than observations. Alternatively, one
can assume that transmission rates for infections transmitted predominantly
through non-sexual social contacts, are proportional to rates of conversational
contact which can be estimated from a contact survey. In general, however,
contacts reported in social contact surveys are proxies of those events by
which transmission may occur and there may exist age-specific characteristics
related to susceptibility and infectiousness which are not captured by the
contact rates. Therefore, in this paper, transmission is modeled as the product
of two age-specific variables: the age-specific contact rate and an
age-specific proportionality factor, which entails an improvement of fit for
the seroprevalence of the varicella-zoster virus (VZV) in Belgium. Furthermore,
we address the impact on the estimation of the basic reproduction number, using
non-parametric bootstrapping to account for different sources of variability
and using multi-model inference to deal with model selection uncertainty. The
proposed method makes it possible to obtain important information on
transmission dynamics that cannot be inferred from approaches traditionally
applied hitherto.
"
"  We introduce the problem of reconstructing a sequence of multidimensional
real vectors where some of the data are missing. This problem contains
regression and mapping inversion as particular cases where the pattern of
missing data is independent of the sequence index. The problem is hard because
it involves possibly multivalued mappings at each vector in the sequence, where
the missing variables can take more than one value given the present variables;
and the set of missing variables can vary from one vector to the next. To solve
this problem, we propose an algorithm based on two redundancy assumptions:
vector redundancy (the data live in a low-dimensional manifold), so that the
present variables constrain the missing ones; and sequence redundancy (e.g.
continuity), so that consecutive vectors constrain each other. We capture the
low-dimensional nature of the data in a probabilistic way with a joint density
model, here the generative topographic mapping, which results in a Gaussian
mixture. Candidate reconstructions at each vector are obtained as all the modes
of the conditional distribution of missing variables given present variables.
The reconstructed sequence is obtained by minimising a global constraint, here
the sequence length, by dynamic programming. We present experimental results
for a toy problem and for inverse kinematics of a robot arm.
"
"  We present an extension of sparse PCA, or sparse dictionary learning, where
the sparsity patterns of all dictionary elements are structured and constrained
to belong to a prespecified set of shapes. This \emph{structured sparse PCA} is
based on a structured regularization recently introduced by [1]. While
classical sparse priors only deal with \textit{cardinality}, the regularization
we use encodes higher-order information about the data. We propose an efficient
and simple optimization procedure to solve this problem. Experiments with two
practical tasks, face recognition and the study of the dynamics of a protein
complex, demonstrate the benefits of the proposed structured approach over
unstructured approaches.
"
"  A large fraction of papers in the climate literature includes erroneous uses
of significance tests. A Bayesian analysis is presented to highlight the
meaning of significance tests and why typical misuse occurs. It is concluded
that a significance test very rarely provides useful quantitative information.
The significance statistic is not a quantitative measure of how confident we
can be of the 'reality' of a given result.
"
"  A functional approximation to implement Bayesian source separation analysis
is introduced and applied to separation of the Cosmic Microwave Background
(CMB) using WMAP data. The approximation allows for tractable full-sky map
reconstructions at the scale of both WMAP and Planck data and models the
spatial smoothness of sources through a Gaussian Markov random field prior. It
is orders of magnitude faster than the usual MCMC approaches. The performance
and limitations of the approximation are also discussed.
"
"  This paper presents two case studies of data sets where the main inferential
goal is to characterize time-varying patterns in model structure. Both of these
examples are seen to be general cases of the so-called ""partition problem,""
where auxiliary information (in this case, time) defines a partition over
sample space, and where different models hold for each element of the
partition. In the first case study, we identify time-varying graphical
structure in the covariance matrix of asset returns from major European equity
indices from 2006--2010. This structure has important implications for
quantifying the notion of financial contagion, a term often mentioned in the
context of the European sovereign debt crisis of this period. In the second
case study, we screen a large database of historical corporate performance in
order to identify specific firms with impressively good (or bad) streaks of
performance.
"
"  We present a generic framework for parallel coordinate descent (CD)
algorithms that includes, as special cases, the original sequential algorithms
Cyclic CD and Stochastic CD, as well as the recent parallel Shotgun algorithm.
We introduce two novel parallel algorithms that are also special
cases---Thread-Greedy CD and Coloring-Based CD---and give performance
measurements for an OpenMP implementation of these.
"
"  In this paper, we derive an explicit sample size formula based a mixed
criterion of absolute and relative errors for estimating means of Poisson
random variables.
"
"  The problem of clustering is considered, for the case when each data point is
a sample generated by a stationary ergodic process. We propose a very natural
asymptotic notion of consistency, and show that simple consistent algorithms
exist, under most general non-parametric assumptions. The notion of consistency
is as follows: two samples should be put into the same cluster if and only if
they were generated by the same distribution. With this notion of consistency,
clustering generalizes such classical statistical problems as homogeneity
testing and process classification. We show that, for the case of a known
number of clusters, consistency can be achieved under the only assumption that
the joint distribution of the data is stationary ergodic (no parametric or
Markovian assumptions, no assumptions of independence, neither between nor
within the samples). If the number of clusters is unknown, consistency can be
achieved under appropriate assumptions on the mixing rates of the processes.
(again, no parametric or independence assumptions). In both cases we give
examples of simple (at most quadratic in each argument) algorithms which are
consistent.
"
"  Mean profiles are widely used as indicators of the electricity consumption
habits of customers. Currently, in \'Electricit\'e De France (EDF), class load
profiles are estimated using point-wise mean function. Unfortunately, it is
well known that the mean is highly sensitive to the presence of outliers, such
as one or more consumers with unusually high-levels of consumption. In this
paper, we propose an alternative to the mean profile: the $L_1$-median profile
which is more robust. When dealing with large datasets of functional data (load
curves for example), survey sampling approaches are useful for estimating the
median profile avoiding storing the whole data. We propose here estimators of
the median trajectory using several sampling strategies and estimators. A
comparison between them is illustrated by means of a test population. We
develop a stratification based on the linearized variable which substantially
improves the accuracy of the estimator compared to simple random sampling
without replacement. We suggest also an improved estimator that takes into
account auxiliary information. Some potential areas for future research are
also highlighted.
"
"  Probabilistic graphical models combine the graph theory and probability
theory to give a multivariate statistical modeling. They provide a unified
description of uncertainty using probability and complexity using the graphical
model. Especially, graphical models provide the following several useful
properties:
  - Graphical models provide a simple and intuitive interpretation of the
structures of probabilistic models. On the other hand, they can be used to
design and motivate new models.
  - Graphical models provide additional insights into the properties of the
model, including the conditional independence properties.
  - Complex computations which are required to perform inference and learning
in sophisticated models can be expressed in terms of graphical manipulations,
in which the underlying mathematical expressions are carried along implicitly.
  The graphical models have been applied to a large number of fields, including
bioinformatics, social science, control theory, image processing, marketing
analysis, among others. However, structure learning for graphical models
remains an open challenge, since one must cope with a combinatorial search over
the space of all possible structures.
  In this paper, we present a comprehensive survey of the existing structure
learning algorithms.
"
"  Inter-coder agreement measures, like Cohen's kappa, correct the relative
frequency of agreement between coders to account for agreement which simply
occurs by chance. However, in some situations these measures exhibit behavior
which make their values difficult to interprete. These properties, e.g. the
""annotator bias"" or the ""problem of prevalence"", refer to a tendency of some of
these measures to indicate counterintuitive high or low values of reliability
depending on conditions which many researchers consider as unrelated to
inter-coder reliability. However, not all researchers agree with this view, and
since there is no commonly accepted formal definition of inter-coder
reliability, it is hard to decide whether this depends upon a different concept
of reliability or simply upon flaws in the measuring algorithms.
  In this note we therefore take an axiomatic approach: we introduce a model
for the rating of items by several coders according to a nominal scale. Based
upon this model we define inter-coder reliability as a probability to assign a
category to an item with certainty. We then discuss under which conditions this
notion of inter-coder reliability is uniquely determined given typical
experimental results, i.e. relative frequencies of category assignments by
different coders.
  In addition we provide an algorithm and conduct numerical simulations which
exhibit the accuracy of this algorithm under different model parameter
settings.
"
"  In this paper, a new method is proposed for sparse PCA based on the recursive
divide-and-conquer methodology. The main idea is to separate the original
sparse PCA problem into a series of much simpler sub-problems, each having a
closed-form solution. By recursively solving these sub-problems in an
analytical way, an efficient algorithm is constructed to solve the sparse PCA
problem. The algorithm only involves simple computations and is thus easy to
implement. The proposed method can also be very easily extended to other sparse
PCA problems with certain constraints, such as the nonnegative sparse PCA
problem. Furthermore, we have shown that the proposed algorithm converges to a
stationary point of the problem, and its computational complexity is
approximately linear in both data size and dimensionality. The effectiveness of
the proposed method is substantiated by extensive experiments implemented on a
series of synthetic and real data in both reconstruction-error-minimization and
data-variance-maximization viewpoints.
"
"  This paper addresses the issue of detecting point objects in a clutter
background and estimating their position by image processing. We are interested
in the specific context where the object signature significantly varies with
its random subpixel location because of aliasing. Conventional matched filter
neglects this phenomenon and causes consistent loss of detection performance.
Thus, alternative detectors are proposed and numerical results show the
improvement brought by approximate and generalized likelihood ratio tests in
comparison with pixel matched filtering. We also study the performance of two
types of subpixel position estimators. Finally, we put forward the major
influence of sensor design on both estimation and point object detection
performance.
"
"  Fast and accurate unveiling of power line outages is of paramount importance
not only for preventing faults that may lead to blackouts, but also for routine
monitoring and control tasks of the smart grid, including state estimation and
optimal power flow. Existing approaches are either challenged by the
\emph{combinatorial complexity} issues involved, and are thus limited to
identifying single- and double-line outages; or, they invoke less pragmatic
assumptions such as \emph{conditionally independent} phasor angle measurements
available across the grid. Using only a subset of voltage phasor angle data,
the present paper develops a near real-time algorithm for identifying multiple
line outages at the affordable complexity of solving a quadratic program via
block coordinate descent iterations. The novel approach relies on reformulating
the DC linear power flow model as a \emph{sparse} overcomplete expansion, and
leveraging contemporary advances in compressive sampling and variable selection
using the least-absolute shrinkage and selection operator (Lasso). Analysis and
simulated tests on the standard IEEE 118-bus system confirm the effectiveness
of lassoing line changes in the smart power grid.
"
"  In this paper we consider sparse approximation problems, that is, general
$l_0$ minimization problems with the $l_0$-""norm"" of a vector being a part of
constraints or objective function. In particular, we first study the
first-order optimality conditions for these problems. We then propose penalty
decomposition (PD) methods for solving them in which a sequence of penalty
subproblems are solved by a block coordinate descent (BCD) method. Under some
suitable assumptions, we establish that any accumulation point of the sequence
generated by the PD methods satisfies the first-order optimality conditions of
the problems. Furthermore, for the problems in which the $l_0$ part is the only
nonconvex part, we show that such an accumulation point is a local minimizer of
the problems. In addition, we show that any accumulation point of the sequence
generated by the BCD method is a saddle point of the penalty subproblem.
Moreover, for the problems in which the $l_0$ part is the only nonconvex part,
we establish that such an accumulation point is a local minimizer of the
penalty subproblem. Finally, we test the performance of our PD methods by
applying them to sparse logistic regression, sparse inverse covariance
selection, and compressed sensing problems. The computational results
demonstrate that our methods generally outperform the existing methods in terms
of solution quality and/or speed.
"
"  Motivated by the need, in some Bayesian likelihood free inference problems,
of imputing a multivariate counting distribution based on its vector of means
and variance-covariance matrix, we define a generic multivariate discrete
distribution. Based on blending the Binomial, Poisson and Negative-Binomial
distributions, and using a normal multivariate copula, the required
distribution is defined. This distribution tends to the Multivariate Normal for
large counts and has an approximate pmf version that is quite simple to
evaluate.
"
"  In this paper we study the approximate learnability of valuations commonly
used throughout economics and game theory for the quantitative encoding of
agent preferences. We provide upper and lower bounds regarding the learnability
of important subclasses of valuation functions that express
no-complementarities. Our main results concern their approximate learnability
in the distributional learning (PAC-style) setting. We provide nearly tight
lower and upper bounds of $\tilde{\Theta}(n^{1/2})$ on the approximation factor
for learning XOS and subadditive valuations, both widely studied superclasses
of submodular valuations. Interestingly, we show that the
$\tilde{\Omega}(n^{1/2})$ lower bound can be circumvented for XOS functions of
polynomial complexity; we provide an algorithm for learning the class of XOS
valuations with a representation of polynomial size achieving an $O(n^{\eps})$
approximation factor in time $O(n^{1/\eps})$ for any $\eps > 0$. This
highlights the importance of considering the complexity of the target function
for polynomial time learning. We also provide new learning results for
interesting subclasses of submodular functions.
  Our upper bounds for distributional learning leverage novel structural
results for all these valuation classes. We show that many of these results
provide new learnability results in the Goemans et al. model (SODA 2009) of
approximate learning everywhere via value queries.
  We also introduce a new model that is more realistic in economic settings, in
which the learner can set prices and observe purchase decisions at these prices
rather than observing the valuation function directly. In this model, most of
our upper bounds continue to hold despite the fact that the learner receives
less information (both for learning in the distributional setting and with
value queries), while our lower bounds naturally extend.
"
"  Dimension-reduction techniques can greatly improve statistical inference in
astronomy. A standard approach is to use Principal Components Analysis (PCA).
In this work we apply a recently-developed technique, diffusion maps, to
astronomical spectra for data parameterization and dimensionality reduction,
and develop a robust, eigenmode-based framework for regression. We show how our
framework provides a computationally efficient means by which to predict
redshifts of galaxies, and thus could inform more expensive redshift estimators
such as template cross-correlation. It also provides a natural means by which
to identify outliers (e.g., misclassified spectra, spectra with anomalous
features). We analyze 3835 SDSS spectra and show how our framework yields a
more than 95% reduction in dimensionality. Finally, we show that the prediction
error of the diffusion map-based regression approach is markedly smaller than
that of a similar approach based on PCA, clearly demonstrating the superiority
of diffusion maps over PCA for this regression task.
"
"  We describe a new technique for computing lower-bounds on the minimum energy
configuration of a planar Markov Random Field (MRF). Our method successively
adds large numbers of constraints and enforces consistency over binary
projections of the original problem state space. These constraints are
represented in terms of subproblems in a dual-decomposition framework that is
optimized using subgradient techniques. The complete set of constraints we
consider enforces cycle consistency over the original graph. In practice we
find that the method converges quickly on most problems with the addition of a
few subproblems and outperforms existing methods for some interesting classes
of hard potentials.
"
"  We find lower and upper bounds for the risk of estimating a manifold in
Hausdorff distance under several models. We also show that there are close
connections between manifold estimation and the problem of deconvolving a
singular measure.
"
"  We consider testing statistical hypotheses about densities of signals in
deconvolution models. A new approach to this problem is proposed. We
constructed score tests for the deconvolution with the known noise density and
efficient score tests for the case of unknown density. The tests are
incorporated with model selection rules to choose reasonable model dimensions
automatically by the data. Consistency of the tests is proved.
"
"  The Lasso is a very well known penalized regression model, which adds an
$L_{1}$ penalty with parameter $\lambda_{1}$ on the coefficients to the squared
error loss function. The Fused Lasso extends this model by also putting an
$L_{1}$ penalty with parameter $\lambda_{2}$ on the difference of neighboring
coefficients, assuming there is a natural ordering. In this paper, we develop a
fast path algorithm for solving the Fused Lasso Signal Approximator that
computes the solutions for all values of $\lambda_1$ and $\lambda_2$. In the
supplement, we also give an algorithm for the general Fused Lasso for the case
with predictor matrix $\bX \in \mathds{R}^{n \times p}$ with
$\text{rank}(\bX)=p$.
"
"  The basic model for high-frequency data in finance is considered, where an
efficient price process is observed under microstructure noise. It is shown
that this nonparametric model is in Le Cam's sense asymptotically equivalent to
a Gaussian shift experiment in terms of the square root of the volatility
function $\sigma$. As an application, simple rate-optimal estimators of the
volatility and efficient estimators of the integrated volatility are
constructed.
"
"  In this paper, we address the general case of a coordinated secondary network
willing to exploit communication opportunities left vacant by a licensed
primary network. Since secondary users (SU) usually have no prior knowledge on
the environment, they need to learn the availability of each channel through
sensing techniques, which however can be prone to detection errors. We argue
that cooperation among secondary users can enable efficient learning and
coordination mechanisms in order to maximize the spectrum exploitation by SUs,
while minimizing the impact on the primary network. To this goal, we provide
three novel contributions in this paper. First, we formulate the spectrum
selection in secondary networks as an instance of the Multi-Armed Bandit (MAB)
problem, and we extend the analysis to the collaboration learning case, in
which each SU learns the spectrum occupation, and shares this information with
other SUs. We show that collaboration among SUs can mitigate the impact of
sensing errors on system performance, and improve the convergence of the
learning process to the optimal solution. Second, we integrate the learning
algorithms with two collaboration techniques based on modified versions of the
Hungarian algorithm and of the Round Robin algorithm that allows reducing the
interference among SUs. Third, we derive fundamental limits to the performance
of cooperative learning algorithms based on Upper Confidence Bound (UCB)
policies in a symmetric scenario where all SU have the same perception of the
quality of the resources. Extensive simulation results confirm the
effectiveness of our joint learning-collaboration algorithm in protecting the
operations of Primary Users (PUs), while maximizing the performance of SUs.
"
"  We present a new boosting algorithm, motivated by the large margins theory
for boosting. We give experimental evidence that the new algorithm is
significantly more robust against label noise than existing boosting algorithm.
"
"  This paper provides the reader with a very brief introduction to some of the
theory and methods of text data mining. The intent of this article is to
introduce the reader to some of the current methodologies that are employed
within this discipline area while at the same time making the reader aware of
some of the interesting challenges that remain to be solved within the area.
Finally, the articles serves as a very rudimentary tutorial on some of
techniques while also providing the reader with a list of references for
additional study.
"
"  The nurse Lucia de Berk was convicted by the Dutch courts as a serial killer
with 7 murders and 3 attempts at murder in three hospitals where she worked.
The nurse however always professed her innocence and indeed was never observed
in such an act of murder. The courts based their decision on circumstantial
evidence and upon the use of statistics. In the appeal court, the use of
statistical calculations was repealed but the use of ""data"" and ""statistical
insights"" were not excluded. The trial hinged importantly on the role of
statistics and data gathering. It appears that data selection and confounding
feature strongly in this case. The notion of ""nominal correlation"" can be used
to highlight those two features. This suggests a mistrial with the conviction
of an innocent person.
"
"  Nicholls and Gray (2008) describe a phylogenetic model for trait data. They
use their model to estimate branching times on Indo-European language trees
from lexical data. Alekseyenko et al. (2008) extended the model and give
applications in genetics. In this paper we extend the inference to handle data
missing at random. When trait data are gathered, traits are thinned in a way
that depends on both the trait and missing-data content. Nicholls and Gray
(2008) treat missing records as absent traits. Hittite has 12% missing trait
records. Its age is poorly predicted in their cross-validation. Our prediction
is consistent with the historical record. Nicholls and Gray (2008) dropped
seven languages with too much missing data. We fit all twenty four languages in
the lexical data of Ringe (2002). In order to model spatial-temporal rate
heterogeneity we add a catastrophe process to the model. When a language passes
through a catastrophe, many traits change at the same time. We fit the full
model in a Bayesian setting, via MCMC.
  We validate our fit using Bayes factors to test known age constraints. We
reject three of thirty historically attested constraints. Our main result is a
unimodel posterior distribution for the age of Proto-Indo-European centered at
8400 years BP with 95% HPD equal 7100-9800 years BP.
"
"  Most professional golfers and analysts think that winning on the PGA Tour
peaks when golfers are in their thirties. Rather than relying on educated
guesses, we can actually use available statistical data to determine the actual
ages at which golfers peak their golf game. We can also test the hypothesis
that age affects winning professional golf tournaments. Using data available
from the Golf Channel, the PGA Tour, and LPGA Tour, I calculated and provided
the mean, the median, and the mode ages at which professional golfers on the
PGA, European PGA, Champions, and LPGA Tours had won over a five-year period.
More specifically, the ages at which golfers on the PGA, European PGA,
Champions Tour, and LPGA Tours peak their wins are 35, 30, 52, and 25,
respectively. The regression analyses I conducted seem to support my hypothesis
that age affects winning professional golf tournaments.
"
"  We propose a non-parametric link prediction algorithm for a sequence of graph
snapshots over time. The model predicts links based on the features of its
endpoints, as well as those of the local neighborhood around the endpoints.
This allows for different types of neighborhoods in a graph, each with its own
dynamics (e.g, growing or shrinking communities). We prove the consistency of
our estimator, and give a fast implementation based on locality-sensitive
hashing. Experiments with simulated as well as five real-world dynamic graphs
show that we outperform the state of the art, especially when sharp
fluctuations or non-linearities are present.
"
"  We consider probabilistic multinomial probit classification using Gaussian
process (GP) priors. The challenges with the multiclass GP classification are
the integration over the non-Gaussian posterior distribution, and the increase
of the number of unknown latent variables as the number of target classes
grows. Expectation propagation (EP) has proven to be a very accurate method for
approximate inference but the existing EP approaches for the multinomial probit
GP classification rely on numerical quadratures or independence assumptions
between the latent values from different classes to facilitate the
computations. In this paper, we propose a novel nested EP approach which does
not require numerical quadratures, and approximates accurately all
between-class posterior dependencies of the latent values, but still scales
linearly in the number of classes. The predictive accuracy of the nested EP
approach is compared to Laplace, variational Bayes, and Markov chain Monte
Carlo (MCMC) approximations with various benchmark data sets. In the
experiments nested EP was the most consistent method with respect to MCMC
sampling, but the differences between the compared methods were small if only
the classification accuracy is concerned.
"
"  In this article we review existing literature on dynamic copulas and then
propose an n-copula which varies in time and space. Our approach makes use of
stochastic differential equations, and gives rise to a dynamic copula which is
able to capture the dependence between multiple Markov diffusion processes.
This model is suitable for pricing basket derivatives in finance and may also
be applicable to other areas such as bioinformatics and environmental science.
"
"  Recently, there has been much interest in finding globally optimal Bayesian
network structures. These techniques were developed for generative scores and
can not be directly extended to discriminative scores, as desired for
classification. In this paper, we propose an exact method for finding network
structures maximizing the probabilistic soft margin, a successfully applied
discriminative score. Our method is based on branch-and-bound techniques within
a linear programming framework and maintains an any-time solution, together
with worst-case sub-optimality bounds. We apply a set of order constraints for
enforcing the network structure to be acyclic, which allows a compact problem
representation and the use of general-purpose optimization techniques. In
classification experiments, our methods clearly outperform generatively trained
network structures and compete with support vector machines.
"
"  We propose a randomized block-coordinate variant of the classic Frank-Wolfe
algorithm for convex optimization with block-separable constraints. Despite its
lower iteration cost, we show that it achieves a similar convergence rate in
duality gap as the full Frank-Wolfe algorithm. We also show that, when applied
to the dual structural support vector machine (SVM) objective, this yields an
online algorithm that has the same low iteration complexity as primal
stochastic subgradient methods. However, unlike stochastic subgradient methods,
the block-coordinate Frank-Wolfe algorithm allows us to compute the optimal
step-size and yields a computable duality gap guarantee. Our experiments
indicate that this simple algorithm outperforms competing structural SVM
solvers.
"
"  In a Bayesian learning setting, the posterior distribution of a predictive
model arises from a trade-off between its prior distribution and the
conditional likelihood of observed data. Such distribution functions usually
rely on additional hyperparameters which need to be tuned in order to achieve
optimum predictive performance; this operation can be efficiently performed in
an Empirical Bayes fashion by maximizing the posterior marginal likelihood of
the observed data. Since the score function of this optimization problem is in
general characterized by the presence of local optima, it is necessary to
resort to global optimization strategies, which require a large number of
function evaluations. Given that the evaluation is usually computationally
intensive and badly scaled with respect to the dataset size, the maximum number
of observations that can be treated simultaneously is quite limited. In this
paper, we consider the case of hyperparameter tuning in Gaussian process
regression. A straightforward implementation of the posterior log-likelihood
for this model requires O(N^3) operations for every iteration of the
optimization procedure, where N is the number of examples in the input dataset.
We derive a novel set of identities that allow, after an initial overhead of
O(N^3), the evaluation of the score function, as well as the Jacobian and
Hessian matrices, in O(N) operations. We prove how the proposed identities,
that follow from the eigendecomposition of the kernel matrix, yield a reduction
of several orders of magnitude in the computation time for the hyperparameter
optimization problem. Notably, the proposed solution provides computational
advantages even with respect to state of the art approximations that rely on
sparse kernel matrices.
"
"  When using deep, multi-layered architectures to build generative models of
data, it is difficult to train all layers at once. We propose a layer-wise
training procedure admitting a performance guarantee compared to the global
optimum. It is based on an optimistic proxy of future performance, the best
latent marginal. We interpret auto-encoders in this setting as generative
models, by showing that they train a lower bound of this criterion. We test the
new learning procedure against a state of the art method (stacked RBMs), and
find it to improve performance. Both theory and experiments highlight the
importance, when training deep architectures, of using an inference model (from
data to hidden variables) richer than the generative model (from hidden
variables to data).
"
"  This paper addresses the estimation of the latent dimensionality in
nonnegative matrix factorization (NMF) with the \beta-divergence. The
\beta-divergence is a family of cost functions that includes the squared
Euclidean distance, Kullback-Leibler and Itakura-Saito divergences as special
cases. Learning the model order is important as it is necessary to strike the
right balance between data fidelity and overfitting. We propose a Bayesian
model based on automatic relevance determination in which the columns of the
dictionary matrix and the rows of the activation matrix are tied together
through a common scale parameter in their prior. A family of
majorization-minimization algorithms is proposed for maximum a posteriori (MAP)
estimation. A subset of scale parameters is driven to a small lower bound in
the course of inference, with the effect of pruning the corresponding spurious
components. We demonstrate the efficacy and robustness of our algorithms by
performing extensive experiments on synthetic data, the swimmer dataset, a
music decomposition example and a stock price prediction task.
"
"  We present a generalization of independent component analysis (ICA), where
instead of looking for a linear transform that makes the data components
independent, we look for a transform that makes the data components well fit by
a tree-structured graphical model. Treating the problem as a semiparametric
statistical problem, we show that the optimal transform is found by minimizing
a contrast function based on mutual information, a function that directly
extends the contrast function used for classical ICA. We provide two
approximations of this contrast function, one using kernel density estimation,
and another using kernel generalized variance. This tree-dependent component
analysis framework leads naturally to an efficient general multivariate density
estimation technique where only bivariate density estimation needs to be
performed.
"
"  A large amount of data is typically collected during a periodontal exam.
Analyzing these data poses several challenges. Several types of measurements
are taken at many locations throughout the mouth. These spatially-referenced
data are a mix of binary and continuous responses, making joint modeling
difficult. Also, most patients have missing teeth. Periodontal disease is a
leading cause of tooth loss, so it is likely that the number and location of
missing teeth informs about the patient's periodontal health. In this paper we
develop a multivariate spatial framework for these data which jointly models
the binary and continuous responses as a function of a single latent spatial
process representing general periodontal health. We also use the latent spatial
process to model the location of missing teeth. We show using simulated and
real data that exploiting spatial associations and jointly modeling the
responses and locations of missing teeth mitigates the problems presented by
these data.
"
"  Dynamical modelling lies at the heart of our understanding of physical
systems. Its role in science is deeper than mere operational forecasting, in
that it allows us to evaluate the adequacy of the mathematical structure of our
models. Despite the importance of model parameters, there is no general method
of parameter estimation outside linear systems. A new relatively simple method
of parameter estimation for nonlinear systems is presented, based on variations
in the accuracy of probability forecasts. It is illustrated on the Logistic
Map, the Henon Map and the 12-D Lorenz96 flow, and its ability to outperform
linear least squares in these systems is explored at various noise levels and
sampling rates. As expected, it is more effective when the forecast error
distributions are non-Gaussian. The new method selects parameter values by
minimizing a proper, local skill score for continuous probability forecasts as
a function of the parameter values. This new approach is easier to implement in
practice than alternative nonlinear methods based on the geometry of attractors
or the ability of the model to shadow the observations. New direct measures of
inadequacy in the model, the ""Implied Ignorance"" and the information deficit
are introduced.
"
"  We approximate the distribution of total expenditure of a retail company over
warranty claims incurred in a fixed period [0, T], say the following quarter.
We consider two kinds of warranty policies, namely, the non-renewing free
replacement warranty policy and the non-renewing pro-rata warranty policy. Our
approximation holds under modest assumptions on the distribution of the sales
process of the warranted item and the nature of arrivals of warranty claims. We
propose a method of using historical data to statistically estimate the
parameters of the approximate distribution. Our methodology is applied to the
warranty claims data from a large car manufacturer for a single car model and
model year.
"
"  We discuss two parameterizations of models for marginal independencies for
discrete distributions which are representable by bi-directed graph models,
under the global Markov property. Such models are useful data analytic tools
especially if used in combination with other graphical models. The first
parameterization, in the saturated case, is also known as the multivariate
logistic transformation, the second is a variant that allows, in some (but not
all) cases, variation independent parameters. An algorithm for maximum
likelihood fitting is proposed, based on an extension of the Aitchison and
Silvey method.
"
"  This paper introduces the Partition Tree Weighting technique, an efficient
meta-algorithm for piecewise stationary sources. The technique works by
performing Bayesian model averaging over a large class of possible partitions
of the data into locally stationary segments. It uses a prior, closely related
to the Context Tree Weighting technique of Willems, that is well suited to data
compression applications. Our technique can be applied to any coding
distribution at an additional time and space cost only logarithmic in the
sequence length. We provide a competitive analysis of the redundancy of our
method, and explore its application in a variety of settings. The order of the
redundancy and the complexity of our algorithm matches those of the best
competitors available in the literature, and the new algorithm exhibits a
superior complexity-performance trade-off in our experiments.
"
"  Software defects rediscovered by a large number of customers affect various
stakeholders and may: 1) hint at gaps in a software manufacturer's Quality
Assurance (QA) processes, 2) lead to an over-load of a software manufacturer's
support and maintenance teams, and 3) consume customers' resources, leading to
a loss of reputation and a decrease in sales.
  Quantifying risk associated with the rediscovery of defects can help all of
these stake-holders. In this chapter we present a set of metrics needed to
quantify the risks. The metrics are designed to help: 1) the QA team to assess
their processes; 2) the support and maintenance teams to allocate their
resources; and 3) the customers to assess the risk associated with using the
software product. The paper includes a validation case study which applies the
risk metrics to industrial data. To calculate the metrics we use mathematical
instruments like the heavy-tailed Kappa distribution and the G/M/k queuing
model.
"
"  We present the discrete infinite logistic normal distribution (DILN), a
Bayesian nonparametric prior for mixed membership models. DILN is a
generalization of the hierarchical Dirichlet process (HDP) that models
correlation structure between the weights of the atoms at the group level. We
derive a representation of DILN as a normalized collection of gamma-distributed
random variables, and study its statistical properties. We consider
applications to topic modeling and derive a variational inference algorithm for
approximate posterior inference. We study the empirical performance of the DILN
topic model on four corpora, comparing performance with the HDP and the
correlated topic model (CTM). To deal with large-scale data sets, we also
develop an online inference algorithm for DILN and compare with online HDP and
online LDA on the Nature magazine, which contains approximately 350,000
articles.
"
"  We consider learning, from strictly behavioral data, the structure and
parameters of linear influence games (LIGs), a class of parametric graphical
games introduced by Irfan and Ortiz (2014). LIGs facilitate causal strategic
inference (CSI): Making inferences from causal interventions on stable behavior
in strategic settings. Applications include the identification of the most
influential individuals in large (social) networks. Such tasks can also support
policy-making analysis. Motivated by the computational work on LIGs, we cast
the learning problem as maximum-likelihood estimation (MLE) of a generative
model defined by pure-strategy Nash equilibria (PSNE). Our simple formulation
uncovers the fundamental interplay between goodness-of-fit and model
complexity: good models capture equilibrium behavior within the data while
controlling the true number of equilibria, including those unobserved. We
provide a generalization bound establishing the sample complexity for MLE in
our framework. We propose several algorithms including convex loss minimization
(CLM) and sigmoidal approximations. We prove that the number of exact PSNE in
LIGs is small, with high probability; thus, CLM is sound. We illustrate our
approach on synthetic data and real-world U.S. congressional voting records. We
briefly discuss our learning framework's generality and potential applicability
to general graphical games.
"
"  The aim of this paper is the introduction of a new method for the numerical
computation of the rank of a three-way array. We show that the rank of a
three-way array over R is intimately related to the real solution set of a
system of polynomial equations. Using this, we present some numerical results
based on the computation of Grobner bases.
  Key words: Tensors; three-way arrays; Candecomp/Parafac; Indscal; generic
rank; typical rank; Veronese variety; Segre variety; Grobner bases.
  AMS classification: Primary 15A69; Secondary 15A72, 15A18.
"
"  We demonstrate a meaningful prospective power analysis for an (admittedly
idealized) illustrative connectome inference task. Modeling neurons as vertices
and synapses as edges in a simple random graph model, we optimize the trade-off
between the number of (putative) edges identified and the accuracy of the edge
identification procedure. We conclude that explicit analysis of the
quantity/quality trade-off is imperative for optimal neuroscientific
experimental design. In particular, more though more errorful edge
identification can yield superior inferential performance.
"
"  The estimation of missing input vector elements in real time processing
applications requires a system that possesses the knowledge of certain
characteristics such as correlations between variables, which are inherent in
the input space. Computational intelligence techniques and maximum likelihood
techniques do possess such characteristics and as a result are important for
imputation of missing data. This paper compares two approaches to the problem
of missing data estimation. The first technique is based on the current state
of the art approach to this problem, that being the use of Maximum Likelihood
(ML) and Expectation Maximisation (EM. The second approach is the use of a
system based on auto-associative neural networks and the Genetic Algorithm as
discussed by Adbella and Marwala3. The estimation ability of both of these
techniques is compared, based on three datasets and conclusions are made.
"
"  We adapt existing statistical modeling techniques for social networks to
study consumption data observed in trophic food webs. These data describe the
feeding volume (non-negative) among organisms grouped into nodes, called
trophic species, that form the food web. Model complexity arises due to the
extensive amount of zeros in the data, as each node in the web is predator/prey
to only a small number of other trophic species. Many of the zeros are regarded
as structural (non-random) in the context of feeding behavior. The presence of
basal prey and top predator nodes (those who never consume and those who are
never consumed, with probability 1) creates additional complexity to the
statistical modeling. We develop a special statistical social network model to
account for such network features. The model is applied to two empirical food
webs; focus is on the web for which the population size of seals is of concern
to various commercial fisheries.
"
"  Modeling species abundance patterns using local environmental features is an
important, current problem in ecology. The Cape Floristic Region (CFR) in South
Africa is a global hot spot of diversity and endemism, and provides a rich
class of species abundance data for such modeling. Here, we propose a
multi-stage Bayesian hierarchical model for explaining species abundance over
this region. Our model is specified at areal level, where the CFR is divided
into roughly $37{,}000$ one minute grid cells; species abundance is observed at
some locations within some cells. The abundance values are ordinally
categorized. Environmental and soil-type factors, likely to influence the
abundance pattern, are included in the model. We formulate the empirical
abundance pattern as a degraded version of the potential pattern, with the
degradation effect accomplished in two stages. First, we adjust for land use
transformation and then we adjust for measurement error, hence
misclassification error, to yield the observed abundance classifications. An
important point in this analysis is that only $28%$ of the grid cells have been
sampled and that, for sampled grid cells, the number of sampled locations
ranges from one to more than one hundred. Still, we are able to develop
potential and transformed abundance surfaces over the entire region. In the
hierarchical framework, categorical abundance classifications are induced by
continuous latent surfaces. The degradation model above is built on the latent
scale. On this scale, an areal level spatial regression model was used for
modeling the dependence of species abundance on the environmental factors.
"
"  Product models of low dimensional experts are a powerful way to avoid the
curse of dimensionality. We present the ``under-complete product of experts'
(UPoE), where each expert models a one dimensional projection of the data. The
UPoE is fully tractable and may be interpreted as a parametric probabilistic
model for projection pursuit. Its ML learning rules are identical to the
approximate learning rules proposed before for under-complete ICA. We also
derive an efficient sequential learning algorithm and discuss its relationship
to projection pursuit density estimation and feature induction algorithms for
additive random field models.
"
"  Inferential summaries of tree estimates are useful in the setting of
evolutionary biology, where phylogenetic trees have been built from DNA data
since the 1960's. In bioinformatics, psychometrics and data mining,
hierarchical clustering techniques output the same mathematical objects, and
practitioners have similar questions about the stability and `generalizability'
of these summaries. This paper provides an implementation of the geometric
distance between trees developed by Billera, Holmes and Vogtmann (2001) [BHV]
equally applicable to phylogenetic trees and hieirarchical clustering trees,
and shows some of the applications in statistical inference for which this
distance can be useful. In particular, since BHV have shown that the space of
trees is negatively curved (a CAT(0) space), a natural representation of a
collection of trees is a tree. We compare this representation to the Euclidean
approximations of treespace made available through Multidimensional Scaling of
the matrix of distances between trees. We also provide applications of the
distances between trees to hierarchical clustering trees constructed from
microarrays. Our method gives a new way of evaluating the influence both of
certain columns (positions, variables or genes) and of certain rows (whether
species, observations or arrays).
"
"  This paper presents a new approach for filter design based on stochastic
distances and tests between distributions. A window is defined around each
pixel, samples are compared and only those which pass a goodness-of-fit test
are used to compute the filtered value. The technique is applied to intensity
Synthetic Aperture Radar (SAR) data, using the Gamma model with varying number
of looks allowing, thus, changes in heterogeneity. Modified Nagao-Matsuyama
windows are used to define the samples. The proposal is compared with the Lee's
filter which is considered a standard, using a protocol based on simulation.
Among the criteria used to quantify the quality of filters, we employ the
equivalent number of looks (related to the signal-to-noise ratio), line
contrast, and edge preservation. Moreover, we also assessed the filters by the
Universal Image Quality Index and the Pearson's correlation between edges.
"
"  Stochastic Gradient Descent (SGD) has become popular for solving large scale
supervised machine learning optimization problems such as SVM, due to their
strong theoretical guarantees. While the closely related Dual Coordinate Ascent
(DCA) method has been implemented in various software packages, it has so far
lacked good convergence analysis. This paper presents a new analysis of
Stochastic Dual Coordinate Ascent (SDCA) showing that this class of methods
enjoy strong theoretical guarantees that are comparable or better than SGD.
This analysis justifies the effectiveness of SDCA for practical applications.
"
"  This review article considers some of the most common methods used in
astronomy for regressing one quantity against another in order to estimate the
model parameters or to predict an observationally expensive quantity using
trends between object values. These methods have to tackle some of the awkward
features prevalent in astronomical data, namely heteroscedastic
(point-dependent) errors, intrinsic scatter, non-ignorable data collection and
selection effects, data structure and non-uniform population (often called
Malmquist bias), non-Gaussian data, outliers and mixtures of regressions. We
outline how least square fits, weighted least squares methods, Maximum
Likelihood, survival analysis, and Bayesian methods have been applied in the
astrophysics literature when one or more of these features is present. In
particular we concentrate on errors-in-variables regression and we advocate
Bayesian techniques.
"
"  Bacteria are the unseen majority on our planet, with millions of species and
comprising most of the living protoplasm. While current methods enable in-depth
study of a small number of communities, a simple tool for breadth studies of
bacterial population composition in a large number of samples is lacking. We
propose a novel approach for reconstruction of the composition of an unknown
mixture of bacteria using a single Sanger-sequencing reaction of the mixture.
This method is based on compressive sensing theory, which deals with
reconstruction of a sparse signal using a small number of measurements.
Utilizing the fact that in many cases each bacterial community is comprised of
a small subset of the known bacterial species, we show the feasibility of this
approach for determining the composition of a bacterial mixture. Using
simulations, we show that sequencing a few hundred base-pairs of the 16S rRNA
gene sequence may provide enough information for reconstruction of mixtures
containing tens of species, out of tens of thousands, even in the presence of
realistic measurement noise. Finally, we show initial promising results when
applying our method for the reconstruction of a toy experimental mixture with
five species. Our approach may have a potential for a practical and efficient
way for identifying bacterial species compositions in biological samples.
"
"  This paper re-visits the spectral method for learning latent variable models
defined in terms of observable operators. We give a new perspective on the
method, showing that operators can be recovered by minimizing a loss defined on
a finite subset of the domain. A non-convex optimization similar to the
spectral method is derived. We also propose a regularized convex relaxation of
this optimization. We show that in practice the availabilty of a continuous
regularization parameter (in contrast with the discrete number of states in the
original method) allows a better trade-off between accuracy and model
complexity. We also prove that in general, a randomized strategy for choosing
the local loss will succeed with high probability.
"
"  We provide a new approach, along with extensions, to results in two important
papers of Worsley, Siegmund and coworkers closely tied to the statistical
analysis of fMRI (functional magnetic resonance imaging) brain data. These
papers studied approximations for the exceedence probabilities of scale and
rotation space random fields, the latter playing an important role in the
statistical analysis of fMRI data. The techniques used there came either from
the Euler characteristic heuristic or via tube formulae, and to a large extent
were carefully attuned to the specific examples of the paper. This paper treats
the same problem, but via calculations based on the so-called Gaussian
kinematic formula. This allows for extensions of the Worsley-Siegmund results
to a wide class of non-Gaussian cases. In addition, it allows one to obtain
results for rotation space random fields in any dimension via reasonably
straightforward Riemannian geometric calculations. Previously only the
two-dimensional case could be covered, and then only via computer algebra. By
adopting this more structured approach to this particular problem, a solution
path for other, related problems becomes clearer.
"
"  Unsupervised estimation of latent variable models is a fundamental problem
central to numerous applications of machine learning and statistics. This work
presents a principled approach for estimating broad classes of such models,
including probabilistic topic models and latent linear Bayesian networks, using
only second-order observed moments. The sufficient conditions for
identifiability of these models are primarily based on weak expansion
constraints on the topic-word matrix, for topic models, and on the directed
acyclic graph, for Bayesian networks. Because no assumptions are made on the
distribution among the latent variables, the approach can handle arbitrary
correlations among the topics or latent factors. In addition, a tractable
learning method via $\ell_1$ optimization is proposed and studied in numerical
experiments.
"
"  We prove that the criterion for Markov equivalence provided by Zhao et al.
(2005) may involve a set of features of a graph that is exponential in the
number of vertices.
"
"  I first met Leo Breiman in 1979 at the beginning of his third career,
Professor of Statistics at Berkeley. He obtained his PhD with Lo\'eve at
Berkeley in 1957. His first career was as a probabilist in the Mathematics
Department at UCLA. After distinguished research, including the
Shannon--Breiman--MacMillan Theorem and getting tenure, he decided that his
real interest was in applied statistics, so he resigned his position at UCLA
and set up as a consultant. Before doing so he produced two classic texts,
Probability, now reprinted as a SIAM Classic in Applied Mathematics, and
Statistics. Both books reflected his strong opinion that intuition and rigor
must be combined. He expressed this in his probability book which he viewed as
a combination of his learning the right hand of probability, rigor, from
Lo\'eve, and the left-hand, intuition, from David Blackwell.
"
"  Data analysis and data mining are concerned with unsupervised pattern finding
and structure determination in data sets. The data sets themselves are
explicitly linked as a form of representation to an observational or otherwise
empirical domain of interest. ""Structure"" has long been understood as symmetry
which can take many forms with respect to any transformation, including point,
translational, rotational, and many others. Beginning with the role of number
theory in expressing data, we show how we can naturally proceed to hierarchical
structures. We show how this both encapsulates traditional paradigms in data
analysis, and also opens up new perspectives towards issues that are on the
order of the day, including data mining of massive, high dimensional,
heterogeneous data sets. Linkages with other fields are also discussed
including computational logic and symbolic dynamics. The structures in data
surveyed here are based on hierarchy, represented as p-adic numbers or an
ultrametric topology.
"
"  This paper explores semi-qualitative probabilistic networks (SQPNs) that
combine numeric and qualitative information. We first show that exact
inferences with SQPNs are NPPP-Complete. We then show that existing qualitative
relations in SQPNs (plus probabilistic logic and imprecise assessments) can be
dealt effectively through multilinear programming. We then discuss learning: we
consider a maximum likelihood method that generates point estimates given a
SQPN and empirical data, and we describe a Bayesian-minded method that employs
the Imprecise Dirichlet Model to generate set-valued estimates.
"
"  We consider active, semi-supervised learning in an offline transductive
setting. We show that a previously proposed error bound for active learning on
undirected weighted graphs can be generalized by replacing graph cut with an
arbitrary symmetric submodular function. Arbitrary non-symmetric submodular
functions can be used via symmetrization. Different choices of submodular
functions give different versions of the error bound that are appropriate for
different kinds of problems. Moreover, the bound is deterministic and holds for
adversarially chosen labels. We show exactly minimizing this error bound is
NP-complete. However, we also introduce for any submodular function an
associated active semi-supervised learning method that approximately minimizes
the corresponding error bound. We show that the error bound is tight in the
sense that there is no other bound of the same form which is better. Our
theoretical results are supported by experiments on real data.
"
"  Rank-order relational data, in which each actor ranks the others according to
some criterion, often arise from sociometric measurements of judgment (e.g.,
self-reported interpersonal interaction) or preference (e.g., relative liking).
We propose a class of exponential-family models for rank-order relational data
and derive a new class of sufficient statistics for such data, which assume no
more than within-subject ordinal properties. Application of MCMC MLE to this
family allows us to estimate effects for a variety of plausible mechanisms
governing rank structure in cross-sectional context, and to model the evolution
of such structures over time. We apply this framework to model the evolution of
relative liking judgments in an acquaintance process, and to model recall of
relative volume of interpersonal interaction among members of a technology
education program.
"
"  In this paper, we consider the multivariate Bernoulli distribution as a model
to estimate the structure of graphs with binary nodes. This distribution is
discussed in the framework of the exponential family, and its statistical
properties regarding independence of the nodes are demonstrated. Importantly
the model can estimate not only the main effects and pairwise interactions
among the nodes but also is capable of modeling higher order interactions,
allowing for the existence of complex clique effects. We compare the
multivariate Bernoulli model with existing graphical inference models - the
Ising model and the multivariate Gaussian model, where only the pairwise
interactions are considered. On the other hand, the multivariate Bernoulli
distribution has an interesting property in that independence and
uncorrelatedness of the component random variables are equivalent. Both the
marginal and conditional distributions of a subset of variables in the
multivariate Bernoulli distribution still follow the multivariate Bernoulli
distribution. Furthermore, the multivariate Bernoulli logistic model is
developed under generalized linear model theory by utilizing the canonical link
function in order to include covariate information on the nodes, edges and
cliques. We also consider variable selection techniques such as LASSO in the
logistic model to impose sparsity structure on the graph. Finally, we discuss
extending the smoothing spline ANOVA approach to the multivariate Bernoulli
logistic model to enable estimation of non-linear effects of the predictor
variables.
"
"  Signal estimation from incomplete observations improves as more signal
structure can be exploited in the inference process. Classic algorithms (e.g.,
Kalman filtering) have exploited strong dynamic structure for time-varying
signals while modern work has often focused on exploiting low-dimensional
signal structure (e.g., sparsity in a basis) for static signals. Few algorithms
attempt to merge both static and dynamic structure to improve estimation for
time-varying sparse signals (e.g., video). In this work we present a
re-weighted l_1 dynamic filtering scheme for causal signal estimation that
utilizes both sparsity assumptions and dynamic structure. Our algorithm
leverages work on hierarchical Laplacian scale mixture models to create a
dynamic probabilistic model. The resulting algorithm incorporates both dynamic
and sparsity priors in the estimation procedure in a robust and efficient
algorithm. We demonstrate the results in simulation using both synthetic and
natural data.
"
"  In this article, we derive concentration inequalities for the
cross-validation estimate of the generalization error for subagged estimators,
both for classification and regressor. General loss functions and class of
predictors with both finite and infinite VC-dimension are considered. We
slightly generalize the formalism introduced by \cite{DUD03} to cover a large
variety of cross-validation procedures including leave-one-out
cross-validation, $k$-fold cross-validation, hold-out cross-validation (or
split sample), and the leave-$\upsilon$-out cross-validation.
  \bigskip
  \noindent An interesting consequence is that the probability upper bound is
bounded by the minimum of a Hoeffding-type bound and a Vapnik-type bounds, and
thus is smaller than 1 even for small learning set. Finally, we give a simple
rule on how to subbag the predictor. \bigskip
"
"  Quantities with right-skewed distributions are ubiquitous in complex social
systems, including political conflict, economics and social networks, and these
systems sometimes produce extremely large events. For instance, the 9/11
terrorist events produced nearly 3000 fatalities, nearly six times more than
the next largest event. But, was this enormous loss of life statistically
unlikely given modern terrorism's historical record? Accurately estimating the
probability of such an event is complicated by the large fluctuations in the
empirical distribution's upper tail. We present a generic statistical algorithm
for making such estimates, which combines semi-parametric models of tail
behavior and a nonparametric bootstrap. Applied to a global database of
terrorist events, we estimate the worldwide historical probability of observing
at least one 9/11-sized or larger event since 1968 to be 11-35%. These results
are robust to conditioning on global variations in economic development,
domestic versus international events, the type of weapon used and a truncated
history that stops at 1998. We then use this procedure to make a data-driven
statistical forecast of at least one similar event over the next decade.
"
"  Mass spectrometry provides a high-throughput way to identify proteins in
biological samples. In a typical experiment, proteins in a sample are first
broken into their constituent peptides. The resulting mixture of peptides is
then subjected to mass spectrometry, which generates thousands of spectra, each
characteristic of its generating peptide. Here we consider the problem of
inferring, from these spectra, which proteins and peptides are present in the
sample. We develop a statistical approach to the problem, based on a nested
mixture model. In contrast to commonly used two-stage approaches, this model
provides a one-stage solution that simultaneously identifies which proteins are
present, and which peptides are correctly identified. In this way our model
incorporates the evidence feedback between proteins and their constituent
peptides. Using simulated data and a yeast data set, we compare and contrast
our method with existing widely used approaches (PeptideProphet/ProteinProphet)
and with a recently published new approach, HSM. For peptide identification,
our single-stage approach yields consistently more accurate results. For
protein identification the methods have similar accuracy in most settings,
although we exhibit some scenarios in which the existing methods perform
poorly.
"
"  This paper deals with the estimation of a sequence of frequencies from a
corresponding sequence of signals. This problem arises in fields such as
Doppler imaging where its specificity is twofold. First, only short noisy data
records are available (typically four sample long) and experimental constraints
may cause spectral aliasing so that measurements provide unreliable, ambiguous
information. Second, the frequency sequence is smooth. Here, this information
is accounted for by a Markov model and application of the Bayes rule yields the
a posteriori density. The maximum a postariori is computed by a combination of
Viterbi and descent procedures. One of the major features of the method is that
it is entirely unsupervised. Adjusting the hyperparameters that balance
data-based and prior-based information is done automatically by ML using an
EM-based gradient algorithm. We compared the proposed estimate to a reference
one and found that it performed better: variance was greatly reduced and
tracking was correct, even beyond the Nyquist frequency.
"
"  Finding sparse solutions of underdetermined systems of linear equations is a
fundamental problem in signal processing and statistics which has become a
subject of interest in recent years. In general, these systems have infinitely
many solutions. However, it may be shown that sufficiently sparse solutions may
be identified uniquely. In other words, the corresponding linear transformation
will be invertible if we restrict its domain to sufficiently sparse vectors.
This property may be used, for example, to solve the underdetermined Blind
Source Separation (BSS) problem, or to find sparse representation of a signal
in an `overcomplete' dictionary of primitive elements (i.e., the so-called
atomic decomposition). The main drawback of current methods of finding sparse
solutions is their computational complexity. In this paper, we will show that
by detecting `active' components of the (potential) solution, i.e., those
components having a considerable value, a framework for fast solution of the
problem may be devised. The idea leads to a family of algorithms, called
`Iterative Detection-Estimation (IDE)', which converge to the solution by
successive detection and estimation of its active part. Comparing the
performance of IDE(s) with one of the most successful method to date, which is
based on Linear Programming (LP), an improvement in speed of about two to three
orders of magnitude is observed.
"
"  Quality of microarray gene expression data has emerged as a new research
topic. As in other areas, microarray quality is assessed by comparing suitable
numerical summaries across microarrays, so that outliers and trends can be
visualized, and poor quality arrays or variable quality sets of arrays can be
identified. Since each single array comprises tens or hundreds of thousands of
measurements, the challenge is to find numerical summaries which can be used to
make accurate quality calls. To this end, several new quality measures are
introduced based on probe level and probeset level information, all obtained as
a by-product of the low-level analysis algorithms RMA/fitPLM for Affymetrix
GeneChips. Quality landscapes spatially localize chip or hybridization
problems. Numerical chip quality measures are derived from the distributions of
Normalized Unscaled Standard Errors and of Relative Log Expressions. Quality of
chip batches is assessed by Residual Scale Factors. These quality assessment
measures are demonstrated on a variety of datasets (spike-in experiments, small
lab experiments, multi-site studies). They are compared with Affymetrix's
individual chip quality report.
"
"  Methods for choosing a fixed set of knot locations in additive spline models
are fairly well established in the statistical literature. While most of these
methods are in principle directly extendable to non-additive surface models,
they are less likely to be successful in that setting because of the curse of
dimensionality, especially when there are more than a couple of covariates. We
propose a regression model for a multivariate Gaussian response that combines
both additive splines and interactive splines, and a highly efficient MCMC
algorithm that updates all the knot locations jointly. We use shrinkage priors
to avoid overfitting with different estimated shrinkage factors for the
additive and surface part of the model, and also different shrinkage parameters
for the different response variables. This makes it possible for the model to
adapt to varying degrees of nonlinearity in different parts of the data in a
parsimonious way. Simulated data and an application to firm leverage data show
that the approach is computationally efficient, and that allowing for freely
estimated knot locations can offer a substantial improvement in out-of-sample
predictive performance.
"
"  We propose a general algorithm for approximating nonstandard Bayesian
posterior distributions. The algorithm minimizes the Kullback-Leibler
divergence of an approximating distribution to the intractable posterior
distribution. Our method can be used to approximate any posterior distribution,
provided that it is given in closed form up to the proportionality constant.
The approximation can be any distribution in the exponential family or any
mixture of such distributions, which means that it can be made arbitrarily
precise. Several examples illustrate the speed and accuracy of our
approximation method in practice.
"
"  In query learning, the goal is to identify an unknown object while minimizing
the number of ""yes or no"" questions (queries) posed about that object. We
consider three extensions of this fundamental problem that are motivated by
practical considerations in real-world, time-critical identification tasks such
as emergency response. First, we consider the problem where the objects are
partitioned into groups, and the goal is to identify only the group to which
the object belongs. Second, we address the situation where the queries are
partitioned into groups, and an algorithm may suggest a group of queries to a
human user, who then selects the actual query. Third, we consider the problem
of query learning in the presence of persistent query noise, and relate it to
group identification. To address these problems we show that a standard
algorithm for query learning, known as the splitting algorithm or generalized
binary search, may be viewed as a generalization of Shannon-Fano coding. We
then extend this result to the group-based settings, leading to new algorithms.
The performance of our algorithms is demonstrated on simulated data and on a
database used by first responders for toxic chemical identification.
"
"  In immunological studies, the characterization of small, functionally
distinct cell subsets from blood and tissue is crucial to decipher system level
biological changes. An increasing number of studies rely on assays that provide
single-cell measurements of multiple genes and proteins from bulk cell samples.
A common problem in the analysis of such data is to identify biomarkers (or
combinations of thereof) that are differentially expressed between two
biological conditions (e.g., before/after vaccination), where expression is
defined as the proportion of cells expressing the biomarker or combination in
the cell subset of interest.
  Here, we present a Bayesian hierarchical framework based on a beta-binomial
mixture model for testing for differential biomarker expression using
single-cell assays. Our model allows inference to be subject specific, as is
typically required when accessing vaccine responses, while borrowing strength
across subjects through common prior distributions. We propose two approaches
for parameter estimation: an empirical-Bayes approach using an
Expectation-Maximization algorithm and a fully Bayesian one based on a Markov
chain Monte Carlo algorithm. We compare our method against frequentist
approaches for single-cell assays including Fisher's exact test, a likelihood
ratio test, and basic log-fold changes. Using several experimental assays
measuring proteins or genes at the single-cell level and simulated data, we
show that our method has higher sensitivity and specificity than alternative
methods. Additional simulations show that our framework is also robust to model
misspecification. Finally, we also demonstrate how our approach can be extended
to testing multivariate differential expression across multiple biomarker
combinations using a Dirichlet-multinomial model and illustrate this
multivariate approach using single-cell gene expression data and simulations.
"
"  We consider the generic regularized optimization problem
$\hat{\mathsf{\beta}}(\lambda)=\arg
\min_{\beta}L({\sf{y}},X{\sf{\beta}})+\lambda J({\sf{\beta}})$. Efron, Hastie,
Johnstone and Tibshirani [Ann. Statist. 32 (2004) 407--499] have shown that for
the LASSO--that is, if $L$ is squared error loss and $J(\beta)=\|\beta\|_1$ is
the $\ell_1$ norm of $\beta$--the optimal coefficient path is piecewise linear,
that is, $\partial \hat{\beta}(\lambda)/\partial \lambda$ is piecewise
constant. We derive a general characterization of the properties of (loss $L$,
penalty $J$) pairs which give piecewise linear coefficient paths. Such pairs
allow for efficient generation of the full regularized coefficient paths. We
investigate the nature of efficient path following algorithms which arise. We
use our results to suggest robust versions of the LASSO for regression and
classification, and to develop new, efficient algorithms for existing problems
in the literature, including Mammen and van de Geer's locally adaptive
regression splines.
"
"  Most machine learning algorithms, such as classification or regression, treat
the individual data point as the object of interest. Here we consider extending
machine learning algorithms to operate on groups of data points. We suggest
treating a group of data points as an i.i.d. sample set from an underlying
feature distribution for that group. Our approach employs kernel machines with
a kernel on i.i.d. sample sets of vectors. We define certain kernel functions
on pairs of distributions, and then use a nonparametric estimator to
consistently estimate those functions based on sample sets. The projection of
the estimated Gram matrix to the cone of symmetric positive semi-definite
matrices enables us to use kernel machines for classification, regression,
anomaly detection, and low-dimensional embedding in the space of distributions.
We present several numerical experiments both on real and simulated datasets to
demonstrate the advantages of our new approach.
"
"  This study proposes a nonhomogeneous birth--death model which captures the
dynamics of a directly transmitted infectious disease. Our model accounts for
an important aspect of observed epidemic data in which only symptomatic
infecteds are observed. The nonhomogeneous birth--death process depends on
survival distributions of reproduction and removal, which jointly yield an
estimate of the effective reproduction number $R(t)$ as a function of epidemic
time. We employ the Burr distribution family for the survival functions and, as
special cases, proportional rate and accelerated event-time models are also
employed for the parameter estimation procedure. As an example, our model is
applied to an outbreak of avian influenza (H7N7) in the Netherlands, 2003,
confirming that the conditional estimate of $R(t)$ declined below unity for the
first time on day 23 since the detection of the index case.
"
"  Supervised statistical classification is a vital tool for satellite image
processing. It is useful not only when a discrete result, such as feature
extraction or surface type, is required, but also for continuum retrievals by
dividing the quantity of interest into discrete ranges. Because of the high
resolution of modern satellite instruments and because of the requirement for
real-time processing, any algorithm has to be fast to be useful. Here we
describe an algorithm based on kernel estimation called Adaptive Gaussian
Filtering that incorporates several innovations to produce superior efficiency
as compared to three other popular methods: k-nearest-neighbour (KNN), Learning
Vector Quantization (LVQ) and Support Vector Machines (SVM). This efficiency is
gained with no compromises: accuracy is maintained, while estimates of the
conditional probabilities are returned. These are useful not only to gauge the
accuracy of an estimate in the absence of its true value, but also to
re-calibrate a retrieved image and as a proxy for a discretized continuum
variable. The algorithm is demonstrated and compared with the other three on a
pair of synthetic test classes and to map the waterways of the Netherlands.
Software may be found at: http://libagf.sourceforge.net.
"
"  We investigate the learning rate of multiple kernel learning (MKL) with
$\ell_1$ and elastic-net regularizations. The elastic-net regularization is a
composition of an $\ell_1$-regularizer for inducing the sparsity and an
$\ell_2$-regularizer for controlling the smoothness. We focus on a sparse
setting where the total number of kernels is large, but the number of nonzero
components of the ground truth is relatively small, and show sharper
convergence rates than the learning rates have ever shown for both $\ell_1$ and
elastic-net regularizations. Our analysis reveals some relations between the
choice of a regularization function and the performance. If the ground truth is
smooth, we show a faster convergence rate for the elastic-net regularization
with less conditions than $\ell_1$-regularization; otherwise, a faster
convergence rate for the $\ell_1$-regularization is shown.
"
"  We present a new approach to learning the structure and parameters of a
Bayesian network based on regularized estimation in an exponential family
representation. Here we show that, given a fixed variable order, the optimal
structure and parameters can be learned efficiently, even without restricting
the size of the parent sets. We then consider the problem of optimizing the
variable order for a given set of features. This is still a computationally
hard problem, but we present a convex relaxation that yields an optimal 'soft'
ordering in polynomial time. One novel aspect of the approach is that we do not
perform a discrete search over DAG structures, nor over variable orders, but
instead solve a continuous relaxation that can then be rounded to obtain a
valid network structure. We conduct an experimental comparison against standard
structure search procedures over standard objectives, which cope with local
minima, and evaluate the advantages of using convex relaxations that reduce the
effects of local minima.
"
"  Random forests are a scheme proposed by Leo Breiman in the 2000's for
building a predictor ensemble with a set of decision trees that grow in
randomly selected subspaces of data. Despite growing interest and practical
use, there has been little exploration of the statistical properties of random
forests, and little is known about the mathematical forces driving the
algorithm. In this paper, we offer an in-depth analysis of a random forests
model suggested by Breiman in \cite{Bre04}, which is very close to the original
algorithm. We show in particular that the procedure is consistent and adapts to
sparsity, in the sense that its rate of convergence depends only on the number
of strong features and not on how many noise variables are present.
"
"  In this report, we derive a non-negative series expansion for the
Jensen-Shannon divergence (JSD) between two probability distributions. This
series expansion is shown to be useful for numerical calculations of the JSD,
when the probability distributions are nearly equal, and for which,
consequently, small numerical errors dominate evaluation.
"
"  Permutation $p$-values have been widely used to assess the significance of
linkage or association in genetic studies. However, the application in
large-scale studies is hindered by a heavy computational burden. We propose a
geometric interpretation of permutation $p$-values, and based on this geometric
interpretation, we develop an efficient permutation $p$-value estimation method
in the context of regression with binary predictors. An application to a study
of gene expression quantitative trait loci (eQTL) shows that our method
provides reliable estimates of permutation $p$-values while requiring less than
5% of the computational time compared with direct permutations. In fact, our
method takes a constant time to estimate permutation $p$-values, no matter how
small the $p$-value. Our method enables a study of the relationship between
nominal $p$-values and permutation $p$-values in a wide range, and provides a
geometric perspective on the effective number of independent tests.
"
"  Information theory provides principled ways to analyze different inference
and learning problems such as hypothesis testing, clustering, dimensionality
reduction, classification, among others. However, the use of information
theoretic quantities as test statistics, that is, as quantities obtained from
empirical data, poses a challenging estimation problem that often leads to
strong simplifications such as Gaussian models, or the use of plug in density
estimators that are restricted to certain representation of the data. In this
paper, a framework to non-parametrically obtain measures of entropy directly
from data using operators in reproducing kernel Hilbert spaces defined by
infinitely divisible kernels is presented. The entropy functionals, which bear
resemblance with quantum entropies, are defined on positive definite matrices
and satisfy similar axioms to those of Renyi's definition of entropy.
Convergence of the proposed estimators follows from concentration results on
the difference between the ordered spectrum of the Gram matrices and the
integral operators associated to the population quantities. In this way,
capitalizing on both the axiomatic definition of entropy and on the
representation power of positive definite kernels, the proposed measure of
entropy avoids the estimation of the probability distribution underlying the
data. Moreover, estimators of kernel-based conditional entropy and mutual
information are also defined. Numerical experiments on independence tests
compare favourably with state of the art.
"
"  The original formulation of BEAMS - Bayesian Estimation Applied to Multiple
Species - showed how to use a dataset contaminated by points of multiple
underlying types to perform unbiased parameter estimation. An example is
cosmological parameter estimation from a photometric supernova sample
contaminated by unknown Type Ibc and II supernovae. Where other methods require
data cuts to increase purity, BEAMS uses all of the data points in conjunction
with their probabilities of being each type. Here we extend the BEAMS formalism
to allow for correlations between the data and the type probabilities of the
objects as can occur in realistic cases. We show with simple simulations that
this extension can be crucial, providing a 50% reduction in parameter
estimation variance when such correlations do exist. We then go on to perform
tests to quantify the importance of the type probabilities, one of which
illustrates the effect of biasing the probabilities in various ways. Finally, a
general presentation of the selection bias problem is given, and discussed in
the context of future photometric supernova surveys and BEAMS, which lead to
specific recommendations for future supernova surveys.
"
"  We introduce a Bernstein-type inequality which serves to uniformly control
quadratic forms of gaussian variables. The latter can for example be used to
derive sharp model selection criteria for linear estimation in linear
regression and linear inverse problems via penalization, and we do not exclude
that its scope of application can be made even broader.
"
"  To understand the structural dynamics of a large-scale social, biological or
technological network, it may be useful to discover behavioral roles
representing the main connectivity patterns present over time. In this paper,
we propose a scalable non-parametric approach to automatically learn the
structural dynamics of the network and individual nodes. Roles may represent
structural or behavioral patterns such as the center of a star, peripheral
nodes, or bridge nodes that connect different communities. Our novel approach
learns the appropriate structural role dynamics for any arbitrary network and
tracks the changes over time. In particular, we uncover the specific global
network dynamics and the local node dynamics of a technological, communication,
and social network. We identify interesting node and network patterns such as
stationary and non-stationary roles, spikes/steps in role-memberships (perhaps
indicating anomalies), increasing/decreasing role trends, among many others.
Our results indicate that the nodes in each of these networks have distinct
connectivity patterns that are non-stationary and evolve considerably over
time. Overall, the experiments demonstrate the effectiveness of our approach
for fast mining and tracking of the dynamics in large networks. Furthermore,
the dynamic structural representation provides a basis for building more
sophisticated models and tools that are fast for exploring large dynamic
networks.
"
"  We present a regularized logistic regression model for evaluating player
contributions in hockey. The traditional metric for this purpose is the
plus-minus statistic, which allocates a single unit of credit (for or against)
to each player on the ice for a goal. However, plus-minus scores measure only
the marginal effect of players, do not account for sample size, and provide a
very noisy estimate of performance. We investigate a related regression
problem: what does each player on the ice contribute, beyond aggregate team
performance and other factors, to the odds that a given goal was scored by
their team? Due to the large-p (number of players) and imbalanced design
setting of hockey analysis, a major part of our contribution is a careful
treatment of prior shrinkage in model estimation. We showcase two recently
developed techniques -- for posterior maximization or simulation -- that make
such analysis feasible. Each approach is accompanied with publicly available
software and we include the simple commands used in our analysis. Our results
show that most players do not stand out as measurably strong (positive or
negative) contributors. This allows the stars to really shine, reveals diamonds
in the rough overlooked by earlier analyses, and argues that some of the
highest paid players in the league are not making contributions worth their
expense.
"
"  By developing data augmentation methods unique to the negative binomial (NB)
distribution, we unite seemingly disjoint count and mixture models under the NB
process framework. We develop fundamental properties of the models and derive
efficient Gibbs sampling inference. We show that the gamma-NB process can be
reduced to the hierarchical Dirichlet process with normalization, highlighting
its unique theoretical, structural and computational advantages. A variety of
NB processes with distinct sharing mechanisms are constructed and applied to
topic modeling, with connections to existing algorithms, showing the importance
of inferring both the NB dispersion and probability parameters.
"
"  Analogical reasoning depends fundamentally on the ability to learn and
generalize about relations between objects. We develop an approach to
relational learning which, given a set of pairs of objects
$\mathbf{S}=\{A^{(1)}:B^{(1)},A^{(2)}:B^{(2)},\ldots,A^{(N)}:B ^{(N)}\}$,
measures how well other pairs A:B fit in with the set $\mathbf{S}$. Our work
addresses the following question: is the relation between objects A and B
analogous to those relations found in $\mathbf{S}$? Such questions are
particularly relevant in information retrieval, where an investigator might
want to search for analogous pairs of objects that match the query set of
interest. There are many ways in which objects can be related, making the task
of measuring analogies very challenging. Our approach combines a similarity
measure on function spaces with Bayesian analysis to produce a ranking. It
requires data containing features of the objects of interest and a link matrix
specifying which relationships exist; no further attributes of such
relationships are necessary. We illustrate the potential of our method on text
analysis and information networks. An application on discovering functional
interactions between pairs of proteins is discussed in detail, where we show
that our approach can work in practice even if a small set of protein pairs is
provided.
"
"  Ecological studies involving counts of abundance, presence-absence or
occupancy rates often produce data having a substantial proportion of zeros.
Furthermore, these types of processes are typically multivariate and only
adequately described by complex nonlinear relationships involving externally
measured covariates. Ignoring these aspects of the data and implementing
standard approaches can lead to models that fail to provide adequate scientific
understanding of the underlying ecological processes, possibly resulting in a
loss of inferential power. One method of dealing with data having excess zeros
is to consider the class of univariate zero-inflated generalized linear models.
However, this class of models fails to address the multivariate and nonlinear
aspects associated with the data usually encountered in practice. Therefore, we
propose a semiparametric bivariate zero-inflated Poisson model that takes into
account both of these data attributes. The general modeling framework is
hierarchical Bayes and is suitable for a broad range of applications. We
demonstrate the effectiveness of our model through a motivating example on
modeling catch per unit area for multiple species using data from the Missouri
River benthic fish study, implemented by the United States Geological Survey.
"
"  The Ward error sum of squares hierarchical clustering method has been very
widely used since its first description by Ward in a 1963 publication. It has
also been generalized in various ways. However there are different
interpretations in the literature and there are different implementations of
the Ward agglomerative algorithm in commonly used software systems, including
differing expressions of the agglomerative criterion. Our survey work and case
studies will be useful for all those involved in developing software for data
analysis using Ward's hierarchical clustering method.
"
"  By providing new insights into the distribution of a protein's torsion
angles, recent statistical models for this data have pointed the way to more
efficient methods for protein structure prediction. Most current approaches
have concentrated on bivariate models at a single sequence position. There is,
however, considerable value in simultaneously modeling angle pairs at multiple
sequence positions in a protein. One area of application for such models is in
structure prediction for the highly variable loop and turn regions. Such
modeling is difficult due to the fact that the number of known protein
structures available to estimate these torsion angle distributions is typically
small. Furthermore, the data is ""sparse"" in that not all proteins have angle
pairs at each sequence position. We propose a new semiparametric model for the
joint distributions of angle pairs at multiple sequence positions. Our model
accommodates sparse data by leveraging known information about the behavior of
protein secondary structure. We demonstrate our technique by predicting the
torsion angles in a loop from the globin fold family. Our results show that a
template-based approach can now be successfully extended to modeling the
notoriously difficult loop and turn regions.
"
"  In this paper we propose a Bayesian nonparametric model for clustering
partial ranking data. We start by developing a Bayesian nonparametric extension
of the popular Plackett-Luce choice model that can handle an infinite number of
choice items. Our framework is based on the theory of random atomic measures,
with the prior specified by a completely random measure. We characterise the
posterior distribution given data, and derive a simple and effective Gibbs
sampler for posterior simulation. We then develop a Dirichlet process mixture
extension of our model and apply it to investigate the clustering of
preferences for college degree programmes amongst Irish secondary school
graduates. The existence of clusters of applicants who have similar preferences
for degree programmes is established and we determine that subject matter and
geographical location of the third level institution characterise these
clusters.
"
"  Targeting at sparse learning, we construct Banach spaces B of functions on an
input space X with the properties that (1) B possesses an l1 norm in the sense
that it is isometrically isomorphic to the Banach space of integrable functions
on X with respect to the counting measure; (2) point evaluations are continuous
linear functionals on B and are representable through a bilinear form with a
kernel function; (3) regularized learning schemes on B satisfy the linear
representer theorem. Examples of kernel functions admissible for the
construction of such spaces are given.
"
"  Interactive applications incorporating high-data rate sensing and computer
vision are becoming possible due to novel runtime systems and the use of
parallel computation resources. To allow interactive use, such applications
require careful tuning of multiple application parameters to meet required
fidelity and latency bounds. This is a nontrivial task, often requiring expert
knowledge, which becomes intractable as resources and application load
characteristics change. This paper describes a method for automatic performance
tuning that learns application characteristics and effects of tunable
parameters online, and constructs models that are used to maximize fidelity for
a given latency constraint. The paper shows that accurate latency models can be
learned online, knowledge of application structure can be used to reduce the
complexity of the learning task, and operating points can be found that achieve
90% of the optimal fidelity by exploring the parameter space only 3% of the
time.
"
"  A procedure for unfolding the true distribution from experimental data is
presented. Machine learning methods are applied for simultaneous identification
of an apparatus function and solving of an inverse problem. A priori
information about the true distribution from theory or previous experiments is
used for Monte-Carlo simulation of the training sample. The training sample can
be used to calculate a transformation from the true distribution to the
measured one. This transformation provides a robust solution for an unfolding
problem with minimal biases and statistical errors for the set of distributions
used to create the training sample. The dimensionality of the solved problem
can be arbitrary. A numerical example is presented to illustrate and validate
the procedure.
"
"  Complex computer codes are often too time expensive to be directly used to
perform uncertainty propagation studies, global sensitivity analysis or to
solve optimization problems. A well known and widely used method to circumvent
this inconvenience consists in replacing the complex computer code by a reduced
model, called a metamodel, or a response surface that represents the computer
code and requires acceptable calculation time. One particular class of
metamodels is studied: the Gaussian process model that is characterized by its
mean and covariance functions. A specific estimation procedure is developed to
adjust a Gaussian process model in complex cases (non linear relations, highly
dispersed or discontinuous output, high dimensional input, inadequate sampling
designs, ...). The efficiency of this algorithm is compared to the efficiency
of other existing algorithms on an analytical test case. The proposed
methodology is also illustrated for the case of a complex hydrogeological
computer code, simulating radionuclide transport in groundwater.
"
"  We present an embedding of stochastic optimal control problems, of the so
called path integral form, into reproducing kernel Hilbert spaces. Using
consistent, sample based estimates of the embedding leads to a model free,
non-parametric approach for calculation of an approximate solution to the
control problem. This formulation admits a decomposition of the problem into an
invariant and task dependent component. Consequently, we make much more
efficient use of the sample data compared to previous sample based approaches
in this domain, e.g., by allowing sample re-use across tasks. Numerical
examples on test problems, which illustrate the sample efficiency, are
provided.
"
"  We consider the problem of estimating the support of a vector $\beta^* \in
\mathbb{R}^{p}$ based on observations contaminated by noise. A significant body
of work has studied behavior of $\ell_1$-relaxations when applied to
measurement matrices drawn from standard dense ensembles (e.g., Gaussian,
Bernoulli). In this paper, we analyze \emph{sparsified} measurement ensembles,
and consider the trade-off between measurement sparsity, as measured by the
fraction $\gamma$ of non-zero entries, and the statistical efficiency, as
measured by the minimal number of observations $n$ required for exact support
recovery with probability converging to one. Our main result is to prove that
it is possible to let $\gamma \to 0$ at some rate, yielding measurement
matrices with a vanishing fraction of non-zeros per row while retaining the
same statistical efficiency as dense ensembles. A variety of simulation results
confirm the sharpness of our theoretical predictions.
"
"  The estimation of cosmological parameters from precision observables is an
important industry with crucial ramifications for particle physics. This
article discusses the statistical methods presently used in cosmological data
analysis, highlighting the main assumptions and uncertainties. The topics
covered are parameter estimation, model selection, multi-model inference, and
experimental design, all primarily from a Bayesian perspective.
"
"  Principal component analysis (PCA) is a widely used technique for data
analysis and dimension reduction with numerous applications in science and
engineering. However, the standard PCA suffers from the fact that the principal
components (PCs) are usually linear combinations of all the original variables,
and it is thus often difficult to interpret the PCs. To alleviate this
drawback, various sparse PCA approaches were proposed in literature [15, 6, 17,
28, 8, 25, 18, 7, 16]. Despite success in achieving sparsity, some important
properties enjoyed by the standard PCA are lost in these methods such as
uncorrelation of PCs and orthogonality of loading vectors. Also, the total
explained variance that they attempt to maximize can be too optimistic. In this
paper we propose a new formulation for sparse PCA, aiming at finding sparse and
nearly uncorrelated PCs with orthogonal loading vectors while explaining as
much of the total variance as possible. We also develop a novel augmented
Lagrangian method for solving a class of nonsmooth constrained optimization
problems, which is well suited for our formulation of sparse PCA. We show that
it converges to a feasible point, and moreover under some regularity
assumptions, it converges to a stationary point. Additionally, we propose two
nonmonotone gradient methods for solving the augmented Lagrangian subproblems,
and establish their global and local convergence. Finally, we compare our
sparse PCA approach with several existing methods on synthetic, random, and
real data, respectively. The computational results demonstrate that the sparse
PCs produced by our approach substantially outperform those by other methods in
terms of total explained variance, correlation of PCs, and orthogonality of
loading vectors.
"
"  The graphical lasso (glasso) is a widely-used fast algorithm for estimating
sparse inverse covariance matrices. The glasso solves an L1 penalized maximum
likelihood problem and is available as an R library on CRAN. The output from
the glasso, a regularized covariance matrix estimate a sparse inverse
covariance matrix estimate, not only identify a graphical model but can also
serve as intermediate inputs into multivariate procedures such as PCA, LDA,
MANOVA, and others. The glasso indeed produces a covariance matrix estimate
which solves the L1 penalized optimization problem in a dual sense; however,
the method for producing the inverse covariance matrix estimator after this
optimization is inexact and may produce asymmetric estimates. This problem is
exacerbated when the amount of L1 regularization that is applied is small,
which in turn is more likely to occur if the true underlying inverse covariance
matrix is not sparse. The lack of symmetry can potentially have consequences.
First, it implies that the covariance and inverse covariance estimates are not
numerical inverses of one another, and second, asymmetry can possibly lead to
negative or complex eigenvalues,rendering many multivariate procedures which
may depend on the inverse covariance estimator unusable. We demonstrate this
problem, explain its causes, and propose possible remedies.
"
"  Statistical emulators of computer simulators have proven to be useful in a
variety of applications. The widely adopted model for emulator building, using
a Gaussian process model with strictly positive correlation function, is
computationally intractable when the number of simulator evaluations is large.
We propose a new model that uses a combination of low-order regression terms
and compactly supported correlation functions to recreate the desired
predictive behavior of the emulator at a fraction of the computational cost.
Following the usual approach of taking the correlation to be a product of
correlations in each input dimension, we show how to impose restrictions on the
ranges of the correlations, giving sparsity, while also allowing the ranges to
trade off against one another, thereby giving good predictive performance. We
illustrate the method using data from a computer simulator of photometric
redshift with 20,000 simulator evaluations and 80,000 predictions.
"
"  We introduce a general framework for measuring risk in the context of Markov
control processes with risk maps on general Borel spaces that generalize known
concepts of risk measures in mathematical finance, operations research and
behavioral economics. Within the framework, applying weighted norm spaces to
incorporate also unbounded costs, we study two types of infinite-horizon
risk-sensitive criteria, discounted total risk and average risk, and solve the
associated optimization problems by dynamic programming. For the discounted
case, we propose a new discount scheme, which is different from the
conventional form but consistent with the existing literature, while for the
average risk criterion, we state Lyapunov-like stability conditions that
generalize known conditions for Markov chains to ensure the existence of
solutions to the optimality equation.
"
"  The mathematical interpretation of L0, L1 and L2 is needed to understand how
we should use these norms for optimization problems. The L0 norm is
combinatorics which is counting certain properties of an object or an operator.
This is the least amplitude dependent norm since it is counted regardless of
the magnitude. The L1 norm could be interpreted as minimal geometric
description. It is somewhat sensitive to amplitude information. In geophysics,
it has been used to edit outliers like spikes in seismic data. This is a good
application of L1 norm. The L2 norm could be interpreted as the numerically
simplest solution to fitting data with a differential equation. It is very
sensitive to amplitude information. Previous application includes least square
migration. In this paper, we will show how to combine the usage of L0 and L1
and L2. We will not be optimizing the 3 norms simultaneously but will go from
one norm to the next norm to optimize the data before the final migration.
"
"  This paper suggests a nonlinear mixed effects model for data points in
$\mathit{SO}(3)$, the set of $3\times3$ rotation matrices, collected according
to a repeated measure design. Each sample individual contributes a sequence of
rotation matrices giving the relative orientations of the right foot with
respect to the right lower leg as its ankle moves. The random effects are the
five angles characterizing the orientation of the two rotation axes of a
subject's right ankle. The fixed parameters are the average value of these
angles and their variances within the population. The algorithms to fit
nonlinear mixed effects models presented in Pinheiro and Bates (2000) are
adapted to the new directional model. The estimation of the random effects are
of interest since they give predictions of the rotation axes of an individual
ankle. The performance of these algorithms is investigated in a Monte Carlo
study. The analysis of two data sets is presented. In the biomechanical
literature, there is no consensus on an in vivo method to estimate the two
rotation axes of the ankle. The new model is promising. The estimates obtained
from a sample of volunteers are shown to be in agreement with the clinically
accepted results of Inman (1976), obtained by manipulating cadavers. The
repeated measure directional model presented in this paper is developed for a
particular application. The approach is, however, general and might be applied
to other models provided that the random directional effects are clustered
around their mean values.
"
"  Motivated by genome-wide association studies, we consider a standard linear
model with one additional random effect in situations where many predictors
have been collected on the same subjects and each predictor is analyzed
separately. Three novel contributions are (1) a transformation between the
linear and log-odds scales which is accurate for the important genetic case of
small effect sizes; (2) a likelihood-maximization algorithm that is an order of
magnitude faster than the previously published approaches; and (3) efficient
methods for computing marginal likelihoods which allow Bayesian model
comparison. The methodology has been successfully applied to a large-scale
association study of multiple sclerosis including over 20,000 individuals and
500,000 genetic variants.
"
"  This paper studies non-asymptotic model selection for the general case of
arbitrary design matrices and arbitrary nonzero entries of the signal. In this
regard, it generalizes the notion of incoherence in the existing literature on
model selection and introduces two fundamental measures of coherence---termed
as the worst-case coherence and the average coherence---among the columns of a
design matrix. It utilizes these two measures of coherence to provide an
in-depth analysis of a simple, model-order agnostic one-step thresholding (OST)
algorithm for model selection and proves that OST is feasible for exact as well
as partial model selection as long as the design matrix obeys an easily
verifiable property. One of the key insights offered by the ensuing analysis in
this regard is that OST can successfully carry out model selection even when
methods based on convex optimization such as the lasso fail due to the rank
deficiency of the submatrices of the design matrix. In addition, the paper
establishes that if the design matrix has reasonably small worst-case and
average coherence then OST performs near-optimally when either (i) the energy
of any nonzero entry of the signal is close to the average signal energy per
nonzero entry or (ii) the signal-to-noise ratio in the measurement system is
not too high. Finally, two other key contributions of the paper are that (i) it
provides bounds on the average coherence of Gaussian matrices and Gabor frames,
and (ii) it extends the results on model selection using OST to low-complexity,
model-order agnostic recovery of sparse signals with arbitrary nonzero entries.
"
"  To recover a sparse signal from an underdetermined system, we often solve a
constrained L1-norm minimization problem. In many cases, the signal sparsity
and the recovery performance can be further improved by replacing the L1 norm
with a ""weighted"" L1 norm. Without any prior information about nonzero elements
of the signal, the procedure for selecting weights is iterative in nature.
Common approaches update the weights at every iteration using the solution of a
weighted L1 problem from the previous iteration.
  In this paper, we present two homotopy-based algorithms that efficiently
solve reweighted L1 problems. First, we present an algorithm that quickly
updates the solution of a weighted L1 problem as the weights change. Since the
solution changes only slightly with small changes in the weights, we develop a
homotopy algorithm that replaces the old weights with the new ones in a small
number of computationally inexpensive steps. Second, we propose an algorithm
that solves a weighted L1 problem by adaptively selecting the weights while
estimating the signal. This algorithm integrates the reweighting into every
step along the homotopy path by changing the weights according to the changes
in the solution and its support, allowing us to achieve a high quality signal
reconstruction by solving a single homotopy problem. We compare the performance
of both algorithms, in terms of reconstruction accuracy and computational
complexity, against state-of-the-art solvers and show that our methods have
smaller computational cost. In addition, we will show that the adaptive
selection of the weights inside the homotopy often yields reconstructions of
higher quality.
"
"  We propose a general information-theoretic approach called Seraph
(SEmi-supervised metRic leArning Paradigm with Hyper-sparsity) for metric
learning that does not rely upon the manifold assumption. Given the probability
parameterized by a Mahalanobis distance, we maximize the entropy of that
probability on labeled data and minimize it on unlabeled data following entropy
regularization, which allows the supervised and unsupervised parts to be
integrated in a natural and meaningful way. Furthermore, Seraph is regularized
by encouraging a low-rank projection induced from the metric. The optimization
of Seraph is solved efficiently and stably by an EM-like scheme with the
analytical E-Step and convex M-Step. Experiments demonstrate that Seraph
compares favorably with many well-known global and local metric learning
methods.
"
"  Using sparse-inducing norms to learn robust models has received increasing
attention from many fields for its attractive properties. Projection-based
methods have been widely applied to learning tasks constrained by such norms.
As a key building block of these methods, an efficient operator for Euclidean
projection onto the intersection of $\ell_1$ and $\ell_{1,q}$ norm balls
$(q=2\text{or}\infty)$ is proposed in this paper. We prove that the projection
can be reduced to finding the root of an auxiliary function which is piecewise
smooth and monotonic. Hence, a bisection algorithm is sufficient to solve the
problem. We show that the time complexity of our solution is $O(n+g\log g)$ for
$q=2$ and $O(n\log n)$ for $q=\infty$, where $n$ is the dimensionality of the
vector to be projected and $g$ is the number of disjoint groups; we confirm
this complexity by experimentation. Empirical study reveals that our method
achieves significantly better performance than classical methods in terms of
running time and memory usage. We further show that embedded with our efficient
projection operator, projection-based algorithms can solve regression problems
with composite norm constraints more efficiently than other methods and give
superior accuracy.
"
"  Singular Value Decomposition (SVD) has been used successfully in recent years
in the area of recommender systems. In this paper we present how this model can
be extended to consider both user ratings and information from Wikipedia. By
mapping items to Wikipedia pages and quantifying their similarity, we are able
to use this information in order to improve recommendation accuracy, especially
when the sparsity is high. Another advantage of the proposed approach is the
fact that it can be easily integrated into any other SVD implementation,
regardless of additional parameters that may have been added to it. Preliminary
experimental results on the MovieLens dataset are encouraging.
"
"  We describe a new optimization scheme for finding high-quality correlation
clusterings in planar graphs that uses weighted perfect matching as a
subroutine. Our method provides lower-bounds on the energy of the optimal
correlation clustering that are typically fast to compute and tight in
practice. We demonstrate our algorithm on the problem of image segmentation
where this approach outperforms existing global optimization techniques in
minimizing the objective and is competitive with the state of the art in
producing high-quality segmentations.
"
"  Additive isotonic regression attempts to determine the relationship between a
multi-dimensional observation variable and a response, under the constraint
that the estimate is the additive sum of univariate component effects that are
monotonically increasing. In this article, we present a new method for such
regression called LASSO Isotone (LISO). LISO adapts ideas from sparse linear
modelling to additive isotonic regression. Thus, it is viable in many
situations with high dimensional predictor variables, where selection of
significant versus insignificant variables are required. We suggest an
algorithm involving a modification of the backfitting algorithm CPAV. We give a
numerical convergence result, and finally examine some of its properties
through simulations. We also suggest some possible extensions that improve
performance, and allow calculation to be carried out when the direction of the
monotonicity is unknown.
"
"  Analyses of serially-sampled data often begin with the assumption that the
observations represent discrete samples from a latent continuous-time
stochastic process. The continuous-time Markov chain (CTMC) is one such
generative model whose popularity extends to a variety of disciplines ranging
from computational finance to human genetics and genomics. A common theme among
these diverse applications is the need to simulate sample paths of a CTMC
conditional on realized data that is discretely observed. Here we present a
general solution to this sampling problem when the CTMC is defined on a
discrete and finite state space. Specifically, we consider the generation of
sample paths, including intermediate states and times of transition, from a
CTMC whose beginning and ending states are known across a time interval of
length $T$. We first unify the literature through a discussion of the three
predominant approaches: (1) modified rejection sampling, (2) direct sampling,
and (3) uniformization. We then give analytical results for the complexity and
efficiency of each method in terms of the instantaneous transition rate matrix
$Q$ of the CTMC, its beginning and ending states, and the length of sampling
time $T$. In doing so, we show that no method dominates the others across all
model specifications, and we give explicit proof of which method prevails for
any given $Q,T,$ and endpoints. Finally, we introduce and compare three
applications of CTMCs to demonstrate the pitfalls of choosing an inefficient
sampler.
"
"  In order to find previously unknown subgroups in biomedical data and generate
testable hypotheses, visually guided exploratory analysis can be of tremendous
importance. In this paper we propose a new dissimilarity measure that can be
used within the Multidimensional Scaling framework to obtain a joint
low-dimensional representation of both the samples and variables of a
multivariate data set, thereby providing an alternative to conventional
biplots. In comparison with biplots, the representations obtained by our
approach are particularly useful for exploratory analysis of data sets where
there are small groups of variables sharing unusually high or low values for a
small group of samples.
"
"  A new generalized Statistical Complexity Measure (SCM) was proposed by Rosso
et al in 2010. It is a functional that captures the notions of order/disorder
and of distance to an equilibrium distribution. The former is computed by a
measure of entropy, while the latter depends on the definition of a stochastic
divergence. When the scene is illuminated by coherent radiation, image data is
corrupted by speckle noise, as is the case of ultrasound-B, sonar, laser and
Synthetic Aperture Radar (SAR) sensors. In the amplitude and intensity formats,
this noise is multiplicative and non-Gaussian requiring, thus, specialized
techniques for image processing and understanding. One of the most successful
family of models for describing these images is the Multiplicative Model which
leads, among other probability distributions, to the G0 law. This distribution
has been validated in the literature as an expressive and tractable model,
deserving the ""universal"" denomination for its ability to describe most types
of targets. In order to compute the statistical complexity of a site in an
image corrupted by speckle noise, we assume that the equilibrium distribution
is that of fully developed speckle, namely the Gamma law in intensity format,
which appears in areas with little or no texture. We use the Shannon entropy
along with the Hellinger distance to measure the statistical complexity of
intensity SAR images, and we show that it is an expressive feature capable of
identifying many types of targets.
"
"  We show that the variational representations for f-divergences currently used
in the literature can be tightened. This has implications to a number of
methods recently proposed based on this representation. As an example
application we use our tighter representation to derive a general f-divergence
estimator based on two i.i.d. samples and derive the dual program for this
estimator that performs well empirically. We also point out a connection
between our estimator and MMD.
"
"  Support vector machines (SVMs) are special kernel based methods and belong to
the most successful learning methods since more than a decade. SVMs can
informally be described as a kind of regularized M-estimators for functions and
have demonstrated their usefulness in many complicated real-life problems.
During the last years a great part of the statistical research on SVMs has
concentrated on the question how to design SVMs such that they are universally
consistent and statistically robust for nonparametric classification or
nonparametric regression purposes. In many applications, some qualitative prior
knowledge of the distribution P or of the unknown function f to be estimated is
present or the prediction function with a good interpretability is desired,
such that a semiparametric model or an additive model is of interest.
  In this paper we mainly address the question how to design SVMs by choosing
the reproducing kernel Hilbert space (RKHS) or its corresponding kernel to
obtain consistent and statistically robust estimators in additive models. We
give an explicit construction of kernels - and thus of their RKHSs - which
leads in combination with a Lipschitz continuous loss function to consistent
and statistically robust SMVs for additive models. Examples are quantile
regression based on the pinball loss function, regression based on the
epsilon-insensitive loss function, and classification based on the hinge loss
function.
"
"  In this paper, we establish convergence theorems for the Non-Local Means
Filter in removing the additive Gaussian noise. We employ the techniques of
""Oracle"" estimation to determine the order of the widths of the similarity
patches and search windows in the aforementioned filter. We propose a practical
choice of these parameters which improve the restoration quality of the filter
compared with the usual choice of parameters.
"
"  Echo state network (ESN) is viewed as a temporal non-orthogonal expansion
with pseudo-random parameters. Such expansions naturally give rise to
regressors of various relevance to a teacher output. We illustrate that often
only a certain amount of the generated echo-regressors effectively explain the
variance of the teacher output and also that sole local regularization is not
able to provide in-depth information concerning the importance of the generated
regressors. The importance is therefore determined by a joint calculation of
the individual variance contributions and Bayesian relevance using locally
regularized orthogonal forward regression (LROFR) algorithm. This information
can be advantageously used in a variety of ways for an in-depth analysis of an
ESN structure and its state-space parameters in relation to the unknown
dynamics of the underlying problem. We present locally regularized linear
readout built using LROFR. The readout may have a different dimensionality than
an ESN model itself, and besides improving robustness and accuracy of an ESN it
relates the echo-regressors to different features of the training data and may
determine what type of an additional readout is suitable for a task at hand.
Moreover, as flexibility of the linear readout has limitations and might
sometimes be insufficient for certain tasks, we also present a radial basis
function (RBF) readout built using LROFR. It is a flexible and parsimonious
readout with excellent generalization abilities and is a viable alternative to
readouts based on a feed-forward neural network (FFNN) or an RBF net built
using relevance vector machine (RVM).
"
"  This article
  * provides an overview of post-election audit sampling research and compares
various approaches to calculating post-election audit sample sizes, focusing on
risklimiting audits,
  * discusses fundamental concepts common to all risk-limiting post-election
audits, presenting new margin error bounds, sampling weights and sampling
probabilities that improve upon existing approaches and work for any size audit
unit and for single or multi-winner election contests,
  * provides two new simple formulas for estimating post-election audit sample
sizes in cases when detailed data, expertise, or tools are not available,
  * summarizes four improved methods for calculating risk-limiting election
audit sample sizes, showing how to apply precise margin error bounds to improve
the accuracy and efficacy of existing methods, and
  * discusses sampling mistakes that reduce post-election audit effectiveness.
"
"  We consider the change-point detection problem of deciding, based on noisy
measurements, whether an unknown signal over a given graph is constant or is
instead piecewise constant over two connected induced subgraphs of relatively
low cut size. We analyze the corresponding generalized likelihood ratio (GLR)
statistics and relate it to the problem of finding a sparsest cut in a graph.
We develop a tractable relaxation of the GLR statistic based on the
combinatorial Laplacian of the graph, which we call the spectral scan
statistic, and analyze its properties. We show how its performance as a testing
procedure depends directly on the spectrum of the graph, and use this result to
explicitly derive its asymptotic properties on few significant graph
topologies. Finally, we demonstrate both theoretically and by simulations that
the spectral scan statistic can outperform naive testing procedures based on
edge thresholding and $\chi^2$ testing.
"
"  This paper presents a novel approach to the robust design of deflection
actions for Near Earth Objects (NEO). In particular, the case of deflection by
means of Solar-pumped Laser ablation is studied here in detail. The basic idea
behind Laser ablation is that of inducing a sublimation of the NEO surface,
which produces a low thrust thereby slowly deviating the asteroid from its
initial Earth threatening trajectory. This work investigates the integrated
design of the Space-based Laser system and the deflection action generated by
laser ablation under uncertainty. The integrated design is formulated as a
multi-objective optimisation problem in which the deviation is maximised and
the total system mass is minimised. Both the model for the estimation of the
thrust produced by surface laser ablation and the spacecraft system model are
assumed to be affected by epistemic uncertainties (partial or complete lack of
knowledge). Evidence Theory is used to quantify these uncertainties and
introduce them in the optimisation process. The propagation of the trajectory
of the NEO under the laser-ablation action is performed with a novel approach
based on an approximated analytical solution of Gauss' Variational Equations.
An example of design of the deflection of asteroid Apophis with a swarm of
spacecraft is presented.
"
"  Variable selection for high-dimensional linear models has received a lot of
attention lately, mostly in the context of l1-regularization. Part of the
attraction is the variable selection effect: parsimonious models are obtained,
which are very suitable for interpretation. In terms of predictive power,
however, these regularized linear models are often slightly inferior to machine
learning procedures like tree ensembles. Tree ensembles, on the other hand,
lack usually a formal way of variable selection and are difficult to visualize.
A Garrote-style convex penalty for trees ensembles, in particular Random
Forests, is proposed. The penalty selects functional groups of nodes in the
trees. These could be as simple as monotone functions of individual predictor
variables. This yields a parsimonious function fit, which lends itself easily
to visualization and interpretation. The predictive power is maintained at
least at the same level as the original tree ensemble. A key feature of the
method is that, once a tree ensemble is fitted, no further tuning parameter
needs to be selected. The empirical performance is demonstrated on a wide array
of datasets.
"
"  The Commission of the European Union, as well the United States Environmental
Protection Agency, have set limit values for some pollutants in the ambient air
that have been shown to have adverse effects on human and environmental health.
It is therefore important to identify regions where the probability of
exceeding those limits is high. We propose a two-step procedure for estimating
the probability of exceeding the legal limits that combines smoothing in the
time domain with spatial interpolation. For illustration, we show an
application to particulate matter with diameter less than 10 microns
(PM$_{10}$) in the North-Italian region Piemonte.
"
"  Material indentation studies, in which a probe is brought into controlled
physical contact with an experimental sample, have long been a primary means by
which scientists characterize the mechanical properties of materials. More
recently, the advent of atomic force microscopy, which operates on the same
fundamental principle, has in turn revolutionized the nanoscale analysis of
soft biomaterials such as cells and tissues. This paper addresses the
inferential problems associated with material indentation and atomic force
microscopy, through a framework for the changepoint analysis of pre- and
post-contact data that is applicable to experiments across a variety of
physical scales. A hierarchical Bayesian model is proposed to account for
experimentally observed changepoint smoothness constraints and measurement
error variability, with efficient Monte Carlo methods developed and employed to
realize inference via posterior sampling for parameters such as Young's
modulus, a key quantifier of material stiffness. These results are the first to
provide the materials science community with rigorous inference procedures and
uncertainty quantification, via optimized and fully automated high-throughput
algorithms, implemented as the publicly available software package BayesCP. To
demonstrate the consistent accuracy and wide applicability of this approach,
results are shown for a variety of data sets from both macro- and
micro-materials experiments--including silicone, neurons, and red blood
cells--conducted by the authors and others.
"
"  Many natural signals exhibit a sparse representation, whenever a suitable
describing model is given. Here, a linear generative model is considered, where
many sparsity-based signal processing techniques rely on such a simplified
model. As this model is often unknown for many classes of the signals, we need
to select such a model based on the domain knowledge or using some exemplar
signals. This paper presents a new exemplar based approach for the linear model
(called the dictionary) selection, for such sparse inverse problems. The
problem of dictionary selection, which has also been called the dictionary
learning in this setting, is first reformulated as a joint sparsity model. The
joint sparsity model here differs from the standard joint sparsity model as it
considers an overcompleteness in the representation of each signal, within the
range of selected subspaces. The new dictionary selection paradigm is examined
with some synthetic and realistic simulations.
"
"  Spatial interaction between two or more classes or species has important
implications in various fields and causes multivariate patterns such as
segregation or association. Segregation occurs when members of a class or
species are more likely to be found near members of the same class or
conspecifics; while association occurs when members of a class or species are
more likely to be found near members of another class or species. The null
patterns considered are random labeling (RL) and complete spatial randomness
(CSR) of points from two or more classes, which is called \emph{CSR
independence}, henceforth. The clustering tests based on nearest neighbor
contingency tables (NNCTs) that are in use in literature are two-sided tests.
In this article, we consider the directional (i.e., one-sided) versions of the
cell-specific NNCT-tests and introduce new directional NNCT-tests for the
two-class case. We analyze the distributional properties; compare the empirical
significant levels and empirical power estimates of the tests using extensive
Monte Carlo simulations. We demonstrate that the new directional tests have
comparable performance with the currently available NNCT-tests in terms of
empirical size and power. We use four example data sets for illustrative
purposes and provide guidelines for using these NNCT-tests.
"
"  We are interested in comparing probability distributions defined on
Riemannian manifold. The traditional approach to study a distribution relies on
locating its mean point and finding the dispersion about that point. On a
general manifold however, even if two distributions are sufficiently
concentrated and have unique means, a comparison of their covariances is not
possible due to the difference in local parametrizations. To circumvent the
problem we associate a covariance field with each distribution and compare them
at common points by applying a similarity invariant function on their
representing matrices. In this way we are able to define distances between
distributions. We also propose new approach for interpolating discrete
distributions and derive some criteria that assure consistent results. Finally,
we illustrate with some experimental results on the unit 2-sphere.
"
"  We present a cosmography analysis of the Local Universe based on the recently
released Two-Micron All-Sky Redshift Survey (2MRS). Our method is based on a
Bayesian Networks Machine Learning algorithm (the Kigen-code) which
self-consistently samples the initial density fluctuations compatible with the
observed galaxy distribution and a structure formation model given by second
order Lagrangian perturbation theory (2LPT). From the initial conditions we
obtain an ensemble of reconstructed density and peculiar velocity fields which
characterize the local cosmic structure with high accuracy unveiling nonlinear
structures like filaments and voids in detail. Coherent redshift space
distortions are consistently corrected within 2LPT. From the ensemble of
cross-correlations between the reconstructions and the galaxy field and the
variance of the recovered density fields we find that our method is extremely
accurate up to k ~ 1 h Mpc^-1 and still yields reliable results down to scales
of about 3-4 h^-1 Mpc. The motion of the local group we obtain within ~ 80 h^-1
Mpc (v_LG=522+-86 km s^-1, l_LG=291^o +- 16^o, b_LG=34^o+-8^o) is in good
agreement with measurements derived from the CMB and from direct observations
of peculiar motions and is consistent with the predictions of LambdaCDM.
"
"  We consider a multi-armed bandit problem in a setting where each arm produces
a noisy reward realization which depends on an observable random covariate. As
opposed to the traditional static multi-armed bandit problem, this setting
allows for dynamically changing rewards that better describe applications where
side information is available. We adopt a nonparametric model where the
expected rewards are smooth functions of the covariate and where the hardness
of the problem is captured by a margin parameter. To maximize the expected
cumulative reward, we introduce a policy called Adaptively Binned Successive
Elimination (abse) that adaptively decomposes the global problem into suitably
""localized"" static bandit problems. This policy constructs an adaptive
partition using a variant of the Successive Elimination (se) policy. Our
results include sharper regret bounds for the se policy in a static bandit
problem and minimax optimal regret bounds for the abse policy in the dynamic
problem.
"
"  We consider the problem of learning a non-negative linear classifier with a
$1$-norm of at most $k$, and a fixed threshold, under the hinge-loss. This
problem generalizes the problem of learning a $k$-monotone disjunction. We
prove that we can learn efficiently in this setting, at a rate which is linear
in both $k$ and the size of the threshold, and that this is the best possible
rate. We provide an efficient online learning algorithm that achieves the
optimal rate, and show that in the batch case, empirical risk minimization
achieves this rate as well. The rates we show are tighter than the uniform
convergence rate, which grows with $k^2$.
"
"  Snoring is extremely common in the general population and when irregular may
indicate the presence of obstructive sleep apnea. We analyze the overnight
sequence of wave packets --- the snore sound --- recorded during full
polysomnography in patients referred to the sleep laboratory due to suspected
obstructive sleep apnea. We hypothesize that irregular snore, with duration in
the range between 10 and 100 seconds, correlates with respiratory obstructive
events. We find that the number of irregular snores --- easily accessible, and
quantified by what we call the snore time interval index (STII) --- is in good
agreement with the well-known apnea-hypopnea index, which expresses the
severity of obstructive sleep apnea and is extracted only from polysomnography.
In addition, the Hurst analysis of the snore sound itself, which calculates the
fluctuations in the signal as a function of time interval, is used to build a
classifier that is able to distinguish between patients with no or mild apnea
and patients with moderate or severe apnea.
"
"  Extreme value statistics (EVS) is applied to the distribution of galaxy
luminosities in the Sloan Digital Sky Survey (SDSS). We analyze the DR8 Main
Galaxy Sample (MGS), as well as the Luminous Red Galaxies (LRG). Maximal
luminosities are sampled from batches consisting of elongated pencil beams in
the radial direction of sight. For the MGS, results suggest a small and
positive tail index $\xi$, effectively ruling out the possibility of having a
finite maximum cutoff luminosity, and implying that the luminosity distribution
function may decay as a power law at the high luminosity end. Assuming,
however, $\xi=0$, a non-parametric comparison of the maximal luminosities with
the Fisher-Tippett-Gumbel distribution (limit distribution for variables
distributed by the Schechter fit) indicates a good agreement provided
uncertainties arising both from the finite batch size and from the batch size
distribution are accounted for. For a volume limited sample of LRGs, results
show that they can be described as being the extremes of a luminosity
distribution with an exponentially decaying tail, provided the uncertainties
related to batch-size distribution are taken care of.
"
"  In many metropolitan areas efforts are made to count the homeless to ensure
proper provision of social services. Some areas are very large, which makes
spatial sampling a viable alternative to an enumeration of the entire terrain.
Counts are observed in sampled regions but must be imputed in unvisited areas.
Along with the imputation process, the costs of underestimating and
overestimating may be different. For example, if precise estimation in areas
with large homeless c ounts is critical, then underestimation should be
penalized more than overestimation in the loss function. We analyze data from
the 2004--2005 Los Angeles County homeless study using an augmentation of $L_1$
stochastic gradient boosting that can weight overestimates and underestimates
asymmetrically. We discuss our choice to utilize stochastic gradient boosting
over other function estimation procedures. In-sample fitted and out-of-sample
imputed values, as well as relationships between the response and predictors,
are analyzed for various cost functions. Practical usage and policy
implications of these results are discussed briefly.
"
"  Consider a sensor network made of remote nodes connected to a common fusion
center. In a recent work Blum and Sadler [1] propose the idea of ordered
transmissions -sensors with more informative samples deliver their messages
first- and prove that optimal detection performance can be achieved using only
a subset of the total messages. Taking to one extreme this approach, we show
that just a single delivering allows making the detection errors as small as
desired, for a sufficiently large network size: a one-bit detection scheme can
be asymptotically consistent. The transmission ordering is based on the modulus
of some local statistic (MO system). We derive analytical results proving the
asymptotic consistency and, for the particular case that the local statistic is
the log-likelihood (\ell-MO system), we also obtain a bound on the error
convergence rate. All the theorems are proved under the general setup of random
number of sensors. Computer experiments corroborate the analysis and address
typical examples of applications including: non-homogeneous Poisson-deployed
networks, detection by per-sensor censoring, monitoring of energy-constrained
phenomenon.
"
"  A prespecified set of genes may be enriched, to varying degrees, for genes
that have altered expression levels relative to two or more states of a cell.
Knowing the enrichment of gene sets defined by functional categories, such as
gene ontology (GO) annotations, is valuable for analyzing the biological
signals in microarray expression data. A common approach to measuring
enrichment is by cross-classifying genes according to membership in a
functional category and membership on a selected list of significantly altered
genes. A small Fisher's exact test $p$-value, for example, in this $2\times2$
table is indicative of enrichment. Other category analysis methods retain the
quantitative gene-level scores and measure significance by referring a
category-level statistic to a permutation distribution associated with the
original differential expression problem. We describe a class of random-set
scoring methods that measure distinct components of the enrichment signal. The
class includes Fisher's test based on selected genes and also tests that
average gene-level evidence across the category. Averaging and selection
methods are compared empirically using Affymetrix data on expression in
nasopharyngeal cancer tissue, and theoretically using a location model of
differential expression. We find that each method has a domain of superiority
in the state space of enrichment problems, and that both methods have benefits
in practice. Our analysis also addresses two problems related to
multiple-category inference, namely, that equally enriched categories are not
detected with equal probability if they are of different sizes, and also that
there is dependence among category statistics owing to shared genes. Random-set
enrichment calculations do not require Monte Carlo for implementation. They are
made available in the R package allez.
"
"  Spatiotemporal simulation of minimum and maximum temperature is a fundamental
requirement for climate impact studies and hydrological or agricultural models.
Particularly over regions with variable orography, these simulations are
difficult to produce due to terrain driven nonstationarity. We develop a
bivariate stochastic model for the spatiotemporal field of minimum and maximum
temperature. The proposed framework splits the bivariate field into two
components of ""local climate"" and ""weather."" The local climate component is a
linear model with spatially varying process coefficients capturing the annual
cycle and yielding local climate estimates at all locations, not only those
within the observation network. The weather component spatially correlates the
bivariate simulations, whose matrix-valued covariance function we estimate
using a nonparametric kernel smoother that retains nonnegative definiteness and
allows for substantial nonstationarity across the simulation domain. The
statistical model is augmented with a spatially varying nugget effect to allow
for locally varying small scale variability. Our model is applied to a daily
temperature data set covering the complex terrain of Colorado, USA, and
successfully accommodates substantial temporally varying nonstationarity in
both the direct-covariance and cross-covariance functions.
"
"  A key prerequisite to optimal reasoning under uncertainty in intelligent
systems is to start with good class probability estimates. This paper improves
on the current best probability estimation trees (Bagged-PETs) and also
presents a new ensemble-based algorithm (MOB-ESP). Comparisons are made using
several benchmark datasets and multiple metrics. These experiments show that
MOB-ESP outputs significantly more accurate class probabilities than either the
baseline BPETs algorithm or the enhanced version presented here (EB-PETs).
These results are based on metrics closely associated with the average accuracy
of the predictions. MOB-ESP also provides much better probability rankings than
B-PETs. The paper further suggests how these estimation techniques can be
applied in concert with a broader category of classifiers.
"
"  In this study, a two-state Markov switching count-data model is proposed as
an alternative to zero-inflated models to account for the preponderance of
zeros sometimes observed in transportation count data, such as the number of
accidents occurring on a roadway segment over some period of time. For this
accident-frequency case, zero-inflated models assume the existence of two
states: one of the states is a zero-accident count state, in which accident
probabilities are so low that they cannot be statistically distinguished from
zero, and the other state is a normal count state, in which counts can be
non-negative integers that are generated by some counting process, for example,
a Poisson or negative binomial. In contrast to zero-inflated models, Markov
switching models allow specific roadway segments to switch between the two
states over time. An important advantage of this Markov switching approach is
that it allows for the direct statistical estimation of the specific
roadway-segment state (i.e., zero or count state) whereas traditional
zero-inflated models do not. To demonstrate the applicability of this approach,
a two-state Markov switching negative binomial model (estimated with Bayesian
inference) and standard zero-inflated negative binomial models are estimated
using five-year accident frequencies on Indiana interstate highway segments. It
is shown that the Markov switching model is a viable alternative and results in
a superior statistical fit relative to the zero-inflated models.
"
"  The literature on statistical learning for time series assumes the asymptotic
independence or ``mixing' of the data-generating process. These mixing
assumptions are never tested, nor are there methods for estimating mixing rates
from data. We give an estimator for the $\beta$-mixing rate based on a single
stationary sample path and show it is $L_1$-risk consistent.
"
"  We study the portfolio problem of maximizing the outperformance probability
over a random benchmark through dynamic trading with a fixed initial capital.
Under a general incomplete market framework, this stochastic control problem
can be formulated as a composite pure hypothesis testing problem. We analyze
the connection between this pure testing problem and its randomized
counterpart, and from latter we derive a dual representation for the maximal
outperformance probability. Moreover, in a complete market setting, we provide
a closed-form solution to the problem of beating a leveraged exchange traded
fund. For a general benchmark under an incomplete stochastic factor model, we
provide the Hamilton-Jacobi-Bellman PDE characterization for the maximal
outperformance probability.
"
"  This paper explores a variety of strategies for understanding the formation,
structure, efficiency and vulnerability of water distribution networks. Water
supply systems are studied as spatially organized networks for which the
practical applications of abstract evaluation methods are critically evaluated.
Empirical data from benchmark networks are used to study the interplay between
network structure and operational efficiency, reliability and robustness.
Structural measurements are undertaken to quantify properties such as
redundancy and optimal-connectivity, herein proposed as constraints in network
design optimization problems. The role of the supply-demand structure towards
system efficiency is studied and an assessment of the vulnerability to failures
based on the disconnection of nodes from the source(s) is undertaken. The
absence of conventional degree-based hubs (observed through uncorrelated
non-heterogeneous sparse topologies) prompts an alternative approach to
studying structural vulnerability based on the identification of network
cut-sets and optimal connectivity invariants. A discussion on the scope,
limitations and possible future directions of this research is provided.
"
"  This paper raises concerns about the advantages of using statistical
significance tests in research assessments as has recently been suggested in
the debate about proper normalization procedures for citation indicators.
Statistical significance tests are highly controversial and numerous criticisms
have been leveled against their use. Based on examples from articles by
proponents of the use statistical significance tests in research assessments,
we address some of the numerous problems with such tests. The issues
specifically discussed are the ritual practice of such tests, their dichotomous
application in decision making, the difference between statistical and
substantive significance, the implausibility of most null hypotheses, the
crucial assumption of randomness, as well as the utility of standard errors and
confidence intervals for inferential purposes. We argue that applying
statistical significance tests and mechanically adhering to their results is
highly problematic and detrimental to critical thinking. We claim that the use
of such tests do not provide any advantages in relation to citation indicators,
interpretations of them, or the decision making processes based upon them. On
the contrary their use may be harmful. Like many other critics, we generally
believe that statistical significance tests are over- and misused in the social
sciences including scientometrics and we encourage a reform on these matters.
"
"  This paper presents the intrinsic geometric model for the solution of power
system planning and its operation. This problem is large-scale and nonlinear,
in general. Thus, we have developed the intrinsic geometric model for the
network reliability and voltage stability, and examined it for the IEEE 5 bus
system. The robustness of the proposed model is illustrated by introducing
variations of the network parameters. Exact analytical results show the
accuracy as well as the efficiency of the proposed solution technique.
"
"  Learning theory has largely focused on two main learning scenarios. The first
is the classical statistical setting where instances are drawn i.i.d. from a
fixed distribution and the second scenario is the online learning, completely
adversarial scenario where adversary at every time step picks the worst
instance to provide the learner with. It can be argued that in the real world
neither of these assumptions are reasonable. It is therefore important to study
problems with a range of assumptions on data. Unfortunately, theoretical
results in this area are scarce, possibly due to absence of general tools for
analysis. Focusing on the regret formulation, we define the minimax value of a
game where the adversary is restricted in his moves. The framework captures
stochastic and non-stochastic assumptions on data. Building on the sequential
symmetrization approach, we define a notion of distribution-dependent
Rademacher complexity for the spectrum of problems ranging from i.i.d. to
worst-case. The bounds let us immediately deduce variation-type bounds. We then
consider the i.i.d. adversary and show equivalence of online and batch
learnability. In the supervised setting, we consider various hybrid assumptions
on the way that x and y variables are chosen. Finally, we consider smoothed
learning problems and show that half-spaces are online learnable in the
smoothed model. In fact, exponentially small noise added to adversary's
decisions turns this problem with infinite Littlestone's dimension into a
learnable problem.
"
"  We introduce the author-topic model, a generative model for documents that
extends Latent Dirichlet Allocation (LDA; Blei, Ng, & Jordan, 2003) to include
authorship information. Each author is associated with a multinomial
distribution over topics and each topic is associated with a multinomial
distribution over words. A document with multiple authors is modeled as a
distribution over topics that is a mixture of the distributions associated with
the authors. We apply the model to a collection of 1,700 NIPS conference papers
and 160,000 CiteSeer abstracts. Exact inference is intractable for these
datasets and we use Gibbs sampling to estimate the topic and author
distributions. We compare the performance with two other generative models for
documents, which are special cases of the author-topic model: LDA (a topic
model) and a simple author model in which each author is associated with a
distribution over words rather than a distribution over topics. We show topics
recovered by the author-topic model, and demonstrate applications to computing
similarity between authors and entropy of author output.
"
"  Dependent nonparametric processes extend distributions over measures, such as
the Dirichlet process and the beta process, to give distributions over
collections of measures, typically indexed by values in some covariate space.
Such models are appropriate priors when exchangeability assumptions do not
hold, and instead we want our model to vary fluidly with some set of
covariates. Since the concept of dependent nonparametric processes was
formalized by MacEachern [1], there have been a number of models proposed and
used in the statistics and machine learning literatures. Many of these models
exhibit underlying similarities, an understanding of which, we hope, will help
in selecting an appropriate prior, developing new models, and leveraging
inference techniques.
"
"  In this paper we consider the task of estimating the non-zero pattern of the
sparse inverse covariance matrix of a zero-mean Gaussian random vector from a
set of iid samples. Note that this is also equivalent to recovering the
underlying graph structure of a sparse Gaussian Markov Random Field (GMRF). We
present two novel greedy approaches to solving this problem. The first
estimates the non-zero covariates of the overall inverse covariance matrix
using a series of global forward and backward greedy steps. The second
estimates the neighborhood of each node in the graph separately, again using
greedy forward and backward steps, and combines the intermediate neighborhoods
to form an overall estimate. The principal contribution of this paper is a
rigorous analysis of the sparsistency, or consistency in recovering the
sparsity pattern of the inverse covariance matrix. Surprisingly, we show that
both the local and global greedy methods learn the full structure of the model
with high probability given just $O(d\log(p))$ samples, which is a
\emph{significant} improvement over state of the art $\ell_1$-regularized
Gaussian MLE (Graphical Lasso) that requires $O(d^2\log(p))$ samples. Moreover,
the restricted eigenvalue and smoothness conditions imposed by our greedy
methods are much weaker than the strong irrepresentable conditions required by
the $\ell_1$-regularization based methods. We corroborate our results with
extensive simulations and examples, comparing our local and global greedy
methods to the $\ell_1$-regularized Gaussian MLE as well as the Neighborhood
Greedy method to that of nodewise $\ell_1$-regularized linear regression
(Neighborhood Lasso).
"
"  We present local discriminative Gaussian (LDG) dimensionality reduction, a
supervised dimensionality reduction technique for classification. The LDG
objective function is an approximation to the leave-one-out training error of a
local quadratic discriminant analysis classifier, and thus acts locally to each
training point in order to find a mapping where similar data can be
discriminated from dissimilar data. While other state-of-the-art linear
dimensionality reduction methods require gradient descent or iterative solution
approaches, LDG is solved with a single eigen-decomposition. Thus, it scales
better for datasets with a large number of feature dimensions or training
examples. We also adapt LDG to the transfer learning setting, and show that it
achieves good performance when the test data distribution differs from that of
the training data.
"
"  A lot of effort has been invested into characterizing the convergence rates
of gradient based algorithms for non-linear convex optimization. Recently,
motivated by large datasets and problems in machine learning, the interest has
shifted towards distributed optimization. In this work we present a distributed
algorithm for strongly convex constrained optimization. Each node in a network
of n computers converges to the optimum of a strongly convex, L-Lipchitz
continuous, separable objective at a rate O(log (sqrt(n) T) / T) where T is the
number of iterations. This rate is achieved in the online setting where the
data is revealed one at a time to the nodes, and in the batch setting where
each node has access to its full local dataset from the start. The same
convergence rate is achieved in expectation when the subgradients used at each
node are corrupted with additive zero-mean noise.
"
"  In psychophysical experiments time and the limited goodwill of participants
is usually a major constraint. This has been the main motivation behind the
early development of adaptive methods for the measurements of psychometric
thresholds. More recently methods have been developed to measure whole
psychometric functions in an adaptive way. Here we describe a Bayesian method
to measure adaptively any aspect of a psychophysical function, taking
inspiration from Kontsevich and Tyler's optimal Bayesian measurement method.
Our method is implemented in a complete and easy-to-use MATLAB package.
"
"  Prediction of various weather quantities is mostly based on deterministic
numerical weather forecasting models. Multiple runs of these models with
different initial conditions result ensembles of forecasts which are applied
for estimating the distribution of future weather quantities. However, the
ensembles are usually under-dispersive and uncalibrated, so post-processing is
required.
  In the present work Bayesian Model Averaging (BMA) is applied for calibrating
ensembles of wind speed forecasts produced by the operational Limited Area
Model Ensemble Prediction System of the Hungarian Meteorological Service (HMS).
  We describe two possible BMA models for wind speed data of the HMS and show
that BMA post-processing significantly improves the calibration and precision
of forecasts.
"
"  This work examines a semi-blind single-channel source separation problem. Our
specific aim is to separate one source whose local structure is approximately
known, from another a priori unspecified background source, given only a single
linear combination of the two sources. We propose a separation technique based
on local sparse approximations along the lines of recent efforts in sparse
representations and dictionary learning. A key feature of our procedure is the
online learning of dictionaries (using only the data itself) to sparsely model
the background source, which facilitates its separation from the
partially-known source. Our approach is applicable to source separation
problems in various application domains; here, we demonstrate the performance
of our proposed approach via simulation on a stylized audio source separation
task.
"
"  Characterization of real-world complex systems increasingly involves the
study of their topological structure using graph theory. Among global network
properties, small-world property, consisting in existence of relatively short
paths together with high clustering of the network, is one of the most
discussed and studied. When dealing with coupled dynamical systems, links among
units of the system are commonly quantified by a measure of pairwise
statistical dependence of observed time series (functional connectivity). We
argue that the functional connectivity approach leads to upwardly biased
estimates of small-world characteristics (with respect to commonly used random
graph models) due to partial transitivity of the accepted functional
connectivity measures such as the correlation coefficient. In particular, this
may lead to observation of small-world characteristics in connectivity graphs
estimated from generic randomly connected dynamical systems. The ubiquity and
robustness of the phenomenon is documented by an extensive parameter study of
its manifestation in a multivariate linear autoregressive process, with
discussion of the potential relevance for nonlinear processes and measures.
"
"  Sparse coding in learned dictionaries has been established as a successful
approach for signal denoising, source separation and solving inverse problems
in general. A dictionary learning method adapts an initial dictionary to a
particular signal class by iteratively computing an approximate factorization
of a training data matrix into a dictionary and a sparse coding matrix. The
learned dictionary is characterized by two properties: the coherence of the
dictionary to observations of the signal class, and the self-coherence of the
dictionary atoms. A high coherence to the signal class enables the sparse
coding of signal observations with a small approximation error, while a low
self-coherence of the atoms guarantees atom recovery and a more rapid residual
error decay rate for the sparse coding algorithm. The two goals of high signal
coherence and low self-coherence are typically in conflict, therefore one seeks
a trade-off between them, depending on the application. We present a dictionary
learning method with an effective control over the self-coherence of the
trained dictionary, enabling a trade-off between maximizing the sparsity of
codings and approximating an equiangular tight frame.
"
"  Background. Drug-drug interaction (DDI) is a major cause of morbidity and
mortality. [...] Biomedical literature mining can aid DDI research by
extracting relevant DDI signals from either the published literature or large
clinical databases. However, though drug interaction is an ideal area for
translational research, the inclusion of literature mining methodologies in DDI
workflows is still very preliminary. One area that can benefit from literature
mining is the automatic identification of a large number of potential DDIs,
whose pharmacological mechanisms and clinical significance can then be studied
via in vitro pharmacology and in populo pharmaco-epidemiology. Experiments. We
implemented a set of classifiers for identifying published articles relevant to
experimental pharmacokinetic DDI evidence. These documents are important for
identifying causal mechanisms behind putative drug-drug interactions, an
important step in the extraction of large numbers of potential DDIs. We
evaluate performance of several linear classifiers on PubMed abstracts, under
different feature transformation and dimensionality reduction methods. In
addition, we investigate the performance benefits of including various
publicly-available named entity recognition features, as well as a set of
internally-developed pharmacokinetic dictionaries. Results. We found that
several classifiers performed well in distinguishing relevant and irrelevant
abstracts. We found that the combination of unigram and bigram textual features
gave better performance than unigram features alone, and also that
normalization transforms that adjusted for feature frequency and document
length improved classification. For some classifiers, such as linear
discriminant analysis (LDA), proper dimensionality reduction had a large impact
on performance. Finally, the inclusion of NER features and dictionaries was
found not to help classification.
"
"  Over the past two decades, a variety of methods have been used to count the
homeless in large metropolitan areas. In this paper, we report on an effort to
count the homeless in Los Angeles County, one that employed the sampling of
census tracts. A number of complications are discussed, including\^{E} the need
to impute homeless counts to areas of \^{E}the County\^{E} not sampled. We
conclude that, despite their imperfections, estimated counts provided useful
and credible information to the stakeholders involved.
"
"  While existing mathematical descriptions can accurately account for phenomena
at microscopic scales (e.g. molecular dynamics), these are often
high-dimensional, stochastic and their applicability over macroscopic time
scales of physical interest is computationally infeasible or impractical. In
complex systems, with limited physical insight on the coherent behavior of
their constituents, the only available information is data obtained from
simulations of the trajectories of huge numbers of degrees of freedom over
microscopic time scales. This paper discusses a Bayesian approach to deriving
probabilistic coarse-grained models that simultaneously address the problems of
identifying appropriate reduced coordinates and the effective dynamics in this
lower-dimensional representation. At the core of the models proposed lie
simple, low-dimensional dynamical systems which serve as the building blocks of
the global model. These approximate the latent, generating sources and
parameterize the reduced-order dynamics. We discuss parallelizable, online
inference and learning algorithms that employ Sequential Monte Carlo samplers
and scale linearly with the dimensionality of the observed dynamics. We propose
a Bayesian adaptive time-integration scheme that utilizes probabilistic
predictive estimates and enables rigorous concurrent s imulation over
macroscopic time scales. The data-driven perspective advocated assimilates
computational and experimental data and thus can materialize data-model fusion.
It can deal with applications that lack a mathematical description and where
only observational data is available. Furthermore, it makes non-intrusive use
of existing computational models.
"
"  The sunspot number (SSN), the total solar irradiance (TSI), a TSI
reconstruction, and the solar flare index (SFI), are analyzed for long-range
persistence (LRP). Standard Hurst analysis yields $H \approx 0.9$, which
suggests strong LRP. However, solar activity time series are non-stationary due
to the almost periodic 11 year smooth component, and the analysis does not give
the correct $H$ for the stochastic component. Better estimates are obtained by
detrended fluctuations analysis (DFA), but estimates are biased and errors are
large due to the short time records. These time series can be modeled as a
stochastic process of the form $x(t)=y(t)+\sigma \sqrt{y(t)}\, w_H(t)$, where
$y(t)$ is the smooth component, and $w_H(t) $ is a stationary fractional noise
with Hurst exponent $H$. From ensembles of numerical solutions to the
stochastic model, and application of Bayes' theorem, we can obtain bias and
error bars on $H$ and also a test of the hypothesis that a process is
uncorrelated ($H=1/2$). The conclusions from the present data sets are that
SSN, TSI and TSI reconstruction almost certainly are long-range persistent, but
with most probable value $H\approx 0.7$. The SFI process, however, is either
very weakly persistent ($H<0.6$) or completely uncorrelated. Some differences
between stochastic properties of the TSI and its reconstruction indicate some
error in the reconstruction scheme.
"
"  Since learning is typically very slow in Boltzmann machines, there is a need
to restrict connections within hidden layers. However, the resulting states of
hidden units exhibit statistical dependencies. Based on this observation, we
propose using $l_1/l_2$ regularization upon the activation possibilities of
hidden units in restricted Boltzmann machines to capture the loacal
dependencies among hidden units. This regularization not only encourages hidden
units of many groups to be inactive given observed data but also makes hidden
units within a group compete with each other for modeling observed data. Thus,
the $l_1/l_2$ regularization on RBMs yields sparsity at both the group and the
hidden unit levels. We call RBMs trained with the regularizer \emph{sparse
group} RBMs. The proposed sparse group RBMs are applied to three tasks:
modeling patches of natural images, modeling handwritten digits and pretaining
a deep networks for a classification task. Furthermore, we illustrate the
regularizer can also be applied to deep Boltzmann machines, which lead to
sparse group deep Boltzmann machines. When adapted to the MNIST data set, a
two-layer sparse group Boltzmann machine achieves an error rate of $0.84\%$,
which is, to our knowledge, the best published result on the
permutation-invariant version of the MNIST task.
"
"  This work explores the role of the intrinsic fluctuations in finite parameter
controller configurations characterizing an ensemble of arbitrary irregular
filter circuits. Our analysis illustrates that the parametric intrinsic
geometric description exhibits a set of exact pair correction functions and
global correlation volume with and without the variation of the mismatch
factor. The present consideration shows that the canonical fluctuations can
precisely be depicted without any approximation. The intrinsic geometric notion
offers a clear picture of the fluctuating controllers, which as the limit of
the ensemble averaging reduce to the specified controller. For the constant
mismatch factor controllers, the Gaussian fluctuations over equilibrium basis
accomplish a well-defined, non-degenerate, flat regular intrinsic Riemannian
surface. An explicit computation further demonstrates that the underlying power
correlations involve ordinary summations, even if we consider the variable
mismatch factor controllers. Our intrinsic geometric framework describes a
definite character to the canonical power fluctuations of the controllers and
constitutes a stable design strategy for the parameters.
"
"  Many of the early works in the quality control literature construct control
limits through the use of graphs and tables as described in Wortham and Ringer
(1972). However, the methods used in this literature are restricted to using
only the values that the graphs and tables can provide and to the case where
the parameters of the underlying distribution are known. In this note, we
briefly describe a technique which can be used to calculate exact control
limits without the use of graphs or tables. We also describe what are commonly
referred to in the literature as fiducial limits. Fiducial limits are often
used as the limits in control charting when the parameters of the underlying
distribution are unknown.
"
"  In the supervised learning setting termed Multiple-Instance Learning (MIL),
the examples are bags of instances, and the bag label is a function of the
labels of its instances. Typically, this function is the Boolean OR. The
learner observes a sample of bags and the bag labels, but not the instance
labels that determine the bag labels. The learner is then required to emit a
classification rule for bags based on the sample. MIL has numerous
applications, and many heuristic algorithms have been used successfully on this
problem, each adapted to specific settings or applications. In this work we
provide a unified theoretical analysis for MIL, which holds for any underlying
hypothesis class, regardless of a specific application or problem domain. We
show that the sample complexity of MIL is only poly-logarithmically dependent
on the size of the bag, for any underlying hypothesis class. In addition, we
introduce a new PAC-learning algorithm for MIL, which uses a regular supervised
learning algorithm as an oracle. We prove that efficient PAC-learning for MIL
can be generated from any efficient non-MIL supervised learning algorithm that
handles one-sided error. The computational complexity of the resulting
algorithm is only polynomially dependent on the bag size.
"
"  To uncover the genetic basis of complex disease, individuals are often
measured at a large number of genetic variants (usually SNPs) across the
genome. GemTools provides computationally efficient tools for modeling genetic
ancestry based on SNP genotypes. The main algorithm creates an eigenmap based
on genetic similarities, and then clusters subjects based on their map
position. This process is continued iteratively until each cluster is
relatively homogeneous. For genetic association studies, GemTools matches cases
and controls based on genetic similarity.
"
"  Constructing gene interaction networks (GINs) from high-throughput gene
expression data is an important and challenging problem in systems biology.
Existing algorithms produce networks that either have undirected and unweighted
edges, or else are constrained to contain no cycles, both of which are
biologically unrealistic. In the present paper we propose a new algorithm,
based on a concept from probability theory known as the phi-mixing coefficient,
that produces networks whose edges are weighted and directed, and are permitted
to contain cycles. Because there is no ""ground truth"" for genome-wide networks
on a human scale, we analyzed the outcomes of several experiments on lung
cancer, and matched the predictions from the inferred networks with
experimental results. Specifically, we inferred three networks (NSCLC,
Neuro-endocrine NSCLC plus SCLC, and normal) from the gene expression
measurements of 157 lung cancer and 59 normal cell lines, compared with the
outcomes of siRNA screening of 19,000+ genes on 11 NSCLC cell lines, and
analyzed data from a ChIP-Seq experiment to determine putative downstream
targets of the lineage specific oncogenic transcription factor ASCL1. The
inferred networks displayed a scale-free or power law behavior between the
degree of a node and the number of nodes with that degree. There was a strong
correlation between the degree of a gene in the inferred NSCLC network and its
essentiality for the survival of the cells. The inferred downstream
neighborhood genes of ASCL1 in the SCLC network were significantly enriched by
ChIP-Seq determined putative target genes, while no such enrichment was found
in the inferred NSCLC network.
"
"  Recent research has shown that performance in signal processing tasks can
often be significantly improved by using signal models based on sparse
representations, where a signal is approximated using a small number of
elements from a fixed dictionary. Unfortunately, inference in this model
involves solving non-smooth optimization problems that are computationally
expensive. While significant efforts have focused on developing digital
algorithms specifically for this problem, these algorithms are inappropriate
for many applications because of the time and power requirements necessary to
solve large optimization problems. Based on recent work in computational
neuroscience, we explore the potential advantages of continuous time dynamical
systems for solving sparse approximation problems if they were implemented in
analog VLSI. Specifically, in the simulated task of recovering synthetic and
MRI data acquired via compressive sensing techniques, we show that these
systems can potentially perform recovery at time scales of 10-20{\mu}s,
supporting datarates of 50-100 kHz (orders of magnitude faster that digital
algorithms). Furthermore, we show analytically that a wide range of sparse
approximation problems can be solved in the same basic architecture, including
approximate $\ell^p$ norms, modified $\ell^1$ norms, re-weighted $\ell^1$ and
$\ell^2$, the block $\ell^1$ norm and classic Tikhonov regularization.
"
"  When response variables are nominal and populations are cross-classified with
respect to multiple polytomies, questions often arise about the degree of
association of the responses with explanatory variables. When populations are
known, we introduce a nominal association vector and matrix to evaluate the
dependence of a response variable with an explanatory variable. These measures
provide detailed evaluations of nominal associations at both local and global
levels. We also define a general class of global association measures which
embraces the well known association measure by Goodman-Kruskal (1954). The
proposed association matrix also gives rise to the expected generalized
confusion matrix in classification. The hierarchy of equivalence relations
defined by the association vector and matrix are also shown.
"
"  To date, testing interactions in high dimensions has been a challenging task.
Existing methods often have issues with sensitivity to modeling assumptions and
heavily asymptotic nominal p-values. To help alleviate these issues, we propose
a permutation-based method for testing marginal interactions with a binary
response. Our method searches for pairwise correlations which differ between
classes. In this manuscript, we compare our method on real and simulated data
to the standard approach of running many pairwise logistic models. On simulated
data our method finds more significant interactions at a lower false discovery
rate (especially in the presence of main effects). On real genomic data,
although there is no gold standard, our method finds apparent signal and tells
a believable story, while logistic regression does not. We also give asymptotic
consistency results under not too restrictive assumptions.
"
"  In practice, several time series exhibit long-range dependence or persistence
in their observations, leading to the development of a number of estimation and
prediction methodologies to account for the slowly decaying autocorrelations.
The autoregressive fractionally integrated moving average (ARFIMA) process is
one of the best-known classes of long-memory models. In the package afmtools
for R, we have implemented some of these statistical tools for analyzing ARFIMA
models. In particular, this package contains functions for parameter
estimation, exact autocovariance calculation, predictive ability testing, and
impulse response function, amongst others. Finally, the implemented methods are
illustrated with applications to real-life time series.
"
"  High-dimensional data pose challenges in statistical learning and modeling.
Sometimes the predictors can be naturally grouped where pursuing the
between-group sparsity is desired. Collinearity may occur in real-world
high-dimensional applications where the popular $l_1$ technique suffers from
both selection inconsistency and prediction inaccuracy. Moreover, the problems
of interest often go beyond Gaussian models. To meet these challenges,
nonconvex penalized generalized linear models with grouped predictors are
investigated and a simple-to-implement algorithm is proposed for computation. A
rigorous theoretical result guarantees its convergence and provides tight
preliminary scaling. This framework allows for grouped predictors and nonconvex
penalties, including the discrete $l_0$ and the `$l_0+l_2$' type penalties.
Penalty design and parameter tuning for nonconvex penalties are examined.
Applications of super-resolution spectrum estimation in signal processing and
cancer classification with joint gene selection in bioinformatics show the
performance improvement by nonconvex penalized estimation.
"
"  Reliable spectrum sensing is a key functionality of a cognitive radio
network. Cooperative spectrum sensing improves the detection reliability of a
cognitive radio system but also increases the system energy consumption which
is a critical factor particularly for low-power wireless technologies. A
censored truncated sequential spectrum sensing technique is considered as an
energy-saving approach. To design the underlying sensing parameters, the
maximum energy consumption per sensor is minimized subject to a lower bounded
global probability of detection and an upper bounded false alarm rate. This way
both the interference to the primary user due to miss detection and the network
throughput as a result of a low false alarm rate is controlled. We compare the
performance of the proposed scheme with a fixed sample size censoring scheme
under different scenarios. It is shown that as the sensing cost of the
cognitive radios increases, the energy efficiency of the censored truncated
sequential approach grows significantly.
"
"  Dimensionality reduction is ubiquitous in analysis of complex dynamics. The
conventional dimensionality reduction techniques, however, focus on reproducing
the underlying configuration space, rather than the dynamics itself. The
constructed low-dimensional space does not provide complete and accurate
description of the dynamics. Here I describe how to perform dimensionality
reduction while preserving the essential properties of the dynamics. The
approach is illustrated by analyzing the chess game - the archetype of complex
dynamics. A variable that provides complete and accurate description of chess
dynamics is constructed. Winning probability is predicted by describing the
game as a random walk on the free energy landscape associated with the
variable. The approach suggests a possible way of obtaining a simple yet
accurate description of many important complex phenomena. The analysis of the
chess game shows that the approach can quantitatively describe the dynamics of
processes where human decision-making plays a central role, e.g., financial and
social dynamics.
"
"  In this paper, we provide $R$-estimators of the location of a rotationally
symmetric distribution on the unit sphere of $\R^k$. In order to do so we first
prove the local asymptotic normality property of a sequence of rotationally
symmetric models; this is a non standard result due to the curved nature of the
unit sphere. We then construct our estimators by adapting the Le Cam one-step
methodology to spherical statistics and ranks. We show that they are
asymptotically normal under any rotationally symmetric distribution and achieve
the efficiency bound under a specific density. Their small sample behavior is
studied via a Monte Carlo simulation and our methodology is illustrated on
geological data.
"
"  The Johnson-Lindenstrauss Lemma allows for the projection of $n$ points in
$p-$dimensional Euclidean space onto a $k-$dimensional Euclidean space, with $k
\ge \frac{24\ln \emph{n}}{3\epsilon^2-2\epsilon^3}$, so that the pairwise
distances are preserved within a factor of $1\pm\epsilon$. Here, working
directly with the distributions of the random distances rather than resorting
to the moment generating function technique, an improvement on the lower bound
for $k$ is obtained. The additional reduction in dimension when compared to
bounds found in the literature, is at least $13\%$, and, in some cases, up to
$30\%$ additional reduction is achieved. Using the moment generating function
technique, we further provide a lower bound for $k$ using pairwise $L_2$
distances in the space of points to be projected and pairwise $L_1$ distances
in the space of the projected points. Comparison with the results obtained in
the literature shows that the bound presented here provides an additional
$36-40\%$ reduction.
"
"  In this paper we consider $l_0$ regularized convex cone programming problems.
In particular, we first propose an iterative hard thresholding (IHT) method and
its variant for solving $l_0$ regularized box constrained convex programming.
We show that the sequence generated by these methods converges to a local
minimizer. Also, we establish the iteration complexity of the IHT method for
finding an $\epsilon$-local-optimal solution. We then propose a method for
solving $l_0$ regularized convex cone programming by applying the IHT method to
its quadratic penalty relaxation and establish its iteration complexity for
finding an $\epsilon$-approximate local minimizer. Finally, we propose a
variant of this method in which the associated penalty parameter is dynamically
updated, and show that every accumulation point is a local minimizer of the
problem.
"
"  Variable selection in high dimensional space has challenged many contemporary
statistical problems from many frontiers of scientific disciplines. Recent
technology advance has made it possible to collect a huge amount of covariate
information such as microarray, proteomic and SNP data via bioimaging
technology while observing survival information on patients in clinical
studies. Thus, the same challenge applies to the survival analysis in order to
understand the association between genomics information and clinical
information about the survival time. In this work, we extend the sure screening
procedure Fan and Lv (2008) to Cox's proportional hazards model with an
iterative version available. Numerical simulation studies have shown
encouraging performance of the proposed method in comparison with other
techniques such as LASSO. This demonstrates the utility and versatility of the
iterative sure independent screening scheme.
"
"  The new generation of radio synthesis arrays, such as LOFAR and SKA, have
been designed to surpass existing arrays in terms of sensitivity, angular
resolution and frequency coverage. This evolution has led to the development of
advanced calibration techniques that ensure the delivery of accurate results at
the lowest possible computational cost. However, the performance of such
calibration techniques is still limited by the compact, bright sources in the
sky, used as calibrators. It is important to have a bright enough source that
is well distinguished from the background noise level in order to achieve
satisfactory results in calibration. We present ""clustered calibration"" as a
modification to traditional radio interferometric calibration, in order to
accommodate faint sources that are almost below the background noise level into
the calibration process. The main idea is to employ the information of the
bright sources' measured signals as an aid to calibrate fainter sources that
are nearby the bright sources. In the case where we do not have bright enough
sources, a source cluster could act as a bright source that can be
distinguished from background noise. We construct a number of source clusters
assuming that the signals of the sources belonging to a single cluster are
corrupted by almost the same errors, and each cluster is calibrated as a single
source, using the combined coherencies of its sources simultaneously. This
upgrades the power of an individual faint source by the effective power of its
cluster. We give performance analysis of clustered calibration to show the
superiority of this approach compared to the traditional unclustered
calibration. We also provide analytical criteria to choose the optimum number
of clusters for a given observation in an efficient manner.
"
"  Ensemble methods for supervised machine learning have become popular due to
their ability to accurately predict class labels with groups of simple,
lightweight ""base learners."" While ensembles offer computationally efficient
models that have good predictive capability they tend to be large and offer
little insight into the patterns or structure in a dataset. We consider an
ensemble technique that returns a model of ranked rules. The model accurately
predicts class labels and has the advantage of indicating which parameter
constraints are most useful for predicting those labels. An example of the rule
ensemble method successfully ranking rules and selecting attributes is given
with a dataset containing images of potential supernovas where the number of
necessary features is reduced from 39 to 21. We also compare the rule ensemble
method on a set of multi-class problems with boosting and bagging, which are
two well known ensemble techniques that use decision trees as base learners,
but do not have a rule ranking scheme.
"
"  In high-dimensional data analysis, penalized likelihood estimators are shown
to provide superior results in both variable selection and parameter
estimation. A new algorithm, APPLE, is proposed for calculating the Approximate
Path for Penalized Likelihood Estimators. Both the convex penalty (such as
LASSO) and the nonconvex penalty (such as SCAD and MCP) cases are considered.
The APPLE efficiently computes the solution path for the penalized likelihood
estimator using a hybrid of the modified predictor-corrector method and the
coordinate-descent algorithm. APPLE is compared with several well-known
packages via simulation and analysis of two gene expression data sets.
"
"  Given a set of k networks, possibly with different sizes and no overlaps in
nodes or edges, how can we quickly assess similarity between them, without
solving the node-correspondence problem? Analogously, how can we extract a
small number of descriptive, numerical features from each graph that
effectively serve as the graph's ""signature""? Having such features will enable
a wealth of graph mining tasks, including clustering, outlier detection,
visualization, etc.
  We propose NetSimile -- a novel, effective, and scalable method for solving
the aforementioned problem. NetSimile has the following desirable properties:
(a) It gives similarity scores that are size-invariant. (b) It is scalable,
being linear on the number of edges for ""signature"" vector extraction. (c) It
does not need to solve the node-correspondence problem. We present extensive
experiments on numerous synthetic and real graphs from disparate domains, and
show NetSimile's superiority over baseline competitors. We also show how
NetSimile enables several mining tasks such as clustering, visualization,
discontinuity detection, network transfer learning, and re-identification
across networks.
"
"  We propose efficient nonparametric statistics to compare medical imaging
modalities in multi-reader multi-test data and to compare markers in
longitudinal ROC data. The proposed methods are based on the weighted area
under the ROC curve which includes the area under the curve and the partial
area under the curve as special cases. The methods maximize the local power for
detecting the difference between imaging modalities. The asymptotic results of
the proposed methods are developed under a complex correlation structure. Our
simulation studies show that the proposed statistics result in much better
powers than existing statistics. We applied the proposed statistics to an
endometriosis diagnosis study.
"
"  We propose a statistical modeling technique, called the Hierarchical
Association Rule Model (HARM), that predicts a patient's possible future
medical conditions given the patient's current and past history of reported
conditions. The core of our technique is a Bayesian hierarchical model for
selecting predictive association rules (such as ""condition 1 and condition 2
$\rightarrow$ condition 3"") from a large set of candidate rules. Because this
method ""borrows strength"" using the conditions of many similar patients, it is
able to provide predictions specialized to any given patient, even when little
information about the patient's history of conditions is available.
"
"  Hyperplane hashing aims at rapidly searching nearest points to a hyperplane,
and has shown practical impact in scaling up active learning with SVMs.
Unfortunately, the existing randomized methods need long hash codes to achieve
reasonable search accuracy and thus suffer from reduced search speed and large
memory overhead. To this end, this paper proposes a novel hyperplane hashing
technique which yields compact hash codes. The key idea is the bilinear form of
the proposed hash functions, which leads to higher collision probability than
the existing hyperplane hash functions when using random projections. To
further increase the performance, we propose a learning based framework in
which the bilinear functions are directly learned from the data. This results
in short yet discriminative codes, and also boosts the search performance over
the random projection based solutions. Large-scale active learning experiments
carried out on two datasets with up to one million samples demonstrate the
overall superiority of the proposed approach.
"
"  This contribution provides a strategy for studying and modelling the
deforestation and soil deterioration in the natural forest reserve of Peten,
Guatemala, using a poor spatial database. A Multispectral Image Processing of
Spot and TM Landsat data permits to understand the behaviour of the past land
cover dynamics; a multi-temporal analysis of Normalized Difference Vegetation
and Hydric Stress index, most informative RGB (according to statistical
criteria) and Principal Components, points out the importance and the direction
of environmental impacts. We gain from the Remote Sensing images new
environmental criteria (distance from roads, oil pipe-line, DEM, etc.) which
influence the spatial allocation of predicted land cover probabilities. We are
comparing the results of different prospective approaches (Markov Chains, Multi
Criteria Evaluation and Cellular Automata; Neural Networks) analysing the
residues for improving the final model of future deforestation risk.
"
"  Standard maximum likelihood estimation cannot be applied to discrete
energy-based models in the general case because the computation of exact model
probabilities is intractable. Recent research has seen the proposal of several
new estimators designed specifically to overcome this intractability, but
virtually nothing is known about their theoretical properties. In this paper,
we present a generalized estimator that unifies many of the classical and
recently proposed estimators. We use results from the standard asymptotic
theory for M-estimators to derive a generic expression for the asymptotic
covariance matrix of our generalized estimator. We apply these results to study
the relative statistical efficiency of classical pseudolikelihood and the
recently-proposed ratio matching estimator.
"
"  We consider the problem of localizing wireless devices in an ad-hoc network
embedded in a d-dimensional Euclidean space. Obtaining a good estimation of
where wireless devices are located is crucial in wireless network applications
including environment monitoring, geographic routing and topology control. When
the positions of the devices are unknown and only local distance information is
given, we need to infer the positions from these local distance measurements.
This problem is particularly challenging when we only have access to
measurements that have limited accuracy and are incomplete. We consider the
extreme case of this limitation on the available information, namely only the
connectivity information is available, i.e., we only know whether a pair of
nodes is within a fixed detection range of each other or not, and no
information is known about how far apart they are. Further, to account for
detection failures, we assume that even if a pair of devices is within the
detection range, it fails to detect the presence of one another with some
probability and this probability of failure depends on how far apart those
devices are. Given this limited information, we investigate the performance of
a centralized positioning algorithm MDS-MAP introduced by Shang et al., and a
distributed positioning algorithm, introduced by Savarese et al., called
HOP-TERRAIN. In particular, for a network consisting of n devices positioned
randomly, we provide a bound on the resulting error for both algorithms. We
show that the error is bounded, decreasing at a rate that is proportional to
R/Rc, where Rc is the critical detection range when the resulting random
network starts to be connected, and R is the detection range of each device.
"
"  We study mathematical models of the collaborative solving of a two-choice
discrimination task. We estimate the difference between the shared performance
for a group of n observers over a single person performance. Our paper is a
theoretical extension of the recent work of Bahrami et al. (2010) from a dyad
(a pair) to a group of n interacting minds. We analyze several models of
communication, decision-making and hierarchical information-aggregation.
  The maximal slope of psychometric function (closely related to the percentage
of right answers vs. easiness of the task) is a convenient parameter
characterizing performance. For every model we investigated, the group
performance turns out to be a product of two numbers: a scaling factor
depending of the group size and an average performance. The scaling factor is a
power function of the group size (with the exponent ranging from 0 to 1),
whereas the average is arithmetic mean, quadratic mean, or maximum of the
individual slopes. Moreover, voting can be almost as efficient as more
elaborate communication models, given the participants have similar individual
performances.
"
"  Substantial statistical research has recently been devoted to the analysis of
large-scale microarray experiments which provide a measure of the simultaneous
expression of thousands of genes in a particular condition. A typical goal is
the comparison of gene expression between two conditions (e.g., diseased vs.
nondiseased) to detect genes which show differential expression. Classical
hypothesis testing procedures have been applied to this problem and more recent
work has employed sophisticated models that allow for the sharing of
information across genes. However, many recent gene expression studies have an
experimental design with several conditions that requires an even more involved
hypothesis testing approach. In this paper, we use a hierarchical Bayesian
model to address the situation where there are many hypotheses that must be
simultaneously tested for each gene. In addition to having many hypotheses
within each gene, our analysis also addresses the more typical multiple
comparison issue of testing many genes simultaneously. We illustrate our
approach with an application to a study of genes involved in obstructive sleep
apnea in humans.
"
"  The main phases of applied statistical work are discussed in general terms.
The account starts with the clarification of objectives and proceeds through
study design, measurement and analysis to interpretation. An attempt is made to
extract some general notions.
"
"  Pac-Bayes bounds are among the most accurate generalization bounds for
classifiers learned from independently and identically distributed (IID) data,
and it is particularly so for margin classifiers: there have been recent
contributions showing how practical these bounds can be either to perform model
selection (Ambroladze et al., 2007) or even to directly guide the learning of
linear classifiers (Germain et al., 2009). However, there are many practical
situations where the training data show some dependencies and where the
traditional IID assumption does not hold. Stating generalization bounds for
such frameworks is therefore of the utmost interest, both from theoretical and
practical standpoints. In this work, we propose the first - to the best of our
knowledge - Pac-Bayes generalization bounds for classifiers trained on data
exhibiting interdependencies. The approach undertaken to establish our results
is based on the decomposition of a so-called dependency graph that encodes the
dependencies within the data, in sets of independent data, thanks to graph
fractional covers. Our bounds are very general, since being able to find an
upper bound on the fractional chromatic number of the dependency graph is
sufficient to get new Pac-Bayes bounds for specific settings. We show how our
results can be used to derive bounds for ranking statistics (such as Auc) and
classifiers trained on data distributed according to a stationary {\ss}-mixing
process. In the way, we show how our approach seemlessly allows us to deal with
U-processes. As a side note, we also provide a Pac-Bayes generalization bound
for classifiers learned on data from stationary $\varphi$-mixing distributions.
"
"  Optimal design under uncertainty has gained much attention in the past ten
years due to the ever increasing need for manufacturers to build robust systems
at the lowest cost. Reliability-based design optimization (RBDO) allows the
analyst to minimize some cost function while ensuring some minimal performances
cast as admissible failure probabilities for a set of performance functions. In
order to address real-world engineering problems in which the performance is
assessed through computational models (e.g., finite element models in
structural mechanics) metamodeling techniques have been developed in the past
decade. This paper introduces adaptive Kriging surrogate models to solve the
RBDO problem. The latter is cast in an augmented space that ""sums up"" the range
of the design space and the aleatory uncertainty in the design parameters and
the environmental conditions. The surrogate model is used (i) for evaluating
robust estimates of the failure probabilities (and for enhancing the
computational experimental design by adaptive sampling) in order to achieve the
requested accuracy and (ii) for applying a gradient-based optimization
algorithm to get optimal values of the design parameters. The approach is
applied to the optimal design of ring-stiffened cylindrical shells used in
submarine engineering under uncertain geometric imperfections. For this
application the performance of the structure is related to buckling which is
addressed here by means of a finite element solution based on the asymptotic
numerical method.
"
"  AUC (area under ROC curve) is an important evaluation criterion, which has
been popularly used in many learning tasks such as class-imbalance learning,
cost-sensitive learning, learning to rank, etc. Many learning approaches try to
optimize AUC, while owing to the non-convexity and discontinuousness of AUC,
almost all approaches work with surrogate loss functions. Thus, the consistency
of AUC is crucial; however, it has been almost untouched before. In this paper,
we provide a sufficient condition for the asymptotic consistency of learning
approaches based on surrogate loss functions. Based on this result, we prove
that exponential loss and logistic loss are consistent with AUC, but hinge loss
is inconsistent. Then, we derive the $q$-norm hinge loss and general hinge loss
that are consistent with AUC. We also derive the consistent bounds for
exponential loss and logistic loss, and obtain the consistent bounds for many
surrogate loss functions under the non-noise setting. Further, we disclose an
equivalence between the exponential surrogate loss of AUC and exponential
surrogate loss of accuracy, and one straightforward consequence of such finding
is that AdaBoost and RankBoost are equivalent.
"
"  Sparse PCA provides a linear combination of small number of features that
maximizes variance across data. Although Sparse PCA has apparent advantages
compared to PCA, such as better interpretability, it is generally thought to be
computationally much more expensive. In this paper, we demonstrate the
surprising fact that sparse PCA can be easier than PCA in practice, and that it
can be reliably applied to very large data sets. This comes from a rigorous
feature elimination pre-processing result, coupled with the favorable fact that
features in real-life data typically have exponentially decreasing variances,
which allows for many features to be eliminated. We introduce a fast block
coordinate ascent algorithm with much better computational complexity than the
existing first-order ones. We provide experimental results obtained on text
corpora involving millions of documents and hundreds of thousands of features.
These results illustrate how Sparse PCA can help organize a large corpus of
text data in a user-interpretable way, providing an attractive alternative
approach to topic models.
"
"  A dynamic coupled modelling is investigated to take temperature into account
in the individual energy consumption forecasting. The objective is both to
avoid the inherent complexity of exhaustive SARIMAX models and to take
advantage of the usual linear relation between energy consumption and
temperature for thermosensitive customers. We first recall some issues related
to individual load curves forecasting. Then, we propose and study the
properties of a dynamic coupled modelling taking temperature into account as an
exogenous contribution and its application to the intraday prediction of energy
consumption. Finally, these theoretical results are illustrated on a real
individual load curve. The authors discuss the relevance of such an approach
and anticipate that it could form a substantial alternative to the commonly
used methods for energy consumption forecasting of individual customers.
"
"  Counting and classifying blood cells is an important diagnostic tool in
medicine. Support Vector Machines are increasingly popular and efficient and
could replace artificial neural network systems. Here a method to classify
blood cells is proposed using SVM. A set of statistics on images are
implemented in C++. The MPEG-7 descriptors Scalable Color Descriptor, Color
Structure Descriptor, Color Layout Descriptor and Homogeneous Texture
Descriptor are extended in size and combined with textural features
corresponding to textural properties perceived visually by humans. From a set
of images of human blood cells these statistics are collected. A SVM is
implemented and trained to classify the cell images. The cell images come from
a CellaVision DM-96 machine which classify cells from images from microscopy.
The output images and classification of the CellaVision machine is taken as
ground truth, a truth that is 90-95% correct. The problem is divided in two --
the primary and the simplified. The primary problem is to classify the same
classes as the CellaVision machine. The simplified problem is to differ between
the five most common types of white blood cells. An encouraging result is
achieved in both cases -- error rates of 10.8% and 3.1% -- considering that the
SVM is misled by the errors in ground truth. Conclusion is that further
investigation of performance is worthwhile.
"
"  I report the results of the internet quiz, where the takers had to tell the
music of Mozart from that of Salieri. The average score earned by over eleven
thousand quiz-takers is 61%. This suggests that the music of Mozart is of about
the same quality as the music of Salieri.
"
"  We present a stochastic setting for optimization problems with nonsmooth
convex separable objective functions over linear equality constraints. To solve
such problems, we propose a stochastic Alternating Direction Method of
Multipliers (ADMM) algorithm. Our algorithm applies to a more general class of
nonsmooth convex functions that does not necessarily have a closed-form
solution by minimizing the augmented function directly. We also demonstrate the
rates of convergence for our algorithm under various structural assumptions of
the stochastic functions: $O(1/\sqrt{t})$ for convex functions and $O(\log
t/t)$ for strongly convex functions. Compared to previous literature, we
establish the convergence rate of ADMM algorithm, for the first time, in terms
of both the objective value and the feasibility violation.
"
"  In this paper we address the following question: Can we approximately sample
from a Bayesian posterior distribution if we are only allowed to touch a small
mini-batch of data-items for every sample we generate?. An algorithm based on
the Langevin equation with stochastic gradients (SGLD) was previously proposed
to solve this, but its mixing rate was slow. By leveraging the Bayesian Central
Limit Theorem, we extend the SGLD algorithm so that at high mixing rates it
will sample from a normal approximation of the posterior, while for slow mixing
rates it will mimic the behavior of SGLD with a pre-conditioner matrix. As a
bonus, the proposed algorithm is reminiscent of Fisher scoring (with stochastic
gradients) and as such an efficient optimizer during burn-in.
"
"  Most existing approaches to clustering gene expression time course data treat
the different time points as independent dimensions and are invariant to
permutations, such as reversal, of the experimental time course. Approaches
utilizing HMMs have been shown to be helpful in this regard, but are hampered
by having to choose model architectures with appropriate complexities. Here we
propose for a clustering application an HMM with a countably infinite state
space; inference in this model is possible by recasting it in the hierarchical
Dirichlet process (HDP) framework (Teh et al. 2006), and hence we call it the
HDP-HMM. We show that the infinite model outperforms model selection methods
over finite models, and traditional time-independent methods, as measured by a
variety of external and internal indices for clustering on two large publicly
available data sets. Moreover, we show that the infinite models utilize more
hidden states and employ richer architectures (e.g. state-to-state transitions)
without the damaging effects of overfitting.
"
"  In the context of clustering, we assume a generative model where each cluster
is the result of sampling points in the neighborhood of an embedded smooth
surface; the sample may be contaminated with outliers, which are modeled as
points sampled in space away from the clusters. We consider a prototype for a
higher-order spectral clustering method based on the residual from a local
linear approximation. We obtain theoretical guarantees for this algorithm and
show that, in terms of both separation and robustness to outliers, it
outperforms the standard spectral clustering algorithm (based on pairwise
distances) of Ng, Jordan and Weiss (NIPS '01). The optimal choice for some of
the tuning parameters depends on the dimension and thickness of the clusters.
We provide estimators that come close enough for our theoretical purposes. We
also discuss the cases of clusters of mixed dimensions and of clusters that are
generated from smoother surfaces. In our experiments, this algorithm is shown
to outperform pairwise spectral clustering on both simulated and real data.
"
"  Latent Dirichlet analysis, or topic modeling, is a flexible latent variable
framework for modeling high-dimensional sparse count data. Various learning
algorithms have been developed in recent years, including collapsed Gibbs
sampling, variational inference, and maximum a posteriori estimation, and this
variety motivates the need for careful empirical comparisons. In this paper, we
highlight the close connections between these approaches. We find that the main
differences are attributable to the amount of smoothing applied to the counts.
When the hyperparameters are optimized, the differences in performance among
the algorithms diminish significantly. The ability of these algorithms to
achieve solutions of comparable accuracy gives us the freedom to select
computationally efficient approaches. Using the insights gained from this
comparative study, we show how accurate topic models can be learned in several
seconds on text corpora with thousands of documents.
"
"  In this paper we adapt online estimation strategies to perform model-based
clustering on large networks. Our work focuses on two algorithms, the first
based on the SAEM algorithm, and the second on variational methods. These two
strategies are compared with existing approaches on simulated and real data. We
use the method to decipher the connexion structure of the political websphere
during the US political campaign in 2008. We show that our online EM-based
algorithms offer a good trade-off between precision and speed, when estimating
parameters for mixture distributions in the context of random graphs.
"
"  Equivalence testing is of emerging importance in genomics studies but has
hitherto been little studied in this content. In this paper, we define the
notion of equivalence of gene expression and determine a `strength of evidence'
measure for gene equivalence. It is common practice in genome-wide studies to
rank genes according to observed gene-specific P-values or adjusted P-values,
which are assumed to measure the strength of evidence against the null
hypothesis of no differential gene expression. We show here, both empirically
and formally, that the equivalence P-value does not satisfy the basic
consistency requirements for a valid strength of evidence measure for
equivalence. This means that the widely-used q-value (Storey, 2002) defined for
each gene to be the minimum positive false discovery rate that would result in
the inclusion of the corresponding P-value in the discovery set, cannot be
translated to the equivalence testing framework. However, when represented as a
posterior probability, we find that the q-value does satisfy some basic
consistency requirements needed to be a credible measure of evidence for
equivalence. We propose a simple estimate for the q-value from posterior
probabilities of equivalence, and analyse data from a mouse stem cell
microarray experiment which demonstrate the theory and methods presented here.
"
"  Emotional disorders and psychological flourishing are the result of complex
interactions between positive and negative affects that depend on external
events and the subject's internal representations. Based on psychological data,
we mathematically model the dynamical balance between positive and negative
affects as a function of the response to external positive and negative events.
This modeling allows the investigation of the relative impact of two leading
forms of therapy on affect balance. The model uses a delay differential
equation to analytically study the complete bifurcation diagram of the system.
We compare the results of the model to psychological data on a single,
recurrently depressed patient that was administered the two types of therapies
considered (viz., coping-focused vs. affect-focused). The model leads to the
prediction that stabilization at a normal state may rely on evaluating one's
emotional state through an historical ongoing emotional state rather than in a
narrow present window. The simple mathematical model proposed here offers a
theoretically grounded quantitative framework for investigating the temporal
process of change and parameters of resilience to relapse.
"
"  Discussion of ""Latent variable graphical model selection via convex
optimization"" by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky
[arXiv:1008.1290].
"
"  AdaBoost is one of the most popular ML algorithms. It is simple to implement
and often found very effective by practitioners, while still being
mathematically elegant and theoretically sound. AdaBoost's interesting behavior
in practice still puzzles the ML community. We address the algorithm's
stability and establish multiple convergence properties of ""Optimal AdaBoost,""
a term coined by Rudin, Daubechies, and Schapire in 2004. We prove, in a
reasonably strong computational sense, the almost universal existence of time
averages, and with that, the convergence of the classifier itself, its
generalization error, and its resulting margins, among many other objects, for
fixed data sets under arguably reasonable conditions. Specifically, we frame
Optimal AdaBoost as a dynamical system and, employing tools from ergodic
theory, prove that, under a condition that Optimal AdaBoost does not have ties
for best weak classifier eventually, a condition for which we provide empirical
evidence from high dimensional real-world datasets, the algorithm's update
behaves like a continuous map. We provide constructive proofs of several
arbitrarily accurate approximations of Optimal AdaBoost; prove that they
exhibit certain cycling behavior in finite time, and that the resulting
dynamical system is ergodic; and establish sufficient conditions for the same
to hold for the actual Optimal-AdaBoost update. We believe that our results
provide reasonably strong evidence for the affirmative answer to two open
conjectures, at least from a broad computational-theory perspective: AdaBoost
always cycles and is an ergodic dynamical system. We present empirical evidence
that cycles are hard to detect while time averages stabilize quickly. Our
results ground future convergence-rate analysis and may help optimize
generalization ability and alleviate a practitioner's burden of deciding how
long to run the algorithm.
"
"  In the conviction of Lucia de Berk an important role was played by a simple
hypergeometric model, used by the expert consulted by the court, which produced
very small probabilities of occurrences of certain numbers of incidents. We
want to draw attention to the fact that, if we take into account the variation
among nurses in incidents they experience during their shifts, these
probabilities can become considerably larger. This points to the danger of
using an oversimplified discrete probability model in these circumstances.
"
"  We introduce a new regression framework, Gaussian process regression networks
(GPRN), which combines the structural properties of Bayesian neural networks
with the non-parametric flexibility of Gaussian processes. This model
accommodates input dependent signal and noise correlations between multiple
response variables, input dependent length-scales and amplitudes, and
heavy-tailed predictive distributions. We derive both efficient Markov chain
Monte Carlo and variational Bayes inference procedures for this model. We apply
GPRN as a multiple output regression and multivariate volatility model,
demonstrating substantially improved performance over eight popular multiple
output (multi-task) Gaussian process models and three multivariate volatility
models on benchmark datasets, including a 1000 dimensional gene expression
dataset.
"
"  We present an alternating augmented Lagrangian method for convex optimization
problems where the cost function is the sum of two terms, one that is separable
in the variable blocks, and a second that is separable in the difference
between consecutive variable blocks. Examples of such problems include Fused
Lasso estimation, total variation denoising, and multi-period portfolio
optimization with transaction costs. In each iteration of our method, the first
step involves separately optimizing over each variable block, which can be
carried out in parallel. The second step is not separable in the variables, but
can be carried out very efficiently. We apply the algorithm to segmentation of
data based on changes inmean (l_1 mean filtering) or changes in variance (l_1
variance filtering). In a numerical example, we show that our implementation is
around 10000 times faster compared with the generic optimization solver SDPT3.
"
"  Automatic classification of scientific articles based on common
characteristics is an interesting problem with many applications in digital
library and information retrieval systems. Properly organized articles can be
useful for automatic generation of taxonomies in scientific writings, textual
summarization, efficient information retrieval etc. Generating article bundles
from a large number of input articles, based on the associated features of the
articles is tedious and computationally expensive task. In this report we
propose an automatic two-step approach for topic extraction and bundling of
related articles from a set of scientific articles in real-time. For topic
extraction, we make use of Latent Dirichlet Allocation (LDA) topic modeling
techniques and for bundling, we make use of hierarchical agglomerative
clustering techniques.
  We run experiments to validate our bundling semantics and compare it with
existing models in use. We make use of an online crowdsourcing marketplace
provided by Amazon called Amazon Mechanical Turk to carry out experiments. We
explain our experimental setup and empirical results in detail and show that
our method is advantageous over existing ones.
"
"  We introduce the Pitman Yor Diffusion Tree (PYDT) for hierarchical
clustering, a generalization of the Dirichlet Diffusion Tree (Neal, 2001) which
removes the restriction to binary branching structure. The generative process
is described and shown to result in an exchangeable distribution over data
points. We prove some theoretical properties of the model and then present two
inference methods: a collapsed MCMC sampler which allows us to model
uncertainty over tree structures, and a computationally efficient greedy
Bayesian EM search algorithm. Both algorithms use message passing on the tree
structure. The utility of the model and algorithms is demonstrated on synthetic
and real world data, both continuous and binary.
"
"  This paper studies convergence behavior of latent mixing measures that arise
in finite and infinite mixture models, using transportation distances (i.e.,
Wasserstein metrics). The relationship between Wasserstein distances on the
space of mixing measures and f-divergence functionals such as Hellinger and
Kullback-Leibler distances on the space of mixture distributions is
investigated in detail using various identifiability conditions. Convergence in
Wasserstein metrics for discrete measures implies convergence of individual
atoms that provide support for the measures, thereby providing a natural
interpretation of convergence of clusters in clustering applications where
mixture models are typically employed. Convergence rates of posterior
distributions for latent mixing measures are established, for both finite
mixtures of multivariate distributions and infinite mixtures based on the
Dirichlet process.
"
"  Bell's [Physics 1 (1964) 195-200] theorem is popularly supposed to establish
the nonlocality of quantum physics. Violation of Bell's inequality in
experiments such as that of Aspect, Dalibard and Roger [Phys. Rev. Lett. 49
(1982) 1804-1807] provides empirical proof of nonlocality in the real world.
This paper reviews recent work on Bell's theorem, linking it to issues in
causality as understood by statisticians. The paper starts with a proof of a
strong, finite sample, version of Bell's inequality and thereby also of Bell's
theorem, which states that quantum theory is incompatible with the conjunction
of three formerly uncontroversial physical principles, here referred to as
locality, realism and freedom. Locality is the principle that the direction of
causality matches the direction of time, and that causal influences need time
to propagate spatially. Realism and freedom are directly connected to
statistical thinking on causality: they relate to counterfactual reasoning, and
to randomisation, respectively. Experimental loopholes in state-of-the-art Bell
type experiments are related to statistical issues of post-selection in
observational studies, and the missing at random assumption. They can be
avoided by properly matching the statistical analysis to the actual
experimental design, instead of by making untestable assumptions of
independence between observed and unobserved variables. Methodological and
statistical issues in the design of quantum Randi challenges (QRC) are
discussed. The paper argues that Bell's theorem (and its experimental
confirmation) should lead us to relinquish not locality, but realism.
"
"  Most network-based protein (or gene) function prediction methods are based on
the assumption that the labels of two adjacent proteins in the network are
likely to be the same. However, assuming the pairwise relationship between
proteins or genes is not complete, the information a group of genes that show
very similar patterns of expression and tend to have similar functions (i.e.
the functional modules) is missed. The natural way overcoming the information
loss of the above assumption is to represent the gene expression data as the
hypergraph. Thus, in this paper, the three un-normalized, random walk, and
symmetric normalized hypergraph Laplacian based semi-supervised learning
methods applied to hypergraph constructed from the gene expression data in
order to predict the functions of yeast proteins are introduced. Experiment
results show that the average accuracy performance measures of these three
hypergraph Laplacian based semi-supervised learning methods are the same.
However, their average accuracy performance measures of these three methods are
much greater than the average accuracy performance measures of un-normalized
graph Laplacian based semi-supervised learning method (i.e. the baseline method
of this paper) applied to gene co-expression network created from the gene
expression data.
"
"  A scattering transform defines a signal representation which is invariant to
translations and Lipschitz continuous relatively to deformations. It is
implemented with a non-linear convolution network that iterates over wavelet
and modulus operators. Lipschitz continuity locally linearizes deformations.
Complex classes of signals and textures can be modeled with low-dimensional
affine spaces, computed with a PCA in the scattering domain. Classification is
performed with a penalized model selection. State of the art results are
obtained for handwritten digit recognition over small training sets, and for
texture classification.
"
"  Directed acyclic graphs are the basic representation of the structure
underlying Bayesian networks, which represent multivariate probability
distributions. In many practical applications, such as the reverse engineering
of gene regulatory networks, not only the estimation of model parameters but
the reconstruction of the structure itself is of great interest. As well as for
the assessment of different structure learning algorithms in simulation
studies, a uniform sample from the space of directed acyclic graphs is required
to evaluate the prevalence of certain structural features. Here we analyse how
to sample acyclic digraphs uniformly at random through recursive enumeration,
an approach previously thought too computationally involved. Based on
complexity considerations, we discuss in particular how the enumeration
directly provides an exact method, which avoids the convergence issues of the
alternative Markov chain methods and is actually computationally much faster.
The limiting behaviour of the distribution of acyclic digraphs then allows us
to sample arbitrarily large graphs. Building on the ideas of recursive
enumeration based sampling we also introduce a novel hybrid Markov chain with
much faster convergence than current alternatives while still being easy to
adapt to various restrictions. Finally we discuss how to include such
restrictions in the combinatorial enumeration and the new hybrid Markov chain
method for efficient uniform sampling of the corresponding graphs.
"
"  In binary classification problems, mainly two approaches have been proposed;
one is loss function approach and the other is uncertainty set approach. The
loss function approach is applied to major learning algorithms such as support
vector machine (SVM) and boosting methods. The loss function represents the
penalty of the decision function on the training samples. In the learning
algorithm, the empirical mean of the loss function is minimized to obtain the
classifier. Against a backdrop of the development of mathematical programming,
nowadays learning algorithms based on loss functions are widely applied to
real-world data analysis. In addition, statistical properties of such learning
algorithms are well-understood based on a lots of theoretical works. On the
other hand, the learning method using the so-called uncertainty set is used in
hard-margin SVM, mini-max probability machine (MPM) and maximum margin MPM. In
the learning algorithm, firstly, the uncertainty set is defined for each binary
label based on the training samples. Then, the best separating hyperplane
between the two uncertainty sets is employed as the decision function. This is
regarded as an extension of the maximum-margin approach. The uncertainty set
approach has been studied as an application of robust optimization in the field
of mathematical programming. The statistical properties of learning algorithms
with uncertainty sets have not been intensively studied. In this paper, we
consider the relation between the above two approaches. We point out that the
uncertainty set is described by using the level set of the conjugate of the
loss function. Based on such relation, we study statistical properties of
learning algorithms using uncertainty sets.
"
"  Traditional approaches to ranking in web search follow the paradigm of
rank-by-score: a learned function gives each query-URL combination an absolute
score and URLs are ranked according to this score. This paradigm ensures that
if the score of one URL is better than another then one will always be ranked
higher than the other. Scoring contradicts prior work in behavioral economics
that showed that users' preferences between two items depend not only on the
items but also on the presented alternatives. Thus, for the same query, users'
preference between items A and B depends on the presence/absence of item C. We
propose a new model of ranking, the Random Shopper Model, that allows and
explains such behavior. In this model, each feature is viewed as a Markov chain
over the items to be ranked, and the goal is to find a weighting of the
features that best reflects their importance. We show that our model can be
learned under the empirical risk minimization framework, and give an efficient
learning algorithm. Experiments on commerce search logs demonstrate that our
algorithm outperforms scoring-based approaches including regression and
listwise ranking.
"
"  From the perspective of the network theory, the present work illustrates how
the parametric intrinsic geometric description exhibits an exact set of pair
correction functions and global correlation volume with and without the
inclusion of the imaginary power flow. The Gaussian fluctuations about the
equilibrium basis accomplish a well-defined, non-degenerate, curved regular
intrinsic Riemannian surfaces for the purely real and the purely imaginary
power flows and their linear combinations. An explicit computation demonstrates
that the underlying real and imaginary power correlations involve ordinary
summations of the power factors, with and without their joint effects. Novel
aspect of the intrinsic geometry constitutes a stable design for the power
systems.
"
"  The relationship between short-term exposure to air pollution and mortality
or morbidity has been the subject of much recent research, in which the
standard method of analysis uses Poisson linear or additive models. In this
paper we use a Bayesian dynamic generalised linear model (DGLM) to estimate
this relationship, which allows the standard linear or additive model to be
extended in two ways: (i) the long-term trend and temporal correlation present
in the health data can be modelled by an autoregressive process rather than a
smooth function of calendar time; (ii) the effects of air pollution are allowed
to evolve over time. The efficacy of these two extensions are investigated by
applying a series of dynamic and non-dynamic models to air pollution and
mortality data from Greater London. A Bayesian approach is taken throughout,
and a Markov chain monte carlo simulation algorithm is presented for inference.
An alternative likelihood based analysis is also presented, in order to allow a
direct comparison with the only previous analysis of air pollution and health
data using a DGLM.
"
"  Structured additive regression provides a general framework for complex
Gaussian and non-Gaussian regression models, with predictors comprising
arbitrary combinations of nonlinear functions and surfaces, spatial effects,
varying coefficients, random effects and further regression terms. The large
flexibility of structured additive regression makes function selection a
challenging and important task, aiming at (1) selecting the relevant
covariates, (2) choosing an appropriate and parsimonious representation of the
impact of covariates on the predictor and (3) determining the required
interactions. We propose a spike-and-slab prior structure for function
selection that allows to include or exclude single coefficients as well as
blocks of coefficients representing specific model terms. A novel
multiplicative parameter expansion is required to obtain good mixing and
convergence properties in a Markov chain Monte Carlo simulation approach and is
shown to induce desirable shrinkage properties. In simulation studies and with
(real) benchmark classification data, we investigate sensitivity to
hyperparameter settings and compare performance to competitors. The flexibility
and applicability of our approach are demonstrated in an additive piecewise
exponential model with time-varying effects for right-censored survival times
of intensive care patients with sepsis. Geoadditive and additive mixed logit
model applications are discussed in an extensive appendix.
"
"  Linear and Quadratic Discriminant analysis (LDA/QDA) are common tools for
classification problems. For these methods we assume observations are normally
distributed within group. We estimate a mean and covariance matrix for each
group and classify using Bayes theorem. With LDA, we estimate a single, pooled
covariance matrix, while for QDA we estimate a separate covariance matrix for
each group. Rarely do we believe in a homogeneous covariance structure between
groups, but often there is insufficient data to separately estimate covariance
matrices. We propose L1- PDA, a regularized model which adaptively pools
elements of the precision matrices. Adaptively pooling these matrices decreases
the variance of our estimates (as in LDA), without overly biasing them. In this
paper, we propose and discuss this method, give an efficient algorithm to fit
it for moderate sized problems, and show its efficacy on real and simulated
datasets.
"
"  Conditional density estimation generalizes regression by modeling a full
density f(yjx) rather than only the expected value E(yjx). This is important
for many tasks, including handling multi-modality and generating prediction
intervals. Though fundamental and widely applicable, nonparametric conditional
density estimators have received relatively little attention from statisticians
and little or none from the machine learning community. None of that work has
been applied to greater than bivariate data, presumably due to the
computational difficulty of data-driven bandwidth selection. We describe the
double kernel conditional density estimator and derive fast dual-tree-based
algorithms for bandwidth selection using a maximum likelihood criterion. These
techniques give speedups of up to 3.8 million in our experiments, and enable
the first applications to previously intractable large multivariate datasets,
including a redshift prediction problem from the Sloan Digital Sky Survey.
"
"  Latent feature models are attractive for image modeling, since images
generally contain multiple objects. However, many latent feature models ignore
that objects can appear at different locations or require pre-segmentation of
images. While the transformed Indian buffet process (tIBP) provides a method
for modeling transformation-invariant features in unsegmented binary images,
its current form is inappropriate for real images because of its computational
cost and modeling assumptions. We combine the tIBP with likelihoods appropriate
for real images and develop an efficient inference, using the cross-correlation
between images and features, that is theoretically and empirically faster than
existing inference techniques. Our method discovers reasonable components and
achieve effective image reconstruction in natural images.
"
"  We derive and analyze a new, efficient, pool-based active learning algorithm
for halfspaces, called ALuMA. Most previous algorithms show exponential
improvement in the label complexity assuming that the distribution over the
instance space is close to uniform. This assumption rarely holds in practical
applications. Instead, we study the label complexity under a large-margin
assumption -- a much more realistic condition, as evident by the success of
margin-based algorithms such as SVM. Our algorithm is computationally efficient
and comes with formal guarantees on its label complexity. It also naturally
extends to the non-separable case and to non-linear kernels. Experiments
illustrate the clear advantage of ALuMA over other active learning algorithms.
"
"  We present a new anytime algorithm that achieves near-optimal regret for any
instance of finite stochastic partial monitoring. In particular, the new
algorithm achieves the minimax regret, within logarithmic factors, for both
""easy"" and ""hard"" problems. For easy problems, it additionally achieves
logarithmic individual regret. Most importantly, the algorithm is adaptive in
the sense that if the opponent strategy is in an ""easy region"" of the strategy
space then the regret grows as if the problem was easy. As an implication, we
show that under some reasonable additional assumptions, the algorithm enjoys an
O(\sqrt{T}) regret in Dynamic Pricing, proven to be hard by Bartok et al.
(2011).
"
"  We present a non-parametric Bayesian approach to structure learning with
hidden causes. Previous Bayesian treatments of this problem define a prior over
the number of hidden causes and use algorithms such as reversible jump Markov
chain Monte Carlo to move between solutions. In contrast, we assume that the
number of hidden causes is unbounded, but only a finite number influence
observable variables. This makes it possible to use a Gibbs sampler to
approximate the distribution over causal structures. We evaluate the
performance of both approaches in discovering hidden causes in simulated data,
and use our non-parametric approach to discover hidden causes in a real medical
dataset.
"
"  In this paper we provide an alternative approach to the works of the
physicists S. Cocco and R. Monasson about a model of DNA molecules. The aim is
to predict the sequence of bases by mechanical stimulations. The model
described by the physicists is a stopped birth and death process with unknown
transition probabilities. We consider two models, a discrete in time and a
continuous in time, as general as possible. We show that explicit formula can
be obtained for the probability to be wrong for a given estimator, and apply it
to evaluate the quality of the prediction. Also we add some generalizations
comparing to the initial model allowing us to answer some questions asked by
the physicists.
"
"  Given a time series of graphs G(t) = (V, E(t)), t = 1, 2, ..., where the
fixed vertex set V represents ""actors"" and an edge between vertex u and vertex
v at time t (uv \in E(t)) represents the existence of a communications event
between actors u and v during the tth time period, we wish to detect anomalies
and/or change points. We consider a collection of graph features, or
invariants, and demonstrate that adaptive fusion provides superior inferential
efficacy compared to naive equal weighting for a certain class of anomaly
detection problems. Simulation results using a latent process model for time
series of graphs, as well as illustrative experimental results for a time
series of graphs derived from the Enron email data, show that a fusion
statistic can provide superior inference compared to individual invariants
alone. These results also demonstrate that an adaptive weighting scheme for
fusion of invariants performs better than naive equal weighting.
"
"  Recent breakthrough results in compressed sensing (CS) have established that
many high dimensional objects can be accurately recovered from a relatively
small number of non- adaptive linear projection observations, provided that the
objects possess a sparse representation in some basis. Subsequent efforts have
shown that the performance of CS can be improved by exploiting the structure in
the location of the non-zero signal coefficients (structured sparsity) or using
some form of online measurement focusing (adaptivity) in the sensing process.
In this paper we examine a powerful hybrid of these two techniques. First, we
describe a simple adaptive sensing procedure and show that it is a provably
effective method for acquiring sparse signals that exhibit structured sparsity
characterized by tree-based coefficient dependencies. Next, employing
techniques from sparse hierarchical dictionary learning, we show that
representations exhibiting the appropriate form of structured sparsity can be
learned from collections of training data. The combination of these techniques
results in an effective and efficient adaptive compressive acquisition
procedure.
"
"  Latent topic models have been successfully applied as an unsupervised topic
discovery technique in large document collections. With the proliferation of
hypertext document collection such as the Internet, there has also been great
interest in extending these approaches to hypertext [6, 9]. These approaches
typically model links in an analogous fashion to how they model words - the
document-link co-occurrence matrix is modeled in the same way that the
document-word co-occurrence matrix is modeled in standard topic models. In this
paper we present a probabilistic generative model for hypertext document
collections that explicitly models the generation of links. Specifically, links
from a word w to a document d depend directly on how frequent the topic of w is
in d, in addition to the in-degree of d. We show how to perform EM learning on
this model efficiently. By not modeling links as analogous to words, we end up
using far fewer free parameters and obtain better link prediction results.
"
"  We propose a distance between two realizations of a random process where for
each realization only sparse and irregularly spaced measurements with
additional measurement errors are available. Such data occur commonly in
longitudinal studies and online trading data. A distance measure then makes it
possible to apply distance-based analysis such as classification, clustering
and multidimensional scaling for irregularly sampled longitudinal data. Once a
suitable distance measure for sparsely sampled longitudinal trajectories has
been found, we apply distance-based clustering methods to eBay online auction
data. We identify six distinct clusters of bidding patterns. Each of these
bidding patterns is found to be associated with a specific chance to obtain the
auctioned item at a reasonable price.
"
"  Poisson regression is a popular tool for modeling count data and is applied
in a vast array of applications from the social to the physical sciences and
beyond. Real data, however, are often over- or under-dispersed and, thus, not
conducive to Poisson regression. We propose a regression model based on the
Conway--Maxwell-Poisson (COM-Poisson) distribution to address this problem. The
COM-Poisson regression generalizes the well-known Poisson and logistic
regression models, and is suitable for fitting count data with a wide range of
dispersion levels. With a GLM approach that takes advantage of exponential
family properties, we discuss model estimation, inference, diagnostics, and
interpretation, and present a test for determining the need for a COM-Poisson
regression over a standard Poisson regression. We compare the COM-Poisson to
several alternatives and illustrate its advantages and usefulness using three
data sets with varying dispersion.
"
"  In this paper, we clarify the relations between the existing sets of
regularity conditions for convergence rates of nonparametric indirect
regression (NPIR) and nonparametric instrumental variables (NPIV) regression
models. We establish minimax risk lower bounds in mean integrated squared error
loss for the NPIR and the NPIV models under two basic regularity conditions
that allow for both mildly ill-posed and severely ill-posed cases. We show that
both a simple projection estimator for the NPIR model, and a sieve minimum
distance estimator for the NPIV model, can achieve the minimax risk lower
bounds, and are rate-optimal uniformly over a large class of structure
functions, allowing for mildly ill-posed and severely ill-posed cases.
"
"  We propose a generalized double Pareto prior for Bayesian shrinkage
estimation and inferences in linear models. The prior can be obtained via a
scale mixture of Laplace or normal distributions, forming a bridge between the
Laplace and Normal-Jeffreys' priors. While it has a spike at zero like the
Laplace density, it also has a Student's $t$-like tail behavior. Bayesian
computation is straightforward via a simple Gibbs sampling algorithm. We
investigate the properties of the maximum a posteriori estimator, as sparse
estimation plays an important role in many problems, reveal connections with
some well-established regularization procedures, and show some asymptotic
results. The performance of the prior is tested through simulations and an
application.
"
"  In several modern applications, ranging from genetics to genomics and
neuroimaging, there is a need to compare observations across different
populations, such as groups of healthy and diseased individuals. The interest
is in detecting a group effect. When the observations are vectorial,
real-valued and follow a multivariate Normal distribution, multivariate
analysis of variance (MANOVA) tests are routinely applied. However, such
traditional procedures are not suitable when dealing with more complex data
structures such as functional (e.g. curves) or graph-structured (e.g. trees and
networks) objects, where the required distributional assumptions may be
violated. In this paper we discuss a distance-based MANOVA-like approach, the
DBF test, for detecting differences between groups for a wider range of data
types. The test statistic, analogously to other distance-based statistics, only
relies on a suitably chosen distance measure that captures the pairwise
dissimilarity among all available samples. An approximate null probability
distribution of the DBF statistic is proposed thus allowing inferences to be
drawn without the need for costly permutation procedures. Through extensive
simulations we provide evidence that the proposed methodology works well for a
range of data types and distances, and generalizes the traditional MANOVA
tests. We also report on an application of the proposed methodology for the
analysis of a multi-locus genome-wide association study of Alzheimer's disease,
which has been carried out using several genetic distance measures.
"
"  This article is a response to an off-the-record discussion that I had at an
international meeting of epidemiologists. It centered on a concern, perhaps
widely spread, that measurement error adjustment methods can induce positive
bias in results of epidemiological studies when there is no true association. I
trace the possible history of this supposition and test it in a simulation
study of both continuous and binary health outcomes under a classical
multiplicative measurement error model. A Bayesian measurement adjustment
method is used. The main conclusion is that adjustment for the presumed
measurement error does not 'induce' positive associations, especially if the
focus of the interpretation of the result is taken away from the point
estimate. This is in line with properties of earlier measurement error
adjustment methods introduced to epidemiologists in the 1990s. An heuristic
argument is provided to support the generalizability of this observation in the
Bayesian framework. I find that when there is no true association, positive
bias can only be induced by indefensible manipulation of the priors, such that
they dominate the data. The misconception about bias induced by measurement
error adjustment should be more clearly explained during the training of
epidemiologists to ensure the appropriate (and wider) use of measurement error
correction procedures. The simple message that can be derived from this paper
is: 'Do not focus on point estimates, but mind the gap between boundaries that
reflect variability in the estimate'. And of course: 'Treat measurement error
as a tractable problem that deserves much more attention than just a
qualitative (throw-away) discussion'.
"
"  In this paper, we propose three approaches for the estimation of the Tucker
decomposition of multi-way arrays (tensors) from partial observations. All
approaches are formulated as convex minimization problems. Therefore, the
minimum is guaranteed to be unique. The proposed approaches can automatically
estimate the number of factors (rank) through the optimization. Thus, there is
no need to specify the rank beforehand. The key technique we employ is the
trace norm regularization, which is a popular approach for the estimation of
low-rank matrices. In addition, we propose a simple heuristic to improve the
interpretability of the obtained factorization. The advantages and
disadvantages of three proposed approaches are demonstrated through numerical
experiments on both synthetic and real world datasets. We show that the
proposed convex optimization based approaches are more accurate in predictive
performance, faster, and more reliable in recovering a known multilinear
structure than conventional approaches.
"
"  We present a new Bayesian non-parametric deprojection algorithm DOPING
(Deprojection of Observed Photometry using and INverse Gambit), that is
designed to extract 3-D luminosity density distributions $\rho$ from observed
surface brightness maps $I$, in generalised geometries, while taking into
account changes in intrinsic shape with radius, using a penalised likelihood
approach and an MCMC optimiser. We provide the most likely solution to the
integral equation that represents deprojection of the measured $I$ to $\rho$.
In order to keep the solution modular, we choose to express $\rho$ as a
function of the line-of-sight (LOS) coordinate $z$. We calculate the extent of
the system along the ${\bf z}$-axis, for a given point on the image that lies
within an identified isophotal annulus. The extent along the LOS is binned and
density is held a constant over each such $z$-bin. The code begins with a seed
density and at the beginning of an iterative step, the trial $\rho$ is updated.
Comparison of the projection of the current choice of $\rho$ and the observed
$I$ defines the likelihood function (which is supplemented by Laplacian
regularisation), the maximal region of which is sought by the optimiser
(Metropolis Hastings). The algorithm is successfully tested on a set of test
galaxies, the morphology of which ranges from an elliptical galaxy with varying
eccentricity to an infinitesimally thin disk galaxy marked by an abruptly
varying eccentricity profile. Applications are made to faint dwarf elliptical
galaxy Ic~3019 and another dwarf elliptical that is characterised by a central
spheroidal nuclear component superimposed upon a more extended flattened
component. The result of deprojection of the X-ray image of triaxial cluster
A1413 is also presented.
"
"  Many problems in sequential decision making and stochastic control often have
natural multiscale structure: sub-tasks are assembled together to accomplish
complex goals. Systematically inferring and leveraging hierarchical structure,
particularly beyond a single level of abstraction, has remained a longstanding
challenge. We describe a fast multiscale procedure for repeatedly compressing,
or homogenizing, Markov decision processes (MDPs), wherein a hierarchy of
sub-problems at different scales is automatically determined. Coarsened MDPs
are themselves independent, deterministic MDPs, and may be solved using
existing algorithms. The multiscale representation delivered by this procedure
decouples sub-tasks from each other and can lead to substantial improvements in
convergence rates both locally within sub-problems and globally across
sub-problems, yielding significant computational savings. A second fundamental
aspect of this work is that these multiscale decompositions yield new transfer
opportunities across different problems, where solutions of sub-tasks at
different levels of the hierarchy may be amenable to transfer to new problems.
Localized transfer of policies and potential operators at arbitrary scales is
emphasized. Finally, we demonstrate compression and transfer in a collection of
illustrative domains, including examples involving discrete and continuous
statespaces.
"
"  A trend in all scientific disciplines, based on advances in technology, is
the increasing availability of high dimensional data in which are buried
important information. A current urgent challenge to statisticians is to
develop effective methods of finding the useful information from the vast
amounts of messy and noisy data available, most of which are noninformative.
This paper presents a general computer intensive approach, based on a method
pioneered by Lo and Zheng for detecting which, of many potential explanatory
variables, have an influence on a dependent variable $Y$. This approach is
suited to detect influential variables, where causal effects depend on the
confluence of values of several variables. It has the advantage of avoiding a
difficult direct analysis, involving possibly thousands of variables, by
dealing with many randomly selected small subsets from which smaller subsets
are selected, guided by a measure of influence $I$. The main objective is to
discover the influential variables, rather than to measure their effects. Once
they are detected, the problem of dealing with a much smaller group of
influential variables should be vulnerable to appropriate analysis. In a sense,
we are confining our attention to locating a few needles in a haystack.
"
"  We present a framework for online inference in the presence of a
nonexhaustively defined set of classes that incorporates supervised
classification with class discovery and modeling. A Dirichlet process prior
(DPP) model defined over class distributions ensures that both known and
unknown class distributions originate according to a common base distribution.
In an attempt to automatically discover potentially interesting class
formations, the prior model is coupled with a suitably chosen data model, and
sequential Monte Carlo sampling is used to perform online inference. Our
research is driven by a biodetection application, where a new class of pathogen
may suddenly appear, and the rapid increase in the number of samples
originating from this class indicates the onset of an outbreak.
"
"  PAQ8 is an open source lossless data compression algorithm that currently
achieves the best compression rates on many benchmarks. This report presents a
detailed description of PAQ8 from a statistical machine learning perspective.
It shows that it is possible to understand some of the modules of PAQ8 and use
this understanding to improve the method. However, intuitive statistical
explanations of the behavior of other modules remain elusive. We hope the
description in this report will be a starting point for discussions that will
increase our understanding, lead to improvements to PAQ8, and facilitate a
transfer of knowledge from PAQ8 to other machine learning methods, such a
recurrent neural networks and stochastic memoizers. Finally, the report
presents a broad range of new applications of PAQ to machine learning tasks
including language modeling and adaptive text prediction, adaptive game
playing, classification, and compression using features from the field of deep
learning.
"
"  Multivariate stochastic volatility models with skew distributions are
proposed. Exploiting Cholesky stochastic volatility modeling, univariate
stochastic volatility processes with leverage effect and generalized hyperbolic
skew t-distributions are embedded to multivariate analysis with time-varying
correlations. Bayesian prior works allow this approach to provide parsimonious
skew structure and to easily scale up for high-dimensional problem. Analyses of
daily stock returns are illustrated. Empirical results show that the
time-varying correlations and the sparse skew structure contribute to improved
prediction performance and VaR forecasts.
"
"  We introduce random survival forests, a random forests method for the
analysis of right-censored survival data. New survival splitting rules for
growing survival trees are introduced, as is a new missing data algorithm for
imputing missing data. A conservation-of-events principle for survival forests
is introduced and used to define ensemble mortality, a simple interpretable
measure of mortality that can be used as a predicted outcome. Several
illustrative examples are given, including a case study of the prognostic
implications of body mass for individuals with coronary artery disease.
Computations for all examples were implemented using the freely available
R-software package, randomSurvivalForest.
"
"  The last two decades have seen intense scientific and regulatory interest in
the health effects of particulate matter (PM). Influential epidemiological
studies that characterize chronic exposure of individuals rely on monitoring
data that are sparse in space and time, so they often assign the same exposure
to participants in large geographic areas and across time. We estimate monthly
PM during 1988--2002 in a large spatial domain for use in studying health
effects in the Nurses' Health Study. We develop a conceptually simple
spatio-temporal model that uses a rich set of covariates. The model is used to
estimate concentrations of $PM_{10}$ for the full time period and $PM_{2.5}$
for a subset of the period. For the earlier part of the period, 1988--1998, few
$PM_{2.5}$ monitors were operating, so we develop a simple extension to the
model that represents $PM_{2.5}$ conditionally on $PM_{10}$ model predictions.
In the epidemiological analysis, model predictions of $PM_{10}$ are more
strongly associated with health effects than when using simpler approaches to
estimate exposure. Our modeling approach supports the application in estimating
both fine-scale and large-scale spatial heterogeneity and capturing space--time
interaction through the use of monthly-varying spatial surfaces. At the same
time, the model is computationally feasible, implementable with standard
software, and readily understandable to the scientific audience. Despite
simplifying assumptions, the model has good predictive performance and
uncertainty characterization.
"
"  We consider a class of learning problems regularized by a structured
sparsity-inducing norm defined as the sum of l_2- or l_infinity-norms over
groups of variables. Whereas much effort has been put in developing fast
optimization techniques when the groups are disjoint or embedded in a
hierarchy, we address here the case of general overlapping groups. To this end,
we present two different strategies: On the one hand, we show that the proximal
operator associated with a sum of l_infinity-norms can be computed exactly in
polynomial time by solving a quadratic min-cost flow problem, allowing the use
of accelerated proximal gradient methods. On the other hand, we use proximal
splitting techniques, and address an equivalent formulation with
non-overlapping groups, but in higher dimension and with additional
constraints. We propose efficient and scalable algorithms exploiting these two
strategies, which are significantly faster than alternative approaches. We
illustrate these methods with several problems such as CUR matrix
factorization, multi-task learning of tree-structured dictionaries, background
subtraction in video sequences, image denoising with wavelets, and topographic
dictionary learning of natural image patches.
"
"  Tuning parameters in supervised learning problems are often estimated by
cross-validation. The minimum value of the cross-validation error can be biased
downward as an estimate of the test error at that same value of the tuning
parameter. We propose a simple method for the estimation of this bias that uses
information from the cross-validation process. As a result, it requires
essentially no additional computation. We apply our bias estimate to a number
of popular classifiers in various settings, and examine its performance.
"
"  Instead of requiring a domain expert to specify the probabilistic
dependencies of the data, in this work we present an approach that uses the
relational DB schema to automatically construct a Bayesian graphical model for
a database. This resulting model contains customized distributions for columns,
latent variables that cluster the data, and factors that reflect and represent
the foreign key links. Experiments demonstrate the accuracy of the model and
the scalability of inference on synthetic and real-world data.
"
"  Classical penalized likelihood regression problems deal with the case that
the independent variables data are known exactly. In practice, however, it is
common to observe data with incomplete covariate information. We are concerned
with a fundamentally important case where some of the observations do not
represent the exact covariate information, but only a probability distribution.
In this case, the maximum penalized likelihood method can be still applied to
estimating the regression function. We first show that the maximum penalized
likelihood estimate exists under a mild condition. In the computation, we
propose a dimension reduction technique to minimize the penalized likelihood
and derive a GACV (Generalized Approximate Cross Validation) to choose the
smoothing parameter. Our methods are extended to handle more complicated
incomplete data problems, such as, covariate measurement error and partially
missing covariates.
"
"  Interest in multioutput kernel methods is increasing, whether under the guise
of multitask learning, multisensor networks or structured output data. From the
Gaussian process perspective a multioutput Mercer kernel is a covariance
function over correlated output functions. One way of constructing such kernels
is based on convolution processes (CP). A key problem for this approach is
efficient inference. Alvarez and Lawrence (2009) recently presented a sparse
approximation for CPs that enabled efficient inference. In this paper, we
extend this work in two directions: we introduce the concept of variational
inducing functions to handle potential non-smooth functions involved in the
kernel CP construction and we consider an alternative approach to approximate
inference based on variational methods, extending the work by Titsias (2009) to
the multiple output case. We demonstrate our approaches on prediction of school
marks, compiler performance and financial time series.
"
"  A statistical model for predicting individual house prices and constructing a
house price index is proposed utilizing information regarding sale price, time
of sale and location (ZIP code). This model is composed of a fixed time effect
and a random ZIP (postal) code effect combined with an autoregressive
component. The former two components are applied to all home sales, while the
latter is applied only to homes sold repeatedly. The time effect can be
converted into a house price index. To evaluate the proposed model and the
resulting index, single-family home sales for twenty US metropolitan areas from
July 1985 through September 2004 are analyzed. The model is shown to have
better predictive abilities than the benchmark S&P/Case--Shiller model, which
is a repeat sales model, and a conventional mixed effects model. Finally, Los
Angeles, CA, is used to illustrate a historical housing market downturn.
"
"  Background: Identification of causal SNPs in most genome wide association
studies relies on approaches that consider each SNP individually. However,
there is a strong correlation structure among SNPs that need to be taken into
account. Hence, increasingly modern computationally expensive regression
methods are employed for SNP selection that consider all markers simultaneously
and thus incorporate dependencies among SNPs.
  Results: We develop a novel multivariate algorithm for large scale SNP
selection using CAR score regression, a promising new approach for prioritizing
biomarkers. Specifically, we propose a computationally efficient procedure for
shrinkage estimation of CAR scores from high-dimensional data. Subsequently, we
conduct a comprehensive comparison study including five advanced regression
approaches (boosting, lasso, NEG, MCP, and CAR score) and a univariate approach
(marginal correlation) to determine the effectiveness in finding true causal
SNPs.
  Conclusions: Simultaneous SNP selection is a challenging task. We demonstrate
that our CAR score-based algorithm consistently outperforms all competing
approaches, both uni- and multivariate, in terms of correctly recovered causal
SNPs and SNP ranking. An R package implementing the approach as well as R code
to reproduce the complete study presented here is available from
http://strimmerlab.org/software/care/ .
"
"  We investigate an application in the automatic tuning of computer codes, an
area of research that has come to prominence alongside the recent rise of
distributed scientific processing and heterogeneity in high-performance
computing environments. Here, the response function is nonlinear and noisy and
may not be smooth or stationary. Clearly needed are variable selection,
decomposition of influence, and analysis of main and secondary effects for both
real-valued and binary inputs and outputs. Our contribution is a novel set of
tools for variable selection and sensitivity analysis based on the recently
proposed dynamic tree model. We argue that this approach is uniquely well
suited to the demands of our motivating example. In illustrations on benchmark
data sets, we show that the new techniques are faster and offer richer feature
sets than do similar approaches in the static tree and computer experiment
literature. We apply the methods in code-tuning optimization, examination of a
cold-cache effect, and detection of transformation errors.
"
"  Structure learning of Bayesian networks is an important problem that arises
in numerous machine learning applications. In this work, we present a novel
approach for learning the structure of Bayesian networks using the solution of
an appropriately constructed traveling salesman problem. In our approach, one
computes an optimal ordering (partially ordered set) of random variables using
methods for the traveling salesman problem. This ordering significantly reduces
the search space for the subsequent greedy optimization that computes the final
structure of the Bayesian network. We demonstrate our approach of learning
Bayesian networks on real world census and weather datasets. In both cases, we
demonstrate that the approach very accurately captures dependencies between
random variables. We check the accuracy of the predictions based on independent
studies in both application domains.
"
"  We present a novel, fast method to recover the density field through the
statistics of the transmitted flux in high redshift quasar absorption spectra.
The proposed technique requires the computation of the probability distribution
function of the transmitted flux (P_F) in the Ly-alpha forest region and, as a
sole assumption, the knowledge of the probability distribution function of the
matter density field (P_Delta). We show that the probability density
conservation of the flux and matter density unveils a flux-density (F-Delta)
relation which can be used to invert the Ly-alpha forest without any assumption
on the physical properties of the intergalactic medium. We test our inversion
method at z=3 through the following steps: [i] simulation of a sample of
synthetic spectra for which P_Delta is known; [ii] computation of P_F; [iii]
inversion of the Ly-alpha forest through the F-Delta relation. Our technique,
when applied to only 10 observed spectra characterized by a signal-to noise
ratio S/N >= 100 provides an exquisite (relative error epsilon_Delta <~ 12 % in
>~ 50 % of the pixels) reconstruction of the density field in >~ 90 % of the
line of sight. We finally discuss strengths and limitations of the method.
"
"  One of the objectives of designing feature selection learning algorithms is
to obtain classifiers that depend on a small number of attributes and have
verifiable future performance guarantees. There are few, if any, approaches
that successfully address the two goals simultaneously. Performance guarantees
become crucial for tasks such as microarray data analysis due to very small
sample sizes resulting in limited empirical evaluation. To the best of our
knowledge, such algorithms that give theoretical bounds on the future
performance have not been proposed so far in the context of the classification
of gene expression data. In this work, we investigate the premise of learning a
conjunction (or disjunction) of decision stumps in Occam's Razor, Sample
Compression, and PAC-Bayes learning settings for identifying a small subset of
attributes that can be used to perform reliable classification tasks. We apply
the proposed approaches for gene identification from DNA microarray data and
compare our results to those of well known successful approaches proposed for
the task. We show that our algorithm not only finds hypotheses with much
smaller number of genes while giving competitive classification accuracy but
also have tight risk guarantees on future performance unlike other approaches.
The proposed approaches are general and extensible in terms of both designing
novel algorithms and application to other domains.
"
"  The Herschel Space Observatory of ESA was launched in May 2009 and is in
operation since. From its distant orbit around L2 it needs to transmit a huge
quantity of information through a very limited bandwidth. This is especially
true for the PACS imaging camera which needs to compress its data far more than
what can be achieved with lossless compression. This is currently solved by
including lossy averaging and rounding steps on board. Recently, a new theory
called compressed-sensing emerged from the statistics community. This theory
makes use of the sparsity of natural (or astrophysical) images to optimize the
acquisition scheme of the data needed to estimate those images. Thus, it can
lead to high compression factors.
  A previous article by Bobin et al. (2008) showed how the new theory could be
applied to simulated Herschel/PACS data to solve the compression requirement of
the instrument. In this article, we show that compressed-sensing theory can
indeed be successfully applied to actual Herschel/PACS data and give
significant improvements over the standard pipeline. In order to fully use the
redundancy present in the data, we perform full sky map estimation and
decompression at the same time, which cannot be done in most other compression
methods. We also demonstrate that the various artifacts affecting the data
(pink noise, glitches, whose behavior is a priori not well compatible with
compressed-sensing) can be handled as well in this new framework. Finally, we
make a comparison between the methods from the compressed-sensing scheme and
data acquired with the standard compression scheme. We discuss improvements
that can be made on ground for the creation of sky maps from the data.
"
"  This letter is a response to the comments of Serang (2012) on Huang and He
(2012) in Bioinformatics. Serang (2012) claimed that the parameters for the
Fido algorithm should be specified using the grid search method in Serang et
al. (2010) so as to generate a deserved accuracy in performance comparison. It
seems that it is an argument on parameter tuning. However, it is indeed the
issue of how to conduct an unbiased performance evaluation for comparing
different protein inference algorithms. In this letter, we would explain why we
don't use the grid search for parameter selection in Huang and He (2012) and
show that this procedure may result in an over-estimated performance that is
unfair to competing algorithms. In fact, this issue has also been pointed out
by Li and Radivojac (2012).
"
"  We consider a class of operator-induced norms, acting as finite-dimensional
surrogates to the L2 norm, and study their approximation properties over
Hilbert subspaces of L2 . The class includes, as a special case, the usual
empirical norm encountered, for example, in the context of nonparametric
regression in reproducing kernel Hilbert spaces (RKHS). Our results have
implications to the analysis of M-estimators in models based on
finite-dimensional linear approximation of functions, and also to some related
packing problems.
"
"  The Washigton Post had published allegations, that results of Russian
elections ""violate Gauss's groundbreaking work on statistics."" I show that
these allegations lack scientific basis.
"
"  We propose a principled algorithm for robust Bayesian filtering and smoothing
in nonlinear stochastic dynamic systems when both the transition function and
the measurement function are described by non-parametric Gaussian process (GP)
models. GPs are gaining increasing importance in signal processing, machine
learning, robotics, and control for representing unknown system functions by
posterior probability distributions. This modern way of ""system identification""
is more robust than finding point estimates of a parametric function
representation. In this article, we present a principled algorithm for robust
analytic smoothing in GP dynamic systems, which are increasingly used in
robotics and control. Our numerical evaluations demonstrate the robustness of
the proposed approach in situations where other state-of-the-art Gaussian
filters and smoothers can fail.
"
"  We compare forecasts of United States inflation from the Survey of
Professional Forecasters (SPF) to predictions made by simple statistical
techniques. In nowcasting, economic expertise is persuasive. When projecting
beyond the current quarter, novel yet simplistic probabilistic no-change
forecasts are equally competitive. We further interpret surveys as ensembles of
forecasts, and show that they can be used similarly to the ways in which
ensemble prediction systems have transformed weather forecasting. Then we
borrow another idea from weather forecasting, in that we apply statistical
techniques to postprocess the SPF forecast, based on experience from the recent
past. The foregoing conclusions remain unchanged after survey postprocessing.
"
"  Discussion of ""Treelets--An adaptive multi-Scale basis for sparse unordered
data"" [arXiv:0707.0481]
"
"  Dynamic Contrast-enhanced Magnetic Resonance Imaging (DCE-MRI) is an
important tool for detecting subtle kinetic changes in cancerous tissue.
Quantitative analysis of DCE-MRI typically involves the convolution of an
arterial input function (AIF) with a nonlinear pharmacokinetic model of the
contrast agent concentration. Parameters of the kinetic model are biologically
meaningful, but the optimization of the non-linear model has significant
computational issues. In practice, convergence of the optimization algorithm is
not guaranteed and the accuracy of the model fitting may be compromised. To
overcome this problems, this paper proposes a semi-parametric penalized spline
smoothing approach, with which the AIF is convolved with a set of B-splines to
produce a design matrix using locally adaptive smoothing parameters based on
Bayesian penalized spline models (P-splines). It has been shown that kinetic
parameter estimation can be obtained from the resulting deconvolved response
function, which also includes the onset of contrast enhancement. Detailed
validation of the method, both with simulated and in vivo data, is provided.
"
"  Star-galaxy classification is one of the most fundamental data-processing
tasks in survey astronomy, and a critical starting point for the scientific
exploitation of survey data. For bright sources this classification can be done
with almost complete reliability, but for the numerous sources close to a
survey's detection limit each image encodes only limited morphological
information. In this regime, from which many of the new scientific discoveries
are likely to come, it is vital to utilise all the available information about
a source, both from multiple measurements and also prior knowledge about the
star and galaxy populations. It is also more useful and realistic to provide
classification probabilities than decisive classifications. All these
desiderata can be met by adopting a Bayesian approach to star-galaxy
classification, and we develop a very general formalism for doing so. An
immediate implication of applying Bayes's theorem to this problem is that it is
formally impossible to combine morphological measurements in different bands
without using colour information as well; however we develop several
approximations that disregard colour information as much as possible. The
resultant scheme is applied to data from the UKIRT Infrared Deep Sky Survey
(UKIDSS), and tested by comparing the results to deep Sloan Digital Sky Survey
(SDSS) Stripe 82 measurements of the same sources. The Bayesian classification
probabilities obtained from the UKIDSS data agree well with the deep SDSS
classifications both overall (a mismatch rate of 0.022, compared to 0.044 for
the UKIDSS pipeline classifier) and close to the UKIDSS detection limit (a
mismatch rate of 0.068 compared to 0.075 for the UKIDSS pipeline classifier).
The Bayesian formalism developed here can be applied to improve the reliability
of any star-galaxy classification schemes based on the measured values of
morphology statistics alone.
"
"  Flow cytometry is often used to characterize the malignant cells in leukemia
and lymphoma patients, traced to the level of the individual cell. Typically,
flow cytometric data analysis is performed through a series of 2-dimensional
projections onto the axes of the data set. Through the years, clinicians have
determined combinations of different fluorescent markers which generate
relatively known expression patterns for specific subtypes of leukemia and
lymphoma -- cancers of the hematopoietic system. By only viewing a series of
2-dimensional projections, the high-dimensional nature of the data is rarely
exploited. In this paper we present a means of determining a low-dimensional
projection which maintains the high-dimensional relationships (i.e.
information) between differing oncological data sets. By using machine learning
techniques, we allow clinicians to visualize data in a low dimension defined by
a linear combination of all of the available markers, rather than just 2 at a
time. This provides an aid in diagnosing similar forms of cancer, as well as a
means for variable selection in exploratory flow cytometric research. We refer
to our method as Information Preserving Component Analysis (IPCA).
"
"  I introduce an algorithm for estimating parameters from multidimensional data
based on forward modelling. In contrast to many machine learning approaches it
avoids fitting an inverse model and the problems associated with this. The
algorithm makes explicit use of the sensitivities of the data to the
parameters, with the goal of better treating parameters which only have a weak
impact on the data. The forward modelling approach provides uncertainty (full
covariance) estimates in the predicted parameters as well as a goodness-of-fit
for observations. I demonstrate the algorithm, ILIUM, with the estimation of
stellar astrophysical parameters (APs) from simulations of the low resolution
spectrophotometry to be obtained by Gaia. The AP accuracy is competitive with
that obtained by a support vector machine. For example, for zero extinction
stars covering a wide range of metallicity, surface gravity and temperature,
ILIUM can estimate Teff to an accuracy of 0.3% at G=15 and to 4% for (lower
signal-to-noise ratio) spectra at G=20. [Fe/H] and logg can be estimated to
accuracies of 0.1-0.4dex for stars with G<=18.5. If extinction varies a priori
over a wide range (Av=0-10mag), then Teff and Av can be estimated quite
accurately (3-4% and 0.1-0.2mag respectively at G=15), but there is a strong
and ubiquitous degeneracy in these parameters which limits our ability to
estimate either accurately at faint magnitudes. Using the forward model we can
map these degeneracies (in advance), and thus provide a complete probability
distribution over solutions. (Abridged)
"
"  To detect changes in the mean of a time series, one may use previsible
detection procedures based on nonparametric kernel prediction smoothers which
cover various classic detection statistics as special cases. Bandwidth
selection, particularly in a data-adaptive way, is a serious issue and not well
studied for detection problems. To ensure data adaptation, we select the
bandwidth by cross-validation, but in a sequential way leading to a functional
estimation approach. This article provides the asymptotic theory for the method
under fairly weak assumptions on the dependence structure of the error terms,
which cover, e.g., GARCH($p,q$) processes, by establishing (sequential)
functional central limit theorems for the cross-validation objective function
and the associated bandwidth selector. It turns out that the proof can be based
in a neat way on \cite{KurtzProtter1996}'s results on the weak convergence of
\ito integrals and a diagonal argument.
  Our gradual change-point model covers multiple change-points in that it
allows for a nonlinear regression function after the first change-point
possibly with further jumps and Lipschitz continuous between those
discontinuities.
  In applications, the time horizon where monitoring stops latest is often
determined by a random experiment, e.g. a first-exit stopping time applied to a
cumulated cost process or a risk measure, possibly stochastically dependent
from the monitored time series. Thus, we also study that case and establish
related limit theorems in the spirit of \citet{Anscombe1952}'s result. The
result has various applications including statistical parameter estimation and
monitoring financial investment strategies with risk-controlled early
termination, which are briefly discussed.
"
"  We discuss briefly the very interesting concept of Brownian distance
covariance developed by Sz\'{e}kely and Rizzo [Ann. Appl. Statist. (2009), to
appear] and describe two possible extensions. The first extension is for high
dimensional data that can be coerced into a Hilbert space, including certain
high throughput screening and functional data settings. The second extension
involves very simple modifications that may yield increased power in some
settings. We commend Sz\'{e}kely and Rizzo for their very interesting work and
recognize that this general idea has potential to have a large impact on the
way in which statisticians evaluate dependency in data. [arXiv:1010.0297]
"
"  While it is widely accepted that lead-based paint and leaded gasoline are
primary sources of elevated concentrations of lead in residential soils,
conclusions regarding their relative contributions are mixed and generally
study specific. We develop a novel nonlinear regression for soil lead
concentrations over time. It is argued that this methodology provides useful
insights into the partitioning of the average soil lead concentration by source
and time over large residential areas. The methodology is used to investigate
soil lead concentrations from the 1987 Minnesota Lead Study and the 1990
National Lead Survey. Potential litigation issues are discussed briefly.
"
"  We address the problem of automatic generation of features for value function
approximation. Bellman Error Basis Functions (BEBFs) have been shown to improve
the error of policy evaluation with function approximation, with a convergence
rate similar to that of value iteration. We propose a simple, fast and robust
algorithm based on random projections to generate BEBFs for sparse feature
spaces. We provide a finite sample analysis of the proposed method, and prove
that projections logarithmic in the dimension of the original space are enough
to guarantee contraction in the error. Empirical results demonstrate the
strength of this method.
"
"  Graphical models trained using maximum likelihood are a common tool for
probabilistic inference of marginal distributions. However, this approach
suffers difficulties when either the inference process or the model is
approximate. In this paper, the inference process is first defined to be the
minimization of a convex function, inspired by free energy approximations.
Learning is then done directly in terms of the performance of the inference
process at univariate marginal prediction. The main novelty is that this is a
direct minimization of emperical risk, where the risk measures the accuracy of
predicted marginals.
"
"  Understanding extreme ocean environments and their interaction with fixed and
floating structures is critical for the design of offshore and coastal
facilities. The joint effect of various ocean variables on extreme responses of
offshore structures is fundamental in determining the design loads. For
example, it is known that mean values of wave periods tend to increase with
increasing storm intensity, and a floating system responds in a complex way to
both variables.
  However, specification of joint extremes in design criteria has often been
somewhat \textit{ad hoc}, being based on fairly arbitrary combinations of
extremes of variables estimated independently. Such approaches are even
outlined in design guidelines. Mathematically more consistent estimates of the
joint occurrence of extreme environmental variables fall into two camps in the
offshore industry -- response-based and response-independent. Both are outlined
here, with emphasis on response-independent methods, particularly those based
on the conditional extremes model recently introduced by Heffernan and Tawn
(2004) which has a solid theoretical motivation. Several applications using the
new methods are presented.
"
"  This brief comment reflects on the historical and current uses of the term
""snowball sampling.""
"
"  A problem of substantial interest is to systematically map variation in
chromatin structure to gene expression regulation across conditions,
environments, or differentiated cell types. We developed and applied a
quantitative framework for determining the existence, strength, and type of
relationship between high-resolution chromatin structure in terms of DNaseI
hypersensitivity (DHS) and genome-wide gene expression levels in 20 diverse
human cell lines. We show that ~25% of genes show cell-type specific expression
explained by alterations in chromatin structure. We find that distal regions of
chromatin structure (e.g., +/- 200kb) capture more genes with this relationship
than local regions (e.g., +/- 2.5kb), yet the local regions show a more
pronounced effect. By exploiting variation across cell-types, we were capable
of pinpointing the most likely hypersensitive sites related to cell-type
specific expression, which we show have a range of contextual usages. This
quantitative framework is likely applicable to other settings aimed at relating
continuous genomic measurements to gene expression variation.
"
"  The LASSO is a recent technique for variable selection in the regression
model \bean y & = & X\beta + z, \eean where $X\in \R^{n\times p}$ and $z$ is a
centered gaussian i.i.d. noise vector $\mathcal N(0,\sigma^2I)$. The LASSO has
been proved to achieve remarkable properties such as exact support recovery of
sparse vectors when the columns are sufficently incoherent and low prediction
error under even less stringent conditions. However, many matrices do not
satisfy small coherence in practical applications and the LASSO estimator may
thus suffer from what is known as the slow rate regime.
  The goal of the present paper is to study the LASSO from a slightly different
perspective by proposing a mixture model for the design matrix which is able to
capture in a natural way the potentially clustered nature of the columns in
many practical situations. In this model, the columns of the design matrix are
drawn from a Gaussian mixture model. Instead of requiring incoherence for the
design matrix $X$, we only require incoherence of the much smaller matrix of
the mixture's centers.
  Our main result states that $X\beta$ can be estimated with the same precision
as for incoherent designs except for a correction term depending on the maximal
variance in the mixture model.
"
"  Structural reliability methods aim at computing the probability of failure of
systems with respect to some prescribed performance functions. In modern
engineering such functions usually resort to running an expensive-to-evaluate
computational model (e.g. a finite element model). In this respect simulation
methods, which may require $10^{3-6}$ runs cannot be used directly. Surrogate
models such as quadratic response surfaces, polynomial chaos expansions or
kriging (which are built from a limited number of runs of the original model)
are then introduced as a substitute of the original model to cope with the
computational cost. In practice it is almost impossible to quantify the error
made by this substitution though. In this paper we propose to use a kriging
surrogate of the performance function as a means to build a quasi-optimal
importance sampling density. The probability of failure is eventually obtained
as the product of an augmented probability computed by substituting the
meta-model for the original performance function and a correction term which
ensures that there is no bias in the estimation even if the meta-model is not
fully accurate. The approach is applied to analytical and finite element
reliability problems and proves efficient up to 100 random variables.
"
"  The decomposition of a sample of images on a relevant subspace is a recurrent
problem in many different fields from Computer Vision to medical image
analysis. We propose in this paper a new learning principle and implementation
of the generative decomposition model generally known as noisy ICA (for
independent component analysis) based on the SAEM algorithm, which is a
versatile stochastic approximation of the standard EM algorithm. We demonstrate
the applicability of the method on a large range of decomposition models and
illustrate the developments with experimental results on various data sets.
"
"  Today's call center managers face multiple operational decision-making tasks.
One of the most common is determining the weekly staffing levels to ensure
customer satisfaction and meeting their needs while minimizing service costs.
An initial step for producing the weekly schedule is forecasting the future
system loads which involves predicting both arrival counts and average service
times. We introduce an arrival count model which is based on a mixed Poisson
process approach. The model is applied to data from an Israeli Telecom company
call center. In our model, we also consider the effect of events such as
billing on the arrival process and we demonstrate how to incorporate them as
exogenous variables in the model. After obtaining the forecasted system load,
in large call centers, a manager can choose to apply the QED
(Quality-Efficiency Driven) regime's ""square-root staffing"" rule in order to
balance the offered-load per server with the quality of service. Implementing
this staffing rule requires that the forecasted values of the arrival counts
and average service times maintain certain levels of precision. We develop
different goodness of fit criteria that help determine our model's practical
performance under the QED regime. These show that during most hours of the day
the model can reach desired precision levels.
"
"  In this paper, we propose and analyze a spectrum sensing method based on
cyclostationarity specifically targeted for receivers with multiple antennas.
This detection method is used for determining the presence or absence of
primary users in cognitive radio networks based on the eigenvalues of the
cyclic covariance matrix of received signals. In particular, the cyclic
correlation significance test is used to detect a specific signal-of-interest
by exploiting knowledge of its cyclic frequencies. Analytical expressions for
the probability of detection and probability of false-alarm under both
spatially uncorrelated or spatially correlated noise are derived and verified
by simulation. The detection performance in a Rayleigh flat-fading environment
is found and verified through simulations. One of the advantages of the
proposed method is that the detection threshold is shown to be independent of
both the number of samples and the noise covariance, effectively eliminating
the dependence on accurate noise estimation. The proposed method is also shown
to provide higher detection probability and better robustness to noise
uncertainty than existing multiple-antenna cyclostationary-based spectrum
sensing algorithms under both AWGN as well as a quasi-static Rayleigh fading
channel.
"
"  Ozone and particulate matter PM2.5 are co-pollutants that have long been
associated with increased public health risks. Information on concentration
levels for both pollutants come from two sources: monitoring sites and output
from complex numerical models that produce concentration surfaces over large
spatial regions. In this paper, we offer a fully-model based approach for
fusing these two sources of information for the pair of co-pollutants which is
computationally feasible over large spatial regions and long periods of time.
Due to the association between concentration levels of the two environmental
contaminants, it is expected that information regarding one will help to
improve prediction of the other. Misalignment is an obvious issue since the
monitoring networks for the two contaminants only partly intersect and because
the collection rate for PM2.5 is typically less frequent than that for ozone.
Extending previous work in Berrocal et al. (2010), we introduce a bivariate
downscaler that provides a flexible class of bivariate space-time assimilation
models. We discuss computational issues for model fitting and analyze a dataset
for ozone and PM2.5 for the ozone season during year 2002. We show a modest
improvement in predictive performance, not surprising in a setting where we can
anticipate only a small gain.
"
"  In this paper, we propose a semiparametric approach, named nonparanormal
skeptic, for efficiently and robustly estimating high dimensional undirected
graphical models. To achieve modeling flexibility, we consider Gaussian Copula
graphical models (or the nonparanormal) as proposed by Liu et al. (2009). To
achieve estimation robustness, we exploit nonparametric rank-based correlation
coefficient estimators, including Spearman's rho and Kendall's tau. In high
dimensional settings, we prove that the nonparanormal skeptic achieves the
optimal parametric rate of convergence in both graph and parameter estimation.
This celebrating result suggests that the Gaussian copula graphical models can
be used as a safe replacement of the popular Gaussian graphical models, even
when the data are truly Gaussian. Besides theoretical analysis, we also conduct
thorough numerical simulations to compare different estimators for their graph
recovery performance under both ideal and noisy settings. The proposed methods
are then applied on a large-scale genomic dataset to illustrate their empirical
usefulness. The R language software package huge implementing the proposed
methods is available on the Comprehensive R Archive Network: http://cran.
r-project.org/.
"
"  Atmospheric aerosols can cause serious damage to human health and life
expectancy. Using the radiances observed by NASA's Multi-angle Imaging
SpectroRadiometer (MISR), the current MISR operational algorithm retrieves
Aerosol Optical Depth (AOD) at a spatial resolution of 17.6 km x 17.6 km. A
systematic study of aerosols and their impact on public health, especially in
highly-populated urban areas, requires a finer-resolution estimate of the
spatial distribution of AOD values.
  We embed MISR's operational weighted least squares criterion and its forward
simulations for AOD retrieval in a likelihood framework and further expand it
into a Bayesian hierarchical model to adapt to a finer spatial scale of 4.4 km
x 4.4 km. To take advantage of AOD's spatial smoothness, our method borrows
strength from data at neighboring pixels by postulating a Gaussian Markov
Random Field prior for AOD. Our model considers both AOD and aerosol mixing
vectors as continuous variables. The inference of AOD and mixing vectors is
carried out using Metropolis-within-Gibbs sampling methods. Retrieval
uncertainties are quantified by posterior variabilities. We also implement a
parallel MCMC algorithm to reduce computational cost. We assess our retrievals
performance using ground-based measurements from the AErosol RObotic NETwork
(AERONET), a hand-held sunphotometer and satellite images from Google Earth.
  Based on case studies in the greater Beijing area, China, we show that a 4.4
km resolution can improve the accuracy and coverage of remotely-sensed aerosol
retrievals, as well as our understanding of the spatial and seasonal behaviors
of aerosols. This improvement is particularly important during high-AOD events,
which often indicate severe air pollution.
"
"  The enhanced Bayesian network (eBN) methodology described in the companion
paper facilitates the assessment of reliability and risk of engineering systems
when information about the system evolves in time. We present the application
of the eBN (a) to the assessment of the life-cycle reliability of a structural
system, (b) to the optimization of a decision on performing measurements in
that structural system, and (c) to the risk assessment of an infrastructure
system subject to natural hazards and deterioration of constituent structures.
In all applications, observations of system performances or the hazards are
made at various points in time and the eBN efficiently includes these
observations in the analysis to provide an updated probabilistic model of the
system at all times.
"
"  Discussion of ""Treelets--An adaptive multi-scale basis for sparse unordered
data"" [arXiv:0707.0481]
"
"  We present a Bayesian method for estimating the age of a renal tumor given
its size. We use a model of tumor growth based on published data from
observations of untreated tumors. We find, for example, that the median age of
a 5 cm tumor is 20 years, with interquartile range 16-23 and 90% confidence
interval 11-30 years.
"
"  We consider the problem of speaker diarization, the problem of segmenting an
audio recording of a meeting into temporal segments corresponding to individual
speakers. The problem is rendered particularly difficult by the fact that we
are not allowed to assume knowledge of the number of people participating in
the meeting. To address this problem, we take a Bayesian nonparametric approach
to speaker diarization that builds on the hierarchical Dirichlet process hidden
Markov model (HDP-HMM) of Teh et al. [J. Amer. Statist. Assoc. 101 (2006)
1566--1581]. Although the basic HDP-HMM tends to over-segment the audio
data---creating redundant states and rapidly switching among them---we describe
an augmented HDP-HMM that provides effective control over the switching rate.
We also show that this augmentation makes it possible to treat emission
distributions nonparametrically. To scale the resulting architecture to
realistic diarization problems, we develop a sampling algorithm that employs a
truncated approximation of the Dirichlet process to jointly resample the full
state sequence, greatly improving mixing rates. Working with a benchmark NIST
data set, we show that our Bayesian nonparametric architecture yields
state-of-the-art speaker diarization results.
"
"  The aim of this short note is to draw attention to a method by which the
partition function and marginal probabilities for a certain class of random
fields on complete graphs can be computed in polynomial time. This class
includes Ising models with homogeneous pairwise potentials but arbitrary
(inhomogeneous) unary potentials. Similarly, the partition function and
marginal probabilities can be computed in polynomial time for random fields on
complete bipartite graphs, provided they have homogeneous pairwise potentials.
We expect that these tractable classes of large scale random fields can be very
useful for the evaluation of approximation algorithms by providing exact error
estimates.
"
"  Since its first publication in 2003, the Gene Set Enrichment Analysis (GSEA)
method, based on the Kolmogorov-Smirnov statistic, has been heavily used,
modified, and also questioned. Recently a simplified approach, using a one
sample t test score to assess enrichment and ignoring gene-gene correlations
was proposed by Irizarry et al. 2009 as a serious contender. The argument
criticizes GSEA's nonparametric nature and its use of an empirical null
distribution as unnecessary and hard to compute. We refute these claims by
careful consideration of the assumptions of the simplified method and its
results, including a comparison with GSEA's on a large benchmark set of 50
datasets. Our results provide strong empirical evidence that gene-gene
correlations cannot be ignored due to the significant variance inflation they
produced on the enrichment scores and should be taken into account when
estimating gene set enrichment significance. In addition, we discuss the
challenges that the complex correlation structure and multi-modality of gene
sets pose more generally for gene set enrichment methods.
"
"  We present techniques for effective Gaussian process (GP) modelling of
multiple short time series. These problems are common when applying GP models
independently to each gene in a gene expression time series data set. Such sets
typically contain very few time points. Naive application of common GP
modelling techniques can lead to severe over-fitting or under-fitting in a
significant fraction of the fitted models, depending on the details of the data
set. We propose avoiding over-fitting by constraining the GP length-scale to
values that focus most of the energy spectrum to frequencies below the Nyquist
frequency corresponding to the sampling frequency in the data set.
Under-fitting can be avoided by more informative priors on observation noise.
Combining these methods allows applying GP methods reliably automatically to
large numbers of independent instances of short time series. This is
illustrated with experiments with both synthetic data and real gene expression
data.
"
"  Latent Dirichlet Allocation models discrete data as a mixture of discrete
distributions, using Dirichlet beliefs over the mixture weights. We study a
variation of this concept, in which the documents' mixture weight beliefs are
replaced with squashed Gaussian distributions. This allows documents to be
associated with elements of a Hilbert space, admitting kernel topic models
(KTM), modelling temporal, spatial, hierarchical, social and other structure
between documents. The main challenge is efficient approximate inference on the
latent Gaussian. We present an approximate algorithm cast around a Laplace
approximation in a transformed basis. The KTM can also be interpreted as a type
of Gaussian process latent variable model, or as a topic model conditional on
document features, uncovering links between earlier work in these areas.
"
"  Discussion on ""Brownian distance covariance"" by G\'{a}bor J. Sz\'{e}kely and
Maria L. Rizzo [arXiv:1010.0297]
"
"  Being among the easiest ways to find meaningful structure from discrete data,
Latent Dirichlet Allocation (LDA) and related component models have been
applied widely. They are simple, computationally fast and scalable,
interpretable, and admit nonparametric priors. In the currently popular field
of network modeling, relatively little work has taken uncertainty of data
seriously in the Bayesian sense, and component models have been introduced to
the field only recently, by treating each node as a bag of out-going links. We
introduce an alternative, interaction component model for communities (ICMc),
where the whole network is a bag of links, stemming from different components.
The former finds both disassortative and assortative structure, while the
alternative assumes assortativity and finds community-like structures like the
earlier methods motivated by physics. With Dirichlet Process priors and an
efficient implementation the models are highly scalable, as demonstrated with a
social network from the Last.fm web site, with 670,000 nodes and 1.89 million
links.
"
"  We extend kernelized matrix factorization with a fully Bayesian treatment and
with an ability to work with multiple side information sources expressed as
different kernels. Kernel functions have been introduced to matrix
factorization to integrate side information about the rows and columns (e.g.,
objects and users in recommender systems), which is necessary for making
out-of-matrix (i.e., cold start) predictions. We discuss specifically bipartite
graph inference, where the output matrix is binary, but extensions to more
general matrices are straightforward. We extend the state of the art in two key
aspects: (i) A fully conjugate probabilistic formulation of the kernelized
matrix factorization problem enables an efficient variational approximation,
whereas fully Bayesian treatments are not computationally feasible in the
earlier approaches. (ii) Multiple side information sources are included,
treated as different kernels in multiple kernel learning that additionally
reveals which side information sources are informative. Our method outperforms
alternatives in predicting drug-protein interactions on two data sets. We then
show that our framework can also be used for solving multilabel learning
problems by considering samples and labels as the two domains where matrix
factorization operates on. Our algorithm obtains the lowest Hamming loss values
on 10 out of 14 multilabel classification data sets compared to five
state-of-the-art multilabel learning algorithms.
"
"  Many applications in data analysis rely on the decomposition of a data matrix
into a low-rank and a sparse component. Existing methods that tackle this task
use the nuclear norm and L1-cost functions as convex relaxations of the rank
constraint and the sparsity measure, respectively, or employ thresholding
techniques. We propose a method that allows for reconstructing and tracking a
subspace of upper-bounded dimension from incomplete and corrupted observations.
It does not require any a priori information about the number of outliers. The
core of our algorithm is an intrinsic Conjugate Gradient method on the set of
orthogonal projection matrices, the so-called Grassmannian. Non-convex sparsity
measures are used for outlier detection, which leads to improved performance in
terms of robustly recovering and tracking the low-rank matrix. In particular,
our approach can cope with more outliers and with an underlying matrix of
higher rank than other state-of-the-art methods.
"
"  This work concerns testing the number of parameters in one hidden layer
multilayer perceptron (MLP). For this purpose we assume that we have
identifiable models, up to a finite group of transformations on the weights,
this is for example the case when the number of hidden units is know. In this
framework, we show that we get a simple asymptotic distribution, if we use the
logarithm of the determinant of the empirical error covariance matrix as cost
function.
"
"  Statistical physics approaches can be used to derive accurate predictions for
the performance of inference methods learning from potentially noisy data, as
quantified by the learning curve defined as the average error versus number of
training examples. We analyse a challenging problem in the area of
non-parametric inference where an effectively infinite number of parameters has
to be learned, specifically Gaussian process regression. When the inputs are
vertices on a random graph and the outputs noisy function values, we show that
replica techniques can be used to obtain exact performance predictions in the
limit of large graphs. The covariance of the Gaussian process prior is defined
by a random walk kernel, the discrete analogue of squared exponential kernels
on continuous spaces. Conventionally this kernel is normalised only globally,
so that the prior variance can differ between vertices; as a more principled
alternative we consider local normalisation, where the prior variance is
uniform.
"
"  We propose a new measure to estimate the direction of information flux in
multivariate time series from complex systems. This measure, based on the slope
of the phase spectrum (Phase Slope Index) has invariance properties that are
important for applications in real physical or biological systems: (a) it is
strictly insensitive to mixtures of arbitrary independent sources, (b) it gives
meaningful results even if the phase spectrum is not linear, and (c) it
properly weights contributions from different frequencies. Simulations of a
class of coupled multivariate random data show that for truly unidirectional
information flow without additional noise contamination our measure detects the
correct direction as good as the standard Granger causality. For random
mixtures of independent sources Granger Causality erroneously yields highly
significant results whereas our measure correctly becomes non-significant. An
application of our novel method to EEG data (88 subjects in eyes-closed
condition) reveals a strikingly clear front-to-back information flow in the
vast majority of subjects and thus contributes to a better understanding of
information processing in the brain.
"
"  [Abridged] The analysis of a sample of 52 clusters with precise and
hypothesis-parsimonious measurements of mass shows that low mass clusters and
groups are not simple scaled-down versions of their massive cousins in terms of
stellar content: lighter clusters have more stars per unit cluster mass. The
same analysis also shows that the stellar content of clusters and groups
displays an intrinsic spread at a given cluster mass, i.e. clusters are not
similar each other in the amount of stars they contain, not even at a fixed
cluster mass. The stellar mass fraction depends on halo mass with (logarithmic)
slope -0.55+/-0.08 and with 0.15+/-0.02 dex of intrinsic scatter at a fixed
cluster mass. The intrinsic scatter at a fixed cluster mass we determine for
gas mass fractions is smaller, 0.06+/-0.01 dex. The intrinsic scatter in both
the stellar and gas mass fractions is a distinctive signature that the regions
from which clusters and groups collected matter, a few tens of Mpc, are yet not
representative, in terms of gas and baryon content, of the mean matter content
of the Universe. The observed stellar mass fraction values are in marked
disagreement with gasdynamics simulations with cooling and star formation of
clusters and groups. We found the the baryon (gas+stellar) fraction is fairly
constant for clusters and groups with 13.7<lg(mass)<15.0 solar masses and it is
offset from the WMAP-derived value by about 6 sigmas. The offset could be
related to the possible non universality of the baryon fraction pointed out by
our measurements of the intrinsic scatter. Our analysis is the first that does
not assume that clusters are identically equal at a given halo mass and it is
also more accurate in many aspects. The data and code used for the stochastic
computation are distributed with the paper.
"
"  In this paper, we develop a multistage approach for estimating the mean of a
bounded variable. We first focus on the multistage estimation of a binomial
parameter and then generalize the estimation methods to the case of general
bounded random variables. A fundamental connection between a binomial parameter
and the mean of a bounded variable is established. Our multistage estimation
methods rigorously guarantee prescribed levels of precision and confidence.
"
"  This letter discusses the problem of testing the degree of randomness within
an image, particularly for a shuffled or encrypted image. Its key contributions
are: 1) a mathematical model of perfectly shuffled images; 2) the derivation of
the theoretical distribution of pixel differences; 3) a new $Z$-test based
approach to differentiate whether or not a test image is perfectly shuffled;
and 4) a randomized algorithm to unbiasedly evaluate the degree of randomness
within a given image. Simulation results show that the proposed method is
robust and effective in evaluating the degree of randomness within an image,
and may often be more suitable for image applications than commonly used
testing schemes designed for binary data like NIST 800-22. The developed method
may be also useful as a first step in determining whether or not a shuffling or
encryption scheme is suitable for a particular cryptographic application.
"
"  We extend multi-way, multivariate ANOVA-type analysis to cases where one
covariate is the view, with features of each view coming from different,
high-dimensional domains. The different views are assumed to be connected by
having paired samples; this is a common setup in recent bioinformatics
experiments, of which we analyze metabolite profiles in different conditions
(disease vs. control and treatment vs. untreated) in different tissues (views).
We introduce a multi-way latent variable model for this new task, by extending
the generative model of Bayesian canonical correlation analysis (CCA) both to
take multi-way covariate information into account as population priors, and by
reducing the dimensionality by an integrated factor analysis that assumes the
metabolites to come in correlated groups.
"
"  In 2004 the Dutch Department of Social Affairs conducted a survey to assess
the extent of noncompliance with social security regulations. The survey was
conducted among 870 recipients of social security benefits and included a
series of sensitive questions about regulatory noncompliance. Due to the
sensitive nature of the questions the randomized response design was used.
Although randomized response protects the privacy of the respondent, it is
unlikely that all respondents followed the design. In this paper we introduce a
model that allows for respondents displaying self-protective response behavior
by consistently giving the nonincriminating response, irrespective of the
outcome of the randomizing device. The dependent variable denoting the total
number of incriminating responses is assumed to be generated by the application
of randomized response to a latent Poisson variable denoting the true number of
rule violations. Since self-protective responses result in an excess of
observed zeros in relation to the Poisson randomized response distribution,
these are modeled as observed zero-inflation. The model includes predictors of
the Poisson parameters, as well as predictors of the probability of
self-protective response behavior.
"
"  Locality-sensitive hashing converts high-dimensional feature vectors, such as
image and speech, into bit arrays and allows high-speed similarity calculation
with the Hamming distance. There is a hashing scheme that maps feature vectors
to bit arrays depending on the signs of the inner products between feature
vectors and the normal vectors of hyperplanes placed in the feature space. This
hashing can be seen as a discretization of the feature space by hyperplanes. If
labels for data are given, one can determine the hyperplanes by using learning
algorithms. However, many proposed learning methods do not consider the
hyperplanes' offsets. Not doing so decreases the number of partitioned regions,
and the correlation between Hamming distances and Euclidean distances becomes
small. In this paper, we propose a lift map that converts learning algorithms
without the offsets to the ones that take into account the offsets. With this
method, the learning methods without the offsets give the discretizations of
spaces as if it takes into account the offsets. For the proposed method, we
input several high-dimensional feature data sets and studied the relationship
between the statistical characteristics of data, the number of hyperplanes, and
the effect of the proposed method.
"
"  Due to the increasing popularity of collaborative tagging systems, the
research on tagged networks, hypergraphs, ontologies, folksonomies and other
related concepts is becoming an important interdisciplinary topic with great
actuality and relevance for practical applications. In most collaborative
tagging systems the tagging by the users is completely ""flat"", while in some
cases they are allowed to define a shallow hierarchy for their own tags.
However, usually no overall hierarchical organisation of the tags is given, and
one of the interesting challenges of this area is to provide an algorithm
generating the ontology of the tags from the available data. In contrast, there
are also other type of tagged networks available for research, where the tags
are already organised into a directed acyclic graph (DAG), encapsulating the
""is a sub-category of"" type of hierarchy between each other. In this paper we
study how this DAG affects the statistical distribution of tags on the nodes
marked by the tags in various real networks. We analyse the relation between
the tag-frequency and the position of the tag in the DAG in two large
sub-networks of the English Wikipedia and a protein-protein interaction
network. We also study the tag co-occurrence statistics by introducing a 2d
tag-distance distribution preserving both the difference in the levels and the
absolute distance in the DAG for the co-occurring pairs of tags. Our most
interesting finding is that the local relevance of tags in the DAG, (i.e.,
their rank or significance as characterised by, e.g., the length of the
branches starting from them) is much more important than their global distance
from the root. Furthermore, we also introduce a simple tagging model based on
random walks on the DAG, capable of reproducing the main statistical features
of tag co-occurrence.
"
"  We propose a new framework for cooperative spectrum sensing in cognitive
radio networks, that is based on a novel class of non-uniform samplers, called
the event-triggered samplers, and sequential detection. In the proposed scheme,
each secondary user computes its local sensing decision statistic based on its
own channel output; and whenever such decision statistic crosses certain
predefined threshold values, the secondary user will send one (or several) bit
of information to the fusion center. The fusion center asynchronously receives
the bits from different secondary users and updates the global sensing decision
statistic to perform a sequential probability ratio test (SPRT), to reach a
sensing decision. We provide an asymptotic analysis for the above scheme, and
under different conditions, we compare it against the cooperative sensing
scheme that is based on traditional uniform sampling and sequential detection.
Simulation results show that the proposed scheme, using even 1 bit, can
outperform its uniform sampling counterpart that uses infinite number of bits
under changing target error probabilities, SNR values, and number of SUs.
"
"  We consider principal component analysis (PCA) in decomposable Gaussian
graphical models. We exploit the prior information in these models in order to
distribute its computation. For this purpose, we reformulate the problem in the
sparse inverse covariance (concentration) domain and solve the global
eigenvalue problem using a sequence of local eigenvalue problems in each of the
cliques of the decomposable graph. We demonstrate the application of our
methodology in the context of decentralized anomaly detection in the Abilene
backbone network. Based on the topology of the network, we propose an
approximate statistical graphical model and distribute the computation of PCA.
"
"  Gridded estimated rainfall intensity values at very high spatial and temporal
resolution levels are needed as main inputs for weather prediction models to
obtain accurate precipitation forecasts, and to verify the performance of
precipitation forecast models. These gridded rainfall fields are also the main
driver for hydrological models that forecast flash floods, and they are
essential for disaster prediction associated with heavy rain. Rainfall
information can be obtained from rain gages that provide relatively accurate
estimates of the actual rainfall values at point-referenced locations, but they
do not characterize well enough the spatial and temporal structure of the
rainfall fields. Doppler radar data offer better spatial and temporal coverage,
but Doppler radar measures effective radar reflectivity ($Ze$) rather than
rainfall rate ($R$). Thus, rainfall estimates from radar data suffer from
various uncertainties due to their measuring principle and the conversion from
$Ze$ to $R$. We introduce a framework to combine radar reflectivity and gage
data, by writing the different sources of rainfall information in terms of an
underlying unobservable spatial temporal process with the true rainfall values.
We use spatial logistic regression to model the probability of rain for both
sources of data in terms of the latent true rainfall process. We characterize
the different sources of bias and error in the gage and radar data and we
estimate the true rainfall intensity with its posterior predictive
distribution, conditioning on the observed data.
"
"  A severe case of scientific misconduct was discovered in a paper from 2005
allegedly showing harmful effects (DNA breakage) of non-thermal mobile phone
electromagnetic field exposure on human and rat cells. Here we describe the way
how the fraudulent data were identified. The low variations of the reported
biological data are shown to be below theoretical lower limits (multinomial
distributions). Another reason for doubts was highly significant non-equal
distributions of last digits, a known hint towards data fabrication. The
Medical University Vienna, where the research was conducted, was informed about
these findings and came to the conclusion that the data in this and another,
related paper by the same group were fabricated, and that both papers should be
retracted.
"
"  I published an interview of Leo Breiman in Statistical Science [Olshen
(2001)], and also the solution to a problem concerning almost sure convergence
of binary tree-structured estimators in regression [Olshen (2007)]. The former
summarized much of my thinking about Leo up to five years before his death. I
discussed the latter with Leo and dedicated that paper to his memory.
Therefore, this note is on other topics. In preparing it I am reminded how much
I miss this man of so many talents and interests. I miss him not because I
always agreed with him, but instead because his comments about statistics in
particular and life in general always elicited my substantial reflection.
"
"  Bayesian networks (BN) are used in a big range of applications but they have
one issue concerning parameter learning. In real application, training data are
always incomplete or some nodes are hidden. To deal with this problem many
learning parameter algorithms are suggested foreground EM, Gibbs sampling and
RBE algorithms. In order to limit the search space and escape from local maxima
produced by executing EM algorithm, this paper presents a learning parameter
algorithm that is a fusion of EM and RBE algorithms. This algorithm
incorporates the range of a parameter into the EM algorithm. This range is
calculated by the first step of RBE algorithm allowing a regularization of each
parameter in bayesian network after the maximization step of the EM algorithm.
The threshold EM algorithm is applied in brain tumor diagnosis and show some
advantages and disadvantages over the EM algorithm.
"
"  The choice of the kernel is critical to the success of many learning
algorithms but it is typically left to the user. Instead, the training data can
be used to learn the kernel by selecting it out of a given family, such as that
of non-negative linear combinations of p base kernels, constrained by a trace
or L1 regularization. This paper studies the problem of learning kernels with
the same family of kernels but with an L2 regularization instead, and for
regression problems. We analyze the problem of learning kernels with ridge
regression. We derive the form of the solution of the optimization problem and
give an efficient iterative algorithm for computing that solution. We present a
novel theoretical analysis of the problem based on stability and give learning
bounds for orthogonal kernels that contain only an additive term O(pp/m) when
compared to the standard kernel ridge regression stability bound. We also
report the results of experiments indicating that L1 regularization can lead to
modest improvements for a small number of kernels, but to performance
degradations in larger-scale cases. In contrast, L2 regularization never
degrades performance and in fact achieves significant improvements with a large
number of kernels.
"
"  We consider the problem of pricing path-dependent options on a basket of
underlying assets using simulations. As an example we develop our studies using
Asian options. Asian options are derivative contracts in which the underlying
variable is the average price of given assets sampled over a period of time.
Due to this structure, Asian options display a lower volatility and are
therefore cheaper than their standard European counterparts. This paper is a
survey of some recent enhancements to improve efficiency when pricing Asian
options by Monte Carlo simulation in the Black-Scholes model. We analyze the
dynamics with constant and time-dependent volatilities of the underlying asset
returns. We present a comparison between the precision of the standard Monte
Carlo method (MC) and the stratified Latin Hypercube Sampling (LHS). In
particular, we discuss the use of low-discrepancy sequences, also known as
Quasi-Monte Carlo method (QMC), and a randomized version of these sequences,
known as Randomized Quasi Monte Carlo (RQMC). The latter has proven to be a
useful variance reduction technique for both problems of up to 20 dimensions
and for very high dimensions. Moreover, we present and test a new path
generation approach based on a Kronecker product approximation (KPA) in the
case of time-dependent volatilities. KPA proves to be a fast generation
technique and reduces the computational cost of the simulation procedure.
"
"  The evaluation of the formative process in the University system has been
assuming an ever increasing importance in the European countries. Within this
context the analysis of student performance and capabilities plays a
fundamental role. In this work we propose a multivariate latent growth model
for studying the performances of a cohort of students of the University of
Bologna. The model proposed is innovative since it is composed by: (1)
multivariate growth models that allow to capture the different dynamics of
student performance indicators over time and (2) a factor model that allows to
measure the general latent student capability. The flexibility of the model
proposed allows its applications in several fields such as socio-economic
settings in which personal behaviours are studied by using panel data.
"
"  Political polls achieve their results by sampling a small number of potential
voters rather than the population as a whole. This leads to sampling error
which most polling agencies dutifully report. But factors such as
nonrepresentative samples, question wording and nonresponse can produce
non-sampling errors. While pollsters are aware of such errors, they are
difficult to quantify and seldom reported. When a polling agency, whether by
intention or not, produces results with non-sampling errors that systematically
favor one candidate over another, then that agency's poll is biased. We
analyzed polling data for the (on-going) 2008 Presidential race, and though our
methods do not allow us to identify which agencies' polls are biased, they do
provide significant evidence that some agencies' polls are.
  We compared polls produced by major television networks with those produced
by Gallup and Rasmussen. We found that, taken as a whole, polls produced by the
networks were significantly to the left of those produced by Gallup and
Rasmussen. We used the available data to provide a tentative ordering of the
major television networks' polls from right to left. Our order was: FOX, CNN,
NBC (which partners with the Wall Street Journal), ABC (which partners with the
Washington Post), CBS (which partners with the New York Times). These results
appear to comport well with the informal perceptions of the political leanings
of these agencies.
  Our findings are preliminary, but they make a case for further research into
the causes of and remedies for polling bias.
"
"  Sparse data models, where data is assumed to be well represented as a linear
combination of a few elements from a dictionary, have gained considerable
attention in recent years, and their use has led to state-of-the-art results in
many signal and image processing tasks. It is now well understood that the
choice of the sparsity regularization term is critical in the success of such
models. Based on a codelength minimization interpretation of sparse coding, and
using tools from universal coding theory, we propose a framework for designing
sparsity regularization terms which have theoretical and practical advantages
when compared to the more standard l0 or l1 ones. The presentation of the
framework and theoretical foundations is complemented with examples that show
its practical advantages in image denoising, zooming and classification.
"
"  It was proved in 1998 by Ben-David and Litman that a concept space has a
sample compression scheme of size d if and only if every finite subspace has a
sample compression scheme of size d. In the compactness theorem, measurability
of the hypotheses of the created sample compression scheme is not guaranteed;
at the same time measurability of the hypotheses is a necessary condition for
learnability. In this thesis we discuss when a sample compression scheme,
created from com- pression schemes on finite subspaces via the compactness
theorem, have measurable hypotheses. We show that if X is a standard Borel
space with a d-maximum and universally separable concept class C, then (X,C)
has a sample compression scheme of size d with universally Borel measurable
hypotheses. Additionally we introduce a new variant of compression scheme
called a copy sample compression scheme.
"
"  Flood quantile estimation is of great importance for many engineering studies
and policy decisions. However, practitioners must often deal with small data
available. Thus, the information must be used optimally. In the last decades,
to reduce the waste of data, inferential methodology has evolved from annual
maxima modeling to peaks over a threshold one. To mitigate the lack of data,
peaks over a threshold are sometimes combined with additional information -
mostly regional and historical information. However, whatever the extra
information is, the most precious information for the practitioner is found at
the target site. In this study, a model that allows inferences on the whole
time series is introduced. In particular, the proposed model takes into account
the dependence between successive extreme observations using an appropriate
extremal dependence structure. Results show that this model leads to more
accurate flood peak quantile estimates than conventional estimators. In
addition, as the time dependence is taken into account, inferences on other
flood characteristics can be performed. An illustration is given on flood
duration. Our analysis shows that the accuracy of the proposed models to
estimate the flood duration is related to specific catchment characteristics.
Some suggestions to increase the flood duration predictions are introduced.
"
"  Many biological characteristics of evolutionary interest are not scalar
variables but continuous functions. Here we use phylogenetic Gaussian process
regression to model the evolution of simulated function-valued traits. Given
function-valued data only from the tips of an evolutionary tree and utilising
independent principal component analysis (IPCA) as a method for dimension
reduction, we construct distributional estimates of ancestral function-valued
traits, and estimate parameters describing their evolutionary dynamics.
"
"  Discussion on ""Brownian distance covariance"" by G\'{a}bor J. Sz\'{e}kely and
Maria L. Rizzo [arXiv:1010.0297]
"
"  Much of the natural variation for a complex trait can be explained by
variation in DNA sequence levels. As part of sequence variation, gene-gene
interaction has been ubiquitously observed in nature, where its role in shaping
the development of an organism has been broadly recognized. The identification
of interactions between genetic factors has been progressively pursued via
statistical or machine learning approaches. A large body of currently adopted
methods, either parametrically or nonparametrically, predominantly focus on
pairwise single marker interaction analysis. As genes are the functional units
in living organisms, analysis by focusing on a gene as a system could
potentially yield more biologically meaningful results. In this work, we
conceptually propose a gene-centric framework for genome-wide gene-gene
interaction detection. We treat each gene as a testing unit and derive a
model-based kernel machine method for two-dimensional genome-wide scanning of
gene-gene interactions. In addition to the biological advantage, our method is
statistically appealing because it reduces the number of hypotheses tested in a
genome-wide scan. Extensive simulation studies are conducted to evaluate the
performance of the method. The utility of the method is further demonstrated
with applications to two real data sets. Our method provides a conceptual
framework for the identification of gene-gene interactions which could shed
novel light on the etiology of complex diseases.
"
"  Doubly stochastic Poisson processes, also known as the Cox processes,
frequently occur in various scientific fields. In this article, motivated
primarily by analyzing Cox process data in biophysics, we propose a
nonparametric kernel-based inference method. We conduct a detailed study,
including an asymptotic analysis, of the proposed method, and provide
guidelines for its practical use, introducing a fast and stable regression
method for bandwidth selection. We apply our method to real photon arrival data
from recent single-molecule biophysical experiments, investigating proteins'
conformational dynamics. Our result shows that conformational fluctuation is
widely present in protein systems, and that the fluctuation covers a broad
range of time scales, highlighting the dynamic and complex nature of proteins'
structure.
"
"  The restricted Boltzmann machine (RBM) is a flexible tool for modeling
complex data, however there have been significant computational difficulties in
using RBMs to model high-dimensional multinomial observations. In natural
language processing applications, words are naturally modeled by K-ary discrete
distributions, where K is determined by the vocabulary size and can easily be
in the hundreds of thousands. The conventional approach to training RBMs on
word observations is limited because it requires sampling the states of K-way
softmax visible units during block Gibbs updates, an operation that takes time
linear in K. In this work, we address this issue by employing a more general
class of Markov chain Monte Carlo operators on the visible units, yielding
updates with computational complexity independent of K. We demonstrate the
success of our approach by training RBMs on hundreds of millions of word
n-grams using larger vocabularies than previously feasible and using the
learned features to improve performance on chunking and sentiment
classification tasks, achieving state-of-the-art results on the latter.
"
"  Biological structure and function depend on complex regulatory interactions
between many genes. A wealth of gene expression data is available from
high-throughput genome-wide measurement technologies, but effective gene
regulatory network inference methods are still needed. Model-based methods
founded on quantitative descriptions of gene regulation are among the most
promising, but many such methods rely on simple, local models or on ad hoc
inference approaches lacking experimental interpretability. We propose an
experimental design and develop an associated statistical method for inferring
a gene network by learning a standard quantitative, interpretable, predictive,
biophysics-based ordinary differential equation model of gene regulation. We
fit the model parameters using gene expression measurements from perturbed
steady-states of the system, like those following overexpression or knockdown
experiments. Although the original model is nonlinear, our design allows us to
transform it into a convex optimization problem by restricting attention to
steady-states and using the lasso for parameter selection. Here, we describe
the model and inference algorithm and apply them to a synthetic six-gene
system, demonstrating that the model is detailed and flexible enough to account
for activation and repression as well as synergistic and self-regulation, and
the algorithm can efficiently and accurately recover the parameters used to
generate the data.
"
"  Divergence estimators based on direct approximation of density-ratios without
going through separate approximation of numerator and denominator densities
have been successfully applied to machine learning tasks that involve
distribution comparison such as outlier detection, transfer learning, and
two-sample homogeneity test. However, since density-ratio functions often
possess high fluctuation, divergence estimation is still a challenging task in
practice. In this paper, we propose to use relative divergences for
distribution comparison, which involves approximation of relative
density-ratios. Since relative density-ratios are always smoother than
corresponding ordinary density-ratios, our proposed method is favorable in
terms of the non-parametric convergence speed. Furthermore, we show that the
proposed divergence estimator has asymptotic variance independent of the model
complexity under a parametric setup, implying that the proposed estimator
hardly overfits even with complex models. Through experiments, we demonstrate
the usefulness of the proposed approach.
"
"  Respondent-driven sampling (RDS) is a procedure to sample from hard-to-reach
populations. It has been widely used in several countries, especially in the
monitoring of HIV/AIDS and other sexually transmitted infections. Hard-to-reach
populations have had a key role in the dynamics of such epidemics and must
inform evidence-based initiatives aiming to curb their spread. In this paper,
we present a simple test for network dependence for a binary response variable.
We estimate the prevalence of the response variable. We also propose a binary
regression model taking into account the RDS structure which is included in the
model through a latent random effect with a correlation structure. The proposed
model is illustrated in a RDS study for HIV and Syphilis in men who have sex
with men implemented in Campinas (Brazil).
"
"  This paper describes a general approach to the compartmental modeling of
nuclear data based on spectral analysis and statistical optimization. We
utilize the renal physiology as test case and validate the method against both
synthetic data and real measurements acquired during two micro-PET experiments
with murine models.
"
"  We investigate the joint description of the interest-rate term stuctures of
Italy and an AAA-rated European country by mean of a --here proposed--
correlated CIR-like bivariate model where one of the state variables is
interpreted as a benchmark risk-free rate and the other as a credit spread. The
model is constructed by requiring the strict positivity of interest rates and
the asymptotic decoupling of the joint distribution of the two state variables
on a long time horizon. The second condition is met by imposing the
reversibility of the process with respect to a product measure, the first is
then implemented by using the tools of potential theory. It turns out that
these conditions select a class of non-affine models, out of which we choose
one that is quadratic in the two state variables both in the drift and
diffusion matrix. We perform a numerical analysis of the model by investigating
a cross section of the term structures comparing the results with those
obtained with an uncoupled bivariate CIR model.
"
"  MCMC methods for sampling from the space of DAGs can mix poorly due to the
local nature of the proposals that are commonly used. It has been shown that
sampling from the space of node orders yields better results [FK03, EW06].
Recently, Koivisto and Sood showed how one can analytically marginalize over
orders using dynamic programming (DP) [KS04, Koi06]. Their method computes the
exact marginal posterior edge probabilities, thus avoiding the need for MCMC.
Unfortunately, there are four drawbacks to the DP technique: it can only use
modular priors, it can only compute posteriors over modular features, it is
difficult to compute a predictive density, and it takes exponential time and
space. We show how to overcome the first three of these problems by using the
DP algorithm as a proposal distribution for MCMC in DAG space. We show that
this hybrid technique converges to the posterior faster than other methods,
resulting in more accurate structure learning and higher predictive likelihoods
on test data.
"
"  Methods for analysis of principal components in discrete data have existed
for some time under various names such as grade of membership modelling,
probabilistic latent semantic analysis, and genotype inference with admixture.
In this paper we explore a number of extensions to the common theory, and
present some application of these methods to some common statistical tasks. We
show that these methods can be interpreted as a discrete version of ICA. We
develop a hierarchical version yielding components at different levels of
detail, and additional techniques for Gibbs sampling. We compare the algorithms
on a text prediction task using support vector machines, and to information
retrieval.
"
"  We consider the problem of sequential prediction and provide tools to study
the minimax value of the associated game. Classical statistical learning theory
provides several useful complexity measures to study learning with i.i.d. data.
Our proposed sequential complexities can be seen as extensions of these
measures to the sequential setting. The developed theory is shown to yield
precise learning guarantees for the problem of sequential prediction. In
particular, we show necessary and sufficient conditions for online learnability
in the setting of supervised learning. Several examples show the utility of our
framework: we can establish learnability without having to exhibit an explicit
online learning algorithm.
"
"  This article is about estimation and inference methods for high dimensional
sparse (HDS) regression models in econometrics. High dimensional sparse models
arise in situations where many regressors (or series terms) are available and
the regression function is well-approximated by a parsimonious, yet unknown set
of regressors. The latter condition makes it possible to estimate the entire
regression function effectively by searching for approximately the right set of
regressors. We discuss methods for identifying this set of regressors and
estimating their coefficients based on $\ell_1$-penalization and describe key
theoretical results. In order to capture realistic practical situations, we
expressly allow for imperfect selection of regressors and study the impact of
this imperfect selection on estimation and inference results. We focus the main
part of the article on the use of HDS models and methods in the instrumental
variables model and the partially linear model. We present a set of novel
inference results for these models and illustrate their use with applications
to returns to schooling and growth regression.
"
"  We propose a multiresolution Gaussian process to capture long-range,
non-Markovian dependencies while allowing for abrupt changes. The
multiresolution GP hierarchically couples a collection of smooth GPs, each
defined over an element of a random nested partition. Long-range dependencies
are captured by the top-level GP while the partition points define the abrupt
changes. Due to the inherent conjugacy of the GPs, one can analytically
marginalize the GPs and compute the conditional likelihood of the observations
given the partition tree. This property allows for efficient inference of the
partition itself, for which we employ graph-theoretic techniques. We apply the
multiresolution GP to the analysis of Magnetoencephalography (MEG) recordings
of brain activity.
"
"  Gaussian belief propagation (GaBP) is an iterative algorithm for computing
the mean of a multivariate Gaussian distribution, or equivalently, the minimum
of a multivariate positive definite quadratic function. Sufficient conditions,
such as walk-summability, that guarantee the convergence and correctness of
GaBP are known, but GaBP may fail to converge to the correct solution given an
arbitrary positive definite quadratic function. As was observed in previous
work, the GaBP algorithm fails to converge if the computation trees produced by
the algorithm are not positive definite. In this work, we will show that the
failure modes of the GaBP algorithm can be understood via graph covers, and we
prove that a parameterized generalization of the min-sum algorithm can be used
to ensure that the computation trees remain positive definite whenever the
input matrix is positive definite. We demonstrate that the resulting algorithm
is closely related to other iterative schemes for quadratic minimization such
as the Gauss-Seidel and Jacobi algorithms. Finally, we observe, empirically,
that there always exists a choice of parameters such that the above
generalization of the GaBP algorithm converges.
"
"  The MEG inverse problem refers to the reconstruction of the neural activity
of the brain from magnetoencephalography (MEG) measurements. We propose a
two-way regularization (TWR) method to solve the MEG inverse problem under the
assumptions that only a small number of locations in space are responsible for
the measured signals (focality), and each source time course is smooth in time
(smoothness). The focality and smoothness of the reconstructed signals are
ensured respectively by imposing a sparsity-inducing penalty and a roughness
penalty in the data fitting criterion. A two-stage algorithm is developed for
fast computation, where a raw estimate of the source time course is obtained in
the first stage and then refined in the second stage by the two-way
regularization. The proposed method is shown to be effective on both synthetic
and real-world examples.
"
"  This short note points out two of the incongruences that I find in the Loredo
(2012) comments on Andreon (2012), i.e. on my chapter written for the book
""Astrostatistical Challenges for the New Astronomy"". First, I find illogic the
Loredo decision of putting my chapter among those presenting simple models,
because one of the models illustrated in my chapter is qualified by him as
""impressing for his complexity"". Second, Loredo criticizes my chapter at one
location confusing it with another paper by another author, because my chapter
do not touch the subject mentioned by Loredo (2012) critics, the comparison
between Bayesian and frequentist fitting models.
"
"  Deducing the structure of neural circuits is one of the central problems of
modern neuroscience. Recently-introduced calcium fluorescent imaging methods
permit experimentalists to observe network activity in large populations of
neurons, but these techniques provide only indirect observations of neural
spike trains, with limited time resolution and signal quality. In this work we
present a Bayesian approach for inferring neural circuitry given this type of
imaging data. We model the network activity in terms of a collection of coupled
hidden Markov chains, with each chain corresponding to a single neuron in the
network and the coupling between the chains reflecting the network's
connectivity matrix. We derive a Monte Carlo Expectation--Maximization
algorithm for fitting the model parameters; to obtain the sufficient statistics
in a computationally-efficient manner, we introduce a specialized
blockwise-Gibbs algorithm for sampling from the joint activity of all observed
neurons given the observed fluorescence data. We perform large-scale
simulations of randomly connected neuronal networks with biophysically
realistic parameters and find that the proposed methods can accurately infer
the connectivity in these networks given reasonable experimental and
computational constraints. In addition, the estimation accuracy may be improved
significantly by incorporating prior knowledge about the sparseness of
connectivity in the network, via standard L$_1$ penalization methods.
"
"  This work gives a simultaneous analysis of both the ordinary least squares
estimator and the ridge regression estimator in the random design setting under
mild assumptions on the covariate/response distributions. In particular, the
analysis provides sharp results on the ``out-of-sample'' prediction error, as
opposed to the ``in-sample'' (fixed design) error. The analysis also reveals
the effect of errors in the estimated covariance structure, as well as the
effect of modeling errors, neither of which effects are present in the fixed
design setting. The proofs of the main results are based on a simple
decomposition lemma combined with concentration inequalities for random vectors
and matrices.
"
"  We investigate the learning rate of multiple kernel leaning (MKL) with
elastic-net regularization, which consists of an $\ell_1$-regularizer for
inducing the sparsity and an $\ell_2$-regularizer for controlling the
smoothness. We focus on a sparse setting where the total number of kernels is
large but the number of non-zero components of the ground truth is relatively
small, and prove that elastic-net MKL achieves the minimax learning rate on the
$\ell_2$-mixed-norm ball. Our bound is sharper than the convergence rates ever
shown, and has a property that the smoother the truth is, the faster the
convergence rate is.
"
"  Temporal networks are ubiquitous and evolve over time by the addition,
deletion, and changing of links, nodes, and attributes. Although many
relational datasets contain temporal information, the majority of existing
techniques in relational learning focus on static snapshots and ignore the
temporal dynamics. We propose a framework for discovering temporal
representations of relational data to increase the accuracy of statistical
relational learning algorithms. The temporal relational representations serve
as a basis for classification, ensembles, and pattern mining in evolving
domains. The framework includes (1) selecting the time-varying relational
components (links, attributes, nodes), (2) selecting the temporal granularity,
(3) predicting the temporal influence of each time-varying relational
component, and (4) choosing the weighted relational classifier. Additionally,
we propose temporal ensemble methods that exploit the temporal-dimension of
relational data. These ensembles outperform traditional and more sophisticated
relational ensembles while avoiding the issue of learning the most optimal
representation. Finally, the space of temporal-relational models are evaluated
using a sample of classifiers. In all cases, the proposed temporal-relational
classifiers outperform competing models that ignore the temporal information.
The results demonstrate the capability and necessity of the temporal-relational
representations for classification, ensembles, and for mining temporal
datasets.
"
"  Statistical tests of earthquake predictions require a null hypothesis to
model occasional chance successes. To define and quantify `chance success' is
knotty. Some null hypotheses ascribe chance to the Earth: Seismicity is modeled
as random. The null distribution of the number of successful predictions -- or
any other test statistic -- is taken to be its distribution when the fixed set
of predictions is applied to random seismicity. Such tests tacitly assume that
the predictions do not depend on the observed seismicity. Conditioning on the
predictions in this way sets a low hurdle for statistical significance.
Consider this scheme: When an earthquake of magnitude 5.5 or greater occurs
anywhere in the world, predict that an earthquake at least as large will occur
within 21 days and within an epicentral distance of 50 km. We apply this rule
to the Harvard centroid-moment-tensor (CMT) catalog for 2000--2004 to generate
a set of predictions. The null hypothesis is that earthquake times are
exchangeable conditional on their magnitudes and locations and on the
predictions--a common ``nonparametric'' assumption in the literature. We
generate random seismicity by permuting the times of events in the CMT catalog.
We consider an event successfully predicted only if (i) it is predicted and
(ii) there is no larger event within 50 km in the previous 21 days. The
$P$-value for the observed success rate is $<0.001$: The method successfully
predicts about 5% of earthquakes, far better than `chance,' because the
predictor exploits the clustering of earthquakes -- occasional foreshocks --
which the null hypothesis lacks. Rather than condition on the predictions and
use a stochastic model for seismicity, it is preferable to treat the observed
seismicity as fixed, and to compare the success rate of the predictions to the
success rate of simple-minded predictions like those just described. If the
proffered predictions do no better than a simple scheme, they have little
value.
"
"  There are (at least) three approaches to quantifying information. The first,
algorithmic information or Kolmogorov complexity, takes events as strings and,
given a universal Turing machine, quantifies the information content of a
string as the length of the shortest program producing it. The second, Shannon
information, takes events as belonging to ensembles and quantifies the
information resulting from observing the given event in terms of the number of
alternate events that have been ruled out. The third, statistical learning
theory, has introduced measures of capacity that control (in part) the expected
risk of classifiers. These capacities quantify the expectations regarding
future data that learning algorithms embed into classifiers.
  This note describes a new method of quantifying information, effective
information, that links algorithmic information to Shannon information, and
also links both to capacities arising in statistical learning theory. After
introducing the measure, we show that it provides a non-universal analog of
Kolmogorov complexity. We then apply it to derive basic capacities in
statistical learning theory: empirical VC-entropy and empirical Rademacher
complexity. A nice byproduct of our approach is an interpretation of the
explanatory power of a learning algorithm in terms of the number of hypotheses
it falsifies, counted in two different ways for the two capacities. We also
discuss how effective information relates to information gain, Shannon and
mutual information.
"
"  Unsupervised discovery of latent representations, in addition to being useful
for density modeling, visualisation and exploratory data analysis, is also
increasingly important for learning features relevant to discriminative tasks.
Autoencoders, in particular, have proven to be an effective way to learn latent
codes that reflect meaningful variations in data. A continuing challenge,
however, is guiding an autoencoder toward representations that are useful for
particular tasks. A complementary challenge is to find codes that are invariant
to irrelevant transformations of the data. The most common way of introducing
such problem-specific guidance in autoencoders has been through the
incorporation of a parametric component that ties the latent representation to
the label information. In this work, we argue that a preferable approach relies
instead on a nonparametric guidance mechanism. Conceptually, it ensures that
there exists a function that can predict the label information, without
explicitly instantiating that function. The superiority of this guidance
mechanism is confirmed on two datasets. In particular, this approach is able to
incorporate invariance information (lighting, elevation, etc.) from the small
NORB object recognition dataset and yields state-of-the-art performance for a
single layer, non-convolutional network.
"
"  This article studies local and global inference for smoothing spline
estimation in a unified asymptotic framework. We first introduce a new
technical tool called functional Bahadur representation, which significantly
generalizes the traditional Bahadur representation in parametric models, that
is, Bahadur [Ann. Inst. Statist. Math. 37 (1966) 577-580]. Equipped with this
tool, we develop four interconnected procedures for inference: (i) pointwise
confidence interval; (ii) local likelihood ratio testing; (iii) simultaneous
confidence band; (iv) global likelihood ratio testing. In particular, our
confidence intervals are proved to be asymptotically valid at any point in the
support, and they are shorter on average than the Bayesian confidence intervals
proposed by Wahba [J. R. Stat. Soc. Ser. B Stat. Methodol. 45 (1983) 133-150]
and Nychka [J. Amer. Statist. Assoc. 83 (1988) 1134-1143]. We also discuss a
version of the Wilks phenomenon arising from local/global likelihood ratio
testing. It is also worth noting that our simultaneous confidence bands are the
first ones applicable to general quasi-likelihood models. Furthermore, issues
relating to optimality and efficiency are carefully addressed. As a by-product,
we discover a surprising relationship between periodic and nonperiodic
smoothing splines in terms of inference.
"
"  We construct an adaptive independent Metropolis-Hastings sampler that uses a
mixture of normals as a proposal distribution. To take full advantage of the
potential of adaptive sampling our algorithm updates the mixture of normals
frequently, starting early in the chain. The algorithm is built for speed and
reliability and its sampling performance is evaluated with real and simulated
examples. Our article outlines conditions for adaptive sampling to hold and
gives a readily accessible proof that under these conditions the sampling
scheme generates iterates that converge to the target distribution.
"
"  Imposition of a lasso penalty shrinks parameter estimates toward zero and
performs continuous model selection. Lasso penalized regression is capable of
handling linear regression problems where the number of predictors far exceeds
the number of cases. This paper tests two exceptionally fast algorithms for
estimating regression coefficients with a lasso penalty. The previously known
$\ell_2$ algorithm is based on cyclic coordinate descent. Our new $\ell_1$
algorithm is based on greedy coordinate descent and Edgeworth's algorithm for
ordinary $\ell_1$ regression. Each algorithm relies on a tuning constant that
can be chosen by cross-validation. In some regression problems it is natural to
group parameters and penalize parameters group by group rather than separately.
If the group penalty is proportional to the Euclidean norm of the parameters of
the group, then it is possible to majorize the norm and reduce parameter
estimation to $\ell_2$ regression with a lasso penalty. Thus, the existing
algorithm can be extended to novel settings. Each of the algorithms discussed
is tested via either simulated or real data or both. The Appendix proves that a
greedy form of the $\ell_2$ algorithm converges to the minimum value of the
objective function.
"
"  Much effort has been directed at algorithms for obtaining the highest
probability configuration in a probabilistic random field model known as the
maximum a posteriori (MAP) inference problem. In many situations, one could
benefit from having not just a single solution, but the top M most probable
solutions known as the M-Best MAP problem. In this paper, we propose an
efficient message-passing based algorithm for solving the M-Best MAP problem.
Specifically, our algorithm solves the recently proposed Linear Programming
(LP) formulation of M-Best MAP [7], while being orders of magnitude faster than
a generic LP-solver. Our approach relies on studying a particular partial
Lagrangian relaxation of the M-Best MAP LP which exposes a natural
combinatorial structure of the problem that we exploit.
"
"  Appropriately designing the proposal kernel of particle filters is an issue
of significant importance, since a bad choice may lead to deterioration of the
particle sample and, consequently, waste of computational power. In this paper
we introduce a novel algorithm adaptively approximating the so-called optimal
proposal kernel by a mixture of integrated curved exponential distributions
with logistic weights. This family of distributions, referred to as mixtures of
experts, is broad enough to be used in the presence of multi-modality or
strongly skewed distributions. The mixtures are fitted, via online-EM methods,
to the optimal kernel through minimisation of the Kullback-Leibler divergence
between the auxiliary target and instrumental distributions of the particle
filter. At each iteration of the particle filter, the algorithm is required to
solve only a single optimisation problem for the whole particle sample,
yielding an algorithm with only linear complexity. In addition, we illustrate
in a simulation study how the method can be successfully applied to optimal
filtering in nonlinear state-space models.
"
"  A dynamic decision-making system that includes a mass of indistinguishable
agents could manifest impressive heterogeneity. This kind of nonhomogeneity is
postulated to result from macroscopic behavioral tactics employed by almost all
involved agents. A State-Space Based (SSB) mass event-history model is
developed here to explore the potential existence of such macroscopic
behaviors. By imposing an unobserved internal state-space variable into the
system, each individual's event-history is made into a composition of a common
state duration and an individual specific time to action. With the common state
modeling of the macroscopic behavior, parametric statistical inferences are
derived under the current-status data structure and conditional independence
assumptions. Identifiability and computation related problems are also
addressed. From the dynamic perspectives of system-wise heterogeneity, this SSB
mass event-history model is shown to be very distinct from a random effect
model via the Principle Component Analysis (PCA) in a numerical experiment.
Real data showing the mass invasion by two species of parasitic nematode into
two species of host larvae are also analyzed. The analysis results not only are
found coherent in the context of the biology of the nematode as a parasite, but
also include new quantitative interpretations.
"
"  In this article, we address the question of how non-knowledge about future
events that influence economic agents' decisions in choice settings has been
formally represented in economic theory up to date. To position our discussion
within the ongoing debate on uncertainty, we provide a brief review of
historical developments in economic theory and decision theory on the
description of economic agents' choice behaviour under conditions of
uncertainty, understood as either (i) ambiguity, or (ii) unawareness.
Accordingly, we identify and discuss two approaches to the formalisation of
non-knowledge: one based on decision-making in the context of a state space
representing the exogenous world, as in Savage's axiomatisation and some
successor concepts (ambiguity as situations with unknown probabilities), and
one based on decision-making over a set of menus of potential future
opportunities, providing the possibility of derivation of agents' subjective
state spaces (unawareness as situation with imperfect subjective knowledge of
all future events possible). We also discuss impeding challenges of the
formalisation of non-knowledge.
"
"  Stochastic processes often exhibit sudden systematic changes in pattern a
short time before certain failure events. Examples include increase in medical
costs before death and decrease in CD4 counts before AIDS diagnosis. To study
such terminal behavior of stochastic processes, a natural and direct way is to
align the processes using failure events as time origins. This paper studies
backward stochastic processes counting time backward from failure events, and
proposes one-sample nonparametric estimation of the mean of backward processes
when follow-up is subject to left truncation and right censoring. We will
discuss benefits of including prevalent cohort data to enlarge the identifiable
region and large sample properties of the proposed estimator with related
extensions. A SEER--Medicare linked data set is used to illustrate the proposed
methodologies.
"
"  The classical mixture of Gaussians model is related to K-means via
small-variance asymptotics: as the covariances of the Gaussians tend to zero,
the negative log-likelihood of the mixture of Gaussians model approaches the
K-means objective, and the EM algorithm approaches the K-means algorithm. Kulis
& Jordan (2012) used this observation to obtain a novel K-means-like algorithm
from a Gibbs sampler for the Dirichlet process (DP) mixture. We instead
consider applying small-variance asymptotics directly to the posterior in
Bayesian nonparametric models. This framework is independent of any specific
Bayesian inference algorithm, and it has the major advantage that it
generalizes immediately to a range of models beyond the DP mixture. To
illustrate, we apply our framework to the feature learning setting, where the
beta process and Indian buffet process provide an appropriate Bayesian
nonparametric prior. We obtain a novel objective function that goes beyond
clustering to learn (and penalize new) groupings for which we relax the mutual
exclusivity and exhaustivity assumptions of clustering. We demonstrate several
other algorithms, all of which are scalable and simple to implement. Empirical
results demonstrate the benefits of the new framework.
"
"  In this paper we apply a two-stage sequential design to item calibration
problems under a three-parameter logistic model assumption. The measurement
errors of the estimates of the latent trait levels of examinees are considered
in our procedure. Moreover, a sequential procedure is employed to guarantee
that the estimates of the parameters reach a prescribed accuracy criterion when
the iteration is stopped, which fully takes the advantage of sequential design.
Statistical properties of both the item parameter estimates and the sequential
procedure are discussed. We compare the performance of the proposed method with
that of the procedures based on some conventional designs using numerical
studies.
"
"  In this paper we present a new approach for tightening upper bounds on the
partition function. Our upper bounds are based on fractional covering bounds on
the entropy function, and result in a concave program to compute these bounds
and a convex program to tighten them. To solve these programs effectively for
general region graphs we utilize the entropy barrier method, thus decomposing
the original programs by their dual programs and solve them with dual block
optimization scheme. The entropy barrier method provides an elegant framework
to generalize the message-passing scheme to high-order region graph, as well as
to solve the block dual steps in closed-form. This is a key for computational
relevancy for large problems with thousands of regions.
"
"  Our concern is selecting the concentration matrix's nonzero coefficients for
a sparse Gaussian graphical model in a high-dimensional setting. This
corresponds to estimating the graph of conditional dependencies between the
variables. We describe a novel framework taking into account a latent structure
on the concentration matrix. This latent structure is used to drive a penalty
matrix and thus to recover a graphical model with a constrained topology. Our
method uses an $\ell_1$ penalized likelihood criterion. Inference of the graph
of conditional dependencies between the variates and of the hidden variables is
performed simultaneously in an iterative \textsc{em}-like algorithm. The
performances of our method is illustrated on synthetic as well as real data,
the latter concerning breast cancer.
"
"  An important problem in wireless sensor networks is to find the minimal
number of randomly deployed sensors making a network connected with a given
probability. In practice sensors are often deployed one by one along a
trajectory of a vehicle, so it is natural to assume that arbitrary probability
density functions of distances between successive sensors in a segment are
given. The paper computes the probability of connectivity and coverage of
1-dimensional networks and gives estimates for a minimal number of sensors for
important distributions.
"
"  In this paper we formulate in general terms an approach to prove strong
consistency of the Empirical Risk Minimisation inductive principle applied to
the prototype or distance based clustering. This approach was motivated by the
Divisive Information-Theoretic Feature Clustering model in probabilistic space
with Kullback-Leibler divergence which may be regarded as a special case within
the Clustering Minimisation framework. Also, we propose clustering
regularization restricting creation of additional clusters which are not
significant or are not essentially different comparing with existing clusters.
"
"  I reflect on the statistical methods of the Christakis-Fowler studies on
network-based contagion of traits by checking the sensitivity of these kinds of
results to various alternate specifications and generative mechanisms. Despite
the honest efforts of all involved, I remain pessimistic about establishing
whether binary health outcomes or product adoptions are contagious if the
evidence comes from simultaneously observed data.
"
"  Large contingency tables arise in many contexts but especially in the
collection of survey and census data by government statistical agencies.
Because the vast majority of the variables in this context have a large number
of categories, agencies and users need a systematic way of constructing tables
which are summaries of such contingency tables. We propose such an approach in
this paper by finding members of a class of restricted log-linear models which
maximize the likelihood of the data and use this to find a parsimonious means
of representing the table. In contrast with more standard approaches for model
search in hierarchical log-linear models (HLLM), our procedure systematically
reduces the number of categories of the variables. Through a series of
examples, we illustrate the extent to which it can preserve the interaction
structure found with HLLMs and be used as a data simplification procedure prior
to HLL modeling. A feature of the procedure is that it can easily be applied to
many tables with millions of cells, providing a new way of summarizing large
data sets in many disciplines. The focus is on information and description
rather than statistical testing. The procedure may treat each variable in the
table in different ways, preserving full detail, treating it as fully nominal,
or preserving ordinality.
"
"  This paper is an invited commentary on Tamas Budavari's presentation, ""On
statistical cross-identification in astronomy,"" for the Statistical Challenges
in Modern Astronomy V conference held at Pennsylvania State University in June
2011. I begin with a brief review of previous work on probabilistic (Bayesian)
assessment of directional and spatio-temporal coincidences in astronomy (e.g.,
cross-matching or cross-identification of objects across multiple catalogs).
Then I discuss an open issue in the recent innovative work of Budavari and his
colleagues on large-scale probabilistic cross-identification: how to assign
prior probabilities that play an important role in the analysis. With a simple
toy problem, I show how Bayesian multilevel modeling (hierarchical Bayes)
provides a principled framework that justifies and generalizes pragmatic rules
of thumb that have been successfully used by Budavari's team to assign priors.
"
"  Biomedical studies have a common interest in assessing relationships between
multiple related health outcomes and high-dimensional predictors. For example,
in reproductive epidemiology, one may collect pregnancy outcomes such as length
of gestation and birth weight and predictors such as single nucleotide
polymorphisms in multiple candidate genes and environmental exposures. In such
settings, there is a need for simple yet flexible methods for selecting true
predictors of adverse health responses from a high-dimensional set of candidate
predictors. To address this problem, one may either consider linear regression
models for the continuous outcomes or convert these outcomes into binary
indicators of adverse responses using pre-defined cutoffs. The former strategy
has the disadvantage of often leading to a poorly fitting model that does not
predict risk well, while the latter approach can be very sensitive to the
cutoff choice. As a simple yet flexible alternative, we propose a method for
adverse subpopulation regression (ASPR), which relies on a two component latent
class model, with the dominant component corresponding to (presumed) healthy
individuals and the risk of falling in the minority component characterized via
a logistic regression. The logistic regression model is designed to accommodate
high-dimensional predictors, as occur in studies with a large number of gene by
environment interactions, through use of a flexible nonparametric multiple
shrinkage approach. The Gibbs sampler is developed for posterior computation.
The methods are evaluated using simulation studies and applied to a genetic
epidemiology study of pregnancy outcomes.
"
"  Cell populations are never truly homogeneous; individual cells exist in
biochemical states that define functional differences between them. New
technology based on microfluidic arrays combined with multiplexed quantitative
polymerase chain reactions (qPCR) now enables high-throughput single-cell gene
expression measurement, allowing assessment of cellular heterogeneity. However
very little analytic tools have been developed specifically for the statistical
and analytical challenges of single-cell qPCR data. We present a statistical
framework for the exploration, quality control, and analysis of single-cell
gene expression data from microfluidic arrays. We assess accuracy and
within-sample heterogeneity of single-cell expression and develop quality
control criteria to filter unreliable cell measurements. We propose a
statistical model accounting for the fact that genes at the single-cell level
can be on (and for which a continuous expression measure is recorded) or
dichotomously off (and the recorded expression is zero). Based on this model,
we derive a combined likelihood-ratio test for differential expression that
incorporates both the discrete and continuous components. Using an experiment
that examines treatment-specific changes in expression, we show that this
combined test is more powerful than either the continuous or dichotomous
component in isolation, or a t-test on the zero-inflated data. While developed
for measurements from a specific platform (Fluidigm), these tools are
generalizable to other multi-parametric measures over large numbers of events.
"
"  Previous studies have suggested a link between alcohol outlets and assaultive
violence. In this paper, we explore the effects of alcohol availability on
assault crimes at the census tract level over time. The statistical analysis is
challenged by several features of the data: (1) the effects of possible
covariates (for example, the alcohol outlet density of each census tract) on
the assaultive crime rates may be complex; (2) the covariates may be highly
correlated with each other; (3) there are a lot of missing inputs in the data;
and (4) spatial correlations exist in the outcome assaultive crime rates. We
propose a hierarchical additive model, where the nonlinear correlations and the
complex interaction effects are modeled using the multiple additive regression
trees (MART) and the spatial variances in the assaultive rates that cannot be
explained by the specified covariates are smoothed trough the Conditional
Autoregressive (CAR) model. We develop a two-stage algorithm that connect the
non-parametric trees with CAR to look for important variables covariates
associated with the assaultive crime rates, while taking account of the spatial
correlations among adjacent census tracts. The proposed methods are applied to
the Los Angeles assaultive data (1990-1999) and compared with traditional
method.
"
"  The following electromagnetism (EM) inverse problem is addressed. It consists
in estimating local radioelectric properties of materials recovering an object
from the global EM scattering measurement, at various incidences and wave
frequencies. This large scale ill-posed inverse problem is explored by an
intensive exploitation of an efficient 2D Maxwell solver, distributed on High
Performance Computing (HPC) machines. Applied to a large training data set, a
statistical analysis reduces the problem to a simpler probabilistic metamodel,
on which Bayesian inference can be performed. Considering the radioelectric
properties as a dynamic stochastic process, evolving in function of the
frequency, it is shown how advanced Markov Chain Monte Carlo methods, called
Sequential Monte Carlo (SMC) or interacting particles, can provide estimations
of the EM properties of each material, and their associated uncertainties.
"
"  This paper introduces a new method for performing computational inference on
log-Gaussian Cox processes. The likelihood is approximated directly by making
novel use of a continuously specified Gaussian random field. We show that for
sufficiently smooth Gaussian random field prior distributions, the
approximation can converge with arbitrarily high order, while an approximation
based on a counting process on a partition of the domain only achieves
first-order convergence. The given results improve on the general theory of
convergence of the stochastic partial differential equation models, introduced
by Lindgren et al. (2011). The new method is demonstrated on a standard point
pattern data set and two interesting extensions to the classical log-Gaussian
Cox process framework are discussed. The first extension considers variable
sampling effort throughout the observation window and implements the method of
Chakraborty et al. (2011). The second extension constructs a log-Gaussian Cox
process on the world's oceans. The analysis is performed using integrated
nested Laplace approximation for fast approximate inference.
"
"  Statistical inference on graphs is a burgeoning field in the applied and
theoretical statistics communities, as well as throughout the wider world of
science, engineering, business, etc. In many applications, we are faced with
the reality of errorfully observed graphs. That is, the existence of an edge
between two vertices is based on some imperfect assessment. In this paper, we
consider a graph $G = (V,E)$. We wish to perform an inference task -- the
inference task considered here is ""vertex classification"". However, we do not
observe $G$; rather, for each potential edge $uv \in {{V}\choose{2}}$ we
observe an ""edge-feature"" which we use to classify $uv$ as edge/not-edge. Thus
we errorfully observe $G$ when we observe the graph $\widetilde{G} =
(V,\widetilde{E})$ as the edges in $\widetilde{E}$ arise from the
classifications of the ""edge-features"", and are expected to be errorful.
Moreover, we face a quantity/quality trade-off regarding the edge-features we
observe -- more informative edge-features are more expensive, and hence the
number of potential edges that can be assessed decreases with the quality of
the edge-features. We studied this problem by formulating a quantity/quality
tradeoff for a simple class of random graphs model, namely the stochastic
blockmodel. We then consider a simple but optimal vertex classifier for
classifying $v$ and we derive the optimal quantity/quality operating point for
subsequent graph inference in the face of this trade-off. The optimal operating
points for the quantity/quality trade-off are surprising and illustrate the
issue that methods for intermediate tasks should be chosen to maximize
performance for the ultimate inference task. Finally, we investigate the
quantity/quality tradeoff for errorful obesrvations of the {\it C.\ elegans}
connectome graph.
"
"  This paper presents algorithms for hierarchical, agglomerative clustering
which perform most efficiently in the general-purpose setup that is given in
modern standard software. Requirements are: (1) the input data is given by
pairwise dissimilarities between data points, but extensions to vector data are
also discussed (2) the output is a ""stepwise dendrogram"", a data structure
which is shared by all implementations in current standard software. We present
algorithms (old and new) which perform clustering in this setting efficiently,
both in an asymptotic worst-case analysis and from a practical point of view.
The main contributions of this paper are: (1) We present a new algorithm which
is suitable for any distance update scheme and performs significantly better
than the existing algorithms. (2) We prove the correctness of two algorithms by
Rohlf and Murtagh, which is necessary in each case for different reasons. (3)
We give well-founded recommendations for the best current algorithms for the
various agglomerative clustering schemes.
"
"  Structured sparse coding and the related structured dictionary learning
problems are novel research areas in machine learning. In this paper we present
a new application of structured dictionary learning for collaborative filtering
based recommender systems. Our extensive numerical experiments demonstrate that
the presented technique outperforms its state-of-the-art competitors and has
several advantages over approaches that do not put structured constraints on
the dictionary elements.
"
"  This paper describes a novel method to approximate the polynomial
coefficients of regression functions, with particular interest on
multi-dimensional classification. The derivation is simple, and offers a fast,
robust classification technique that is resistant to over-fitting.
"
"  Using former maps, geographers intend to study the evolution of the land
cover in order to have a prospective approach on the future landscape;
predictions of the future land cover, by the use of older maps and
environmental variables, are usually done through the GIS (Geographic
Information System). We propose here to confront this classical geographical
approach with statistical approaches: a linear parametric model (polychotomous
regression modeling) and a nonparametric one (multilayer perceptron). These
methodologies have been tested on two real areas on which the land cover is
known at various dates; this allows us to emphasize the benefit of these two
statistical approaches compared to GIS and to discuss the way GIS could be
improved by the use of statistical models.
"
"  We describe $k$-MLE, a fast and efficient local search algorithm for learning
finite statistical mixtures of exponential families such as Gaussian mixture
models. Mixture models are traditionally learned using the
expectation-maximization (EM) soft clustering technique that monotonically
increases the incomplete (expected complete) likelihood. Given prescribed
mixture weights, the hard clustering $k$-MLE algorithm iteratively assigns data
to the most likely weighted component and update the component models using
Maximum Likelihood Estimators (MLEs). Using the duality between exponential
families and Bregman divergences, we prove that the local convergence of the
complete likelihood of $k$-MLE follows directly from the convergence of a dual
additively weighted Bregman hard clustering. The inner loop of $k$-MLE can be
implemented using any $k$-means heuristic like the celebrated Lloyd's batched
or Hartigan's greedy swap updates. We then show how to update the mixture
weights by minimizing a cross-entropy criterion that implies to update weights
by taking the relative proportion of cluster points, and reiterate the mixture
parameter update and mixture weight update processes until convergence. Hard EM
is interpreted as a special case of $k$-MLE when both the component update and
the weight update are performed successively in the inner loop. To initialize
$k$-MLE, we propose $k$-MLE++, a careful initialization of $k$-MLE guaranteeing
probabilistically a global bound on the best possible complete likelihood.
"
"  We have developed a strategy for the analysis of newly available binary data
to improve outcome predictions based on existing data (binary or non-binary).
Our strategy involves two modeling approaches for the newly available data, one
combining binary covariate selection via LASSO with logistic regression and one
based on logic trees. The results of these models are then compared to the
results of a model based on existing data with the objective of combining model
results to achieve the most accurate predictions. The combination of model
predictions is aided by the use of support vector machines to identify
subspaces of the covariate space in which specific models lead to successful
predictions. We demonstrate our approach in the analysis of single nucleotide
polymorphism (SNP) data and traditional clinical risk factors for the
prediction of coronary heart disease.
"
"  A new approach called RESID is proposed in this paper for estimating
reliability of a software allowing for imperfect debugging. Unlike earlier
approaches based on counting number of bugs or modelling inter-failure time
gaps, RESID focuses on the probability of ""bugginess"" of different parts of a
program buggy. This perspective allows an easy way to incorporate the structure
of the software under test, as well as imperfect debugging. One main design
objective behind RESID is ease of implementation in practical scenarios.
"
"  Recently, a lot of effort has been paid to the efficient computation of
Kriging predictors when observations are assimilated sequentially. In
particular, Kriging update formulae enabling significant computational savings
were derived in Barnes and Watson (1992), Gao et al. (1996), and Emery (2009).
Taking advantage of the previous Kriging mean and variance calculations helps
avoiding a costly $(n+1) \times (n+1)$ matrix inversion when adding one
observation to the $n$ already available ones. In addition to traditional
update formulae taking into account a single new observation, Emery (2009) also
proposed formulae for the batch-sequential case, i.e. when $r > 1$ new
observations are simultaneously assimilated. However, the Kriging variance and
covariance formulae given without proof in Emery (2009) for the
batch-sequential case are not correct. In this paper we fix this issue and
establish corrected expressions for updated Kriging variances and covariances
when assimilating several observations in parallel.
"
"  Respondent-Driven Sampling (RDS) is an approach to sampling design and
inference in hard-to-reach human populations. Typically, a sampling frame is
not available, and population members are difficult to identify or recruit from
broader sampling frames. Common examples include injecting drug users, men who
have sex with men, and female sex workers. Most analysis of RDS data has
focused on estimating aggregate characteristics, such as disease prevalence.
However, RDS is often conducted in settings where the population size is
unknown and of great independent interest. This paper presents an approach to
estimating the size of a target population based on data collected through RDS.
  The proposed approach uses a successive sampling approximation to RDS to
leverage information in the ordered sequence of observed personal network
sizes. The inference uses the Bayesian framework, allowing for the
incorporation of prior knowledge. A flexible class of priors for the population
size is proposed that aids elicitation. An extensive simulation study provides
insight into the performance of the method for estimating population size under
a broad range of conditions. A further study shows the approach also improves
estimation of aggregate characteristics. A particular choice of the prior
produces interval estimates with good frequentist properties. Finally, the
method demonstrates sensible results when used to estimate the numbers of
sub-populations most at risk for HIV in two cities in El Salvador.
"
"  This paper analyses the problem of Gaussian process (GP) bandits with
deterministic observations. The analysis uses a branch and bound algorithm that
is related to the UCB algorithm of (Srinivas et al., 2010). For GPs with
Gaussian observation noise, with variance strictly greater than zero, (Srinivas
et al., 2010) proved that the regret vanishes at the approximate rate of
$O(\frac{1}{\sqrt{t}})$, where t is the number of observations. To complement
their result, we attack the deterministic case and attain a much faster
exponential convergence rate. Under some regularity assumptions, we show that
the regret decreases asymptotically according to $O(e^{-\frac{\tau t}{(\ln
t)^{d/4}}})$ with high probability. Here, d is the dimension of the search
space and $\tau$ is a constant that depends on the behaviour of the objective
function near its global maximum.
"
"  We propose a novel statistical hypothesis testing method for detection of
objects in noisy images. The method uses results from percolation theory and
random graph theory. We present an algorithm that allows to detect objects of
unknown shapes in the presence of nonparametric noise of unknown level and of
unknown distribution. No boundary shape constraints are imposed on the object,
only a weak bulk condition for the object's interior is required. The algorithm
has linear complexity and exponential accuracy and is appropriate for real-time
systems. In this paper, we develop further the mathematical formalism of our
method and explore important connections to the mathematical theory of
percolation and statistical physics. We prove results on consistency and
algorithmic complexity of our testing procedure. In addition, we address not
only an asymptotic behavior of the method, but also a finite sample performance
of our test.
"
"  Important information concerning a multivariate data set, such as clusters
and modal regions, is contained in the derivatives of the probability density
function. Despite this importance, nonparametric estimation of higher order
derivatives of the density functions have received only relatively scant
attention. Kernel estimators of density functions are widely used as they
exhibit excellent theoretical and practical properties, though their
generalization to density derivatives has progressed more slowly due to the
mathematical intractabilities encountered in the crucial problem of bandwidth
(or smoothing parameter) selection. This paper presents the first fully
automatic, data-based bandwidth selectors for multivariate kernel density
derivative estimators. This is achieved by synthesizing recent advances in
matrix analytic theory which allow mathematically and computationally tractable
representations of higher order derivatives of multivariate vector valued
functions. The theoretical asymptotic properties as well as the finite sample
behaviour of the proposed selectors are studied. {In addition, we explore in
detail the applications of the new data-driven methods for two other
statistical problems: clustering and bump hunting. The introduced techniques
are combined with the mean shift algorithm to develop novel automatic,
nonparametric clustering procedures which are shown to outperform mixture-model
cluster analysis and other recent nonparametric approaches in practice.
Furthermore, the advantage of the use of smoothing parameters designed for
density derivative estimation for feature significance analysis for bump
hunting is illustrated with a real data example.
"
"  The aim of this work is to address the question of whether we can in
principle design rational decision-making agents or artificial intelligences
embedded in computable physics such that their decisions are optimal in
reasonable mathematical senses. Recent developments in rare event probability
estimation, recursive bayesian inference, neural networks, and probabilistic
planning are sufficient to explicitly approximate reinforcement learners of the
AIXI style with non-trivial model classes (here, the class of resource-bounded
Turing machines). Consideration of the effects of resource limitations in a
concrete implementation leads to insights about possible architectures for
learning systems using optimal decision makers as components.
"
"  We demonstrate that almost all non-parametric dimensionality reduction
methods can be expressed by a simple procedure: regularized loss minimization
plus singular value truncation. By distinguishing the role of the loss and
regularizer in such a process, we recover a factored perspective that reveals
some gaps in the current literature. Beyond identifying a useful new loss for
manifold unfolding, a key contribution is to derive new convex regularizers
that combine distance maximization with rank reduction. These regularizers can
be applied to any loss.
"
"  Recent research has studied the role of sparsity in high dimensional
regression and signal reconstruction, establishing theoretical limits for
recovering sparse models from sparse data. This line of work shows that
$\ell_1$-regularized least squares regression can accurately estimate a sparse
linear model from $n$ noisy examples in $p$ dimensions, even if $p$ is much
larger than $n$. In this paper we study a variant of this problem where the
original $n$ input variables are compressed by a random linear transformation
to $m \ll n$ examples in $p$ dimensions, and establish conditions under which a
sparse linear model can be successfully recovered from the compressed data. A
primary motivation for this compression procedure is to anonymize the data and
preserve privacy by revealing little information about the original data. We
characterize the number of random projections that are required for
$\ell_1$-regularized compressed regression to identify the nonzero coefficients
in the true model with probability approaching one, a property called
``sparsistence.'' In addition, we show that $\ell_1$-regularized compressed
regression asymptotically predicts as well as an oracle linear model, a
property called ``persistence.'' Finally, we characterize the privacy
properties of the compression procedure in information-theoretic terms,
establishing upper bounds on the mutual information between the compressed and
uncompressed data that decay to zero.
"
"  This study introduces a new method of visualizing complex tree structured
objects. The usefulness of this method is illustrated in the context of
detecting unexpected features in a data set of very large trees. The major
contribution is a novel two-dimensional graphical representation of each tree,
with a covariate coded by color. The motivating data set contains three
dimensional representations of brain artery systems of 105 subjects. Due to
inaccuracies inherent in the medical imaging techniques, issues with the
reconstruction algo- rithms and inconsistencies introduced by manual
adjustment, various discrepancies are present in the data. The proposed
representation enables quick visual detection of the most common discrepancies.
For our driving example, this tool led to the modification of 10% of the artery
trees and deletion of 6.7%. The benefits of our cleaning method are
demonstrated through a statistical hypothesis test on the effects of aging on
vessel structure. The data cleaning resulted in improved significance levels.
"
"  We address challenges of active learning under scarce informational resources
in non-stationary environments. In real-world settings, data labeled and
integrated into a predictive model may become invalid over time. However, the
data can become informative again with switches in context and such changes may
indicate unmodeled cyclic or other temporal dynamics. We explore principles for
discarding, caching, and recalling labeled data points in active learning based
on computations of value of information. We review key concepts and study the
value of the methods via investigations of predictive performance and costs of
acquiring data for simulated and real-world data sets.
"
"  The problem of multiple hypothesis testing arises when there are more than
one hypothesis to be tested simultaneously for statistical significance. This
is a very common situation in many data mining applications. For instance,
assessing simultaneously the significance of all frequent itemsets of a single
dataset entails a host of hypothesis, one for each itemset. A multiple
hypothesis testing method is needed to control the number of false positives
(Type I error). Our contribution in this paper is to extend the multiple
hypothesis framework to be used with a generic data mining algorithm. We
provide a method that provably controls the family-wise error rate (FWER, the
probability of at least one false positive) in the strong sense. We evaluate
the performance of our solution on both real and generated data. The results
show that our method controls the FWER while maintaining the power of the test.
"
"  We develop a Bayesian inference method that allows the efficient
determination of several interesting parameters from complicated
high-energy-density experiments performed on the National Ignition Facility
(NIF). The model is based on an exploration of phase space using the
hydrodynamic code HYDRA. A linear model is used to describe the effect of
nuisance parameters on the analysis, allowing an analytic likelihood to be
derived that can be determined from a small number of HYDRA runs and then used
in existing advanced statistical analysis methods. This approach is applied to
a recent experiment in order to determine the carbon opacity and X-ray drive;
it is found that the inclusion of prior expert knowledge and fluctuations in
capsule dimensions and chemical composition significantly improve the agreement
between experiment and theoretical opacity calculations. A parameterisation of
HYDRA results is used to test the application of both Markov chain Monte Carlo
(MCMC) and genetic algorithm (GA) techniques to explore the posterior. These
approaches have distinct advantages and we show that both can allow the
efficient analysis of high energy density experiments.
"
"  Kalman smoothers reconstruct the state of a dynamical system starting from
noisy output samples. While the classical estimator relies on quadratic
penalization of process deviations and measurement errors, extensions that
exploit Piecewise Linear Quadratic (PLQ) penalties have been recently proposed
in the literature. These new formulations include smoothers robust with respect
to outliers in the data, and smoothers that keep better track of fast system
dynamics, e.g. jumps in the state values. In addition to L2, well known
examples of PLQ penalties include the L1, Huber and Vapnik losses. In this
paper, we use a dual representation for PLQ penalties to build a statistical
modeling framework and a computational theory for Kalman smoothing.
  We develop a statistical framework by establishing conditions required to
interpret PLQ penalties as negative logs of true probability densities. Then,
we present a computational framework, based on interior-point methods, that
solves the Kalman smoothing problem with PLQ penalties and maintains the linear
complexity in the size of the time series, just as in the L2 case. The
framework presented extends the computational efficiency of the Mayne-Fraser
and Rauch-Tung-Striebel algorithms to a much broader non-smooth setting, and
includes many known robust and sparse smoothers as special cases.
"
"  Probe-level models have led to improved performance in microarray studies but
the various sources of probe-level contamination are still poorly understood.
Data-driven analysis of probe performance can be used to quantify the
uncertainty in individual probes and to highlight the relative contribution of
different noise sources. Improved understanding of the probe-level effects can
lead to improved preprocessing techniques and microarray design.
  We have implemented probabilistic tools for probe performance analysis and
summarization on short oligonucleotide arrays. In contrast to standard
preprocessing approaches, the methods provide quantitative estimates of
probe-specific noise and affinity terms and tools to investigate these
parameters. Tools to incorporate prior information of the probes in the
analysis are provided as well. Comparisons to known probe-level error sources
and spike-in data sets validate the approach.
  Implementation is freely available in R/BioConductor:
http://www.bioconductor.org/packages/release/bioc/html/RPA.html
"
"  In this paper, we derive Hybrid, Bayesian and Marginalized Cram\'{e}r-Rao
lower bounds (HCRB, BCRB and MCRB) for the single and multiple measurement
vector Sparse Bayesian Learning (SBL) problem of estimating compressible
vectors and their prior distribution parameters. We assume the unknown vector
to be drawn from a compressible Student-t prior distribution. We derive CRBs
that encompass the deterministic or random nature of the unknown parameters of
the prior distribution and the regression noise variance. We extend the MCRB to
the case where the compressible vector is distributed according to a general
compressible prior distribution, of which the generalized Pareto distribution
is a special case. We use the derived bounds to uncover the relationship
between the compressibility and Mean Square Error (MSE) in the estimates.
Further, we illustrate the tightness and utility of the bounds through
simulations, by comparing them with the MSE performance of two popular
SBL-based estimators. It is found that the MCRB is generally the tightest among
the bounds derived and that the MSE performance of the Expectation-Maximization
(EM) algorithm coincides with the MCRB for the compressible vector. Through
simulations, we demonstrate the dependence of the MSE performance of SBL based
estimators on the compressibility of the vector for several values of the
number of observations and at different signal powers.
"
"  Recent publications have described and applied a novel metric that quantifies
the genetic distance of an individual with respect to two population samples,
and have suggested that the metric makes it possible to infer the presence of
an individual of known genotype in a sample for which only the marginal allele
frequencies are known. However, the assumptions, limitations, and utility of
this metric remained incompletely characterized. Here we present an exploration
of the strengths and limitations of that method. In addition to analytical
investigations of the underlying assumptions, we use both real and simulated
genotypes to test empirically the method's accuracy. The results reveal that,
when used as a means by which to identify individuals as members of a
population sample, the specificity is low in several circumstances. We find
that the misclassifications stem from violations of assumptions that are
crucial to the technique yet hard to control in practice, and we explore the
feasibility of several methods to improve the sensitivity. Additionally, we
find that the specificity may still be lower than expected even in ideal
circumstances. However, despite the metric's inadequacies for identifying the
presence of an individual in a sample, our results suggest potential avenues
for future research on tuning this method to problems of ancestry inference or
disease prediction. By revealing both the strengths and limitations of the
proposed method, we hope to elucidate situations in which this distance metric
may be used in an appropriate manner. We also discuss the implications of our
findings in forensics applications and in the protection of GWAS participant
privacy.
"
"  The amplified fragment length polymorphism (AFLP) method produces anonymous
genetic markers from throughout a genome. We extend the nucleotide substitution
model of AFLP evolution to additionally include insertion and deletion
processes. The new Sub-ID model relaxes the common assumption that markers are
independent and homologous. We build a Markov chain Monte Carlo methodology
tailored for the Sub-ID model to implement a Bayesian approach to infer AFLP
marker evolution. The method allows us to infer both the phylogenies and the
subset of markers that are possibly homologous. In addition, we can infer the
genome-wide relative rate of indels versus substitutions. In a case study with
AFLP markers from sedges, a grass-like plant common in North America, we find
that accounting for insertion and deletion makes a difference in phylogenetic
inference. The inference of topologies is not sensitive to the prior settings
and the Jukes--Cantor assumption for nucleotide substitution. The model for
insertion and deletion we introduce has potential value in other phylogenetic
applications.
"
"  This paper considers the problem of model selection within the context of
finite element model updating. Given that a number of FEM updating models, with
different updating parameters, can be designed, this paper proposes using the
Bayesian evidence statistic to assess the probability of each updating model.
This makes it possible then to evaluate the need for alternative updating
parameters in the updating of the initial FE model. The model evidences are
compared using the Bayes factor, which is the ratio of evidences. The Jeffrey
scale is used to determine the differences in the models. The Bayesian evidence
is calculated by integrating the likelihood of the data given the model and its
parameters over the a priori model parameter space using the new nested
sampling algorithm. The nested algorithm samples this likelihood distribution
by using a hard likelihood-value constraint on the sampling region while
providing the posterior samples of the updating model parameters as a
by-product. This method is used to calculate the evidence of a number of
plausible finite element models.
"
"  Communities in social networks or graphs are sets of well-connected,
overlapping vertices. The effectiveness of a community detection algorithm is
determined by accuracy in finding the ground-truth communities and ability to
scale with the size of the data. In this work, we provide three contributions.
First, we show that a popular measure of accuracy known as the F1 score, which
is between 0 and 1, with 1 being perfect detection, has an information lower
bound is 0.5. We provide a trivial algorithm that produces communities with an
F1 score of 0.5 for any graph! Somewhat surprisingly, we find that popular
algorithms such as modularity optimization, BigClam and CESNA have F1 scores
less than 0.5 for the popular IMDB graph. To rectify this, as the second
contribution we propose a generative model for community formation, the
sequential community graph, which is motivated by the formation of social
networks. Third, motivated by our generative model, we propose the
leader-follower algorithm (LFA). We prove that it recovers all communities for
sequential community graphs by establishing a structural result that sequential
community graphs are chordal. For a large number of popular social networks, it
recovers communities with a much higher F1 score than other popular algorithms.
For the IMDB graph, it obtains an F1 score of 0.81. We also propose a
modification to the LFA called the fast leader-follower algorithm (FLFA) which
in addition to being highly accurate, is also fast, with a scaling that is
almost linear in the network size.
"
"  In many medical studies, patients are followed longitudinally and interest is
on assessing the relationship between longitudinal measurements and time to an
event. Recently, various authors have proposed joint modeling approaches for
longitudinal and time-to-event data for a single longitudinal variable. These
joint modeling approaches become intractable with even a few longitudinal
variables. In this paper we propose a regression calibration approach for
jointly modeling multiple longitudinal measurements and discrete time-to-event
data. Ideally, a two-stage modeling approach could be applied in which the
multiple longitudinal measurements are modeled in the first stage and the
longitudinal model is related to the time-to-event data in the second stage.
Biased parameter estimation due to informative dropout makes this direct
two-stage modeling approach problematic. We propose a regression calibration
approach which appropriately accounts for informative dropout. We approximate
the conditional distribution of the multiple longitudinal measurements given
the event time by modeling all pairwise combinations of the longitudinal
measurements using a bivariate linear mixed model which conditions on the event
time. Complete data are then simulated based on estimates from these pairwise
conditional models, and regression calibration is used to estimate the
relationship between longitudinal data and time-to-event data using the
complete data. We show that this approach performs well in estimating the
relationship between multivariate longitudinal measurements and the
time-to-event data and in estimating the parameters of the multiple
longitudinal process subject to informative dropout. We illustrate this
methodology with simulations and with an analysis of primary biliary cirrhosis
(PBC) data.
"
"  Many practitioners who use the EM algorithm complain that it is sometimes
slow. When does this happen, and what can be done about it? In this paper, we
study the general class of bound optimization algorithms - including
Expectation-Maximization, Iterative Scaling and CCCP - and their relationship
to direct optimization algorithms such as gradient-based methods for parameter
learning. We derive a general relationship between the updates performed by
bound optimization methods and those of gradient and second-order methods and
identify analytic conditions under which bound optimization algorithms exhibit
quasi-Newton behavior, and conditions under which they possess poor,
first-order convergence. Based on this analysis, we consider several specific
algorithms, interpret and analyze their convergence properties and provide some
recipes for preprocessing input to these algorithms to yield faster convergence
behavior. We report empirical results supporting our analysis and showing that
simple data preprocessing can result in dramatically improved performance of
bound optimizers in practice.
"
"  We introduce a new discrepancy score between two distributions that gives an
indication on their similarity. While much research has been done to determine
if two samples come from exactly the same distribution, much less research
considered the problem of determining if two finite samples come from similar
distributions. The new score gives an intuitive interpretation of similarity;
it optimally perturbs the distributions so that they best fit each other. The
score is defined between distributions, and can be efficiently estimated from
samples. We provide convergence bounds of the estimated score, and develop
hypothesis testing procedures that test if two data sets come from similar
distributions. The statistical power of this procedures is presented in
simulations. We also compare the score's capacity to detect similarity with
that of other known measures on real data.
"
"  By using laboratory experimental data, we test the uncertainty of social
strategy transitions in various competing environments of fixed paired
two-person constant sum $2 \times 2$ games. It firstly shows that, the
distributions of social strategy transitions are not erratic but obey the
principle of the maximum entropy (MaxEnt). This finding indicates that human
subject social systems and natural systems could have wider common backgrounds.
"
"  In this paper we propose and discuss variance reduction techniques for the
estimation of quantiles of the output of a complex model with random input
parameters. These techniques are based on the use of a reduced model, such as a
metamodel or a response surface. The reduced model can be used as a control
variate; or a rejection method can be implemented to sample the realizations of
the input parameters in prescribed relevant strata; or the reduced model can be
used to determine a good biased distribution of the input parameters for the
implementation of an importance sampling strategy. The different strategies are
analyzed and the asymptotic variances are computed, which shows the benefit of
an adaptive controlled stratification method. This method is finally applied to
a real example (computation of the peak cladding temperature during a
large-break loss of coolant accident in a nuclear reactor).
"
"  Standard Gaussian graphical models (GGMs) implicitly assume that the
conditional independence among variables is common to all observations in the
sample. However, in practice, observations are usually collected form
heterogeneous populations where such assumption is not satisfied, leading in
turn to nonlinear relationships among variables. To tackle these problems we
explore mixtures of GGMs; in particular, we consider both infinite mixture
models of GGMs and infinite hidden Markov models with GGM emission
distributions. Such models allow us to divide a heterogeneous population into
homogenous groups, with each cluster having its own conditional independence
structure. The main advantage of considering infinite mixtures is that they
allow us easily to estimate the number of number of subpopulations in the
sample. As an illustration, we study the trends in exchange rate fluctuations
in the pre-Euro era. This example demonstrates that the models are very
flexible while providing extremely interesting interesting insights into
real-life applications.
"
"  Many successful applications of computer vision to image or video
manipulation are interactive by nature. However, parameters of such systems are
often trained neglecting the user. Traditionally, interactive systems have been
treated in the same manner as their fully automatic counterparts. Their
performance is evaluated by computing the accuracy of their solutions under
some fixed set of user interactions. This paper proposes a new evaluation and
learning method which brings the user in the loop. It is based on the use of an
active robot user - a simulated model of a human user. We show how this
approach can be used to evaluate and learn parameters of state-of-the-art
interactive segmentation systems. We also show how simulated user models can be
integrated into the popular max-margin method for parameter learning and
propose an algorithm to solve the resulting optimisation problem.
"
"  In this paper, we address the problem of learning the structure of a pairwise
graphical model from samples in a high-dimensional setting. Our first main
result studies the sparsistency, or consistency in sparsity pattern recovery,
properties of a forward-backward greedy algorithm as applied to general
statistical models. As a special case, we then apply this algorithm to learn
the structure of a discrete graphical model via neighborhood estimation. As a
corollary of our general result, we derive sufficient conditions on the number
of samples n, the maximum node-degree d and the problem size p, as well as
other conditions on the model parameters, so that the algorithm recovers all
the edges with high probability. Our result guarantees graph selection for
samples scaling as n = Omega(d^2 log(p)), in contrast to existing
convex-optimization based algorithms that require a sample complexity of
\Omega(d^3 log(p)). Further, the greedy algorithm only requires a restricted
strong convexity condition which is typically milder than irrepresentability
assumptions. We corroborate these results using numerical simulations at the
end.
"
"  For supervised and unsupervised learning, positive definite kernels allow to
use large and potentially infinite dimensional feature spaces with a
computational cost that only depends on the number of observations. This is
usually done through the penalization of predictor functions by Euclidean or
Hilbertian norms. In this paper, we explore penalizing by sparsity-inducing
norms such as the l1-norm or the block l1-norm. We assume that the kernel
decomposes into a large sum of individual basis kernels which can be embedded
in a directed acyclic graph; we show that it is then possible to perform kernel
selection through a hierarchical multiple kernel learning framework, in
polynomial time in the number of selected kernels. This framework is naturally
applied to non linear variable selection; our extensive simulations on
synthetic datasets and datasets from the UCI repository show that efficiently
exploring the large feature space through sparsity-inducing norms leads to
state-of-the-art predictive performance.
"
"  Cooperative diversity systems are wireless communication systems designed to
exploit cooperation among users to mitigate the effects of multipath fading. In
fairly general conditions, it has been shown that these systems can achieve the
diversity order of an equivalent MISO channel and, if the node geometry
permits, virtually the same outage probability can be achieved as that of the
equivalent MISO channel for a wide range of applicable SNR. However, much of
the prior analysis has been performed under the assumption of perfect timing
and frequency offset synchronization. In this paper, we derive the estimation
bounds and associated maximum likelihood estimators for frequency offset
estimation in a cooperative communication system. We show the benefit of
adaptively tuning the frequency of the relay node in order to reduce estimation
error at the destination. We also derive an efficient estimation algorithm,
based on the correlation sequence of the data, which has mean squared error
close to the Cramer-Rao Bound.
"
"  Statistics is a uniquely difficult field to convey to the uninitiated. It
sits astride the abstract and the concrete, the theoretical and the applied. It
has a mathematical flavor and yet it is not simply a branch of mathematics. Its
core problems blend into those of the disciplines that probe into the nature of
intelligence and thought, in particular philosophy, psychology and artificial
intelligence. Debates over foundational issues have waxed and waned, but the
field has not yet arrived at a single foundational perspective.
"
"  Most classification methods provide either a prediction of class membership
or an assessment of class membership probability. In the case of two-group
classification the predicted probability can be described as ""risk"" of
belonging to a ""special"" class . When the required output is a set of
ordinal-risk groups, a discretization of the continuous risk prediction is
achieved by two common methods: by constructing a set of models that describe
the conditional risk function at specific points (quantile regression) or by
dividing the output of an ""optimal"" classification model into adjacent
intervals that correspond to the desired risk groups. By defining a new error
measure for the distribution of risk onto intervals we are able to identify
lower bounds on the accuracy of these methods, showing sub-optimality both in
their distribution of risk and in the efficiency of their resulting partition
into intervals. By adding a new form of constraint to the existing maximum
likelihood optimization framework and by introducing a penalty function to
avoid degenerate solutions, we show how existing methods can be augmented to
solve the ordinal risk-group classification problem. We implement our method
for logistic regression (LR) and show a numeric example.
"
"  This paper presents a Bayesian approach to symbol and phase inference in a
phase-unsynchronized digital receiver. It primarily extends [Quinn 2011] to the
multi-symbol case, using the variational Bayes (VB) approximation to deal with
the combinatorial complexity of the phase inference in this case. The work
provides a fully Bayesian extension of the EM-based framework underlying
current turbo-synchronization methods, since it induces a von Mises prior on
the time-invariant phase parmeter. As a result, we achieve tractable iterative
algorithms with improved robustness in low SNR regimes, compared to the current
EM-based approaches. As a corollary to our analysis we also discover the
importance of prior regularization in elegantly tackling the significant
problem of phase ambiguity.
"
"  The problem of multilabel classification when the labels are related through
a hierarchical categorization scheme occurs in many application domains such as
computational biology. For example, this problem arises naturally when trying
to automatically assign gene function using a controlled vocabularies like Gene
Ontology. However, most existing approaches for predicting gene functions solve
independent classification problems to predict genes that are involved in a
given function category, independently of the rest. Here, we propose two simple
methods for incorporating information about the hierarchical nature of the
categorization scheme. In the first method, we use information about a gene's
previous annotation to set an initial prior on its label. In a second approach,
we extend a graph-based semi-supervised learning algorithm for predicting gene
function in a hierarchy. We show that we can efficiently solve this problem by
solving a linear system of equations. We compare these approaches with a
previous label reconciliation-based approach. Results show that using the
hierarchy information directly, compared to using reconciliation methods,
improves gene function prediction.
"
"  Expo 2010 Shanghai China was a successful, splendid and unforgettable event,
remaining us with valuable experiences. The visitor flow pattern of Expo is
investigated in this paper. The Hurst exponent, mean value and standard
deviation of visitor volume prove that the visitor flow is fractal with
long-term stability and correlation as well as obvious fluctuation in short
period. Then the time series of visitor volume is converted to complex network
by visibility algorithm. It can be inferred from the topological properties of
the visibility graph that the network is scale-free, small-world and
hierarchically constructed, conforming that the time series are fractal and
close relationship exit between the visitor volume on different days.
Furthermore, it is inevitable to show some extreme visitor volume in the
original visitor flow, and these extreme points may appear in group to a great
extent.
"
"  Information-maximization clustering learns a probabilistic classifier in an
unsupervised manner so that mutual information between feature vectors and
cluster assignments is maximized. A notable advantage of this approach is that
it only involves continuous optimization of model parameters, which is
substantially easier to solve than discrete optimization of cluster
assignments. However, existing methods still involve non-convex optimization
problems, and therefore finding a good local optimal solution is not
straightforward in practice. In this paper, we propose an alternative
information-maximization clustering method based on a squared-loss variant of
mutual information. This novel approach gives a clustering solution
analytically in a computationally efficient way via kernel eigenvalue
decomposition. Furthermore, we provide a practical model selection procedure
that allows us to objectively optimize tuning parameters included in the kernel
function. Through experiments, we demonstrate the usefulness of the proposed
approach.
"
"  The power of sparse signal modeling with learned over-complete dictionaries
has been demonstrated in a variety of applications and fields, from signal
processing to statistical inference and machine learning. However, the
statistical properties of these models, such as under-fitting or over-fitting
given sets of data, are still not well characterized in the literature. As a
result, the success of sparse modeling depends on hand-tuning critical
parameters for each data and application. This work aims at addressing this by
providing a practical and objective characterization of sparse models by means
of the Minimum Description Length (MDL) principle -- a well established
information-theoretic approach to model selection in statistical inference. The
resulting framework derives a family of efficient sparse coding and dictionary
learning algorithms which, by virtue of the MDL principle, are completely
parameter free. Furthermore, such framework allows to incorporate additional
prior information to existing models, such as Markovian dependencies, or to
define completely new problem formulations, including in the matrix analysis
area, in a natural way. These virtues will be demonstrated with parameter-free
algorithms for the classic image denoising and classification problems, and for
low-rank matrix recovery in video applications.
"
"  Multinomial logistic regression is one of the most popular models for
modelling the effect of explanatory variables on a subject choice between a set
of specified options. This model has found numerous applications in machine
learning, psychology or economy. Bayesian inference in this model is non
trivial and requires, either to resort to a MetropolisHastings algorithm, or
rejection sampling within a Gibbs sampler. In this paper, we propose an
alternative model to multinomial logistic regression. The model builds on the
Plackett-Luce model, a popular model for multiple comparisons. We show that the
introduction of a suitable set of auxiliary variables leads to an
Expectation-Maximization algorithm to find Maximum A Posteriori estimates of
the parameters. We further provide a full Bayesian treatment by deriving a
Gibbs sampler, which only requires to sample from highly standard
distributions. We also propose a variational approximate inference scheme. All
are very simple to implement. One property of our Plackett-Luce regression
model is that it learns a sparse set of feature weights. We compare our method
to sparse Bayesian multinomial logistic regression and show that it is
competitive, especially in presence of polychotomous data.
"
"  Variance estimation for estimators of state, county, and school district
quantities derived from the Census 2000 long form are discussed. The variance
estimator must account for (1) uncertainty due to imputation, and (2) raking to
census population controls. An imputation procedure that imputes more than one
value for each missing item using donors that are neighbors is described and
the procedure using two nearest neighbors is applied to the Census long form.
The Kim and Fuller [Biometrika 91 (2004) 559--578] method for variance
estimation under fractional hot deck imputation is adapted for application to
the long form data. Numerical results from the 2000 long form data are
presented.
"
"  We consider learning on graphs, guided by kernels that encode similarity
between vertices. Our focus is on random walk kernels, the analogues of squared
exponential kernels in Euclidean spaces. We show that on large, locally
treelike, graphs these have some counter-intuitive properties, specifically in
the limit of large kernel lengthscales. We consider using these kernels as
covariance matrices of e.g.\ Gaussian processes (GPs). In this situation one
typically scales the prior globally to normalise the average of the prior
variance across vertices. We demonstrate that, in contrast to the Euclidean
case, this generically leads to significant variation in the prior variance
across vertices, which is undesirable from the probabilistic modelling point of
view. We suggest the random walk kernel should be normalised locally, so that
each vertex has the same prior variance, and analyse the consequences of this
by studying learning curves for Gaussian process regression. Numerical
calculations as well as novel theoretical predictions for the learning curves
using belief propagation make it clear that one obtains distinctly different
probabilistic models depending on the choice of normalisation. Our method for
predicting the learning curves using belief propagation is significantly more
accurate than previous approximations and should become exact in the limit of
large random graphs.
"
"  Autoencoder neural network is implemented to estimate the missing data.
Genetic algorithm is implemented for network optimization and estimating the
missing data. Missing data is treated as Missing At Random mechanism by
implementing maximum likelihood algorithm. The network performance is
determined by calculating the mean square error of the network prediction. The
network is further optimized by implementing Decision Forest. The impact of
missing data is then investigated and decision forrests are found to improve
the results.
"
"  In many applications, it is of interest to assess the dependence structure in
multivariate longitudinal data. Discovering such dependence is challenging due
to the dimensionality involved. By concatenating the random effects from
component models for each response, dependence within and across longitudinal
responses can be characterized through a large random effects covariance
matrix. Motivated by the common problems in estimating this matrix, especially
the off-diagonal elements, we propose a Bayesian approach that relies on
shrinkage priors for parameters in a modified Cholesky decomposition. Without
adjustment, such priors and previous related approaches are order-dependent and
tend to shrink strongly toward an ARtype structure. We propose moment-matching
(MM) priors to mitigate such problems. Efficient Gibbs samplers are developed
for posterior computation. The methods are illustrated through simulated
examples and are applied to a longitudinal epidemiologic study of hormones and
oxidative stress.
"
"  Graphical models are widely used in scienti fic and engineering research to
represent conditional independence structures between random variables. In many
controlled experiments, environmental changes or external stimuli can often
alter the conditional dependence between the random variables, and potentially
produce significant structural changes in the corresponding graphical models.
Therefore, it is of great importance to be able to detect such structural
changes from data, so as to gain novel insights into where and how the
structural changes take place and help the system adapt to the new environment.
Here we report an effective learning strategy to extract structural changes in
Gaussian graphical model using l1-regularization based convex optimization. We
discuss the properties of the problem formulation and introduce an efficient
implementation by the block coordinate descent algorithm. We demonstrate the
principle of the approach on a numerical simulation experiment, and we then
apply the algorithm to the modeling of gene regulatory networks under different
conditions and obtain promising yet biologically plausible results.
"
"  Combining principles with pragmatism, a new approach and accompanying
algorithm are presented to a longstanding problem in applied statistics: the
interpretation of principal components. Following Rousson and Gasser [53 (2004)
539--555] @p250pt@ the ultimate goal is not to propose a method that leads
automatically to a unique solution, but rather to develop tools for assisting
the user in his or her choice of an interpretable solution. Accordingly, our
approach is essentially exploratory. Calling a vector 'simple' if it has small
integer elements, it poses the open question: @p250pt@ What sets of simply
interpretable orthogonal axes---if any---are angle-close to the principal
components of interest? its answer being presented in summary form as an
automated visual display of the solutions found, ordered in terms of overall
measures of simplicity, accuracy and star quality, from which the user may
choose. Here, 'star quality' refers to striking overall patterns in the sets of
axes found, deserving to be especially drawn to the user's attention precisely
because they have emerged from the data, rather than being imposed on it by
(implicitly) adopting a model. Indeed, other things being equal, explicit
models can be checked by seeing if their fits occur in our exploratory
analysis, as we illustrate. Requiring orthogonality, attractive visualization
and dimension reduction features of principal component analysis are retained.
"
"  Functional brain connectivity, as revealed through distant correlations in
the signals measured by functional Magnetic Resonance Imaging (fMRI), is a
promising source of biomarkers of brain pathologies. However, establishing and
using diagnostic markers requires probabilistic inter-subject comparisons.
Principled comparison of functional-connectivity structures is still a
challenging issue. We give a new matrix-variate probabilistic model suitable
for inter-subject comparison of functional connectivity matrices on the
manifold of Symmetric Positive Definite (SPD) matrices. We show that this model
leads to a new algorithm for principled comparison of connectivity coefficients
between pairs of regions. We apply this model to comparing separately
post-stroke patients to a group of healthy controls. We find
neurologically-relevant connection differences and show that our model is more
sensitive that the standard procedure. To the best of our knowledge, these
results are the first report of functional connectivity differences between a
single-patient and a group and thus establish an important step toward using
functional connectivity as a diagnostic tool.
"
"  We study the spatial distribution of clusters associated to the aftershocks
of the megathrust Maule earthquake MW 8.8 of 27 February 2010. We used a recent
clustering method which hinges on a nonparametric estimation of the underlying
probability density function to detect subsets of points forming clusters
associated with high density areas. In addition, we estimate the probability
density function using a nonparametric kernel method for each of these
clusters. This allows us to identify a set of regions where there is an
association between frequency of events and coseismic slip. Our results suggest
that high coseismic slip spatially correlates with high aftershock frequency.
"
"  We consider the problem of recognizing a vocabulary--a collection of words
(sequences) over a finite alphabet--from a potential subsequence of one of its
words. We assume the given subsequence is received through a deletion channel
as a result of transmission of a random word from one of the two generic
underlying vocabularies. An exact maximum a posterior (MAP) solution for this
problem counts the number of ways a given subsequence can be derived from
particular subsets of candidate vocabularies, requiring exponential time or
space.
  We present a polynomial approximation algorithm for this problem. The
algorithm makes no prior assumption about the rules and patterns governing the
structure of vocabularies. Instead, through off-line processing of
vocabularies, it extracts data regarding regularity patterns in the
subsequences of each vocabulary. In the recognition phase, the algorithm just
uses this data, called subsequence-histogram, to decide in favor of one of the
vocabularies. We provide examples to demonstrate the performance of the
algorithm and show that it can achieve the same performance as MAP in some
situations.
  Potential applications include bioinformatics, storage systems, and search
engines.
"
"  We present a new estimation method for mapping the gravitational lensing
potential from observed CMB intensity and polarization fields. Our method uses
Bayesian techniques to estimate the average curvature of the potential over
small local regions. These local curvatures are then used to construct an
estimate of a low pass filter of the gravitational potential. By utilizing
Bayesian/likelihood methods one can easily overcome problems with missing
and/or non-uniform pixels and problems with partial sky observations (E and B
mode mixing, for example). Moreover, our methods are local in nature which
allow us to easily model spatially varying beams and are highly parallelizable.
We note that our estimates do not rely on the typical Taylor approximation
which is used to construct estimates of the gravitational potential by Fourier
coupling. We present our methodology with a flat sky simulation under nearly
ideal experimental conditions with a noise level of 1 $\mu K$-arcmin for the
temperature field, $\sqrt{2}$ $\mu K$-arcmin for the polarization fields, with
an instrumental beam full width at half maximum (FWHM) of 0.25 arcmin.
"
"  Introducing the discussion paper by Sz\'{e}kely and Rizzo
"
"  Bias - variance decomposition of the expected error defined for regression
and classification problems is an important tool to study and compare different
algorithms, to find the best areas for their application. Here the
decomposition is introduced for the survival analysis problem. In our
experiments, we study bias -variance parts of the expected error for two
algorithms: original Cox proportional hazard regression and CoxPath, path
algorithm for L1-regularized Cox regression, on the series of increased
training sets. The experiments demonstrate that, contrary expectations, CoxPath
does not necessarily have an advantage over Cox regression.
"
"  We prove in this paper that the weighted volume of the set of integral
transportation matrices between two integral histograms r and c of equal sum is
a positive definite kernel of r and c when the set of considered weights forms
a positive definite matrix. The computation of this quantity, despite being the
subject of a significant research effort in algebraic statistics, remains an
intractable challenge for histograms of even modest dimensions. We propose an
alternative kernel which, rather than considering all matrices of the
transportation polytope, only focuses on a sub-sample of its vertices known as
its Northwestern corner solutions. The resulting kernel is positive definite
and can be computed with a number of operations O(R^2d) that grows linearly in
the complexity of the dimension d, where R^2, the total amount of sampled
vertices, is a parameter that controls the complexity of the kernel.
"
"  Assessing immune responses to study vaccines as surrogates of protection
plays a central role in vaccine clinical trials. Motivated by three ongoing or
pending HIV vaccine efficacy trials, we consider such surrogate endpoint
assessment in a randomized placebo-controlled trial with case-cohort sampling
of immune responses and a time to event endpoint. Based on the principal
surrogate definition under the principal stratification framework proposed by
Frangakis and Rubin [Biometrics 58 (2002) 21--29] and adapted by Gilbert and
Hudgens (2006), we introduce estimands that measure the value of an immune
response as a surrogate of protection in the context of the Cox proportional
hazards model. The estimands are not identified because the immune response to
vaccine is not measured in placebo recipients. We formulate the problem as a
Cox model with missing covariates, and employ novel trial designs for
predicting the missing immune responses and thereby identifying the estimands.
The first design utilizes information from baseline predictors of the immune
response, and bridges their relationship in the vaccine recipients to the
placebo recipients. The second design provides a validation set for the
unmeasured immune responses of uninfected placebo recipients by immunizing them
with the study vaccine after trial closeout. A maximum estimated likelihood
approach is proposed for estimation of the parameters. Simulated data examples
are given to evaluate the proposed designs and study their properties.
"
"  In this paper we introduce a micro-clustering strategy for Functional
Boxplots. The aim is to summarize a set of streaming time series splitted in
non overlapping windows. It is a two step strategy which performs at first, an
on-line summarization by means of functional data structures, named Functional
Boxplot micro-clusters; then it reveals the final summarization by processing,
off-line, the functional data structures. Our main contribute consists in
providing a new definition of micro-cluster based on Functional Boxplots and,
in defining a proximity measure which allows to compare and update them. This
allows to get a finer graphical summarization of the streaming time series by
five functional basic statistics of data. The obtained synthesis will be able
to keep track of the dynamic evolution of the multiple streams.
"
"  We provide the first algorithm for online bandit linear optimization whose
regret after T rounds is of order sqrt{Td ln N} on any finite class X of N
actions in d dimensions, and of order d*sqrt{T} (up to log factors) when X is
infinite. These bounds are not improvable in general. The basic idea utilizes
tools from convex geometry to construct what is essentially an optimal
exploration basis. We also present an application to a model of linear bandits
with expert advice. Interestingly, these results show that bandit linear
optimization with expert advice in d dimensions is no more difficult (in terms
of the achievable regret) than the online d-armed bandit problem with expert
advice (where EXP4 is optimal).
"
"  We propose dual regression as an alternative to the quantile regression
process for the global estimation of conditional distribution functions under
minimal assumptions. Dual regression provides all the interpretational power of
the quantile regression process while avoiding the need for repairing the
intersecting conditional quantile surfaces that quantile regression often
produces in practice. Our approach introduces a mathematical programming
characterization of conditional distribution functions which, in its simplest
form, is the dual program of a simultaneous estimator for linear location-scale
models. We apply our general characterization to the specification and
estimation of a flexible class of conditional distribution functions, and
present asymptotic theory for the corresponding empirical dual regression
process.
"
"  We introduce an efficient method for training the linear ranking support
vector machine. The method combines cutting plane optimization with red-black
tree based approach to subgradient calculations, and has O(m*s+m*log(m)) time
complexity, where m is the number of training examples, and s the average
number of non-zero features per example. Best previously known training
algorithms achieve the same efficiency only for restricted special cases,
whereas the proposed approach allows any real valued utility scores in the
training data. Experiments demonstrate the superior scalability of the proposed
approach, when compared to the fastest existing RankSVM implementations.
"
"  Dendrograms used in data analysis are ultrametric spaces, hence objects of
nonarchimedean geometry. It is known that there exist $p$-adic representation
of dendrograms. Completed by a point at infinity, they can be viewed as
subtrees of the Bruhat-Tits tree associated to the $p$-adic projective line.
The implications are that certain moduli spaces known in algebraic geometry are
$p$-adic parameter spaces of (families of) dendrograms, and stochastic
classification can also be handled within this framework. At the end, we
calculate the topology of the hidden part of a dendrogram.
"
"  We address the online linear optimization problem when the actions of the
forecaster are represented by binary vectors. Our goal is to understand the
magnitude of the minimax regret for the worst possible set of actions. We study
the problem under three different assumptions for the feedback: full
information, and the partial information models of the so-called ""semi-bandit"",
and ""bandit"" problems. We consider both $L_\infty$-, and $L_2$-type of
restrictions for the losses assigned by the adversary.
  We formulate a general strategy using Bregman projections on top of a
potential-based gradient descent, which generalizes the ones studied in the
series of papers Gyorgy et al. (2007), Dani et al. (2008), Abernethy et al.
(2008), Cesa-Bianchi and Lugosi (2009), Helmbold and Warmuth (2009), Koolen et
al. (2010), Uchiya et al. (2010), Kale et al. (2010) and Audibert and Bubeck
(2010). We provide simple proofs that recover most of the previous results. We
propose new upper bounds for the semi-bandit game. Moreover we derive lower
bounds for all three feedback assumptions. With the only exception of the
bandit game, the upper and lower bounds are tight, up to a constant factor.
Finally, we answer a question asked by Koolen et al. (2010) by showing that the
exponentially weighted average forecaster is suboptimal against $L_{\infty}$
adversaries.
"
"  This paper continues study, both theoretical and empirical, of the method of
Venn prediction, concentrating on binary prediction problems. Venn predictors
produce probability-type predictions for the labels of test objects which are
guaranteed to be well calibrated under the standard assumption that the
observations are generated independently from the same distribution. We give a
simple formalization and proof of this property. We also introduce Venn-Abers
predictors, a new class of Venn predictors based on the idea of isotonic
regression, and report promising empirical results both for Venn-Abers
predictors and for their more computationally efficient simplified version.
"
"  This manuscript considers the following ""graph classification"" question:
given a collection of graphs and associated classes, how can one predict the
class of a newly observed graph? To address this question we propose a
statistical model for graph/class pairs. This model naturally leads to a set of
estimators to identify the class-conditional signal, or ""signal-subgraph,""
defined as the collection of edges that are probabilistically different between
the classes. The estimators admit classifiers which are asymptotically optimal
and efficient, but differ by their assumption about the ""coherency"" of the
signal-subgraph (coherency is the extent to which the signal-edges ""stick
together"" around a common subset of vertices). Via simulation, the best
estimator is shown to be not just a function of the coherency of the model, but
also the number of training samples. These estimators are employed to address a
contemporary neuroscience question: can we classify ""connectomes""
(brain-graphs) according to sex? The answer is yes, and significantly better
than all benchmark algorithms considered. Synthetic data analysis demonstrates
that even when the model is correct, given the relatively small number of
training samples, the estimated signal-subgraph should be taken with a grain of
salt. We conclude by discussing several possible extensions.
"
"  MAP inference for general energy functions remains a challenging problem.
While most efforts are channeled towards improving the linear programming (LP)
based relaxation, this work is motivated by the quadratic programming (QP)
relaxation. We propose a novel MAP relaxation that penalizes the
Kullback-Leibler divergence between the LP pairwise auxiliary variables, and QP
equivalent terms given by the product of the unaries. We develop two efficient
algorithms based on variants of this relaxation. The algorithms minimize the
non-convex objective using belief propagation and dual decomposition as
building blocks. Experiments on synthetic and real-world data show that the
solutions returned by our algorithms substantially improve over the LP
relaxation.
"
"  We consider a novel approach of measuring the homology of DNA sequences based
of the variety of optimal alignments in the longest common subsequence sense.
The proposed approach is compared with BLAST in measuring the homology of four
genes.
"
"  Online (also called ""recursive"" or ""adaptive"") estimation of fixed model
parameters in hidden Markov models is a topic of much interest in times series
modelling. In this work, we propose an online parameter estimation algorithm
that combines two key ideas. The first one, which is deeply rooted in the
Expectation-Maximization (EM) methodology consists in reparameterizing the
problem using complete-data sufficient statistics. The second ingredient
consists in exploiting a purely recursive form of smoothing in HMMs based on an
auxiliary recursion. Although the proposed online EM algorithm resembles a
classical stochastic approximation (or Robbins-Monro) algorithm, it is
sufficiently different to resist conventional analysis of convergence. We thus
provide limited results which identify the potential limiting points of the
recursion as well as the large-sample behavior of the quantities involved in
the algorithm. The performance of the proposed algorithm is numerically
evaluated through simulations in the case of a noisily observed Markov chain.
In this case, the algorithm reaches estimation results that are comparable to
that of the maximum likelihood estimator for large sample sizes.
"
"  Follow-the-Leader (FTL) is an intuitive sequential prediction strategy that
guarantees constant regret in the stochastic setting, but has terrible
performance for worst-case data. Other hedging strategies have better
worst-case guarantees but may perform much worse than FTL if the data are not
maximally adversarial. We introduce the FlipFlop algorithm, which is the first
method that provably combines the best of both worlds.
  As part of our construction, we develop AdaHedge, which is a new way of
dynamically tuning the learning rate in Hedge without using the doubling trick.
AdaHedge refines a method by Cesa-Bianchi, Mansour and Stoltz (2007), yielding
slightly improved worst-case guarantees. By interleaving AdaHedge and FTL, the
FlipFlop algorithm achieves regret within a constant factor of the FTL regret,
without sacrificing AdaHedge's worst-case guarantees.
  AdaHedge and FlipFlop do not need to know the range of the losses in advance;
moreover, unlike earlier methods, both have the intuitive property that the
issued weights are invariant under rescaling and translation of the losses. The
losses are also allowed to be negative, in which case they may be interpreted
as gains.
"
"  This paper consider the problem of determining the reliability of a software
system which can be decomposed in a number of modules. We have derived the
expression of the reliability of a system using the Markovian model for the
transfer of control between modules in order. We have given the expression of
reliability by considering both benign and catastrophic failure. The expression
of reliability presented in this work is applicable for some control software
which are designed to detect its own internal errors.
"
"  Distributions over rankings are used to model data in various settings such
as preference analysis and political elections. The factorial size of the space
of rankings, however, typically forces one to make structural assumptions, such
as smoothness, sparsity, or probabilistic independence about these underlying
distributions. We approach the modeling problem from the computational
principle that one should make structural assumptions which allow for efficient
calculation of typical probabilistic queries. For ranking models, ""typical""
queries predominantly take the form of partial ranking queries (e.g., given a
user's top-k favorite movies, what are his preferences over remaining movies?).
In this paper, we argue that riffled independence factorizations proposed in
recent literature [7, 8] are a natural structural assumption for ranking
distributions, allowing for particularly efficient processing of partial
ranking queries.
"
"  Many statistical methods have been proposed to estimate causal models in
classical situations with fewer variables than observations (p<n, p: the number
of variables and n: the number of observations). However, modern datasets
including gene expression data need high-dimensional causal modeling in
challenging situations with orders of magnitude more variables than
observations (p>>n). In this paper, we propose a method to find exogenous
variables in a linear non-Gaussian causal model, which requires much smaller
sample sizes than conventional methods and works even when p>>n. The key idea
is to identify which variables are exogenous based on non-Gaussianity instead
of estimating the entire structure of the model. Exogenous variables work as
triggers that activate a causal chain in the model, and their identification
leads to more efficient experimental designs and better understanding of the
causal mechanism. We present experiments with artificial data and real-world
gene expression data to evaluate the method.
"
"  We consider the smoothing probabilities of hidden Markov model (HMM). We show
that under fairly general conditions for HMM, the exponential forgetting still
holds, and the smoothing probabilities can be well approximated with the ones
of double sided HMM. This makes it possible to use ergodic theorems. As an
applications we consider the pointwise maximum a posteriori segmentation, and
show that the corresponding risks converge.
"
"  Comparing allele frequencies among populations that differ in environment has
long been a tool for detecting loci involved in local adaptation. However, such
analyses are complicated by an imperfect knowledge of population allele
frequencies and neutral correlations of allele frequencies among populations
due to shared population history and gene flow. Here we develop a set of
methods to robustly test for unusual allele frequency patterns, and
correlations between environmental variables and allele frequencies while
accounting for these complications based on a Bayesian model previously
implemented in the software Bayenv. Using this model, we calculate a set of
`standardized allele frequencies' that allows investigators to apply tests of
their choice to multiple populations, while accounting for sampling and
covariance due to population history. We illustrate this first by showing that
these standardized frequencies can be used to calculate powerful tests to
detect non-parametric correlations with environmental variables, which are also
less prone to spurious results due to outlier populations. We then demonstrate
how these standardized allele frequencies can be used to construct a test to
detect SNPs that deviate strongly from neutral population structure. This test
is conceptually related to FST but should be more powerful as we account for
population history. We also extend the model to next-generation sequencing of
population pools, which is a cost-efficient way to estimate population allele
frequencies, but it implies an additional level of sampling noise. The utility
of these methods is demonstrated in simulations and by re-analyzing human SNP
data from the HGDP populations. An implementation of our method will be
available from http://gcbias.org.
"
"  Starting with the 2004 recall referendum, an important opposition sector to
President Chavez has questioned the integrity of the Venezuelan electoral
system, and casts doubt on the legitimacy and impartiality of the upcoming 2012
presidential elections on October 7. After carrying out a forensic analysis on
Venezuelan elections and referendums celebrated since 1998 until 2012, we reach
two controversial conclusions: on one hand, we cannot rule out the hypothesis
of fraud in elections run by the current regime. On the other, if fraud has
been committed, it has not been decisive on results of past elections. In other
words, the winner would have been the same in clean elections. Only in a
scenario of tight results, as 2012 elections could be, fraud would constitute a
decisive factor.
  ----
  A partir del refer\'endum revocatorio del 2004, un importante sector opositor
al Presidente Ch\'avez ha cuestionado la integridad del sistema electoral
venezolano y tiene dudas acerca de la legitimidad e imparcialidad de las
futuras elecciones presidenciales del 7 de Octubre del 2012. Practicando un
an\'alisis forense a elecciones y refer\'endums venezolanos desde 1998 hasta
2012 llegamos a dos controversiales conclusiones: por un lado, no podemos
descartar la hip\'otesis de fraude en comicios administrados por el actual
r\'egimen. Por otro lado, de haberse cometido fraude en pasadas elecciones,
este no ha sido determinante en los resultados. Es decir, el ganador hubiese
sido el mismo en elecciones limpias. S\'olo en un escenario de resultados
ajustados, como pudiera ser el 2012, el fraude podr\'ia ser determinante.
"
"  Belief Propagation (BP) is one of the most popular methods for inference in
probabilistic graphical models. BP is guaranteed to return the correct answer
for tree structures, but can be incorrect or non-convergent for loopy graphical
models. Recently, several new approximate inference algorithms based on cavity
distribution have been proposed. These methods can account for the effect of
loops by incorporating the dependency between BP messages. Alternatively,
region-based approximations (that lead to methods such as Generalized Belief
Propagation) improve upon BP by considering interactions within small clusters
of variables, thus taking small loops within these clusters into account. This
paper introduces an approach, Generalized Loop Correction (GLC), that benefits
from both of these types of loop correction. We show how GLC relates to these
two families of inference methods, then provide empirical evidence that GLC
works effectively in general, and can be significantly more accurate than both
correction schemes.
"
"  A Bayesian multiple change-point model is proposed to analyse violations of
air quality standards by pollutants such as nitrogen oxides (NO2 and NO) and
carbon monoxide (CO). The model is built on the assumption that the occurrence
of threshold exceedances may be described by a non-homogeneous Poisson process
with a step rate function. Unlike earlier approaches, our model is not
restricted by a predetermined number of change-points, nor does it involve any
covariates. Possible short-range correlations in the exceedance data (e.g., due
to chemical and meteorological factors) are removed via declusterisation. The
unknown rate function is estimated using a reversible jump MCMC sampling
algorithm adapted from Green (1995), which allows for transitions between
parameter subspaces of varying dimension. This technique is applied to the
17-year (1993-2009) daily NO2, NO and CO concentration data in the City of
Leeds, UK. The results are validated by running the MCMC estimator on simulated
data replicated via a posterior estimate of the rate function. The findings are
interpreted and discussed in relation to some known traffic control actions.
The proposed methodology may be useful in the air quality management context by
providing quantitative objective means to measure the efficacy of pollution
control programmes.
"
"  Undirected graphical models encode in a graph $G$ the dependency structure of
a random vector $Y$. In many applications, it is of interest to model $Y$ given
another random vector $X$ as input. We refer to the problem of estimating the
graph $G(x)$ of $Y$ conditioned on $X=x$ as ``graph-valued regression.'' In
this paper, we propose a semiparametric method for estimating $G(x)$ that
builds a tree on the $X$ space just as in CART (classification and regression
trees), but at each leaf of the tree estimates a graph. We call the method
``Graph-optimized CART,'' or Go-CART. We study the theoretical properties of
Go-CART using dyadic partitioning trees, establishing oracle inequalities on
risk minimization and tree partition consistency. We also demonstrate the
application of Go-CART to a meteorological dataset, showing how graph-valued
regression can provide a useful tool for analyzing complex data.
"
"  The causal assumptions, the study design and the data are the elements
required for scientific inference in empirical research. The research is
adequately communicated only if all of these elements and their relations are
described precisely. Causal models with design describe the study design and
the missing data mechanism together with the causal structure and allow the
direct application of causal calculus in the estimation of the causal effects.
The flow of the study is visualized by ordering the nodes of the causal diagram
in two dimensions by their causal order and the time of the observation.
Conclusions whether a causal or observational relationship can be estimated
from the collected incomplete data can be made directly from the graph. Causal
models with design offer a systematic and unifying view scientific inference
and increase the clarity and speed of communication. Examples on the causal
models for a case-control study, a nested case-control study, a clinical trial
and a two-stage case-cohort study are presented.
"
"  Conditional modeling x \to y is a central problem in machine learning. A
substantial research effort is devoted to such modeling when x is high
dimensional. We consider, instead, the case of a high dimensional y, where x is
either low dimensional or high dimensional. Our approach is based on selecting
a small subset y_L of the dimensions of y, and proceed by modeling (i) x \to
y_L and (ii) y_L \to y. Composing these two models, we obtain a conditional
model x \to y that possesses convenient statistical properties. Multi-label
classification and multivariate regression experiments on several datasets show
that this model outperforms the one vs. all approach as well as several
sophisticated multiple output prediction methods.
"
"  The global sensitivity analysis of a complex numerical model often calls for
the estimation of variance-based importance measures, named Sobol' indices.
Metamodel-based techniques have been developed in order to replace the cpu
time-expensive computer code with an inexpensive mathematical function, which
predicts the computer code output. The common metamodel-based sensitivity
analysis methods are well-suited for computer codes with scalar outputs.
However, in the environmental domain, as in many areas of application, the
numerical model outputs are often spatial maps, which may also vary with time.
In this paper, we introduce an innovative method to obtain a spatial map of
Sobol' indices with a minimal number of numerical model computations. It is
based upon the functional decomposition of the spatial output onto a wavelet
basis and the metamodeling of the wavelet coefficients by the Gaussian process.
An analytical example is presented to clarify the various steps of our
methodology. This technique is then applied to a real hydrogeological case: for
each model input variable, a spatial map of Sobol' indices is thus obtained.
"
"  The Yarowsky algorithm is a rule-based semi-supervised learning algorithm
that has been successfully applied to some problems in computational
linguistics. The algorithm was not mathematically well understood until (Abney
2004) which analyzed some specific variants of the algorithm, and also proposed
some new algorithms for bootstrapping. In this paper, we extend Abney's work
and show that some of his proposed algorithms actually optimize (an upper-bound
on) an objective function based on a new definition of cross-entropy which is
based on a particular instantiation of the Bregman distance between probability
distributions. Moreover, we suggest some new algorithms for rule-based
semi-supervised learning and show connections with harmonic functions and
minimum multi-way cuts in graph-based semi-supervised learning.
"
"  The analysis of the fully three-dimensional and time-varying polarization
characteristics of a modulated trivariate, or three-component, oscillation is
addressed. The use of the analytic operator enables the instantaneous
three-dimensional polarization state of any square-integrable trivariate signal
to be uniquely defined. Straightforward expressions are given which permit the
ellipse parameters to be recovered from data. The notions of instantaneous
frequency and instantaneous bandwidth, generalized to the trivariate case, are
related to variations in the ellipse properties. Rates of change of the ellipse
parameters are found to be intimately linked to the first few moments of the
signal's spectrum, averaged over the three signal components. In particular,
the trivariate instantaneous bandwidth---a measure of the instantaneous
departure of the signal from a single pure sinusoidal oscillation---is found to
contain five contributions: three essentially two-dimensional effects due to
the motion of the ellipse within a fixed plane, and two effects due to the
motion of the plane containing the ellipse. The resulting analysis method is an
informative means of describing nonstationary trivariate signals, as is
illustrated with an application to a seismic record.
"
"  We consider problems of Bayesian inference for a spatial epidemic on a graph,
where the final state of the epidemic corresponds to bond percolation, and
where only the set or number of finally infected sites is observed. We develop
appropriate Markov chain Monte Carlo algorithms, demonstrating their
effectiveness, and we study problems of optimal experimental design. In
particular, we demonstrate that for lattice-based processes an experiment on a
sparsified lattice can yield more information on model parameters than one
conducted on a complete lattice. We also prove some probabilistic results about
the behaviour of estimators associated with large infected clusters.
"
"  PD curve calibration refers to the transformation of a set of rating grade
level probabilities of default (PDs) to another average PD level that is
determined by a change of the underlying portfolio-wide PD. This paper presents
a framework that allows to explore a variety of calibration approaches and the
conditions under which they are fit for purpose. We test the approaches
discussed by applying them to publicly available datasets of agency rating and
default statistics that can be considered typical for the scope of application
of the approaches. We show that the popular 'scaled PDs' approach is
theoretically questionable and identify an alternative calibration approach
('scaled likelihood ratio') that is both theoretically sound and performs
better on the test datasets.
  Keywords: Probability of default, calibration, likelihood ratio, Bayes'
formula, rating profile, binary classification.
"
"  In this paper, we propose two algorithms for solving linear inverse problems
when the observations are corrupted by noise. A proper data fidelity term
(log-likelihood) is introduced to reflect the statistics of the noise (e.g.
Gaussian, Poisson). On the other hand, as a prior, the images to restore are
assumed to be positive and sparsely represented in a dictionary of waveforms.
Piecing together the data fidelity and the prior terms, the solution to the
inverse problem is cast as the minimization of a non-smooth convex functional.
We establish the well-posedness of the optimization problem, characterize the
corresponding minimizers, and solve it by means of primal and primal-dual
proximal splitting algorithms originating from the field of non-smooth convex
optimization theory. Experimental results on deconvolution, inpainting and
denoising with some comparison to prior methods are also reported.
"
"  The uncertainty or the variability of the data may be treated by considering,
rather than a single value for each data, the interval of values in which it
may fall. This paper studies the derivation of basic description statistics for
interval-valued datasets. We propose a geometrical approach in the
determination of summary statistics (central tendency and dispersion measures)
for interval-valued variables.
"
"  We propose a Bayesian nonparametric approach to the problem of jointly
modeling multiple related time series. Our approach is based on the discovery
of a set of latent, shared dynamical behaviors. Using a beta process prior, the
size of the set and the sharing pattern are both inferred from data. We develop
efficient Markov chain Monte Carlo methods based on the Indian buffet process
representation of the predictive distribution of the beta process, without
relying on a truncated model. In particular, our approach uses the sum-product
algorithm to efficiently compute Metropolis-Hastings acceptance probabilities,
and explores new dynamical behaviors via birth and death proposals. We examine
the benefits of our proposed feature-based model on several synthetic datasets,
and also demonstrate promising results on unsupervised segmentation of visual
motion capture data.
"
"  Simulated annealing is a popular method for approaching the solution of a
global optimization problem. Existing results on its performance apply to
discrete combinatorial optimization where the optimization variables can assume
only a finite set of possible values. We introduce a new general formulation of
simulated annealing which allows one to guarantee finite-time performance in
the optimization of functions of continuous variables. The results hold
universally for any optimization problem on a bounded domain and establish a
connection between simulated annealing and up-to-date theory of convergence of
Markov chain Monte Carlo methods on continuous domains. This work is inspired
by the concept of finite-time learning with known accuracy and confidence
developed in statistical learning theory.
"
"  The paper considers a class of multi-agent Markov decision processes (MDPs),
in which the network agents respond differently (as manifested by the
instantaneous one-stage random costs) to a global controlled state and the
control actions of a remote controller. The paper investigates a distributed
reinforcement learning setup with no prior information on the global state
transition and local agent cost statistics. Specifically, with the agents'
objective consisting of minimizing a network-averaged infinite horizon
discounted cost, the paper proposes a distributed version of $Q$-learning,
$\mathcal{QD}$-learning, in which the network agents collaborate by means of
local processing and mutual information exchange over a sparse (possibly
stochastic) communication network to achieve the network goal. Under the
assumption that each agent is only aware of its local online cost data and the
inter-agent communication network is \emph{weakly} connected, the proposed
distributed scheme is almost surely (a.s.) shown to yield asymptotically the
desired value function and the optimal stationary control policy at each
network agent. The analytical techniques developed in the paper to address the
mixed time-scale stochastic dynamics of the \emph{consensus + innovations}
form, which arise as a result of the proposed interactive distributed scheme,
are of independent interest.
"
"  We present details of the analysis of the nonlinear quality of life index for
171 countries. This index is based on four indicators: GDP per capita by
Purchasing Power Parities, Life expectancy at birth, Infant mortality rate, and
Tuberculosis incidence. We analyze the structure of the data in order to find
the optimal and independent on expert's opinion way to map several numerical
indicators from a multidimensional space onto the one-dimensional space of the
quality of life. In the 4D space we found a principal curve that goes ""through
the middle"" of the dataset and project the data points on this curve. The order
along this principal curve gives us the ranking of countries. Projection onto
the principal curve provides a solution to the classical problem of
unsupervised ranking of objects. It allows us to find the independent on
expert's opinion way to project several numerical indicators from a
multidimensional space onto the one-dimensional space of the index values. This
projection is, in some sense, optimal and preserves as much information as
possible. For computation we used ViDaExpert, a tool for visualization and
analysis of multidimensional vectorial data (arXiv:1406.5550).
"
"  We present a novel and simple method to numerically calculate Fisher
Information Matrices for stochastic chemical kinetics models. The linear noise
approximation is used to derive model equations and a likelihood function which
leads to an efficient computational algorithm. Our approach reduces the problem
of calculating the Fisher Information Matrix to solving a set of ordinary
differential equations. {This is the first method to compute Fisher Information
for stochastic chemical kinetics models without the need for Monte Carlo
simulations.} This methodology is then used to study sensitivity, robustness
and parameter identifiability in stochastic chemical kinetics models. We show
that significant differences exist between stochastic and deterministic models
as well as between stochastic models with time-series and time-point
measurements. We demonstrate that these discrepancies arise from the
variability in molecule numbers, correlations between species, and temporal
correlations and show how this approach can be used in the analysis and design
of experiments probing stochastic processes at the cellular level. The
algorithm has been implemented as a Matlab package and is available from the
authors upon request.
"
"  Equilibrium systems evolve according to Detailed Balance (DB). This principe
guided development of the Monte-Carlo sampling techniques, of which
Metropolis-Hastings (MH) algorithm is the famous representative. It is also
known that DB is sufficient but not necessary. We construct irreversible
deformation of a given reversible algorithm capable of dramatic improvement of
sampling from known distribution. Our transformation modifies transition rates
keeping the structure of transitions intact. To illustrate the general scheme
we design an Irreversible version of Metropolis-Hastings (IMH) and test it on
example of a spin cluster. Standard MH for the model suffers from the critical
slowdown, while IMH is free from critical slowdown.
"
"  We consider the problem of learning the structure of Ising models (pairwise
binary Markov random fields) from i.i.d. samples. While several methods have
been proposed to accomplish this task, their relative merits and limitations
remain somewhat obscure. By analyzing a number of concrete examples, we show
that low-complexity algorithms systematically fail when the Markov random field
develops long-range correlations. More precisely, this phenomenon appears to be
related to the Ising model phase transition (although it does not coincide with
it).
"
"  We consider nonparametric estimation of the state price density encapsulated
in option prices. Unlike usual density estimation problems, we only observe
option prices and their corresponding strike prices rather than samples from
the state price density. We propose to model the state price density directly
with a nonparametric mixture and estimate it using least squares. We show that
although the minimization is taken over an infinitely dimensional function
space, the minimizer always admits a finite dimensional representation and can
be computed efficiently. We also prove that the proposed estimate of the state
price density function converges to the truth at a ``nearly parametric'' rate.
"
"  Studies of functional MRI data are increasingly concerned with the estimation
of differences in spatio-temporal networks across groups of subjects or
experimental conditions. Unsupervised clustering and independent component
analysis (ICA) have been used to identify such spatio-temporal networks. While
these approaches have been useful for estimating these networks at the
subject-level, comparisons over groups or experimental conditions require
further methodological development. In this paper, we tackle this problem by
showing how self-organizing maps (SOMs) can be compared within a Frechean
inferential framework. Here, we summarize the mean SOM in each group as a
Frechet mean with respect to a metric on the space of SOMs. We consider the use
of different metrics, and introduce two extensions of the classical sum of
minimum distance (SMD) between two SOMs, which take into account the
spatio-temporal pattern of the fMRI data. The validity of these methods is
illustrated on synthetic data. Through these simulations, we show that the
three metrics of interest behave as expected, in the sense that the ones
capturing temporal, spatial and spatio-temporal aspects of the SOMs are more
likely to reach significance under simulated scenarios characterized by
temporal, spatial and spatio-temporal differences, respectively. In addition, a
re-analysis of a classical experiment on visually-triggered emotions
demonstrates the usefulness of this methodology. In this study, the
multivariate functional patterns typical of the subjects exposed to pleasant
and unpleasant stimuli are found to be more similar than the ones of the
subjects exposed to emotionally neutral stimuli. Taken together, these results
indicate that our proposed methods can cast new light on existing data by
adopting a global analytical perspective on functional MRI paradigms.
"
"  A number of recent work studied the effectiveness of feature selection using
Lasso. It is known that under the restricted isometry properties (RIP), Lasso
does not generally lead to the exact recovery of the set of nonzero
coefficients, due to the looseness of convex relaxation. This paper considers
the feature selection property of nonconvex regularization, where the solution
is given by a multi-stage convex relaxation scheme. Under appropriate
conditions, we show that the local solution obtained by this procedure recovers
the set of nonzero coefficients without suffering from the bias of Lasso
relaxation, which complements parameter estimation results of this procedure.
"
"  We investigate the role of the initialization for the stability of the
k-means clustering algorithm. As opposed to other papers, we consider the
actual k-means algorithm and do not ignore its property of getting stuck in
local optima. We are interested in the actual clustering, not only in the costs
of the solution. We analyze when different initializations lead to the same
local optimum, and when they lead to different local optima. This enables us to
prove that it is reasonable to select the number of clusters based on stability
scores.
"
"  We study the problem of partitioning a small sample of $n$ individuals from a
mixture of $k$ product distributions over a Boolean cube $\{0, 1\}^K$ according
to their distributions. Each distribution is described by a vector of allele
frequencies in $\R^K$. Given two distributions, we use $\gamma$ to denote the
average $\ell_2^2$ distance in frequencies across $K$ dimensions, which
measures the statistical divergence between them. We study the case assuming
that bits are independently distributed across $K$ dimensions. This work
demonstrates that, for a balanced input instance for $k = 2$, a certain
graph-based optimization function returns the correct partition with high
probability, where a weighted graph $G$ is formed over $n$ individuals, whose
pairwise hamming distances between their corresponding bit vectors define the
edge weights, so long as $K = \Omega(\ln n/\gamma)$ and $Kn = \tilde\Omega(\ln
n/\gamma^2)$. The function computes a maximum-weight balanced cut of $G$, where
the weight of a cut is the sum of the weights across all edges in the cut. This
result demonstrates a nice property in the high-dimensional feature space: one
can trade off the number of features that are required with the size of the
sample to accomplish certain tasks like clustering.
"
"  Consider a two-class classification problem where the number of features is
much larger than the sample size. The features are masked by Gaussian noise
with mean zero and covariance matrix $\Sigma$, where the precision matrix
$\Omega=\Sigma^{-1}$ is unknown but is presumably sparse. The useful features,
also unknown, are sparse and each contributes weakly (i.e., rare and weak) to
the classification decision. By obtaining a reasonably good estimate of
$\Omega$, we formulate the setting as a linear regression model. We propose a
two-stage classification method where we first select features by the method of
Innovated Thresholding (IT), and then use the retained features and Fisher's
LDA for classification. In this approach, a crucial problem is how to set the
threshold of IT. We approach this problem by adapting the recent innovation of
Higher Criticism Thresholding (HCT). We find that when useful features are rare
and weak, the limiting behavior of HCT is essentially just as good as the
limiting behavior of ideal threshold, the threshold one would choose if the
underlying distribution of the signals is known (if only). Somewhat
surprisingly, when $\Omega$ is sufficiently sparse, its off-diagonal
coordinates usually do not have a major influence over the classification
decision. Compared to recent work in the case where $\Omega$ is the identity
matrix [Proc. Natl. Acad. Sci. USA 105 (2008) 14790-14795; Philos. Trans. R.
Soc. Lond. Ser. A Math. Phys. Eng. Sci. 367 (2009) 4449-4470], the current
setting is much more general, which needs a new approach and much more
sophisticated analysis. One key component of the analysis is the intimate
relationship between HCT and Fisher's separation. Another key component is the
tight large-deviation bounds for empirical processes for data with
unconventional correlation structures, where graph theory on vertex coloring
plays an important role.
"
"  The measure of the bullwhip effect, a phenomenon in which demand variability
increases as one moves up the supply chain, is a major issue in Supply Chain
Management. Although it is simply defined (it is the ratio of the unconditional
variance of the order process to that of the demand process), explicit formulas
are difficult to obtain. In this paper we investigate the theoretical and
practical issues of Zhang [Manufacturing and Services Operations Management 6-2
(2004b) 195] with the purpose of quantifying the bullwhip effect. Considering a
two-stage supply chain, the bullwhip effect is measured for an ARMA(p,q) demand
process admitting an infinite moving average representation. As particular
cases of this time series model, the AR(p), MA(q), ARMA(1,1), AR(1) and AR(2)
are discussed. For some of them, explicit formulas are obtained. We show that
for certain types of demand processes, the use of the optimal forecasting
procedure that minimizes the mean squared forecasting error leads to
significant reduction in the safety stock level. This highlights the potential
economic benefits resulting from the use of this time series analysis. Finally,
an R function called SCperf is programmed to calculate the bullwhip effect and
other supply chain performance variables. It leads to a simple but powerful
tool which could benefit both managers and researchers.
"
"  Networks are ubiquitous in science and have become a focal point for
discussion in everyday life. Formal statistical models for the analysis of
network data have emerged as a major topic of interest in diverse areas of
study, and most of these involve a form of graphical representation.
Probability models on graphs date back to 1959. Along with empirical studies in
social psychology and sociology from the 1960s, these early works generated an
active network community and a substantial literature in the 1970s. This effort
moved into the statistical literature in the late 1970s and 1980s, and the past
decade has seen a burgeoning network literature in statistical physics and
computer science. The growth of the World Wide Web and the emergence of online
networking communities such as Facebook, MySpace, and LinkedIn, and a host of
more specialized professional network communities has intensified interest in
the study of networks and network data. Our goal in this review is to provide
the reader with an entry point to this burgeoning literature. We begin with an
overview of the historical development of statistical network modeling and then
we introduce a number of examples that have been studied in the network
literature. Our subsequent discussion focuses on a number of prominent static
and dynamic network models and their interconnections. We emphasize formal
model descriptions, and pay special attention to the interpretation of
parameters and their estimation. We end with a description of some open
problems and challenges for machine learning and statistics.
"
"  Many scholars have recently begun to dispute the assumed link between
individual wellbeing and economic conditions and the extent to which the latter
matters (Easterlin, 1995; Stevenson and Wolfers 2008; Tella and MacCulloch
2008). This dilemma is empirically demonstrated in the Latin America Public
Opinion Project (LAPOP, 2011), which surveyed North and Latin America in terms
of perceived life satisfaction. Higher measures found in the less developed
countries of Brazil, Costa Rica, and Panama than in North America pose an
intriguing quandary to traditional economic theory. In light of this
predicament this paper aims to construct a sensible measure of the national
happiness level for the United States on a year by year basis; and regress this
against indicators of the national economy to provide insight into this
puzzling enigma between national happiness and economic forces
"
"  A new procedure, called DDa-procedure, is developed to solve the problem of
classifying d-dimensional objects into q >= 2 classes. The procedure is
completely nonparametric; it uses q-dimensional depth plots and a very
efficient algorithm for discrimination analysis in the depth space [0,1]^q.
Specifically, the depth is the zonoid depth, and the algorithm is the
alpha-procedure. In case of more than two classes several binary
classifications are performed and a majority rule is applied. Special
treatments are discussed for 'outsiders', that is, data having zero depth
vector. The DDa-classifier is applied to simulated as well as real data, and
the results are compared with those of similar procedures that have been
recently proposed. In most cases the new procedure has comparable error rates,
but is much faster than other classification approaches, including the SVM.
"
"  Given two graphs, the graph matching problem is to align the two vertex sets
so as to minimize the number of adjacency disagreements between the two graphs.
The seeded graph matching problem is the graph matching problem when we are
first given a partial alignment that we are tasked with completing. In this
paper, we modify the state-of-the-art approximate graph matching algorithm
""FAQ"" of Vogelstein et al. (2015) to make it a fast approximate seeded graph
matching algorithm, adapt its applicability to include graphs with differently
sized vertex sets, and extend the algorithm so as to provide, for each
individual vertex, a nomination list of likely matches. We demonstrate the
effectiveness of our algorithm via simulation and real data experiments;
indeed, knowledge of even a few seeds can be extremely effective when our
seeded graph matching algorithm is used to recover a naturally existing
alignment that is only partially observed.
"
"  As data sets grow in size, the ability of learning methods to find structure
in them is increasingly hampered by the time needed to search the large spaces
of possibilities and generate a score for each that takes all of the observed
data into account. For instance, Bayesian networks, the model chosen in this
paper, have a super-exponentially large search space for a fixed number of
variables. One possible method to alleviate this problem is to use a proxy,
such as a Gaussian Process regressor, in place of the true scoring function,
training it on a selection of sampled networks. We prove here that the use of
such a proxy is well-founded, as we can bound the smoothness of a commonly-used
scoring function for Bayesian network structure learning. We show here that,
compared to an identical search strategy using the network?s exact scores, our
proxy-based search is able to get equivalent or better scores on a number of
data sets in a fraction of the time.
"
"  This work concerns estimation of multidimensional nonlinear regression models
using multilayer perceptron (MLP). The main problem with such model is that we
have to know the covariance matrix of the noise to get optimal estimator.
however we show that, if we choose as cost function the logarithm of the
determinant of the empirical error covariance matrix, we get an asymptotically
optimal estimator.
"
"  In this article we evaluate the statistical evidence that a population of
students learn about the sub-game perfect Nash equilibrium of the centipede
game via repeated play of the game. This is done by formulating a model in
which a player's error in assessing the utility of decisions changes as they
gain experience with the game. We first estimate parameters in a statistical
model where the probabilities of choices of the players are given by a Quantal
Response Equilibrium (QRE) (McKelvey and Palfrey, 1995, 1996, 1998), but are
allowed to change with repeated play. This model gives a better fit to the data
than similar models previously considered. However, substantial correlation of
outcomes of games having a common player suggests that a statistical model that
captures within-subject correlation is more appropriate. Thus we then estimate
parameters in a model which allows for within-player correlation of decisions
and rates of learning. Through out the paper we also consider and compare the
use of randomization tests and posterior predictive tests in the context of
exploratory and confirmatory data analyses.
"
"  Combining the mutual information criterion with a forward feature selection
strategy offers a good trade-off between optimality of the selected feature
subset and computation time. However, it requires to set the parameter(s) of
the mutual information estimator and to determine when to halt the forward
procedure. These two choices are difficult to make because, as the
dimensionality of the subset increases, the estimation of the mutual
information becomes less and less reliable. This paper proposes to use
resampling methods, a K-fold cross-validation and the permutation test, to
address both issues. The resampling methods bring information about the
variance of the estimator, information which can then be used to automatically
set the parameter and to calculate a threshold to stop the forward procedure.
The procedure is illustrated on a synthetic dataset as well as on real-world
examples.
"
"  We show that an economic system populated by multiple agents generates an
equilibrium distribution in the form of multiple scaling laws of conditional
PDFs, which are sufficient for characterizing the probability distribution. The
existence of the double scaling law is demonstrated empirically for the sales
and the labor of one million Japanese firms. Theoretical study of the scaling
laws suggests lognormal joint distributions of sales and labor and a scaling
law for labor productivity, both of which are confirmed empirically. This
framework offers characterization of the equilibrium distribution with a small
number of scaling indices, which determine macroscopic quantities, thus setting
the stage for an equivalence with statistical physics, bridging micro- and
macro-economics.
"
"  Given a reproducing kernel Hilbert space H of real-valued functions and a
suitable measure mu over the source space D (subset of R), we decompose H as
the sum of a subspace of centered functions for mu and its orthogonal in H.
This decomposition leads to a special case of ANOVA kernels, for which the
functional ANOVA representation of the best predictor can be elegantly derived,
either in an interpolation or regularization framework. The proposed kernels
appear to be particularly convenient for analyzing the e ffect of each (group
of) variable(s) and computing sensitivity indices without recursivity.
"
"  This paper derives the rate of convergence and asymptotic distribution for a
class of Kolmogorov-Smirnov style test statistics for conditional moment
inequality models for parameters on the boundary of the identified set under
general conditions. In contrast to other moment inequality settings, the rate
of convergence is faster than root-$n$, and the asymptotic distribution depends
entirely on nonbinding moments. The results require the development of new
techniques that draw a connection between moment selection, irregular
identification, bandwidth selection and nonstandard M-estimation. Using these
results, I propose tests that are more powerful than existing approaches for
choosing critical values for this test statistic. I quantify the power
improvement by showing that the new tests can detect alternatives that converge
to points on the identified set at a faster rate than those detected by
existing approaches. A monte carlo study confirms that the tests and the
asymptotic approximations they use perform well in finite samples. In an
application to a regression of prescription drug expenditures on income with
interval data from the Health and Retirement Study, confidence regions based on
the new tests are substantially tighter than those based on existing methods.
"
"  The Cancer Genome Atlas (TCGA) provides researchers with clinicopathological
data and genomic characterizations of various carcinomas. These data sets
include expression microarrays for genes and microRNAs -- short, non-coding
strands of RNA that downregulate gene expression through RNA interference -- as
well as days_to_death and days_to_last_followup fields for each tumor sample.
Our aim is to develop a software tool that screens TCGA data sets for
genes/miRNAs with functional involvement in specific cancers. Furthermore, our
computational pipeline is intended to produce a set of visualizations, or
profiles, that place our screened outputs in a pathway-centric context. We
accomplish our 'screening' by ranking genes/miRNAs by the correlation of their
expression misregulation with differential patient survival. In other words, if
a gene/miRNA is consistently misregulated in patients with poor survival rates
and, on the other hand, is expressed more 'normally' in patients with longer
survival rates, then it is ranked highly; if its misregulation has no such
correlation with good/bad survival in patients, then its rank is low. Our
pathway profiling pipeline produces several outputs, which allow us to examine
the functional roles played by highly ranked genes discovered by our screening.
Running the OV (ovarian serous cystadenocarcinoma) data set through our
analysis pipeline, we find that several highly ranked pathways and functional
groups of genes (VEGF, Jun, Fos, etc.) have already been shown to play some
part in the development of epithelial ovarian carcinomas. We also observe that
the dysfunction of the Wnt signaling pathway, which regulates cell-fate
specification and progenitor cell differentiation, has a disproportionate
impact on the survival of ovarian cancer patients.
"
"  Longitudinal data tracking repeated measurements on individuals are highly
valued for research because they offer controls for unmeasured individual
heterogeneity that might otherwise bias results. Random effects or mixed models
approaches, which treat individual heterogeneity as part of the model error
term and use generalized least squares to estimate model parameters, are often
criticized because correlation between unobserved individual effects and other
model variables can lead to biased and inconsistent parameter estimates.
Starting with an examination of the relationship between random effects and
fixed effects estimators in the standard unobserved effects model, this article
demonstrates through analysis and simulation that the mixed model approach has
a ``bias compression'' property under a general model for individual
heterogeneity that can mitigate bias due to uncontrolled differences among
individuals. The general model is motivated by the complexities of longitudinal
student achievement measures, but the results have broad applicability to
longitudinal modeling.
"
"  We present a mixture Poisson model for claims counts in which the number of
components in the mixture are estimated by reversible jump MCMC methods.
"
"  We develop an unsupervised, nonparametric, and scalable statistical learning
method for detection of unknown objects in noisy images. The method uses
results from percolation theory and random graph theory. We present an
algorithm that allows to detect objects of unknown shapes and sizes in the
presence of nonparametric noise of unknown level. The noise density is assumed
to be unknown and can be very irregular. The algorithm has linear complexity
and exponential accuracy and is appropriate for real-time systems. We prove
strong consistency and scalability of our method in this setup with minimal
assumptions.
"
"  Sunspot numbers form a comprehensive, long-duration proxy of solar activity
and have been used numerous times to empirically investigate the properties of
the solar cycle. A number of correlations have been discovered over the 24
cycles for which observational records are available. Here we carry out a
sophisticated statistical analysis of the sunspot record that reaffirms these
correlations, and sets up an empirical predictive framework for future cycles.
An advantage of our approach is that it allows for rigorous assessment of both
the statistical significance of various cycle features and the uncertainty
associated with predictions. We summarize the data into three sequential
relations that estimate the amplitude, duration, and time of rise to maximum
for any cycle, given the values from the previous cycle. We find that there is
no indication of a persistence in predictive power beyond one cycle, and
conclude that the dynamo does not retain memory beyond one cycle. Based on
sunspot records up to October 2011, we obtain, for Cycle 24, an estimated
maximum smoothed monthly sunspot number of 97 +- 15, to occur in
January--February 2014 +- 6 months.
"
"  We present a novel nonparametric Bayesian approach based on L\'{e}vy Adaptive
Regression Kernels (LARK) to model spectral data arising from MALDI-TOF (Matrix
Assisted Laser Desorption Ionization Time-of-Flight) mass spectrometry. This
model-based approach provides identification and quantification of proteins
through model parameters that are directly interpretable as the number of
proteins, mass and abundance of proteins and peak resolution, while having the
ability to adapt to unknown smoothness as in wavelet based methods. Informative
prior distributions on resolution are key to distinguishing true peaks from
background noise and resolving broad peaks into individual peaks for multiple
protein species. Posterior distributions are obtained using a reversible jump
Markov chain Monte Carlo algorithm and provide inference about the number of
peaks (proteins), their masses and abundance. We show through simulation
studies that the procedure has desirable true-positive and false-discovery
rates. Finally, we illustrate the method on five example spectra: a blank
spectrum, a spectrum with only the matrix of a low-molecular-weight substance
used to embed target proteins, a spectrum with known proteins, and a single
spectrum and average of ten spectra from an individual lung cancer patient.
"
"  Admixture mapping is a popular tool to identify regions of the genome
associated with traits in a recently admixed population. Existing methods have
been developed primarily for identification of a single locus influencing a
dichotomous trait within a case-control study design. We propose a generalized
admixture mapping (GLEAM) approach, a flexible and powerful regression method
for both quantitative and qualitative traits, which is able to test for
association between the trait and local ancestries in multiple loci
simultaneously and adjust for covariates. The new method is based on the
generalized linear model and utilizes a quadratic normal moment prior to
incorporate admixture prior information. Through simulation, we demonstrate
that GLEAM achieves lower type I error rate and higher power than existing
methods both for qualitative traits and more significantly for quantitative
traits. We applied GLEAM to genome-wide SNP data from the Illumina African
American panel derived from a cohort of black woman participating in the
Healthy Pregnancy, Healthy Baby study and identified a locus on chromosome 2
associated with the averaged maternal mean arterial pressure during 24 to 28
weeks of pregnancy.
"
"  mlpy is a Python Open Source Machine Learning library built on top of
NumPy/SciPy and the GNU Scientific Libraries. mlpy provides a wide range of
state-of-the-art machine learning methods for supervised and unsupervised
problems and it is aimed at finding a reasonable compromise among modularity,
maintainability, reproducibility, usability and efficiency. mlpy is
multiplatform, it works with Python 2 and 3 and it is distributed under GPL3 at
the website http://mlpy.fbk.eu.
"
"  Principal components analysis (PCA) is a standard tool for identifying good
low-dimensional approximations to data in high dimension. Many data sets of
interest contain private or sensitive information about individuals. Algorithms
which operate on such data should be sensitive to the privacy risks in
publishing their outputs. Differential privacy is a framework for developing
tradeoffs between privacy and the utility of these outputs. In this paper we
investigate the theory and empirical performance of differentially private
approximations to PCA and propose a new method which explicitly optimizes the
utility of the output. We show that the sample complexity of the proposed
method differs from the existing procedure in the scaling with the data
dimension, and that our method is nearly optimal in terms of this scaling. We
furthermore illustrate our results, showing that on real data there is a large
performance gap between the existing method and our method.
"
"  We present a two-stage approach for learning dictionaries for object
classification tasks based on the principle of information maximization. The
proposed method seeks a dictionary that is compact, discriminative, and
generative. In the first stage, dictionary atoms are selected from an initial
dictionary by maximizing the mutual information measure on dictionary
compactness, discrimination and reconstruction. In the second stage, the
selected dictionary atoms are updated for improved reconstructive and
discriminative power using a simple gradient ascent algorithm on mutual
information. Experiments using real datasets demonstrate the effectiveness of
our approach for image classification tasks.
"
"  The role of kernels is central to machine learning. Motivated by the
importance of power-law distributions in statistical modeling, in this paper,
we propose the notion of power-law kernels to investigate power-laws in
learning problem. We propose two power-law kernels by generalizing Gaussian and
Laplacian kernels. This generalization is based on distributions, arising out
of maximization of a generalized information measure known as nonextensive
entropy that is very well studied in statistical mechanics. We prove that the
proposed kernels are positive definite, and provide some insights regarding the
corresponding Reproducing Kernel Hilbert Space (RKHS). We also study practical
significance of both kernels in classification and regression, and present some
simulation results.
"
"  Regularized kernel methods such as, e.g., support vector machines and
least-squares support vector regression constitute an important class of
standard learning algorithms in machine learning. Theoretical investigations
concerning asymptotic properties have manly focused on rates of convergence
during the last years but there are only very few and limited (asymptotic)
results on statistical inference so far. As this is a serious limitation for
their use in mathematical statistics, the goal of the article is to fill this
gap. Based on asymptotic normality of many of these methods, the article
derives a strongly consistent estimator for the unknown covariance matrix of
the limiting normal distribution. In this way, we obtain asymptotically correct
confidence sets for $\psi(f_{P,\lambda_0})$ where $f_{P,\lambda_0}$ denotes the
minimizer of the regularized risk in the reproducing kernel Hilbert space $H$
and $\psi:H\rightarrow\mathds{R}^m$ is any Hadamard-differentiable functional.
Applications include (multivariate) pointwise confidence sets for values of
$f_{P,\lambda_0}$ and confidence sets for gradients, integrals, and norms.
"
"  The contractive auto-encoder learns a representation of the input data that
captures the local manifold structure around each data point, through the
leading singular vectors of the Jacobian of the transformation from input to
representation. The corresponding singular values specify how much local
variation is plausible in directions associated with the corresponding singular
vectors, while remaining in a high-density region of the input space. This
paper proposes a procedure for generating samples that are consistent with the
local structure captured by a contractive auto-encoder. The associated
stochastic process defines a distribution from which one can sample, and which
experimentally appears to converge quickly and mix well between modes, compared
to Restricted Boltzmann Machines and Deep Belief Networks. The intuitions
behind this procedure can also be used to train the second layer of contraction
that pools lower-level features and learns to be invariant to the local
directions of variation discovered in the first layer. We show that this can
help learn and represent invariances present in the data and improve
classification error.
"
"  A neural network combined to a neural classifier is used in a real time
forecasting of hourly maximum ozone in the centre of France, in an urban
atmosphere. This neural model is based on the MultiLayer Perceptron (MLP)
structure. The inputs of the statistical network are model output statistics of
the weather predictions from the French National Weather Service. With this
neural classifier, the Success Index of forecasting is 78% whereas it is from
65% to 72% with the classical MLPs. During the validation phase, in the Summer
of 2003, six ozone peaks above the threshold were detected. They actually were
seven.
"
"  Early, reliable detection of disease outbreaks is a critical problem today.
This paper reports an investigation of the use of causal Bayesian networks to
model spatio-temporal patterns of a non-contagious disease (respiratory anthrax
infection) in a population of people. The number of parameters in such a
network can become enormous, if not carefully managed. Also, inference needs to
be performed in real time as population data stream in. We describe techniques
we have applied to address both the modeling and inference challenges. A key
contribution of this paper is the explication of assumptions and techniques
that are sufficient to allow the scaling of Bayesian network modeling and
inference to millions of nodes for real-time surveillance applications. The
results reported here provide a proof-of-concept that Bayesian networks can
serve as the foundation of a system that effectively performs Bayesian
biosurveillance of disease outbreaks.
"
"  The paper considers a particular family of set--valued stochastic processes
modeling birth--and--growth processes. The proposed setting allows us to
investigate the nucleation and the growth processes. A decomposition theorem is
established to characterize the nucleation and the growth. As a consequence,
different consistent set--valued estimators are studied for growth process.
Moreover, the nucleation process is studied via the hitting function, and a
consistent estimator of the nucleation hitting function is derived.
"
"  We consider the problem of regression learning for deterministic design and
independent random errors. We start by proving a sharp PAC-Bayesian type bound
for the exponentially weighted aggregate (EWA) under the expected squared
empirical loss. For a broad class of noise distributions the presented bound is
valid whenever the temperature parameter $\beta$ of the EWA is larger than or
equal to $4\sigma^2$, where $\sigma^2$ is the noise variance. A remarkable
feature of this result is that it is valid even for unbounded regression
functions and the choice of the temperature parameter depends exclusively on
the noise level. Next, we apply this general bound to the problem of
aggregating the elements of a finite-dimensional linear space spanned by a
dictionary of functions $\phi_1,...,\phi_M$. We allow $M$ to be much larger
than the sample size $n$ but we assume that the true regression function can be
well approximated by a sparse linear combination of functions $\phi_j$. Under
this sparsity scenario, we propose an EWA with a heavy tailed prior and we show
that it satisfies a sparsity oracle inequality with leading constant one.
Finally, we propose several Langevin Monte-Carlo algorithms to approximately
compute such an EWA when the number $M$ of aggregated functions can be large.
We discuss in some detail the convergence of these algorithms and present
numerical experiments that confirm our theoretical findings.
"
"  The CUR matrix decomposition is an important extension of Nystr\""{o}m
approximation to a general matrix. It approximates any data matrix in terms of
a small number of its columns and rows. In this paper we propose a novel
randomized CUR algorithm with an expected relative-error bound. The proposed
algorithm has the advantages over the existing relative-error CUR algorithms
that it possesses tighter theoretical bound and lower time complexity, and that
it can avoid maintaining the whole data matrix in main memory. Finally,
experiments on several real-world datasets demonstrate significant improvement
over the existing relative-error algorithms.
"
"  Boosting is a popular way to derive powerful learners from simpler hypothesis
classes. Following previous work (Mason et al., 1999; Friedman, 2000) on
general boosting frameworks, we analyze gradient-based descent algorithms for
boosting with respect to any convex objective and introduce a new measure of
weak learner performance into this setting which generalizes existing work. We
present the weak to strong learning guarantees for the existing gradient
boosting work for strongly-smooth, strongly-convex objectives under this new
measure of performance, and also demonstrate that this work fails for
non-smooth objectives. To address this issue, we present new algorithms which
extend this boosting approach to arbitrary convex loss functions and give
corresponding weak to strong convergence results. In addition, we demonstrate
experimental results that support our analysis and demonstrate the need for the
new algorithms we present.
"
"  The estimation of a density profile from experimental data points is a
challenging problem, usually tackled by plotting a histogram. Prior assumptions
on the nature of the density, from its smoothness to the specification of its
form, allow the design of more accurate estimation procedures, such as Maximum
Likelihood. Our aim is to construct a procedure that makes no explicit
assumptions, but still providing an accurate estimate of the density. We
introduce the self-consistent estimate: the power spectrum of a candidate
density is given, and an estimation procedure is constructed on the assumption,
to be released \emph{a posteriori}, that the candidate is correct. The
self-consistent estimate is defined as a prior candidate density that precisely
reproduces itself. Our main result is to derive the exact expression of the
self-consistent estimate for any given dataset, and to study its properties.
Applications of the method require neither priors on the form of the density
nor the subjective choice of parameters. A cutoff frequency, akin to a bin size
or a kernel bandwidth, emerges naturally from the derivation. We apply the
self-consistent estimate to artificial data generated from various
distributions and show that it reaches the theoretical limit for the scaling of
the square error with the dataset size.
"
"  Many recent statistical applications involve inference under complex models,
where it is computationally prohibitive to calculate likelihoods but possible
to simulate data. Approximate Bayesian Computation (ABC) is devoted to these
complex models because it bypasses evaluations of the likelihood function using
comparisons between observed and simulated summary statistics. We introduce the
R abc package that implements several ABC algorithms for performing parameter
estimation and model selection. In particular, the recently developed
non-linear heteroscedastic regression methods for ABC are implemented. The abc
package also includes a cross-validation tool for measuring the accuracy of ABC
estimates, and to calculate the misclassification probabilities when performing
model selection. The main functions are accompanied by appropriate summary and
plotting tools. Considering an example of demographic inference with population
genetics data, we show the potential of the R package.
  R is already widely used in bioinformatics and several fields of biology. The
R abc package will make the ABC algorithms available to the large number of R
users. abc is a freely available R package under the GPL license, and it can be
downloaded at http://cran.r-project.org/web/packages/abc/index.html.
"
"  A general framework is proposed for Bayesian model-based designs of Phase I
cancer trials, in which a general criterion for coherence (Cheung, 2005) of a
design is also developed. This framework can incorporate both ""individual"" and
""collective"" ethics into the design of the trial. We propose a new design which
minimizes a risk function composed of two terms, with one representing the
individual risk of the current dose and the other representing the collective
risk. The performance of this design, which is measured in terms of the
accuracy of the estimated target dose at the end of the trial, the toxicity and
overdose rates, and certain loss functions reflecting the individual and
collective ethics, is studied and compared with existing Bayesian model-based
designs and is shown to have better performance than existing designs.
"
"  The examples of rhythmical signals with variable period are considered. The
definition of periodic function with the variable period is given as a model of
such signals. The examples of such functions are given and their variable
periods are written in the explicit form. The system of trigonometric functions
with the variable period is considered and its orthogonality is proved. The
generalized system of trigonometric functions with the variable period is also
suggested; some conditions of its existence are considered.
"
"  Fitting probabilistic models to data is often difficult, due to the general
intractability of the partition function and its derivatives. Here we propose a
new parameter estimation technique that does not require computing an
intractable normalization factor or sampling from the equilibrium distribution
of the model. This is achieved by establishing dynamics that would transform
the observed data distribution into the model distribution, and then setting as
the objective the minimization of the KL divergence between the data
distribution and the distribution produced by running the dynamics for an
infinitesimal time. Score matching, minimum velocity learning, and certain
forms of contrastive divergence are shown to be special cases of this learning
technique. We demonstrate parameter estimation in Ising models, deep belief
networks and an independent component analysis model of natural scenes. In the
Ising model case, current state of the art techniques are outperformed by at
least an order of magnitude in learning time, with lower error in recovered
coupling parameters.
"
"  We consider the problem of learning object arrangements in a 3D scene. The
key idea here is to learn how objects relate to human poses based on their
affordances, ease of use and reachability. In contrast to modeling
object-object relationships, modeling human-object relationships scales
linearly in the number of objects. We design appropriate density functions
based on 3D spatial features to capture this. We learn the distribution of
human poses in a scene using a variant of the Dirichlet process mixture model
that allows sharing of the density function parameters across the same object
types. Then we can reason about arrangements of the objects in the room based
on these meaningful human poses. In our extensive experiments on 20 different
rooms with a total of 47 objects, our algorithm predicted correct placements
with an average error of 1.6 meters from ground truth. In arranging five real
scenes, it received a score of 4.3/5 compared to 3.7 for the best baseline
method.
"
"  Discussion of ""A statistical analysis of multiple temperature proxies: Are
reconstructions of surface temperatures over the last 1000 years reliable?"" by
B.B. McShane and A.J. Wyner [arXiv:1104.4002]
"
"  We propose a novel model for generating graphs similar to a given example
graph. Unlike standard approaches that compute features of graphs in Euclidean
space, our approach obtains features on a surface of a hypersphere. We then
utilize a von Mises-Fisher distribution, an exponential family distribution on
the surface of a hypersphere, to define a model over possible feature values.
While our approach bears similarity to a popular exponential random graph model
(ERGM), unlike ERGMs, it does not suffer from degeneracy, a situation when a
significant probability mass is placed on unrealistic graphs. We propose a
parameter estimation approach for our model, and a procedure for drawing
samples from the distribution. We evaluate the performance of our approach both
on the small domain of all 8-node graphs as well as larger real-world social
networks.
"
"  This paper proposes confidence regions for the identified set in conditional
moment inequality models using Kolmogorov-Smirnov statistics with a truncated
inverse variance weighting with increasing truncation points. The new weighting
differs from those proposed in the literature in two important ways. First,
confidence regions based on KS tests with the weighting function I propose
converge to the identified set at a faster rate than existing procedures based
on bounded weight functions in a broad class of models. This provides a
theoretical justification for inverse variance weighting in this context, and
contrasts with analogous results for conditional moment equalities in which
optimal weighting only affects the asymptotic variance. Second, the new
weighting changes the asymptotic behavior, including the rate of convergence,
of the KS statistic itself, requiring a new asymptotic theory in choosing the
critical value, which I provide. To make these comparisons, I derive rates of
convergence for the confidence regions I propose along with new results for
rates of convergence of existing estimators under a general set of conditions.
A series of examples illustrates the broad applicability of the conditions. A
monte carlo study examines the finite sample behavior of the confidence
regions.
"
"  Recently, metric learning and similarity learning have attracted a large
amount of interest. Many models and optimisation algorithms have been proposed.
However, there is relatively little work on the generalization analysis of such
methods. In this paper, we derive novel generalization bounds of metric and
similarity learning. In particular, we first show that the generalization
analysis reduces to the estimation of the Rademacher average over
""sums-of-i.i.d."" sample-blocks related to the specific matrix norm. Then, we
derive generalization bounds for metric/similarity learning with different
matrix-norm regularisers by estimating their specific Rademacher complexities.
Our analysis indicates that sparse metric/similarity learning with $L^1$-norm
regularisation could lead to significantly better bounds than those with
Frobenius-norm regularisation. Our novel generalization analysis develops and
refines the techniques of U-statistics and Rademacher complexity analysis.
"
"  The aim of the present paper is to develop a strategy for solving
reliability-based design optimization (RBDO) problems that remains applicable
when the performance models are expensive to evaluate. Starting with the
premise that simulation-based approaches are not affordable for such problems,
and that the most-probable-failure-point-based approaches do not permit to
quantify the error on the estimation of the failure probability, an approach
based on both metamodels and advanced simulation techniques is explored. The
kriging metamodeling technique is chosen in order to surrogate the performance
functions because it allows one to genuinely quantify the surrogate error. The
surrogate error onto the limit-state surfaces is propagated to the failure
probabilities estimates in order to provide an empirical error measure. This
error is then sequentially reduced by means of a population-based adaptive
refinement technique until the kriging surrogates are accurate enough for
reliability analysis. This original refinement strategy makes it possible to
add several observations in the design of experiments at the same time.
Reliability and reliability sensitivity analyses are performed by means of the
subset simulation technique for the sake of numerical efficiency. The adaptive
surrogate-based strategy for reliability estimation is finally involved into a
classical gradient-based optimization algorithm in order to solve the RBDO
problem. The kriging surrogates are built in a so-called augmented reliability
space thus making them reusable from one nested RBDO iteration to the other.
The strategy is compared to other approaches available in the literature on
three academic examples in the field of structural mechanics.
"
"  In the past, we have observed several large blackouts, i.e. loss of power to
large areas. It has been noted by several researchers that these large
blackouts are a result of a cascade of failures of various components. As a
power grid is made up of several thousands or even millions of components
(relays, breakers, transformers, etc.), it is quite plausible that a few of
these components do not perform their function as desired. Their
failure/misbehavior puts additional burden on the working components causing
them to misbehave, and thus leading to a cascade of failures.
  The complexity of the entire power grid makes it difficult to model each and
every individual component and study the stability of the entire system. For
this reason, it is often the case that abstract models of the working of the
power grid are constructed and then analyzed. These models need to be
computationally tractable while serving as a reasonable model for the entire
system. In this work, we construct one such model for the power grid, and
analyze it.
"
"  Products of Hidden Markov Models(PoHMMs) are an interesting class of
generative models which have received little attention since their
introduction. This maybe in part due to their more computationally expensive
gradient-based learning algorithm,and the intractability of computing the log
likelihood of sequences under the model. In this paper, we demonstrate how the
partition function can be estimated reliably via Annealed Importance Sampling.
We perform experiments using contrastive divergence learning on rainfall data
and data captured from pairs of people dancing. Our results suggest that
advances in learning and evaluation for undirected graphical models and recent
increases in available computing power make PoHMMs worth considering for
complex time-series modeling tasks.
"
"  This paper presents the R package gRapHD for efficient selection of
high-dimensional undirected graphical models. The package provides tools for
selecting trees, forests and decomposable models minimizing information
criteria such as AIC or BIC, and for displaying the independence graphs of the
models. It has also some useful tools for analysing graphical structures. It
supports the use of discrete, continuous, or both types of variables
simultaneously.
"
"  Correction to Annals of Applied Statistics 1 (2007) 17--35
[doi:10.1214/07-AOAS114]
"
"  In latent Dirichlet allocation (LDA), topics are multinomial distributions
over the entire vocabulary. However, the vocabulary usually contains many words
that are not relevant in forming the topics. We adopt a variable selection
method widely used in statistical modeling as a dimension reduction tool and
combine it with LDA. In this variable selection model for LDA (vsLDA), topics
are multinomial distributions over a subset of the vocabulary, and by excluding
words that are not informative for finding the latent topic structure of the
corpus, vsLDA finds topics that are more robust and discriminative. We compare
three models, vsLDA, LDA with symmetric priors, and LDA with asymmetric priors,
on heldout likelihood, MCMC chain consistency, and document classification. The
performance of vsLDA is better than symmetric LDA for likelihood and
classification, better than asymmetric LDA for consistency and classification,
and about the same in the other comparisons.
"
"  We present the multidimensional membership mixture (M3) models where every
dimension of the membership represents an independent mixture model and each
data point is generated from the selected mixture components jointly. This is
helpful when the data has a certain shared structure. For example, three unique
means and three unique variances can effectively form a Gaussian mixture model
with nine components, while requiring only six parameters to fully describe it.
In this paper, we present three instantiations of M3 models (together with the
learning and inference algorithms): infinite, finite, and hybrid, depending on
whether the number of mixtures is fixed or not. They are built upon Dirichlet
process mixture models, latent Dirichlet allocation, and a combination
respectively. We then consider two applications: topic modeling and learning 3D
object arrangements. Our experiments show that our M3 models achieve better
performance using fewer topics than many classic topic models. We also observe
that topics from the different dimensions of M3 models are meaningful and
orthogonal to each other.
"
"  One important aspect of the relationship between spoken and written Chinese
is the ranked syllable-to-character mapping spectrum, which is the ranked list
of syllables by the number of characters that map to the syllable. Previously,
this spectrum is analyzed for more than 400 syllables without distinguishing
the four intonations. In the current study, the spectrum with 1280 toned
syllables is analyzed by logarithmic function, Beta rank function, and
piecewise logarithmic function. Out of the three fitting functions, the
two-piece logarithmic function fits the data the best, both by the smallest sum
of squared errors (SSE) and by the lowest Akaike information criterion (AIC)
value. The Beta rank function is the close second. By sampling from a Poisson
distribution whose parameter value is chosen from the observed data, we
empirically estimate the $p$-value for testing the
two-piece-logarithmic-function being better than the Beta rank function
hypothesis, to be 0.16. For practical purposes, the piecewise logarithmic
function and the Beta rank function can be considered a tie.
"
"  Forensic science is usually taken to mean the application of a broad spectrum
of scientific tools to answer questions of interest to the legal system.
Despite such popular television series as CSI: Crime Scene Investigation and
its spinoffs--CSI: Miami and CSI: New York--on which the forensic scientists
use the latest high-tech scientific tools to identify the perpetrator of a
crime and always in under an hour, forensic science is under assault, in the
public media, popular magazines [Talbot (2007), Toobin (2007)] and in the
scientific literature [Kennedy (2003), Saks and Koehler (2005)]. Ironically,
this growing controversy over forensic science has occurred precisely at the
time that DNA evidence has become the ``gold standard'' in the courts, leading
to the overturning of hundreds of convictions many of which were based on
clearly less credible forensic evidence, including eyewitness testimony [Berger
(2006)].
"
"  In machine learning, Domain Adaptation (DA) arises when the distribution gen-
erating the test (target) data differs from the one generating the learning
(source) data. It is well known that DA is an hard task even under strong
assumptions, among which the covariate-shift where the source and target
distributions diverge only in their marginals, i.e. they have the same labeling
function. Another popular approach is to consider an hypothesis class that
moves closer the two distributions while implying a low-error for both tasks.
This is a VC-dim approach that restricts the complexity of an hypothesis class
in order to get good generalization. Instead, we propose a PAC-Bayesian
approach that seeks for suitable weights to be given to each hypothesis in
order to build a majority vote. We prove a new DA bound in the PAC-Bayesian
context. This leads us to design the first DA-PAC-Bayesian algorithm based on
the minimization of the proposed bound. Doing so, we seek for a \rho-weighted
majority vote that takes into account a trade-off between three quantities. The
first two quantities being, as usual in the PAC-Bayesian approach, (a) the
complexity of the majority vote (measured by a Kullback-Leibler divergence) and
(b) its empirical risk (measured by the \rho-average errors on the source
sample). The third quantity is (c) the capacity of the majority vote to
distinguish some structural difference between the source and target samples.
"
"  Understanding the mutual relationships between information flows and social
activity in society today is one of the cornerstones of the social sciences. In
financial economics, the key issue in this regard is understanding and
quantifying how news of all possible types (geopolitical, environmental,
social, financial, economic, etc.) affect trading and the pricing of firms in
organized stock markets. In this article, we seek to address this issue by
performing an analysis of more than 24 million news records provided by
Thompson Reuters and of their relationship with trading activity for 206 major
stocks in the S&P US stock index. We show that the whole landscape of news that
affect stock price movements can be automatically summarized via simple
regularized regressions between trading activity and news information pieces
decomposed, with the help of simple topic modeling techniques, into their
""thematic"" features. Using these methods, we are able to estimate and quantify
the impacts of news on trading. We introduce network-based visualization
techniques to represent the whole landscape of news information associated with
a basket of stocks. The examination of the words that are representative of the
topic distributions confirms that our method is able to extract the significant
pieces of information influencing the stock market. Our results show that one
of the most puzzling stylized fact in financial economies, namely that at
certain times trading volumes appear to be ""abnormally large,"" can be partially
explained by the flow of news. In this sense, our results prove that there is
no ""excess trading,"" when restricting to times when news are genuinely novel
and provide relevant financial information.
"
"  Feature selection is a technique to screen out less important features. Many
existing supervised feature selection algorithms use redundancy and relevancy
as the main criteria to select features. However, feature interaction,
potentially a key characteristic in real-world problems, has not received much
attention. As an attempt to take feature interaction into account, we propose
L1-LSMI, an L1-regularization based algorithm that maximizes a squared-loss
variant of mutual information between selected features and outputs. Numerical
results show that L1-LSMI performs well in handling redundancy, detecting
non-linear dependency, and considering feature interaction.
"
"  We revisit the problem of feature selection in linear discriminant analysis
(LDA), that is, when features are correlated. First, we introduce a pooled
centroids formulation of the multiclass LDA predictor function, in which the
relative weights of Mahalanobis-transformed predictors are given by
correlation-adjusted $t$-scores (cat scores). Second, for feature selection we
propose thresholding cat scores by controlling false nondiscovery rates (FNDR).
Third, training of the classifier is based on James--Stein shrinkage estimates
of correlations and variances, where regularization parameters are chosen
analytically without resampling. Overall, this results in an effective and
computationally inexpensive framework for high-dimensional prediction with
natural feature selection. The proposed shrinkage discriminant procedures are
implemented in the R package ``sda'' available from the R repository CRAN.
"
"  Crossover clinical trials can provide substantial benefits by eliminating
inter-patient variation from treatment comparisons and by allowing multiple
observations of each patient. They are particularly useful when sample sizes
are necessarily small. These advantages proved particularly valuable in an
assessment of clot prevention in children undergoing haemodialysis. Only small
numbers of children are treated at any given time in any single unit, but each
patient is obliged to attend two or three times each week, suggesting the use
of a crossover trial with many periods. Standard crossover trials described in
the literature a) typically have fewer than 10 periods and b) are based on a
model of questionable applicability to this study. This paper describes the
derivation of an optimal crossover trial with 30 periods which was used to
compare the treatments using nine patients.
"
"  We propose a nonlinear voter model to study the emergence of global consensus
in opinion dynamics. In our model, agent $i$ agrees with one of binary opinions
with the probability that is a power function of the number of agents holding
this opinion among agent $i$ and its nearest neighbors, where an adjustable
parameter $\alpha$ controls the effect of herd behavior on consensus. We find
that there exists an optimal value of $\alpha$ leading to the fastest consensus
for lattices, random graphs, small-world networks and scale-free networks.
Qualitative insights are obtained by examining the spatiotemporal evolution of
the opinion clusters.
"
"  When releasing data to the public, data stewards are ethically and often
legally obligated to protect the confidentiality of data subjects' identities
and sensitive attributes. They also strive to release data that are informative
for a wide range of secondary analyses. Achieving both objectives is
particularly challenging when data stewards seek to release highly resolved
geographical information. We present an approach for protecting the
confidentiality of data with geographic identifiers based on multiple
imputation. The basic idea is to convert geography to latitude and longitude,
estimate a bivariate response model conditional on attributes, and simulate new
latitude and longitude values from these models. We illustrate the proposed
methods using data describing causes of death in Durham, North Carolina. In the
context of the application, we present a straightforward tool for generating
simulated geographies and attributes based on regression trees, and we present
methods for assessing disclosure risks with such simulated data.
"
"  We present a very fast algorithm for general matrix factorization of a data
matrix for use in the statistical analysis of high-dimensional data via latent
factors. Such data are prevalent across many application areas and generate an
ever-increasing demand for methods of dimension reduction in order to undertake
the statistical analysis of interest. Our algorithm uses a gradient-based
approach which can be used with an arbitrary loss function provided the latter
is differentiable. The speed and effectiveness of our algorithm for dimension
reduction is demonstrated in the context of supervised classification of some
real high-dimensional data sets from the bioinformatics literature.
"
"  Recently, there has been an explosion of work on network routing in hostile
environments. Hostile environments tend to be dynamic, and the motivation for
this work stems from the scenario of IED placements by insurgents in a
logistical network. For discussion, we consider here a sub-network abstracted
from a real network, and propose a framework for route selection. What
distinguishes our work from related work is its decision theoretic foundation,
and statistical considerations pertaining to probability assessments. The
latter entails the fusion of data from diverse sources, modeling the
socio-psychological behavior of adversaries, and likelihood functions that are
induced by simulation. This paper demonstrates the role of statistical
inference and data analysis on problems that have traditionally belonged in the
domain of computer science, communications, transportation science, and
operations research.
"
"  We derive relations between theoretical properties of restricted Boltzmann
machines (RBMs), popular machine learning models which form the building blocks
of deep learning models, and several natural notions from discrete mathematics
and convex geometry. We give implications and equivalences relating
RBM-representable probability distributions, perfectly reconstructible inputs,
Hamming modes, zonotopes and zonosets, point configurations in hyperplane
arrangements, linear threshold codes, and multi-covering numbers of hypercubes.
As a motivating application, we prove results on the relative representational
power of mixtures of product distributions and products of mixtures of pairs of
product distributions (RBMs) that formally justify widely held intuitions about
distributed representations. In particular, we show that a mixture of products
requiring an exponentially larger number of parameters is needed to represent
the probability distributions which can be obtained as products of mixtures.
"
"  With the increasing size of today's data sets, finding the right parameter
configuration in model selection via cross-validation can be an extremely
time-consuming task. In this paper we propose an improved cross-validation
procedure which uses nonparametric testing coupled with sequential analysis to
determine the best parameter set on linearly increasing subsets of the data. By
eliminating underperforming candidates quickly and keeping promising candidates
as long as possible, the method speeds up the computation while preserving the
capability of the full cross-validation. Theoretical considerations underline
the statistical power of our procedure. The experimental evaluation shows that
our method reduces the computation time by a factor of up to 120 compared to a
full cross-validation with a negligible impact on the accuracy.
"
"  Societal stress may cause far reaching political, economic and even
geological effects. Nevertheless, it is still scarcely investigated, contrary
to social stress, which an individual faces in their interactions within a
society. It is natural to suppose that in its adaptation, society demonstrates
the same objective laws that biological population does, since they are, in
fact, the closest systems. In the survey, the hypothesis is tested that the
collective stress effect holds true in society, which must appear (as it
happens according to correlation adaptometry method in biological systems) in
escalation of both correlations between societal characteristics and their
dispersion. Both tends are observed in Ukrainian society during 2009-2012, as a
result of political elections that affect societal anxiety.
"
"  We consider the sampling problem for functional PCA (fPCA), where the
simplest example is the case of taking time samples of the underlying
functional components. More generally, we model the sampling operation as a
continuous linear map from $\mathcal{H}$ to $\mathbb{R}^m$, where the
functional components to lie in some Hilbert subspace $\mathcal{H}$ of $L^2$,
such as a reproducing kernel Hilbert space of smooth functions. This model
includes time and frequency sampling as special cases. In contrast to classical
approach in fPCA in which access to entire functions is assumed, having a
limited number m of functional samples places limitations on the performance of
statistical procedures. We study these effects by analyzing the rate of
convergence of an M-estimator for the subspace spanned by the leading
components in a multi-spiked covariance model. The estimator takes the form of
regularized PCA, and hence is computationally attractive. We analyze the
behavior of this estimator within a nonasymptotic framework, and provide bounds
that hold with high probability as a function of the number of statistical
samples n and the number of functional samples m. We also derive lower bounds
showing that the rates obtained are minimax optimal.
"
"  This work studies formal utility and privacy guarantees for a simple
multiplicative database transformation, where the data are compressed by a
random linear or affine transformation, reducing the number of data records
substantially, while preserving the number of original input variables. We
provide an analysis framework inspired by a recent concept known as
differential privacy (Dwork 06). Our goal is to show that, despite the general
difficulty of achieving the differential privacy guarantee, it is possible to
publish synthetic data that are useful for a number of common statistical
learning applications. This includes high dimensional sparse regression (Zhou
et al. 07), principal component analysis (PCA), and other statistical measures
(Liu et al. 06) based on the covariance of the initial data.
"
"  We consider cooperative spectrum sensing for cognitive radios. We develop an
energy efficient detector with low detection delay using sequential hypothesis
testing. Sequential Probability Ratio Test (SPRT) is used at both the local
nodes and the fusion center. We also analyse the performance of this algorithm
and compare with the simulations. Modelling uncertainties in the distribution
parameters are considered. Slow fading with and without perfect channel state
information at the cognitive radios is taken into account.
"
"  The spatial scan statistic is widely used in epidemiology and medical studies
as a tool to identify hotspots of diseases. The classical spatial scan
statistic assumes the number of disease cases in different locations have
independent Poisson distributions, while in practice the data may exhibit
overdispersion and spatial correlation. In this work, we examine the behavior
of the spatial scan statistic when overdispersion and spatial correlation are
present, and propose a modified spatial scan statistic to account for that.
Some theoretical results are provided to demonstrate that ignoring the
overdispersion and spatial correlation leads to an increased rate of false
positives, which is verified through a simulation study. Simulation studies
also show that our modified procedure can substantially reduce the rate of
false alarms. Two data examples involving brain cancer cases in New Mexico and
chickenpox incidence data in France are used to illustrate the practical
relevance of the modified procedure.
"
"  Deep belief networks are a powerful way to model complex probability
distributions. However, learning the structure of a belief network,
particularly one with hidden units, is difficult. The Indian buffet process has
been used as a nonparametric Bayesian prior on the directed structure of a
belief network with a single infinitely wide hidden layer. In this paper, we
introduce the cascading Indian buffet process (CIBP), which provides a
nonparametric prior on the structure of a layered, directed belief network that
is unbounded in both depth and width, yet allows tractable inference. We use
the CIBP prior with the nonlinear Gaussian belief network so each unit can
additionally vary its behavior between discrete and continuous representations.
We provide Markov chain Monte Carlo algorithms for inference in these belief
networks and explore the structures learned on several image data sets.
"
"  Many data are naturally modeled by an unobserved hierarchical structure. In
this paper we propose a flexible nonparametric prior over unknown data
hierarchies. The approach uses nested stick-breaking processes to allow for
trees of unbounded width and depth, where data can live at any node and are
infinitely exchangeable. One can view our model as providing infinite mixtures
where the components have a dependency structure corresponding to an
evolutionary diffusion down a tree. By using a stick-breaking approach, we can
apply Markov chain Monte Carlo methods based on slice sampling to perform
Bayesian inference and simulate from the posterior distribution on trees. We
apply our method to hierarchical clustering of images and topic modeling of
text data.
"
"  The question of the optimality of Thompson Sampling for solving the
stochastic multi-armed bandit problem had been open since 1933. In this paper
we answer it positively for the case of Bernoulli rewards by providing the
first finite-time analysis that matches the asymptotic rate given in the Lai
and Robbins lower bound for the cumulative regret. The proof is accompanied by
a numerical comparison with other optimal policies, experiments that have been
lacking in the literature until now for the Bernoulli case.
"
"  We consider the structure learning problem for graphical models that we call
loosely connected Markov random fields, in which the number of short paths
between any pair of nodes is small, and present a new conditional independence
test based algorithm for learning the underlying graph structure. The novel
maximization step in our algorithm ensures that the true edges are detected
correctly even when there are short cycles in the graph. The number of samples
required by our algorithm is C*log p, where p is the size of the graph and the
constant C depends on the parameters of the model. We show that several
previously studied models are examples of loosely connected Markov random
fields, and our algorithm achieves the same or lower computational complexity
than the previously designed algorithms for individual cases. We also get new
results for more general graphical models, in particular, our algorithm learns
general Ising models on the Erdos-Renyi random graph G(p, c/p) correctly with
running time O(np^5).
"
"  Using the theory of group action, we first introduce the concept of the
automorphism group of an exponential family or a graphical model, thus
formalizing the general notion of symmetry of a probabilistic model. This
automorphism group provides a precise mathematical framework for lifted
inference in the general exponential family. Its group action partitions the
set of random variables and feature functions into equivalent classes (called
orbits) having identical marginals and expectations. Then the inference problem
is effectively reduced to that of computing marginals or expectations for each
class, thus avoiding the need to deal with each individual variable or feature.
We demonstrate the usefulness of this general framework in lifting two classes
of variational approximation for MAP inference: local LP relaxation and local
LP relaxation with cycle constraints; the latter yields the first lifted
inference that operate on a bound tighter than local constraints. Initial
experimental results demonstrate that lifted MAP inference with cycle
constraints achieved the state of the art performance, obtaining much better
objective function values than local approximation while remaining relatively
efficient.
"
"  This paper presents a natural extension of stagewise ranking to the the case
of infinitely many items. We introduce the infinite generalized Mallows model
(IGM), describe its properties and give procedures to estimate it from data.
For estimation of multimodal distributions we introduce the
Exponential-Blurring-Mean-Shift nonparametric clustering algorithm. The
experiments highlight the properties of the new model and demonstrate that
infinite models can be simple, elegant and practical.
"
"  We present the Infinite Latent Events Model, a nonparametric hierarchical
Bayesian distribution over infinite dimensional Dynamic Bayesian Networks with
binary state representations and noisy-OR-like transitions. The distribution
can be used to learn structure in discrete timeseries data by simultaneously
inferring a set of latent events, which events fired at each timestep, and how
those events are causally linked. We illustrate the model on a sound
factorization task, a network topology identification task, and a video game
task.
"
"  A new family of tree models is proposed, which we call ""differential trees.""
A differential tree model is constructed from multiple data sets and aims to
detect distributional differences between them. The new methodology differs
from the existing difference and change detection techniques in its
nonparametric nature, model construction from multiple data sets, and
applicability to high-dimensional data. Through a detailed study of an arson
case in New Zealand, where an individual is known to have been laying
vegetation fires within a certain time period, we illustrate how these models
can help detect changes in the frequencies of event occurrences and uncover
unusual clusters of events in a complex environment.
"
"  We introduce a Gaussian process model of functions which are additive. An
additive function is one which decomposes into a sum of low-dimensional
functions, each depending on only a subset of the input variables. Additive GPs
generalize both Generalized Additive Models, and the standard GP models which
use squared-exponential kernels. Hyperparameter learning in this model can be
seen as Bayesian Hierarchical Kernel Learning (HKL). We introduce an expressive
but tractable parameterization of the kernel function, which allows efficient
evaluation of all input interaction terms, whose number is exponential in the
input dimension. The additional structure discoverable by this model results in
increased interpretability, as well as state-of-the-art predictive power in
regression tasks.
"
"  We study the estimation of $\beta$ for the nonlinear model $y =
f(X\sp{\top}\beta) + \epsilon$ when $f$ is a nonlinear transformation that is
known, $\beta$ has sparse nonzero coordinates, and the number of observations
can be much smaller than that of parameters ($n\ll p$). We show that in order
to bound the $L_2$ error of the $L_0$ regularized estimator $\hat\beta$, i.e.,
$\|\hat\beta - \beta\|_2$, it is sufficient to establish two conditions. Based
on this, we obtain bounds of the $L_2$ error for (1) $L_0$ regularized maximum
likelihood estimation (MLE) for exponential linear models and (2) $L_0$
regularized least square (LS) regression for the more general case where $f$ is
analytic. For the analytic case, we rely on power series expansion of $f$,
which requires taking into account the singularities of $f$.
"
"  Evidences are presented concerning tantalizing regularities in cities'
population-flows in what regards to space and time correlations. The former
exhibit a distance-behavior (for large distances) compatible with the inverse
square law, following an overall Lorentzian dependence with an scale-parameter
of $74\pm6$ km. The later decay exponentially with a characteristic time of
$17.2\pm1.3$ years. These features can be explained by a dynamical model for
cities' population-growth of a Lagevinian nature. Numerical simulations based
on the model confirm its applicability. The model also allows for the
identification of collective normal modes of city-growth dynamics that can be
empirically identified.
"
"  Spatial Independent Component Analysis (ICA) decomposes the time by space
functional MRI (fMRI) matrix into a set of 1-D basis time courses and their
associated 3-D spatial maps that are optimized for mutual independence. When
applied to resting state fMRI (rsfMRI), ICA produces several spatial
independent components (ICs) that seem to have biological relevance - the
so-called resting state networks (RSNs). The ICA problem is well posed when the
true data generating process follows a linear mixture of ICs model in terms of
the identifiability of the mixing matrix. However, the contrast function used
for promoting mutual independence in ICA is dependent on the finite amount of
observed data and is potentially non-convex with multiple local minima. Hence,
each run of ICA could produce potentially different IC estimates even for the
same data. One technique to deal with this run-to-run variability of ICA was
proposed by Yang et al. (2008) in their algorithm RAICAR which allows for the
selection of only those ICs that have a high run-to-run reproducibility. We
propose an enhancement to the original RAICAR algorithm that enables us to
assign reproducibility p-values to each IC and allows for an objective
assessment of both within subject and across subjects reproducibility. We call
the resulting algorithm RAICAR-N (N stands for null hypothesis test), and we
have applied it to publicly available human rsfMRI data (http://www.nitrc.org).
Our reproducibility analyses indicated that many of the published RSNs in
rsfMRI literature are highly reproducible. However, we found several other RSNs
that are highly reproducible but not frequently listed in the literature.
"
"  Super-resolution methods form high-resolution images from low-resolution
images. In this paper, we develop a new Bayesian nonparametric model for
super-resolution. Our method uses a beta-Bernoulli process to learn a set of
recurring visual patterns, called dictionary elements, from the data. Because
it is nonparametric, the number of elements found is also determined from the
data. We test the results on both benchmark and natural images, comparing with
several other models from the research literature. We perform large-scale human
evaluation experiments to assess the visual quality of the results. In a first
implementation, we use Gibbs sampling to approximate the posterior. However,
this algorithm is not feasible for large-scale data. To circumvent this, we
then develop an online variational Bayes (VB) algorithm. This algorithm finds
high quality dictionaries in a fraction of the time needed by the Gibbs
sampler.
"
"  Cyber-physical systems, such as mobile robots, must respond adaptively to
dynamic operating conditions. Effective operation of these systems requires
that sensing and actuation tasks are performed in a timely manner.
Additionally, execution of mission specific tasks such as imaging a room must
be balanced against the need to perform more general tasks such as obstacle
avoidance. This problem has been addressed by maintaining relative utilization
of shared resources among tasks near a user-specified target level. Producing
optimal scheduling strategies requires complete prior knowledge of task
behavior, which is unlikely to be available in practice. Instead, suitable
scheduling strategies must be learned online through interaction with the
system. We consider the sample complexity of reinforcement learning in this
domain, and demonstrate that while the problem state space is countably
infinite, we may leverage the problem's structure to guarantee efficient
learning.
"
"  We examine the recovery of block sparse signals and extend the framework in
two important directions; one by exploiting signals' intra-block correlation
and the other by generalizing signals' block structure. We propose two families
of algorithms based on the framework of block sparse Bayesian learning (BSBL).
One family, directly derived from the BSBL framework, requires knowledge of the
block structure. Another family, derived from an expanded BSBL framework, is
based on a weaker assumption on the block structure, and can be used when the
block structure is completely unknown. Using these algorithms we show that
exploiting intra-block correlation is very helpful in improving recovery
performance. These algorithms also shed light on how to modify existing
algorithms or design new ones to exploit such correlation and improve
performance.
"
"  The systematic biases seen in people's probability judgments are typically
taken as evidence that people do not reason about probability using the rules
of probability theory, but instead use heuristics which sometimes yield
reasonable judgments and sometimes systematic biases. This view has had a major
impact in economics, law, medicine, and other fields; indeed, the idea that
people cannot reason with probabilities has become a widespread truism. We
present a simple alternative to this view, where people reason about
probability according to probability theory but are subject to random variation
or noise in the reasoning process. In this account the effect of noise is
cancelled for some probabilistic expressions: analysing data from two
experiments we find that, for these expressions, people's probability judgments
are strikingly close to those required by probability theory. For other
expressions this account produces systematic deviations in probability
estimates. These deviations explain four reliable biases in human probabilistic
reasoning (conservatism, subadditivity, conjunction and disjunction fallacies).
These results suggest that people's probability judgments embody the rules of
probability theory, and that biases in those judgments are due to the effects
of random noise.
"
"  We examine a fundamental problem that models various active sampling setups,
such as network tomography. We analyze sampling of a multivariate normal
distribution with an unknown expectation that needs to be estimated: in our
setup it is possible to sample the distribution from a given set of linear
functionals, and the difficulty addressed is how to optimally select the
combinations to achieve low estimation error. Although this problem is in the
heart of the field of optimal design, no efficient solutions for the case with
many functionals exist. We present some bounds and an efficient sub-optimal
solution for this problem for more structured sets such as binary functionals
that are induced by graph walks.
"
"  We study linear models under heavy-tailed priors from a probabilistic
viewpoint. Instead of computing a single sparse most probable (MAP) solution as
in standard deterministic approaches, the focus in the Bayesian compressed
sensing framework shifts towards capturing the full posterior distribution on
the latent variables, which allows quantifying the estimation uncertainty and
learning model parameters using maximum likelihood. The exact posterior
distribution under the sparse linear model is intractable and we concentrate on
variational Bayesian techniques to approximate it. Repeatedly computing
Gaussian variances turns out to be a key requisite and constitutes the main
computational bottleneck in applying variational techniques in large-scale
problems. We leverage on the recently proposed Perturb-and-MAP algorithm for
drawing exact samples from Gaussian Markov random fields (GMRF). The main
technical contribution of our paper is to show that estimating Gaussian
variances using a relatively small number of such efficiently drawn random
samples is much more effective than alternative general-purpose variance
estimation techniques. By reducing the problem of variance estimation to
standard optimization primitives, the resulting variational algorithms are
fully scalable and parallelizable, allowing Bayesian computations in extremely
large-scale problems with the same memory and time complexity requirements as
conventional point estimation techniques. We illustrate these ideas with
experiments in image deblurring.
"
"  This paper explores Bayesian inference for a biased sampling model in
situations where the population of interest cannot be sampled directly, but
rather through an indirect and inherently biased method. Observations are
viewed as being the result of a multinomial sampling process from a tagged
population which is, in turn, a biased sample from the original population of
interest. This paper presents several Gibbs Sampling techniques to estimate the
joint posterior distribution of the original population based on the observed
counts of the tagged population. These algorithms efficiently sample from the
joint posterior distribution of a very large multinomial parameter vector.
Samples from this method can be used to generate both joint and marginal
posterior inferences. We also present an iterative optimization procedure based
upon the conditional distributions of the Gibbs Sampler which directly computes
the mode of the posterior distribution. To illustrate our approach, we apply it
to a tagged population of messanger RNAs (mRNA) generated using a common
high-throughput technique, Serial Analysis of Gene Expression (SAGE).
Inferences for the mRNA expression levels in the yeast Saccharomyces cerevisiae
are reported.
"
"  As research into community finding in social networks progresses, there is a
need for algorithms capable of detecting overlapping community structure. Many
algorithms have been proposed in recent years that are capable of assigning
each node to more than a single community. The performance of these algorithms
tends to degrade when the ground-truth contains a more highly overlapping
community structure, with nodes assigned to more than two communities. Such
highly overlapping structure is likely to exist in many social networks, such
as Facebook friendship networks. In this paper we present a scalable algorithm,
MOSES, based on a statistical model of community structure, which is capable of
detecting highly overlapping community structure, especially when there is
variance in the number of communities each node is in. In evaluation on
synthetic data MOSES is found to be superior to existing algorithms, especially
at high levels of overlap. We demonstrate MOSES on real social network data by
analyzing the networks of friendship links between students of five US
universities.
"
"  The influence of speed limits on roadway safety has been a subject of
continuous debate in the State of Indiana and nationwide. In Indiana,
highway-related accidents result in about 900 fatalities and forty thousand
injuries annually and place an incredible social and economic burden on the
state. Still, speed limits posted on highways and other roads are routinely
exceeded as individual drivers try to balance safety, mobility (speed), and the
risks and penalties associated with law enforcement efforts. The
speed-limit/safety issue has been a matter of considerable concern in Indiana
since the state raised its speed limits on rural interstates and selected
multilane highways on July 1, 2005. In this paper, the influence of the posted
speed limit on the severity of vehicle accidents is studied using Indiana
accident data from 2004 (the year before speed limits were raised) and 2006
(the year after speed limits were raised on rural interstates and some
multi-lane non-interstate routes). Statistical models of the injury severity of
different types of accidents on various roadway classes were estimated. The
results of the model estimations showed that, for the speed limit ranges
currently used, speed limits did not have a statistically significant effect on
the severity of accidents on interstate highways. However, for some
non-interstate highways, higher speed limits were found to be associated with
higher accident severities - suggesting that future speed limit changes, on
non-interstate highways in particular, need to be carefully assessed on a
case-by-case basis.
"
"  We propose a method called ideal regression for approximating an arbitrary
system of polynomial equations by a system of a particular type. Using
techniques from approximate computational algebraic geometry, we show how we
can solve ideal regression directly without resorting to numerical
optimization. Ideal regression is useful whenever the solution to a learning
problem can be described by a system of polynomial equations. As an example, we
demonstrate how to formulate Stationary Subspace Analysis (SSA), a source
separation problem, in terms of ideal regression, which also yields a
consistent estimator for SSA. We then compare this estimator in simulations
with previous optimization-based approaches for SSA.
"
"  A number of variable selection methods have been proposed involving nonconvex
penalty functions. These methods, which include the smoothly clipped absolute
deviation (SCAD) penalty and the minimax concave penalty (MCP), have been
demonstrated to have attractive theoretical properties, but model fitting is
not a straightforward task, and the resulting solutions may be unstable. Here,
we demonstrate the potential of coordinate descent algorithms for fitting these
models, establishing theoretical convergence properties and demonstrating that
they are significantly faster than competing approaches. In addition, we
demonstrate the utility of convexity diagnostics to determine regions of the
parameter space in which the objective function is locally convex, even though
the penalty is not. Our simulation study and data examples indicate that
nonconvex penalties like MCP and SCAD are worthwhile alternatives to the lasso
in many applications. In particular, our numerical results suggest that MCP is
the preferred approach among the three methods.
"
"  The paper proposes Monte Carlo algorithms for the computation of the
information rate of two-dimensional source/channel models. The focus of the
paper is on binary-input channels with constraints on the allowed input
configurations. The problem of numerically computing the information rate, and
even the noiseless capacity, of such channels has so far remained largely
unsolved. Both problems can be reduced to computing a Monte Carlo estimate of a
partition function. The proposed algorithms use tree-based Gibbs sampling and
multilayer (multitemperature) importance sampling. The viability of the
proposed algorithms is demonstrated by simulation results.
"
"  Much of uncertainty quantification to date has focused on determining the
effect of variables modeled probabilistically, and with a known distribution,
on some physical or engineering system. We develop methods to obtain
information on the system when the distributions of some variables are known
exactly, others are known only approximately, and perhaps others are not
modeled as random variables at all. The main tool used is the duality between
risk-sensitive integrals and relative entropy, and we obtain explicit bounds on
standard performance measures (variances, exceedance probabilities) over
families of distributions whose distance from a nominal distribution is
measured by relative entropy. The evaluation of the risk-sensitive expectations
is based on polynomial chaos expansions, which help keep the computational
aspects tractable.
"
"  Hierarchical parametric models consisting of observable and latent variables
are widely used for unsupervised learning tasks. For example, a mixture model
is a representative hierarchical model for clustering. From the statistical
point of view, the models can be regular or singular due to the distribution of
data. In the regular case, the models have the identifiability; there is
one-to-one relation between a probability density function for the model
expression and the parameter. The Fisher information matrix is positive
definite, and the estimation accuracy of both observable and latent variables
has been studied. In the singular case, on the other hand, the models are not
identifiable and the Fisher matrix is not positive definite. Conventional
statistical analysis based on the inverse Fisher matrix is not applicable.
Recently, an algebraic geometrical analysis has been developed and is used to
elucidate the Bayes estimation of observable variables. The present paper
applies this analysis to latent-variable estimation and determines its
theoretical performance. Our results clarify behavior of the convergence of the
posterior distribution. It is found that the posterior of the
observable-variable estimation can be different from the one in the
latent-variable estimation. Because of the difference, the Markov chain Monte
Carlo method based on the parameter and the latent variable cannot construct
the desired posterior distribution.
"
"  Observed associations in a database may be due in whole or part to variations
in unrecorded (latent) variables. Identifying such variables and their causal
relationships with one another is a principal goal in many scientific and
practical domains. Previous work shows that, given a partition of observed
variables such that members of a class share only a single latent common cause,
standard search algorithms for causal Bayes nets can infer structural relations
between latent variables. We introduce an algorithm for discovering such
partitions when they exist. Uniquely among available procedures, the algorithm
is (asymptotically) correct under standard assumptions in causal Bayes net
search algorithms, requires no prior knowledge of the number of latent
variables, and does not depend on the mathematical form of the relationships
among the latent variables. We evaluate the algorithm on a variety of simulated
data sets.
"
"  Nuclear Magnetic Resonance (NMR) spectra are widely used in metabolomics to
obtain profiles of metabolites dissolved in biofluids such as cell
supernatants. Methods for estimating metabolite concentrations from these
spectra are presently confined to manual peak fitting and to binning procedures
for integrating resonance peaks. Extensive information on the patterns of
spectral resonance generated by human metabolites is now available in online
databases. By incorporating this information into a Bayesian model we can
deconvolve resonance peaks from a spectrum and obtain explicit concentration
estimates for the corresponding metabolites. Spectral resonances that cannot be
deconvolved in this way may also be of scientific interest so we model them
jointly using wavelets.
  We describe a Markov chain Monte Carlo algorithm which allows us to sample
from the joint posterior distribution of the model parameters, using
specifically designed block updates to improve mixing. The strong prior on
resonance patterns allows the algorithm to identify peaks corresponding to
particular metabolites automatically, eliminating the need for manual peak
assignment.
  We assess our method for peak alignment and concentration estimation. Except
in cases when the target resonance signal is very weak, alignment is unbiased
and precise. We compare the Bayesian concentration estimates to those obtained
from a conventional numerical integration method and find that our point
estimates have sixfold lower mean squared error.
  Finally, we apply our method to a spectral dataset taken from an
investigation of the metabolic response of yeast to recombinant protein
expression. We estimate the concentrations of 26 metabolites and compare to
manual quantification by five expert spectroscopists. We discuss the reason for
discrepancies and the robustness of our methods concentration estimates.
"
"  Spatial Independent Component Analysis (ICA) is an increasingly used
data-driven method to analyze functional Magnetic Resonance Imaging (fMRI)
data. To date, it has been used to extract meaningful patterns without prior
information. However, ICA is not robust to mild data variation and remains a
parameter-sensitive algorithm. The validity of the extracted patterns is hard
to establish, as well as the significance of differences between patterns
extracted from different groups of subjects. We start from a generative model
of the fMRI group data to introduce a probabilistic ICA pattern-extraction
algorithm, called CanICA (Canonical ICA). Thanks to an explicit noise model and
canonical correlation analysis, our method is auto-calibrated and identifies
the group-reproducible data subspace before performing ICA. We compare our
method to state-of-the-art multi-subject fMRI ICA methods and show that the
features extracted are more reproducible.
"
"  When analyzing microarray data, hierarchical models are often used to share
information across genes when estimating means and variances or identifying
differential expression. Many methods utilize some form of the two-level
hierarchical model structure suggested by Kendziorski et al. [Stat. Med. (2003)
22 3899-3914] in which the first level describes the distribution of latent
mean expression levels among genes and among differentially expressed
treatments within a gene. The second level describes the conditional
distribution, given a latent mean, of repeated observations for a single gene
and treatment. Many of these models, including those used in Kendziorski's et
al. [Stat. Med. (2003) 22 3899-3914] EBarrays package, assume that expression
level changes due to treatment effects have the same distribution as expression
level changes from gene to gene. We present empirical evidence that this
assumption is often inadequate and propose three-level hierarchical models as
extensions to the two-level log-normal based EBarrays models to address this
inadequacy. We demonstrate that use of our three-level models dramatically
changes analysis results for a variety of microarray data sets and verify the
validity and improved performance of our suggested method in a series of
simulation studies. We also illustrate the importance of accounting for the
uncertainty of gene-specific error variance estimates when using hierarchical
models to identify differentially expressed genes.
"
"  We introduce a copula mixture model to perform dependency-seeking clustering
when co-occurring samples from different data sources are available. The model
takes advantage of the great flexibility offered by the copulas framework to
extend mixtures of Canonical Correlation Analysis to multivariate data with
arbitrary continuous marginal densities. We formulate our model as a
non-parametric Bayesian mixture, while providing efficient MCMC inference.
Experiments on synthetic and real data demonstrate that the increased
flexibility of the copula mixture significantly improves the clustering and the
interpretability of the results.
"
"  In this analytical study we derive the optimal unbiased value estimator (MVU)
and compare its statistical risk to three well known value estimators: Temporal
Difference learning (TD), Monte Carlo estimation (MC) and Least-Squares
Temporal Difference Learning (LSTD). We demonstrate that LSTD is equivalent to
the MVU if the Markov Reward Process (MRP) is acyclic and show that both differ
for most cyclic MRPs as LSTD is then typically biased. More generally, we show
that estimators that fulfill the Bellman equation can only be unbiased for
special cyclic MRPs. The main reason being the probability measures with which
the expectations are taken. These measure vary from state to state and due to
the strong coupling by the Bellman equation it is typically not possible for a
set of value estimators to be unbiased with respect to each of these measures.
Furthermore, we derive relations of the MVU to MC and TD. The most important
one being the equivalence of MC to the MVU and to LSTD for undiscounted MRPs in
which MC has the same amount of information. In the discounted case this
equivalence does not hold anymore. For TD we show that it is essentially
unbiased for acyclic MRPs and biased for cyclic MRPs. We also order estimators
according to their risk and present counter-examples to show that no general
ordering exists between the MVU and LSTD, between MC and LSTD and between TD
and MC. Theoretical results are supported by examples and an empirical
evaluation.
"
"  A number of modern learning tasks involve estimation from heterogeneous
information sources. This includes classification with labeled and unlabeled
data as well as other problems with analogous structure such as competitive
(game theoretic) problems. The associated estimation problems can be typically
reduced to solving a set of fixed point equations (consistency conditions). We
introduce a general method for combining a preferred information source with
another in this setting by evolving continuous paths of fixed points at
intermediate allocations. We explicitly identify critical points along the
unique paths to either increase the stability of estimation or to ensure a
significant departure from the initial source. The homotopy continuation
approach is guaranteed to terminate at the second source, and involves no
combinatorial effort. We illustrate the power of these ideas both in
classification tasks with labeled and unlabeled data, as well as in the context
of a competitive (min-max) formulation of DNA sequence motif discovery.
"
"  We derive generalization error bounds for traditional time-series forecasting
models. Our results hold for many standard forecasting tools including
autoregressive models, moving average models, and, more generally, linear
state-space models. These non-asymptotic bounds need only weak assumptions on
the data-generating process, yet allow forecasters to select among competing
models and to guarantee, with high probability, that their chosen model will
perform well. We motivate our techniques with and apply them to standard
economic and financial forecasting tools---a GARCH model for predicting equity
volatility and a dynamic stochastic general equilibrium model (DSGE), the
standard tool in macroeconomic forecasting. We demonstrate in particular how
our techniques can aid forecasters and policy makers in choosing models which
behave well under uncertainty and mis-specification.
"
"  The problem of finding groups in data (cluster analysis) has been extensively
studied by researchers from the fields of Statistics and Computer Science,
among others. However, despite its popularity it is widely recognized that the
investigation of some theoretical aspects of clustering has been relatively
sparse. One of the main reasons for this lack of theoretical results is surely
the fact that, unlike the situation with other statistical problems as
regression or classification, for some of the cluster methodologies it is quite
difficult to specify a population goal to which the data-based clustering
algorithms should try to get close. This paper aims to provide some insight
into the theoretical foundations of the usual nonparametric approach to
clustering, which understands clusters as regions of high density, by
presenting an explicit formulation for the ideal population clustering.
"
"  Bayesian inference methods are applied within a Bayesian hierarchical
modelling framework to the problems of joint state and parameter estimation,
and of state forecasting. We explore and demonstrate the ideas in the context
of a simple nonlinear marine biogeochemical model. A novel approach is proposed
to the formulation of the stochastic process model, in which ecophysiological
properties of plankton communities are represented by autoregressive stochastic
processes. This approach captures the effects of changes in plankton
communities over time, and it allows the incorporation of literature metadata
on individual species into prior distributions for process model parameters.
The approach is applied to a case study at Ocean Station Papa, using Particle
Markov chain Monte Carlo computational techniques. The results suggest that, by
drawing on objective prior information, it is possible to extract useful
information about model state and a subset of parameters, and even to make
useful long-term forecasts, based on sparse and noisy observations.
"
"  In observational studies the assignment of units to treatments is not under
control. Consequently, the estimation and comparison of treatment effects based
on the empirical distribution of the responses can be biased since the units
exposed to the various treatments could differ in important unknown
pretreatment characteristics, which are related to the response. An important
example studied in this article is the question of whether private schools
offer better quality of education than public schools. In order to address this
question, we use data collected in the year 2000 by OECD for the Programme for
International Student Assessment (PISA). Focusing for illustration on scores in
mathematics of 15-year-old pupils in Ireland, we find that the raw average
score of pupils in private schools is higher than of pupils in public schools.
However, application of a newly proposed method for observational studies
suggests that the less able pupils tend to enroll in public schools, such that
their lower scores are not necessarily an indication of bad quality of the
public schools. Indeed, when comparing the average score in the two types of
schools after adjusting for the enrollment effects, we find quite surprisingly
that public schools perform better on average. This outcome is supported by the
methods of instrumental variables and latent variables, commonly used by
econometricians for analyzing and evaluating social programs.
"
"  Overlapping clustering problem is an important learning issue in which
clusters are not mutually exclusive and each object may belongs simultaneously
to several clusters. This paper presents a kernel based method that produces
overlapping clusters on a high feature space using mercer kernel techniques to
improve separability of input patterns. The proposed method, called
OKM-K(Overlapping $k$-means based kernel method), extends OKM (Overlapping
$k$-means) method to produce overlapping schemes. Experiments are performed on
overlapping dataset and empirical results obtained with OKM-K outperform
results obtained with OKM.
"
"  We characterize and study variable importance (VIMP) and pairwise variable
associations in binary regression trees. A key component involves the node mean
squared error for a quantity we refer to as a maximal subtree. The theory
naturally extends from single trees to ensembles of trees and applies to
methods like random forests. This is useful because while importance values
from random forests are used to screen variables, for example they are used to
filter high throughput genomic data in Bioinformatics, very little theory
exists about their properties.
"
"  We propose a Bayesian methodology for one-mode projecting a bipartite network
that is being observed across a series of discrete time steps. The resulting
one mode network captures the uncertainty over the presence/absence of each
link and provides a probability distribution over its possible weight values.
Additionally, the incorporation of prior knowledge over previous states makes
the resulting network less sensitive to noise and missing observations that
usually take place during the data collection process. The methodology consists
of computationally inexpensive update rules and is scalable to large problems,
via an appropriate distributed implementation.
"
"  I study the sunspot area fluctuations over the epoch of 12 solar cycles
(12-23). Lately, I found three significant quasi-periodicities at 10, 17 and 23
solar rotations, but two longer periods could be treated as subharmonics of the
10-rotation period. Thus, I search this period during the low- and the
high-activity periods of each solar cycles. Because of the N-S asymmetry I
consider each solar hemisphere separately. The skewness of each fluctuation
probability distribution suggests that the positive and the negative
fluctuations could be are examined separately. To avoid the problem when a few
strong fluctuations could create an auto-correlation or a wavelet peak, I also
analyse the transformations of fluctuations for which the amplitudes at the
high- and the low-activity periods are almost the same. The auto-correlation
and the wavelet analyses show that the 10-rotation period is mainly detected
during the high-activity periods, but it also exists during a few low-activity
periods.
"
"  Chaos and oscillations continue to capture the interest of both the
scientific and public domains. Yet despite the importance of these qualitative
features, most attempts at constructing mathematical models of such phenomena
have taken an indirect, quantitative approach, e.g. by fitting models to a
finite number of data-points. Here we develop a qualitative inference framework
that allows us to both reverse engineer and design systems exhibiting these and
other dynamical behaviours by directly specifying the desired characteristics
of the underlying dynamical attractor. This change in perspective from
quantitative to qualitative dynamics, provides fundamental and new insights
into the properties of dynamical systems.
"
"  We derive the sampling probability density function (pdf) of an ideal
localized random electromagnetic field, its amplitude and intensity in an
electromagnetic environment that is quasi-statically time-varying statistically
homogeneous or static statistically inhomogeneous. The results allow for the
estimation of field statistics and confidence intervals when a single spatial
or temporal stochastic process produces randomization of the field. Results for
both coherent and incoherent detection techniques are derived, for Cartesian,
planar and full-vectorial fields. We show that the functional form of the
sampling pdf depends on whether the random variable is dimensioned (e.g., the
sampled electric field proper) or is expressed in dimensionless standardized or
normalized form (e.g., the sampled electric field divided by its sampled
standard deviation). For dimensioned quantities, the electric field, its
amplitude and intensity exhibit different types of
  Bessel $K$ sampling pdfs, which differ significantly from the asymptotic
Gauss normal and $\chi^{(2)}_{2p}$ ensemble pdfs when $\nu$ is relatively
small. By contrast, for the corresponding standardized quantities, Student $t$,
Fisher-Snedecor $F$ and root-$F$ sampling pdfs are obtained that exhibit
heavier tails than comparable Bessel $K$ pdfs. Statistical uncertainties
obtained from classical small-sample theory for dimensionless quantities are
shown to be overestimated compared to dimensioned quantities. Differences in
the sampling pdfs arising from de-normalization versus de-standardization are
obtained.
"
"  Studying the topology of so-called real networks, that is networks obtained
from sociological or biological data for instance, has become a major field of
interest in the last decade. One way to deal with it is to consider that
networks are built from small functional units called motifs, which can be
found by looking for small subgraphs whose numbers of occurrences in the whole
network are surprisingly high. In this article, we propose to define motifs
through a local overrepresentation in the network and develop a statistic to
detect them without relying on simulations. We then illustrate the performance
of our procedure on simulated and real data, recovering already known
biologically relevant motifs. Moreover, we explain how our method gives some
information about the respective roles of the vertices in a motif.
"
"  The covariance graph (aka bi-directed graph) of a probability distribution
$p$ is the undirected graph $G$ where two nodes are adjacent iff their
corresponding random variables are marginally dependent in $p$. In this paper,
we present a graphical criterion for reading dependencies from $G$, under the
assumption that $p$ satisfies the graphoid properties as well as weak
transitivity and composition. We prove that the graphical criterion is sound
and complete in certain sense. We argue that our assumptions are not too
restrictive. For instance, all the regular Gaussian probability distributions
satisfy them.
"
"  In this work we are interested in the problems of supervised learning and
variable selection when the input-output dependence is described by a nonlinear
function depending on a few variables. Our goal is to consider a sparse
nonparametric model, hence avoiding linear or additive models. The key idea is
to measure the importance of each variable in the model by making use of
partial derivatives. Based on this intuition we propose a new notion of
nonparametric sparsity and a corresponding least squares regularization scheme.
Using concepts and results from the theory of reproducing kernel Hilbert spaces
and proximal methods, we show that the proposed learning algorithm corresponds
to a minimization problem which can be provably solved by an iterative
procedure. The consistency properties of the obtained estimator are studied
both in terms of prediction and selection performance. An extensive empirical
analysis shows that the proposed method performs favorably with respect to the
state-of-the-art methods.
"
"  We present two sets of theoretical results on the grouped lasso with overlap
of Jacob, Obozinski and Vert (2009) in the linear regression setting. This
method allows for joint selection of predictors in sparse regression, allowing
for complex structured sparsity over the predictors encoded as a set of groups.
This flexible framework suggests that arbitrarily complex structures can be
encoded with an intricate set of groups. Our results show that this strategy
results in unexpected theoretical consequences for the procedure. In
particular, we give two sets of results: (1) finite sample bounds on prediction
and estimation, and (2) asymptotic distribution and selection. Both sets of
results give insight into the consequences of choosing an increasingly complex
set of groups for the procedure, as well as what happens when the set of groups
cannot recover the true sparsity pattern. Additionally, these results
demonstrate the differences and similarities between the the grouped lasso
procedure with and without overlapping groups. Our analysis shows the set of
groups must be chosen with caution - an overly complex set of groups will
damage the analysis.
"
"  In this paper we study iterative procedures for stationary equilibria in
games with large number of players. Most of learning algorithms for games with
continuous action spaces are limited to strict contraction best reply maps in
which the Banach-Picard iteration converges with geometrical convergence rate.
When the best reply map is not a contraction, Ishikawa-based learning is
proposed. The algorithm is shown to behave well for Lipschitz continuous and
pseudo-contractive maps. However, the convergence rate is still unsatisfactory.
Several acceleration techniques are presented. We explain how cognitive users
can improve the convergence rate based only on few number of measurements. The
methodology provides nice properties in mean field games where the payoff
function depends only on own-action and the mean of the mean-field (first
moment mean-field games). A learning framework that exploits the structure of
such games, called, mean-field learning, is proposed. The proposed mean-field
learning framework is suitable not only for games but also for non-convex
global optimization problems. Then, we introduce mean-field learning without
feedback and examine the convergence to equilibria in beauty contest games,
which have interesting applications in financial markets. Finally, we provide a
fully distributed mean-field learning and its speedup versions for satisfactory
solution in wireless networks. We illustrate the convergence rate improvement
with numerical examples.
"
"  This article reviews the Author-Topic Model and presents a new non-parametric
extension based on the Hierarchical Dirichlet Process. The extension is
especially suitable when no prior information about the number of components
necessary is available. A blocked Gibbs sampler is described and focus put on
staying as close as possible to the original model with only the minimum of
theoretical and implementation overhead necessary.
"
"  There are very significant changes taking place in the university sector and
in related higher education institutes in many parts of the world. In this work
we look at financial data from 2010 and 2011 from the UK higher education
sector. Situating ourselves to begin with in the context of teaching versus
research in universities, we look at the data in order to explore the new
divergence between the broad agendas of teaching and research in universities.
The innovation agenda has become at least equal to the research and teaching
objectives of universities. From the financial data, published in the Times
Higher Education weekly newspaper, we explore the interesting contrast, and
very opposite orientations, in specialization of universities in the UK. We
find a polarity in specialism that goes considerably beyond the usual one of
research-led elite versus more teaching-oriented new universities. Instead we
point to the role of medical/bioscience research income in the former, and
economic and business sectoral niche player roles in the latter.
"
"  Online optimization has emerged as powerful tool in large scale optimization.
In this paper, we introduce efficient online algorithms based on the
alternating directions method (ADM). We introduce a new proof technique for ADM
in the batch setting, which yields the O(1/T) convergence rate of ADM and forms
the basis of regret analysis in the online setting. We consider two scenarios
in the online setting, based on whether the solution needs to lie in the
feasible set or not. In both settings, we establish regret bounds for both the
objective function as well as constraint violation for general and strongly
convex functions. Preliminary results are presented to illustrate the
performance of the proposed algorithms.
"
"  In nonparametric classification and regression problems, regularized kernel
methods, in particular support vector machines, attract much attention in
theoretical and in applied statistics. In an abstract sense, regularized kernel
methods (simply called SVMs here) can be seen as regularized M-estimators for a
parameter in a (typically infinite dimensional) reproducing kernel Hilbert
space. For smooth loss functions, it is shown that the difference between the
estimator, i.e.\ the empirical SVM, and the theoretical SVM is asymptotically
normal with rate $\sqrt{n}$. That is, the standardized difference converges
weakly to a Gaussian process in the reproducing kernel Hilbert space. As common
in real applications, the choice of the regularization parameter may depend on
the data. The proof is done by an application of the functional delta-method
and by showing that the SVM-functional is suitably Hadamard-differentiable.
"
"  Functional Magnetic Resonance Imaging (fMRI) provides dynamical access into
the complex functioning of the human brain, detailing the hemodynamic activity
of thousands of voxels during hundreds of sequential time points. One approach
towards illuminating the connection between fMRI and cognitive function is
through decoding; how do the time series of voxel activities combine to provide
information about internal and external experience? Here we seek models of fMRI
decoding which are balanced between the simplicity of their interpretation and
the effectiveness of their prediction. We use signals from a subject immersed
in virtual reality to compare global and local methods of prediction applying
both linear and nonlinear techniques of dimensionality reduction. We find that
the prediction of complex stimuli is remarkably low-dimensional, saturating
with less than 100 features. In particular, we build effective models based on
the decorrelated components of cognitive activity in the classically-defined
Brodmann areas. For some of the stimuli, the top predictive areas were
surprisingly transparent, including Wernicke's area for verbal instructions,
visual cortex for facial and body features, and visual-temporal regions for
velocity. Direct sensory experience resulted in the most robust predictions,
with the highest correlation ($c \sim 0.8$) between the predicted and
experienced time series of verbal instructions. Techniques based on non-linear
dimensionality reduction (Laplacian eigenmaps) performed similarly. The
interpretability and relative simplicity of our approach provides a conceptual
basis upon which to build more sophisticated techniques for fMRI decoding and
offers a window into cognitive function during dynamic, natural experience.
"
"  By taking into account the nonlinear effect of the cause, the inner noise
effect, and the measurement distortion effect in the observed variables, the
post-nonlinear (PNL) causal model has demonstrated its excellent performance in
distinguishing the cause from effect. However, its identifiability has not been
properly addressed, and how to apply it in the case of more than two variables
is also a problem. In this paper, we conduct a systematic investigation on its
identifiability in the two-variable case. We show that this model is
identifiable in most cases; by enumerating all possible situations in which the
model is not identifiable, we provide sufficient conditions for its
identifiability. Simulations are given to support the theoretical results.
Moreover, in the case of more than two variables, we show that the whole causal
structure can be found by applying the PNL causal model to each structure in
the Markov equivalent class and testing if the disturbance is independent of
the direct causes for each variable. In this way the exhaustive search over all
possible causal structures is avoided.
"
"  As machine learning algorithms enter applications in industrial settings,
there is increased interest in controlling their cpu-time during testing. The
cpu-time consists of the running time of the algorithm and the extraction time
of the features. The latter can vary drastically when the feature set is
diverse. In this paper, we propose an algorithm, the Greedy Miser, that
incorporates the feature extraction cost during training to explicitly minimize
the cpu-time during testing. The algorithm is a straightforward extension of
stage-wise regression and is equally suitable for regression or multi-class
classification. Compared to prior work, it is significantly more cost-effective
and scales to larger data sets.
"
"  In this work, Pseudo-Random Bit Generation (PRBG) based on 2D chaotic
mappings of logistic type is considered. The sequences generated with two
Pseudorandom Bit Generators (PRBGs) of this type are statistically tested and
the computational effectiveness of the generators is estimated. The role played
by the symmetry and the geometrical properties of the underlying chaotic
attractors is also explored. Considering these PRBGs valid for cryptography,
the size of the available key spaces are calculated. Additionally, a novel
mechanism called 'symmetry-swap' is introduced in order to enhance the PRBG
algorithm. It is shown that it can increase the degrees of freedom of the key
space, while maintaining the speed and performance in the PRBG.
"
"  Discussion of ""Network routing in a dynamic environment"" by N.D. Singpurwalla
[arXiv:1107.4852]
"
"  We consider the hypothesis testing problem of detecting a shift between the
means of two multivariate normal distributions in the high-dimensional setting,
allowing for the data dimension p to exceed the sample size n. Specifically, we
propose a new test statistic for the two-sample test of means that integrates a
random projection with the classical Hotelling T^2 statistic. Working under a
high-dimensional framework with (p,n) tending to infinity, we first derive an
asymptotic power function for our test, and then provide sufficient conditions
for it to achieve greater power than other state-of-the-art tests. Using ROC
curves generated from synthetic data, we demonstrate superior performance
against competing tests in the parameter regimes anticipated by our theoretical
results. Lastly, we illustrate an advantage of our procedure's false positive
rate with comparisons on high-dimensional gene expression data involving the
discrimination of different types of cancer.
"
"  High-dimensional classification has become an increasingly important problem.
In this paper we propose a ""Multivariate Adaptive Stochastic Search"" (MASS)
approach which first reduces the dimension of the data space and then applies a
standard classification method to the reduced space. One key advantage of MASS
is that it automatically adjusts to mimic variable selection type methods, such
as the Lasso, variable combination methods, such as PCA, or methods that
combine these two approaches. The adaptivity of MASS allows it to perform well
in situations where pure variable selection or variable combination methods
fail. Another major advantage of our approach is that MASS can accurately
project the data into very low-dimensional non-linear, as well as linear,
spaces. MASS uses a stochastic search algorithm to select a handful of optimal
projection directions from a large number of random directions in each
iteration. We provide some theoretical justification for MASS and demonstrate
its strengths on an extensive range of simulation studies and real world data
sets by comparing it to many classical and modern classification methods.
"
"  Choice models, which capture popular preferences over objects of interest,
play a key role in making decisions whose eventual outcome is impacted by human
choice behavior. In most scenarios, the choice model, which can effectively be
viewed as a distribution over permutations, must be learned from observed data.
The observed data, in turn, may frequently be viewed as (partial, noisy)
information about marginals of this distribution over permutations. As such,
the search for an appropriate choice model boils down to learning a
distribution over permutations that is (near-)consistent with observed
information about this distribution.
  In this work, we pursue a non-parametric approach which seeks to learn a
choice model (i.e. a distribution over permutations) with {\em sparsest}
possible support, and consistent with observed data. We assume that the data
observed consists of noisy information pertaining to the marginals of the
choice model we seek to learn. We establish that {\em any} choice model admits
a `very' sparse approximation in the sense that there exists a choice model
whose support is small relative to the dimension of the observed data and whose
marginals approximately agree with the observed marginal information. We
further show that under, what we dub, `signature' conditions, such a sparse
approximation can be found in a computationally efficiently fashion relative to
a brute force approach. An empirical study using the American Psychological
Association election data-set suggests that our approach manages to unearth
useful structural properties of the underlying choice model using the sparse
approximation found. Our results further suggest that the signature condition
is a potential alternative to the recently popularized Restricted Null Space
condition for efficient recovery of sparse models.
"
"  Variational Bayesian inference and (collapsed) Gibbs sampling are the two
important classes of inference algorithms for Bayesian networks. Both have
their advantages and disadvantages: collapsed Gibbs sampling is unbiased but is
also inefficient for large count values and requires averaging over many
samples to reduce variance. On the other hand, variational Bayesian inference
is efficient and accurate for large count values but suffers from bias for
small counts. We propose a hybrid algorithm that combines the best of both
worlds: it samples very small counts and applies variational updates to large
counts. This hybridization is shown to significantly improve testset perplexity
relative to variational inference at no computational cost.
"
"  Background: Predictive, stable and interpretable gene signatures are
generally seen as an important step towards a better personalized medicine.
During the last decade various methods have been proposed for that purpose.
However, one important obstacle for making gene signatures a standard tool in
clinics is the typical low reproducibility of these signatures combined with
the difficulty to achieve a clear biological interpretation. For that purpose
in the last years there has been a growing interest in approaches that try to
integrate information from molecular interaction networks. Results: We propose
a novel algorithm, called FrSVM, which integrates protein-protein interaction
network information into gene selection for prognostic biomarker discovery. Our
method is a simple filter based approach, which focuses on central genes with
large differences in their expression. Compared to several other competing
methods our algorithm reveals a significantly better prediction performance and
higher signature stability. More- over, obtained gene lists are highly enriched
with known disease genes and drug targets. We extendd our approach further by
integrating information on candidate disease genes and targets of disease
associated Transcript Factors (TFs).
"
"  Looking for associations among multiple variables is a topical issue in
statistics due to the increasing amount of data encountered in biology,
medicine and many other domains involving statistical applications. Graphical
models have recently gained popularity for this purpose in the statistical
literature. Following the ideas of the LASSO procedure designed for the linear
regression framework, recent developments dealing with graphical model
selection have been based on $\ell_1$-penalization. In the binary case,
however, exact inference is generally very slow or even intractable because of
the form of the so-called log-partition function. Various approximate methods
have recently been proposed in the literature and the main objective of this
paper is to compare them. Through an extensive simulation study, we show that a
simple modification of a method relying on a Gaussian approximation achieves
good performance and is very fast. We present a real application in which we
search for associations among causes of death recorded on French death
certificates.
"
"  Food and beverage authentication is the process by which foods or beverages
are verified as complying with its label description, for example, verifying if
the denomination of origin of an olive oil bottle is correct or if the variety
of a certain bottle of wine matches its label description. The common way to
deal with an authentication process is to measure a number of attributes on
samples of food and then use these as input for a classification problem. Our
motivation stems from data consisting of measurements of nine chemical
compounds denominated Anthocyanins, obtained from samples of Chilean red wines
of grape varieties Cabernet Sauvignon, Merlot and Carm\'{e}n\`{e}re. We
consider a model-based approach to authentication through a semiparametric
multivariate hierarchical linear mixed model for the mean responses, and
covariance matrices that are specific to the classification categories.
Specifically, we propose a model of the ANOVA-DDP type, which takes advantage
of the fact that the available covariates are discrete in nature. The results
suggest that the model performs well compared to other parametric alternatives.
This is also corroborated by application to simulated data.
"
"  The radio astronomy community is currently building a number of phased array
telescopes. The calibration of these telescopes is hampered by the fact that
covariances of signals from closely spaced antennas are sensitive to noise
coupling and to variations in sky brightness on large spatial scales. These
effects are difficult and computationally expensive to model. We propose to
model them phenomenologically using a non-diagonal noise covariance matrix. The
parameters can be estimated using a weighted alternating least squares (WALS)
algorithm iterating between the calibration parameters and the additive
nuisance parameters. We demonstrate the effectiveness of our method using data
from the low frequency array (LOFAR) prototype station.
"
"  We study Bayesian discriminative inference given a model family $p(c,\x,
\theta)$ that is assumed to contain all our prior information but still known
to be incorrect. This falls in between ""standard"" Bayesian generative modeling
and Bayesian regression, where the margin $p(\x,\theta)$ is known to be
uninformative about $p(c|\x,\theta)$. We give an axiomatic proof that
discriminative posterior is consistent for conditional inference; using the
discriminative posterior is standard practice in classical Bayesian regression,
but we show that it is theoretically justified for model families of joint
densities as well. A practical benefit compared to Bayesian regression is that
the standard methods of handling missing values in generative modeling can be
extended into discriminative inference, which is useful if the amount of data
is small. Compared to standard generative modeling, discriminative posterior
results in better conditional inference if the model family is incorrect. If
the model family contains also the true model, the discriminative posterior
gives the same result as standard Bayesian generative modeling. Practical
computation is done with Markov chain Monte Carlo.
"
"  This paper studies iteration convergence of Kronecker graphical lasso
(KGLasso) algorithms for estimating the covariance of an i.i.d. Gaussian random
sample under a sparse Kronecker-product covariance model and MSE convergence
rates. The KGlasso model, originally called the transposable regularized
covariance model by Allen [""Transposable regularized covariance models with an
application to missing data imputation,"" Ann. Appl. Statist., vol. 4, no. 2,
pp. 764-790, 2010], implements a pair of $\ell_1$ penalties on each Kronecker
factor to enforce sparsity in the covariance estimator. The KGlasso algorithm
generalizes Glasso, introduced by Yuan and Lin [""Model selection and estimation
in the Gaussian graphical model,"" Biometrika, vol. 94, pp. 19-35, 2007] and
Banerjee [""Model selection through sparse maximum likelihood estimation for
multivariate Gaussian or binary data,"" J. Mach. Learn. Res., vol. 9, pp.
485-516, Mar. 2008], to estimate covariances having Kronecker product form. It
also generalizes the unpenalized ML flip-flop (FF) algorithm of Dutilleul [""The
MLE algorithm for the matrix normal distribution,"" J. Statist. Comput. Simul.,
vol. 64, pp. 105-123, 1999] and Werner [""On estimation of covariance matrices
with Kronecker product structure,"" IEEE Trans. Signal Process., vol. 56, no. 2,
pp. 478-491, Feb. 2008] to estimation of sparse Kronecker factors. We establish
that the KGlasso iterates converge pointwise to a local maximum of the
penalized likelihood function. We derive high dimensional rates of convergence
to the true covariance as both the number of samples and the number of
variables go to infinity. Our results establish that KGlasso has significantly
faster asymptotic convergence than Glasso and FF. Simulations are presented
that validate the results of our analysis.
"
"  Fitting models for non-Poisson point processes is complicated by the lack of
tractable models for much of the data. By using large samples of independent
and identically distributed realizations and statistical learning, it is
possible to identify absence of fit through finding a classification rule that
can efficiently identify single realizations of each type. The method requires
a much wider range of descriptive statistics than are currently in use, and a
new concept of model fitting which is derive from how physical laws are judged
to fit data.
"
"  Demand functions for goods are generally cyclical in nature with
characteristics such as trend or stochasticity. Most existing demand
forecasting techniques in literature are designed to manage and forecast this
type of demand functions. However, if the demand function is lumpy in nature,
then the general demand forecasting techniques may fail given the unusual
characteristics of the function. Proper identification of the underlying demand
function and using the most appropriate forecasting technique becomes critical.
In this paper, we will attempt to explore the key characteristics of the
different types of demand function and relate them to known statistical
distributions. By fitting statistical distributions to actual past demand data,
we are then able to identify the correct demand functions, so that the the most
appropriate forecasting technique can be applied to obtain improved forecasting
results. We applied the methodology to a real case study to show the reduction
in forecasting errors obtained.
"
"  A main goal of regression is to derive statistical conclusions on the
conditional distribution of the output variable Y given the input values x. Two
of the most important characteristics of a single distribution are location and
scale. Support vector machines (SVMs) are well established to estimate location
functions like the conditional median or the conditional mean. We investigate
the estimation of scale functions by SVMs when the conditional median is
unknown, too. Estimation of scale functions is important e.g. to estimate the
volatility in finance. We consider the median absolute deviation (MAD) and the
interquantile range (IQR) as measures of scale. Our main result shows the
consistency of MAD-type SVMs.
"
"  In large scale multiple testing, the use of an empirical null distribution
rather than the theoretical null distribution can be critical for correct
inference. This paper proposes a ``mode matching'' method for fitting an
empirical null when the theoretical null belongs to any exponential family.
Based on the central matching method for $z$-scores, mode matching estimates
the null density by fitting an appropriate exponential family to the histogram
of the test statistics by Poisson regression in a region surrounding the mode.
The empirical null estimate is then used to estimate local and tail false
discovery rate (FDR) for inference. Delta-method covariance formulas and
approximate asymptotic bias formulas are provided, as well as simulation
studies of the effect of the tuning parameters of the procedure on the
bias-variance trade-off. The standard FDR estimates are found to be biased down
at the far tails. Correlation between test statistics is taken into account in
the covariance estimates, providing a generalization of Efron's ``wing
function'' for exponential families. Applications with $\chi^2$ statistics are
shown in a family-based genome-wide association study from the Framingham Heart
Study and an anatomical brain imaging study of dyslexia in children.
"
"  We derive a novel norm that corresponds to the tightest convex relaxation of
sparsity combined with an $\ell_2$ penalty. We show that this new {\em
$k$-support norm} provides a tighter relaxation than the elastic net and is
thus a good replacement for the Lasso or the elastic net in sparse prediction
problems. Through the study of the $k$-support norm, we also bound the
looseness of the elastic net, thus shedding new light on it and providing
justification for its use.
"
"  Convex regression is a promising area for bridging statistical estimation and
deterministic convex optimization. New piecewise linear convex regression
methods are fast and scalable, but can have instability when used to
approximate constraints or objective functions for optimization. Ensemble
methods, like bagging, smearing and random partitioning, can alleviate this
problem and maintain the theoretical properties of the underlying estimator. We
empirically examine the performance of ensemble methods for prediction and
optimization, and then apply them to device modeling and constraint
approximation for geometric programming based circuit design.
"
"  Multi-parameter evidence synthesis (MPES) is receiving growing attention from
the epidemiological community as a coherent and flexible analytical framework
to accommodate a disparate body of evidence available to inform disease
incidence and prevalence estimation. MPES is the statistical methodology
adopted by the Health Protection Agency in the UK for its annual national
assessment of the HIV epidemic, and is acknowledged by the World Health
Organization and UNAIDS as a valuable technique for the estimation of adult HIV
prevalence from surveillance data. This paper describes the results of
utilizing a Bayesian MPES approach to model HIV prevalence in the Netherlands
at the end of 2007, using an array of field data from different study designs
on various population risk subgroups and with a varying degree of regional
coverage. Auxiliary data and expert opinion were additionally incorporated to
resolve issues arising from biased, insufficient or inconsistent evidence. This
case study offers a demonstration of the ability of MPES to naturally integrate
and critically reconcile disparate and heterogeneous sources of evidence, while
producing reliable estimates of HIV prevalence used to support public health
decision-making.
"
"  In recent years, several methods have been proposed for the discovery of
causal structure from non-experimental data (Spirtes et al. 2000; Pearl 2000).
Such methods make various assumptions on the data generating process to
facilitate its identification from purely observational data. Continuing this
line of research, we show how to discover the complete causal structure of
continuous-valued data, under the assumptions that (a) the data generating
process is linear, (b) there are no unobserved confounders, and (c) disturbance
variables have non-gaussian distributions of non-zero variances. The solution
relies on the use of the statistical method known as independent component
analysis (ICA), and does not require any pre-specified time-ordering of the
variables. We provide a complete Matlab package for performing this LiNGAM
analysis (short for Linear Non-Gaussian Acyclic Model), and demonstrate the
effectiveness of the method using artificially generated data.
"
"  This paper presents an approach to phenology, one based on the use of a
method developed by the authors for event history data. Of specific interest is
the prediction of the so-called ""bloom--date"" of fruit trees in the agriculture
industry and it is this application which we consider, although the method is
much more broadly applicable. Our approach provides sensible estimate for a
parameter that interests phenologists -- Tbase, the thresholding parameter in
the definition of the growing degree days (GDD). Our analysis supports
scientists' empirical finding: the timing of a phenological event of a prenniel
crop is related the cumulative sum of GDDs. Our prediction of future
bloom--dates are quite accurate, but the predictive uncertainty is high,
possibly due to our crude climate model for predicting future temperature, the
time-dependent covariate in our regression model for phenological events. We
found that if we can manage to get accurate prediction of future temperature,
our prediction of bloom--date is more accurate and the predictive uncertainty
is much lower.
"
"  Managing risk in dynamic decision problems is of cardinal importance in many
fields such as finance and process control. The most common approach to
defining risk is through various variance related criteria such as the Sharpe
Ratio or the standard deviation adjusted reward. It is known that optimizing
many of the variance related risk criteria is NP-hard. In this paper we devise
a framework for local policy gradient style algorithms for reinforcement
learning for variance related criteria. Our starting point is a new formula for
the variance of the cost-to-go in episodic tasks. Using this formula we develop
policy gradient algorithms for criteria that involve both the expected cost and
the variance of the cost. We prove the convergence of these algorithms to local
minima and demonstrate their applicability in a portfolio planning problem.
"
"  Recent results in Compressive Sensing have shown that, under certain
conditions, the solution to an underdetermined system of linear equations with
sparsity-based regularization can be accurately recovered by solving convex
relaxations of the original problem. In this work, we present a novel
primal-dual analysis on a class of sparsity minimization problems. We show that
the Lagrangian bidual (i.e., the Lagrangian dual of the Lagrangian dual) of the
sparsity minimization problems can be used to derive interesting convex
relaxations: the bidual of the $\ell_0$-minimization problem is the
$\ell_1$-minimization problem; and the bidual of the $\ell_{0,1}$-minimization
problem for enforcing group sparsity on structured data is the
$\ell_{1,\infty}$-minimization problem. The analysis provides a means to
compute per-instance non-trivial lower bounds on the (group) sparsity of the
desired solutions. In a real-world application, the bidual relaxation improves
the performance of a sparsity-based classification framework applied to robust
face recognition.
"
"  In this article, we have proposed several approaches for post processing a
large ensemble of prediction models or rules. The results from our simulations
show that the post processing methods we have considered here are promising. We
have used the techniques developed here for estimation of quantitative traits
from markers, on the benchmark ""Bostob Housing""data set and in some
simulations. In most cases, the produced models had better prediction
performance than, for example, the ones produced by the random forest or the
rulefit algorithms.
"
"  The seemingly disjoint problems of count and mixture modeling are united
under the negative binomial (NB) process. A gamma process is employed to model
the rate measure of a Poisson process, whose normalization provides a random
probability measure for mixture modeling and whose marginalization leads to an
NB process for count modeling. A draw from the NB process consists of a Poisson
distributed finite number of distinct atoms, each of which is associated with a
logarithmic distributed number of data samples. We reveal relationships between
various count- and mixture-modeling distributions and construct a
Poisson-logarithmic bivariate distribution that connects the NB and Chinese
restaurant table distributions. Fundamental properties of the models are
developed, and we derive efficient Bayesian inference. It is shown that with
augmentation and normalization, the NB process and gamma-NB process can be
reduced to the Dirichlet process and hierarchical Dirichlet process,
respectively. These relationships highlight theoretical, structural and
computational advantages of the NB process. A variety of NB processes,
including the beta-geometric, beta-NB, marked-beta-NB, marked-gamma-NB and
zero-inflated-NB processes, with distinct sharing mechanisms, are also
constructed. These models are applied to topic modeling, with connections made
to existing algorithms under Poisson factor analysis. Example results show the
importance of inferring both the NB dispersion and probability parameters.
"
"  Clustering in high-dimensional spaces is nowadays a recurrent problem in many
scientific domains but remains a difficult task from both the clustering
accuracy and the result understanding points of view. This paper presents a
discriminative latent mixture (DLM) model which fits the data in a latent
orthonormal discriminative subspace with an intrinsic dimension lower than the
dimension of the original space. By constraining model parameters within and
between groups, a family of 12 parsimonious DLM models is exhibited which
allows to fit onto various situations. An estimation algorithm, called the
Fisher-EM algorithm, is also proposed for estimating both the mixture
parameters and the discriminative subspace. Experiments on simulated and real
datasets show that the proposed approach performs better than existing
clustering methods while providing a useful representation of the clustered
data. The method is as well applied to the clustering of mass spectrometry
data.
"
"  An important question in health services research is the estimation of the
proportion of medical expenditures that exceed a given threshold. Typically,
medical expenditures present highly skewed, heavy tailed distributions, for
which (a) simple variable transformations are insufficient to achieve a
tractable low-dimensional parametric form and (b) nonparametric methods are not
efficient in estimating exceedance probabilities for large thresholds.
Motivated by this context, in this paper we propose a general Bayesian approach
for the estimation of tail probabilities of heavy-tailed distributions, based
on a mixture of gamma distributions in which the mixing occurs over the shape
parameter. This family provides a flexible and novel approach for modeling
heavy-tailed distributions, it is computationally efficient, and it only
requires to specify a prior distribution for a single parameter. By carrying
out simulation studies, we compare our approach with commonly used methods,
such as the log-normal model and nonparametric alternatives. We found that the
mixture-gamma model significantly improves predictive performance in estimating
tail probabilities, compared to these alternatives. We also applied our method
to the Medical Current Beneficiary Survey (MCBS), for which we estimate the
probability of exceeding a given hospitalization cost for smoking attributable
diseases. We have implemented the method in the open source GSM package,
available from the Comprehensive R Archive Network.
"
"  In this work, we propose a PAC-Bayes bound for the generalization risk of the
Gibbs classifier in the multi-class classification framework. The novelty of
our work is the critical use of the confusion matrix of a classifier as an
error measure; this puts our contribution in the line of work aiming at dealing
with performance measure that are richer than mere scalar criterion such as the
misclassification rate. Thanks to very recent and beautiful results on matrix
concentration inequalities, we derive two bounds showing that the true
confusion risk of the Gibbs classifier is upper-bounded by its empirical risk
plus a term depending on the number of training examples in each class. To the
best of our knowledge, this is the first PAC-Bayes bounds based on confusion
matrices.
"
"  Recently developed spatial capture-recapture (SCR) models represent a major
advance over traditional capture-recapture (CR) models because they yield
explicit estimates of animal density instead of population size within an
unknown area. Furthermore, unlike nonspatial CR methods, SCR models account for
heterogeneity in capture probability arising from the juxtaposition of animal
activity centers and sample locations. Although the utility of SCR methods is
gaining recognition, the requirement that all individuals can be uniquely
identified excludes their use in many contexts. In this paper, we develop
models for situations in which individual recognition is not possible, thereby
allowing SCR concepts to be applied in studies of unmarked or partially marked
populations. The data required for our model are spatially referenced counts
made on one or more sample occasions at a collection of closely spaced sample
units such that individuals can be encountered at multiple locations. Our
approach includes a spatial point process for the animal activity centers and
uses the spatial correlation in counts as information about the number and
location of the activity centers. Camera-traps, hair snares, track plates,
sound recordings, and even point counts can yield spatially correlated count
data, and thus our model is widely applicable. A simulation study demonstrated
that while the posterior mean exhibits frequentist bias on the order of 5-10%
in small samples, the posterior mode is an accurate point estimator as long as
adequate spatial correlation is present. Marking a subset of the population
substantially increases posterior precision and is recommended whenever
possible. We applied our model to avian point count data collected on an
unmarked population of the northern parula (Parula americana) and obtained a
density estimate (posterior mode) of 0.38 (95% CI: 0.19-1.64) birds/ha. Our
paper challenges sampling and analytical conventions in ecology by
demonstrating that neither spatial independence nor individual recognition is
needed to estimate population density - rather, spatial dependence can be
informative about individual distribution and density.
"
"  Generalized autoregressive conditional heteroscedasticity (GARCH) models have
long been considered as one of the most successful families of approaches for
volatility modeling in financial return series. In this paper, we propose an
alternative approach based on methodologies widely used in the field of
statistical machine learning. Specifically, we propose a novel nonparametric
Bayesian mixture of Gaussian process regression models, each component of which
models the noise variance process that contaminates the observed data as a
separate latent Gaussian process driven by the observed data. This way, we
essentially obtain a mixture Gaussian process conditional heteroscedasticity
(MGPCH) model for volatility modeling in financial return series. We impose a
nonparametric prior with power-law nature over the distribution of the model
mixture components, namely the Pitman-Yor process prior, to allow for better
capturing modeled data distributions with heavy tails and skewness. Finally, we
provide a copula- based approach for obtaining a predictive posterior for the
covariances over the asset returns modeled by means of a postulated MGPCH
model. We evaluate the efficacy of our approach in a number of benchmark
scenarios, and compare its performance to state-of-the-art methodologies.
"
"  Distributed strategic learning has been getting attention in recent years. As
systems become distributed finding Nash equilibria in a distributed fashion is
becoming more important for various applications. In this paper, we develop a
distributed strategic learning framework for seeking Nash equilibria under
stochastic state-dependent payoff functions. We extend the work of Krstic
et.al. in [1] to the case of stochastic state dependent payoff functions. We
develop an iterative distributed algorithm for Nash seeking and examine its
convergence to a limiting trajectory defined by an Ordinary Differential
Equation (ODE). We show convergence of our proposed algorithm for vanishing
step size and provide an error bound for fixed step size. Finally, we conduct a
stability analysis and apply the proposed scheme in a generic wireless
networks. We also present numerical results which corroborate our claim.
"
"  We outline a representation for discrete multivariate distributions in terms
of interventional potential functions that are globally normalized. This
representation can be used to model the effects of interventions, and the
independence properties encoded in this model can be represented as a directed
graph that allows cycles. In addition to discussing inference and sampling with
this representation, we give an exponential family parametrization that allows
parameter estimation to be stated as a convex optimization problem; we also
give a convex relaxation of the task of simultaneous parameter and structure
learning using group l1-regularization. The model is evaluated on simulated
data and intracellular flow cytometry data.
"
"  We consider the problem of estimating a function $f\_{0}$ in logistic
regression model. We propose to estimate this function $f\_{0}$ by a sparse
approximation build as a linear combination of elements of a given dictionary
of $p$ functions. This sparse approximation is selected by the Lasso or Group
Lasso procedure. In this context, we state non asymptotic oracle inequalities
for Lasso and Group Lasso under restricted eigenvalues assumption as introduced
in \cite{BRT}.
"
"  We study the efficiency of V-fold cross-validation (VFCV) for model selection
from the non-asymptotic viewpoint, and suggest an improvement on it, which we
call ``V-fold penalization''. Considering a particular (though simple)
regression problem, we prove that VFCV with a bounded V is suboptimal for model
selection, because it ``overpenalizes'' all the more that V is large. Hence,
asymptotic optimality requires V to go to infinity. However, when the
signal-to-noise ratio is low, it appears that overpenalizing is necessary, so
that the optimal V is not always the larger one, despite of the variability
issue. This is confirmed by some simulated data. In order to improve on the
prediction performance of VFCV, we define a new model selection procedure,
called ``V-fold penalization'' (penVF). It is a V-fold subsampling version of
Efron's bootstrap penalties, so that it has the same computational cost as
VFCV, while being more flexible. In a heteroscedastic regression framework,
assuming the models to have a particular structure, we prove that penVF
satisfies a non-asymptotic oracle inequality with a leading constant that tends
to 1 when the sample size goes to infinity. In particular, this implies
adaptivity to the smoothness of the regression function, even with a highly
heteroscedastic noise. Moreover, it is easy to overpenalize with penVF,
independently from the V parameter. A simulation study shows that this results
in a significant improvement on VFCV in non-asymptotic situations.
"
"  One of the most interesting unsolved questions in science today is the
question of life on other planets. At the present time it is safe to say that
we do not have much of an idea as to whether life is common or exceedingly rare
in the universe, and this will probably not be solved for certain unless
definitive evidence of extraterrestrial life is found in the future. Our
presence on Earth is just as consistent with the hypothesis that life is
extremely rare as it is with the hypothesis that it is common, since if there
was only one planet with intelligent life, we would find ourselves on it.
However, we have more information than this, such as the the surprisingly short
length of time it took for life to arise on Earth. Previous authors have
analysed this information, concluding that it is evidence that the probability
of abiogenesis is moderate ($>$ 13% with 95% probability) and cannot be
extremely small. In this paper I use simple probabilistic model to show that
this conclusion was based more on an unintentional assumption than on the data.
While the early formation of life on Earth provides some evidence in the
direction of life being common, it is far from conclusive, and in particular
does not rule out the possibility that abiogenesis has only occurred once in
the history of the universe.
"
"  A model for network panel data is discussed, based on the assumption that the
observed data are discrete observations of a continuous-time Markov process on
the space of all directed graphs on a given node set, in which changes in tie
variables are independent conditional on the current graph. The model for tie
changes is parametric and designed for applications to social network analysis,
where the network dynamics can be interpreted as being generated by choices
made by the social actors represented by the nodes of the graph. An algorithm
for calculating the Maximum Likelihood estimator is presented, based on data
augmentation and stochastic approximation. An application to an evolving
friendship network is given and a small simulation study is presented which
suggests that for small data sets the Maximum Likelihood estimator is more
efficient than the earlier proposed Method of Moments estimator.
"
"  We propose a general and formal statistical framework for multiple tests of
association between known fixed features of a genome and unknown parameters of
the distribution of variable features of this genome in a population of
interest. The known gene-annotation profiles, corresponding to the fixed
features of the genome, may concern Gene Ontology (GO) annotation, pathway
membership, regulation by particular transcription factors, nucleotide
sequences, or protein sequences. The unknown gene-parameter profiles,
corresponding to the variable features of the genome, may be, for example,
regression coefficients relating possibly censored biological and clinical
outcomes to genome-wide transcript levels, DNA copy numbers, and other
covariates. A generic question of great interest in current genomic research
regards the detection of associations between biological annotation metadata
and genome-wide expression measures. This biological question may be translated
as the test of multiple hypotheses concerning association measures between
gene-annotation profiles and gene-parameter profiles. A general and rigorous
formulation of the statistical inference question allows us to apply the
multiple hypothesis testing methodology developed in [Multiple Testing
Procedures with Applications to Genomics (2008) Springer, New York] and related
articles, to control a broad class of Type I error rates, defined as
generalized tail probabilities and expected values for arbitrary functions of
the numbers of Type I errors and rejected hypotheses. The resampling-based
single-step and stepwise multiple testing procedures of [Multiple Testing
Procedures with Applications to Genomics (2008) Springer, New York] take into
account the joint distribution of the test statistics and provide Type I error
control in testing problems involving general data generating distributions
(with arbitrary dependence structures among variables), null hypotheses, and
test statistics.
"
"  This paper introduces a new specialized algorithm for equilibrium Monte Carlo
sampling of binary-valued systems, which allows for large moves in the state
space. This is achieved by constructing self-avoiding walks (SAWs) in the state
space. As a consequence, many bits are flipped in a single MCMC step. We name
the algorithm SARDONICS, an acronym for Self-Avoiding Random Dynamics on
Integer Complex Systems. The algorithm has several free parameters, but we show
that Bayesian optimization can be used to automatically tune them. SARDONICS
performs remarkably well in a broad number of sampling tasks: toroidal
ferromagnetic and frustrated Ising models, 3D Ising models, restricted
Boltzmann machines and chimera graphs arising in the design of quantum
computers.
"
"  We define a class of Euclidean distances on weighted graphs, enabling to
perform thermodynamic soft graph clustering. The class can be constructed form
the ""raw coordinates"" encountered in spectral clustering, and can be extended
by means of higher-dimensional embeddings (Schoenberg transformations).
Geographical flow data, properly conditioned, illustrate the procedure as well
as visualization aspects.
"
"  We study decision making in environments where the reward is only partially
observed, but can be modeled as a function of an action and an observed
context. This setting, known as contextual bandits, encompasses a wide variety
of applications including health-care policy and Internet advertising. A
central task is evaluation of a new policy given historic data consisting of
contexts, actions and received rewards. The key challenge is that the past data
typically does not faithfully represent proportions of actions taken by a new
policy. Previous approaches rely either on models of rewards or models of the
past policy. The former are plagued by a large bias whereas the latter have a
large variance.
  In this work, we leverage the strength and overcome the weaknesses of the two
approaches by applying the doubly robust technique to the problems of policy
evaluation and optimization. We prove that this approach yields accurate value
estimates when we have either a good (but not necessarily consistent) model of
rewards or a good (but not necessarily consistent) model of past policy.
Extensive empirical comparison demonstrates that the doubly robust approach
uniformly improves over existing techniques, achieving both lower variance in
value estimation and better policies. As such, we expect the doubly robust
approach to become common practice.
"
"  The ETS has recently released new estimates of validities of the GRE for
predicting cumulative graduate GPA. They average in the middle thirties - twice
as high as those previously reported by a number of independent investigators.
It is shown in the first part of this paper that this unexpected finding can be
traced to a flawed methodology that tends to inflate multiple correlation
estimates, especially those of populations values near zero. Secondly, the
issue of upward corrections of validity estimates for restriction of range is
taken up. It is shown that they depend on assumptions that are rarely met by
the data. Finally, it is argued more generally that conventional test theory,
which is couched in terms of correlations and variances, is not only
unnecessarily abstract but, more importantly, incomplete, since the practical
utility of a test does not only depend on its validity, but also on base-rates
and admission quotas. A more direct and conclusive method for gauging the
utility of a test involves misclassification rates, and entirely dispenses with
questionable assumptions and post-hoc ""corrections"". On applying this approach
to the GRE, it emerges (1) that the GRE discriminates against ethnic and
economic minorities, and (2) that it often produces more erroneous decisions
than a purely random admissions policy would.
"
"  Screening is the problem of finding a superset of the set of non-zero entries
in an unknown p-dimensional vector \beta* given n noisy observations.
Naturally, we want this superset to be as small as possible. We propose a novel
framework for screening, which we refer to as Multiple Grouping (MuG), that
groups variables, performs variable selection over the groups, and repeats this
process multiple number of times to estimate a sequence of sets that contains
the non-zero entries in \beta*. Screening is done by taking an intersection of
all these estimated sets. The MuG framework can be used in conjunction with any
group based variable selection algorithm. In the high-dimensional setting,
where p >> n, we show that when MuG is used with the group Lasso estimator,
screening can be consistently performed without using any tuning parameter. Our
numerical simulations clearly show the merits of using the MuG framework in
practice.
"
"  We provide a unifying framework linking two classes of statistics used in
two-sample and independence testing: on the one hand, the energy distances and
distance covariances from the statistics literature; on the other, maximum mean
discrepancies (MMD), that is, distances between embeddings of distributions to
reproducing kernel Hilbert spaces (RKHS), as established in machine learning.
In the case where the energy distance is computed with a semimetric of negative
type, a positive definite kernel, termed distance kernel, may be defined such
that the MMD corresponds exactly to the energy distance. Conversely, for any
positive definite kernel, we can interpret the MMD as energy distance with
respect to some negative-type semimetric. This equivalence readily extends to
distance covariance using kernels on the product space. We determine the class
of probability distributions for which the test statistics are consistent
against all alternatives. Finally, we investigate the performance of the family
of distance kernels in two-sample and independence tests: we show in particular
that the energy distance most commonly employed in statistics is just one
member of a parametric family of kernels, and that other choices from this
family can yield more powerful tests.
"
"  Boosted decision trees typically yield good accuracy, precision, and ROC
area. However, because the outputs from boosting are not well calibrated
posterior probabilities, boosting yields poor squared error and cross-entropy.
We empirically demonstrate why AdaBoost predicts distorted probabilities and
examine three calibration methods for correcting this distortion: Platt
Scaling, Isotonic Regression, and Logistic Correction. We also experiment with
boosting using log-loss instead of the usual exponential loss. Experiments show
that Logistic Correction and boosting with log-loss work well when boosting
weak models such as decision stumps, but yield poor performance when boosting
more complex models such as full decision trees. Platt Scaling and Isotonic
Regression, however, significantly improve the probabilities predicted by
"
"  Bayesian model averaging enables one to combine the disparate predictions of
a number of models in a coherent fashion, leading to superior predictive
performance. The improvement in performance arises from averaging models that
make different predictions. In this work, we tap into perhaps the biggest
driver of different predictions---different analysts---in order to gain the
full benefits of model averaging. In a standard implementation of our method,
several data analysts work independently on portions of a data set, eliciting
separate models which are eventually updated and combined through a specific
weighting method. We call this modeling procedure Bayesian Synthesis. The
methodology helps to alleviate concerns about the sizable gap between the
foundational underpinnings of the Bayesian paradigm and the practice of
Bayesian statistics. In experimental work we show that human modeling has
predictive performance superior to that of many automatic modeling techniques,
including AIC, BIC, Smoothing Splines, CART, Bagged CART, Bayes CART, BMA and
LARS, and only slightly inferior to that of BART. We also show that Bayesian
Synthesis further improves predictive performance. Additionally, we examine the
predictive performance of a simple average across analysts, which we dub Convex
Synthesis, and find that it also produces an improvement.
"
"  We show that the disagreement coefficient of certain smooth hypothesis
classes is $O(m)$, where $m$ is the dimension of the hypothesis space, thereby
answering a question posed in \cite{friedman09}.
"
"  In this paper, we formulate the Canonical Correlation Analysis (CCA) problem
on matrix manifolds. This framework provides a natural way for dealing with
matrix constraints and tools for building efficient algorithms even in an
adaptive setting. Finally, an adaptive CCA algorithm is proposed and applied to
a change detection problem in EEG signals.
"
"  Recent advances of information technology in biomedical sciences and other
applied areas have created numerous large diverse data sets with a high
dimensional feature space, which provide us a tremendous amount of information
and new opportunities for improving the quality of human life. Meanwhile, great
challenges are also created driven by the continuous arrival of new data that
requires researchers to convert these raw data into scientific knowledge in
order to benefit from it. Association studies of complex diseases using SNP
data have become more and more popular in biomedical research in recent years.
In this paper, we present a review of recent statistical advances and
challenges for analyzing correlated high dimensional SNP data in genomic
association studies for complex diseases. The review includes both general
feature reduction approaches for high dimensional correlated data and more
specific approaches for SNPs data, which include unsupervised haplotype
mapping, tag SNP selection, and supervised SNPs selection using statistical
testing/scoring, statistical modeling and machine learning methods with an
emphasis on how to identify interacting loci.
"
"  We consider a multidimensional It\^o process $Y=(Y_t)_{t\in[0,T]}$ with some
unknown drift coefficient process $b_t$ and volatility coefficient
$\sigma(X_t,\theta)$ with covariate process $X=(X_t)_{t\in[0,T]}$, the function
$\sigma(x,\theta)$ being known up to $\theta\in\Theta$. For this model we
consider a change point problem for the parameter $\theta$ in the volatility
component. The change is supposed to occur at some point $t^*\in (0,T)$. Given
discrete time observations from the process $(X,Y)$, we propose quasi-maximum
likelihood estimation of the change point. We present the rate of convergence
of the change point estimator and the limit thereoms of aymptotically mixed
type.
"
"  In this paper, we propose the distributed tree kernels (DTK) as a novel
method to reduce time and space complexity of tree kernels. Using a linear
complexity algorithm to compute vectors for trees, we embed feature spaces of
tree fragments in low-dimensional spaces where the kernel computation is
directly done with dot product. We show that DTKs are faster, correlate with
tree kernels, and obtain a statistically similar performance in two natural
language processing tasks.
"
"  Graphical Gaussian models are popular tools for the estimation of
(undirected) gene association networks from microarray data. A key issue when
the number of variables greatly exceeds the number of samples is the estimation
of the matrix of partial correlations. Since the (Moore-Penrose) inverse of the
sample covariance matrix leads to poor estimates in this scenario, standard
methods are inappropriate and adequate regularization techniques are needed. In
this article, we investigate a general framework for combining regularized
regression methods with the estimation of Graphical Gaussian models. This
framework includes various existing methods as well as two new approaches based
on ridge regression and adaptive lasso, respectively. These methods are
extensively compared both qualitatively and quantitatively within a simulation
study and through an application to six diverse real data sets. In addition,
all proposed algorithms are implemented in the R package ""parcor"", available
from the R repository CRAN.
"
"  Least squares fitting is in general not useful for high-dimensional linear
models, in which the number of predictors is of the same or even larger order
of magnitude than the number of samples. Theory developed in recent years has
coined a paradigm according to which sparsity-promoting regularization is
regarded as a necessity in such setting. Deviating from this paradigm, we show
that non-negativity constraints on the regression coefficients may be similarly
effective as explicit regularization if the design matrix has additional
properties, which are met in several applications of non-negative least squares
(NNLS). We show that for these designs, the performance of NNLS with regard to
prediction and estimation is comparable to that of the lasso. We argue further
that in specific cases, NNLS may have a better $\ell_{\infty}$-rate in
estimation and hence also advantages with respect to support recovery when
combined with thresholding. From a practical point of view, NNLS does not
depend on a regularization parameter and is hence easier to use.
"
"  Sparse estimation methods are aimed at using or obtaining parsimonious
representations of data or models. While naturally cast as a combinatorial
optimization problem, variable or feature selection admits a convex relaxation
through the regularization by the $\ell_1$-norm. In this paper, we consider
situations where we are not only interested in sparsity, but where some
structural prior knowledge is available as well. We show that the $\ell_1$-norm
can then be extended to structured norms built on either disjoint or
overlapping groups of variables, leading to a flexible framework that can deal
with various structures. We present applications to unsupervised learning, for
structured sparse principal component analysis and hierarchical dictionary
learning, and to supervised learning in the context of non-linear variable
selection.
"
"  Ultrahigh-dimensional variable selection plays an increasingly important role
in contemporary scientific discoveries and statistical research. Among others,
Fan and Lv [J. R. Stat. Soc. Ser. B Stat. Methodol. 70 (2008) 849-911] propose
an independent screening framework by ranking the marginal correlations. They
showed that the correlation ranking procedure possesses a sure independence
screening property within the context of the linear model with Gaussian
covariates and responses. In this paper, we propose a more general version of
the independent learning with ranking the maximum marginal likelihood estimates
or the maximum marginal likelihood itself in generalized linear models. We show
that the proposed methods, with Fan and Lv [J. R. Stat. Soc. Ser. B Stat.
Methodol. 70 (2008) 849-911] as a very special case, also possess the sure
screening property with vanishing false selection rate. The conditions under
which the independence learning possesses a sure screening is surprisingly
simple. This justifies the applicability of such a simple method in a wide
spectrum. We quantify explicitly the extent to which the dimensionality can be
reduced by independence screening, which depends on the interactions of the
covariance matrix of covariates and true parameters. Simulation studies are
used to illustrate the utility of the proposed approaches. In addition, we
establish an exponential inequality for the quasi-maximum likelihood estimator
which is useful for high-dimensional statistical learning.
"
"  Binary trait data record the presence or absence of distinguishing traits in
individuals. We treat the problem of estimating ancestral trees with time depth
from binary trait data. Simple analysis of such data is problematic. Each
homology class of traits has a unique birth event on the tree, and the birth
event of a trait visible at the leaves is biased towards the leaves. We propose
a model-based analysis of such data, and present an MCMC algorithm that can
sample from the resulting posterior distribution. Our model is based on using a
birth-death process for the evolution of the elements of sets of traits. Our
analysis correctly accounts for the removal of singleton traits, which are
commonly discarded in real data sets. We illustrate Bayesian inference for two
binary-trait data sets which arise in historical linguistics. The Bayesian
approach allows for the incorporation of information from ancestral languages.
The marginal prior distribution of the root time is uniform. We present a
thorough analysis of the robustness of our results to model mispecification,
through analysis of predictive distributions for external data, and fitting
data simulated under alternative observation models. The reconstructed ages of
tree nodes are relatively robust, whilst posterior probabilities for topology
are not reliable.
"
"  In this work we deal with parameter estimation in a latent variable model,
namely the multiple-hidden i.i.d. model, which is derived from multiple
alignment algorithms. We first provide a rigorous formalism for the homology
structure of k sequences related by a star-shaped phylogenetic tree in the
context of multiple alignment based on indel evolution models. We discuss
possible definitions of likelihoods and compare them to the criterion used in
multiple alignment algorithms. Existence of two different Information
divergence rates is established and a divergence property is shown under
additional assumptions. This would yield consistency for the parameter in
parametrization schemes for which the divergence property holds. We finally
extend the definition of the multiple-hidden i.i.d. model and the results
obtained to the case in which the sequences are related by an arbitrary
phylogenetic tree. Simulations illustrate different cases which are not covered
by our results.
"
"  Whenever eye movements are measured, a central part of the analysis has to do
with where subjects fixate, and why they fixated where they fixated. To a first
approximation, a set of fixations can be viewed as a set of points in space:
this implies that fixations are spatial data and that the analysis of fixation
locations can be beneficially thought of as a spatial statistics problem. We
argue that thinking of fixation locations as arising from point processes is a
very fruitful framework for eye movement data, helping turn qualitative
questions into quantitative ones.
  We provide a tutorial introduction to some of the main ideas of the field of
spatial statistics, focusing especially on spatial Poisson processes. We show
how point processes help relate image properties to fixation locations. In
particular we show how point processes naturally express the idea that image
features' predictability for fixations may vary from one image to another. We
review other methods of analysis used in the literature, show how they relate
to point process theory, and argue that thinking in terms of point processes
substantially extends the range of analyses that can be performed and clarify
their interpretation.
"
"  The runtime for Kernel Partial Least Squares (KPLS) to compute the fit is
quadratic in the number of examples. However, the necessity of obtaining
sensitivity measures as degrees of freedom for model selection or confidence
intervals for more detailed analysis requires cubic runtime, and thus
constitutes a computational bottleneck in real-world data analysis. We propose
a novel algorithm for KPLS which not only computes (a) the fit, but also (b)
its approximate degrees of freedom and (c) error bars in quadratic runtime. The
algorithm exploits a close connection between Kernel PLS and the Lanczos
algorithm for approximating the eigenvalues of symmetric matrices, and uses
this approximation to compute the trace of powers of the kernel matrix in
quadratic runtime.
"
"  Most traditional online learning algorithms are based on variants of mirror
descent or follow-the-leader. In this paper, we present an online algorithm
based on a completely different approach, tailored for transductive settings,
which combines ""random playout"" and randomized rounding of loss subgradients.
As an application of our approach, we present the first computationally
efficient online algorithm for collaborative filtering with trace-norm
constrained matrices. As a second application, we solve an open question
linking batch learning and transductive online learning
"
"  We consider the problem of rank loss minimization in the setting of
multilabel classification, which is usually tackled by means of convex
surrogate losses defined on pairs of labels. Very recently, this approach was
put into question by a negative result showing that commonly used pairwise
surrogate losses, such as exponential and logistic losses, are inconsistent. In
this paper, we show a positive result which is arguably surprising in light of
the previous one: the simpler univariate variants of exponential and logistic
surrogates (i.e., defined on single labels) are consistent for rank loss
minimization. Instead of directly proving convergence, we give a much stronger
result by deriving regret bounds and convergence rates. The proposed losses
suggest efficient and scalable algorithms, which are tested experimentally.
"
"  Consider a multinomial regression model where the response, which indicates a
unit's membership in one of several possible unordered classes, is associated
with a set of predictor variables. Such models typically involve a matrix of
regression coefficients, with the $(j,k)$ element of this matrix modulating the
effect of the $k$th predictor on the propensity of the unit to belong to the
$j$th class. Thus, a supposition that only a subset of the available predictors
are associated with the response corresponds to some of the columns of the
coefficient matrix being zero. Under the Bayesian paradigm, the subset of
predictors which are associated with the response can be treated as an unknown
parameter, leading to typical Bayesian model selection and model averaging
procedures. As an alternative, we investigate model selection and averaging,
whereby a subset of individual elements of the coefficient matrix are zero.
That is, the subset of predictors associated with the propensity to belong to a
class varies with the class. We refer to this as class-specific predictor
selection. We argue that such a scheme can be attractive on both conceptual and
computational grounds.
"
"  We give polynomial-time algorithms for the exact computation of lowest-energy
(ground) states, worst margin violators, log partition functions, and marginal
edge probabilities in certain binary undirected graphical models. Our approach
provides an interesting alternative to the well-known graph cut paradigm in
that it does not impose any submodularity constraints; instead we require
planarity to establish a correspondence with perfect matchings (dimer
coverings) in an expanded dual graph. We implement a unified framework while
delegating complex but well-understood subproblems (planar embedding,
maximum-weight perfect matching) to established algorithms for which efficient
implementations are freely available. Unlike graph cut methods, we can perform
penalized maximum-likelihood as well as maximum-margin parameter estimation in
the associated conditional random fields (CRFs), and employ marginal posterior
probabilities as well as maximum a posteriori (MAP) states for prediction.
Maximum-margin CRF parameter estimation on image denoising and segmentation
problems shows our approach to be efficient and effective. A C++ implementation
is available from http://nic.schraudolph.org/isinf/
"
"  Simulated tempering (ST) is an established Markov chain Monte Carlo (MCMC)
method for sampling from a multimodal density $\pi(\theta)$. Typically, ST
involves introducing an auxiliary variable $k$ taking values in a finite subset
of $[0,1]$ and indexing a set of tempered distributions, say $\pi_k(\theta)
\propto \pi(\theta)^k$. In this case, small values of $k$ encourage better
mixing, but samples from $\pi$ are only obtained when the joint chain for
$(\theta,k)$ reaches $k=1$. However, the entire chain can be used to estimate
expectations under $\pi$ of functions of interest, provided that importance
sampling (IS) weights are calculated. Unfortunately this method, which we call
importance tempering (IT), can disappoint. This is partly because the most
immediately obvious implementation is na\""ive and can lead to high variance
estimators. We derive a new optimal method for combining multiple IS estimators
and prove that the resulting estimator has a highly desirable property related
to the notion of effective sample size. We briefly report on the success of the
optimal combination in two modelling scenarios requiring reversible-jump MCMC,
where the na\""ive approach fails.
"
"  The problem of stochastic convex optimization with bandit feedback (in the
learning community) or without knowledge of gradients (in the optimization
community) has received much attention in recent years, in the form of
algorithms and performance upper bounds. However, much less is known about the
inherent complexity of these problems, and there are few lower bounds in the
literature, especially for nonlinear functions. In this paper, we investigate
the attainable error/regret in the bandit and derivative-free settings, as a
function of the dimension d and the available number of queries T. We provide a
precise characterization of the attainable performance for strongly-convex and
smooth functions, which also imply a non-trivial lower bound for more general
problems. Moreover, we prove that in both the bandit and derivative-free
setting, the required number of queries must scale at least quadratically with
the dimension. Finally, we show that on the natural class of quadratic
functions, it is possible to obtain a ""fast"" O(1/T) error rate in terms of T,
under mild assumptions, even without having access to gradients. To the best of
our knowledge, this is the first such rate in a derivative-free stochastic
setting, and holds despite previous results which seem to imply the contrary.
"
"  Nodes in real-world networks are usually organized in local modules. These
groups, called communities, are intuitively defined as sub-graphs with a larger
density of internal connections than of external links. In this work, we
introduce a new measure aimed at quantifying the statistical significance of
single communities. Extreme and Order Statistics are used to predict the
statistics associated with individual clusters in random graphs. These
distributions allows us to define one community significance as the probability
that a generic clustering algorithm finds such a group in a random graph. The
method is successfully applied in the case of real-world networks for the
evaluation of the significance of their communities.
"
"  Functional neuroimaging can measure the brain?s response to an external
stimulus. It is used to perform brain mapping: identifying from these
observations the brain regions involved. This problem can be cast into a linear
supervised learning task where the neuroimaging data are used as predictors for
the stimulus. Brain mapping is then seen as a support recovery problem. On
functional MRI (fMRI) data, this problem is particularly challenging as i) the
number of samples is small due to limited acquisition time and ii) the
variables are strongly correlated. We propose to overcome these difficulties
using sparse regression models over new variables obtained by clustering of the
original variables. The use of randomization techniques, e.g. bootstrap
samples, and clustering of the variables improves the recovery properties of
sparse methods. We demonstrate the benefit of our approach on an extensive
simulation study as well as two fMRI datasets.
"
"  This paper uses statistical analysis of function words used in legal
judgments written by United States Supreme Court justices, to determine which
justices have the most variable writing style (which may indicated greater
reliance on their law clerks when writing opinions), and also the extent to
which different justices' writing styles are distinguishable from each other.
"
"  In this article supervised learning problems are solved using soft rule
ensembles. We first review the importance sampling learning ensembles (ISLE)
approach that is useful for generating hard rules. The soft rules are then
obtained with logistic regression from the corresponding hard rules. In order
to deal with the perfect separation problem related to the logistic regression,
Firth's bias corrected likelihood is used. Various examples and simulation
results show that soft rule ensembles can improve predictive performance over
hard rule ensembles.
"
"  This paper presents a general coding method where data in a Hilbert space are
represented by finite dimensional coding vectors. The method is based on
empirical risk minimization within a certain class of linear operators, which
map the set of coding vectors to the Hilbert space. Two results bounding the
expected reconstruction error of the method are derived, which highlight the
role played by the codebook and the class of linear operators. The results are
specialized to some cases of practical importance, including K-means
clustering, nonnegative matrix factorization and other sparse coding methods.
"
"  This paper treats the problem of detecting periodicity in a sequence of
photon arrival times, which occurs, for example, in attempting to detect
gamma-ray pulsars. A particular focus is on how auxiliary information,
typically source intensity, background intensity, and incidence angles and
energies associated with each photon arrival should be used to maximize the
detection power. We construct a class of likelihood-based tests, score tests,
which give rise to event weighting in a principled and natural way, and derive
expressions quantifying the power of the tests. These results can be used to
compare the efficacies of different weight functions, including cuts in energy
and incidence angle. The test is targeted toward a template for the periodic
lightcurve, and we quantify how deviation from that template affects the power
of detection.
"
"  We show that the two-stage adaptive Lasso procedure (Zou, 2006) is consistent
for high-dimensional model selection in linear and Gaussian graphical models.
Our conditions for consistency cover more general situations than those
accomplished in previous work: we prove that restricted eigenvalue conditions
(Bickel et al., 2008) are also sufficient for sparse structure estimation.
"
"  The free energy functional has recently been proposed as a variational
principle for bounded rational decision-making, since it instantiates a natural
trade-off between utility gains and information processing costs that can be
axiomatically derived. Here we apply the free energy principle to general
decision trees that include both adversarial and stochastic environments. We
derive generalized sequential optimality equations that not only include the
Bellman optimality equations as a limit case, but also lead to well-known
decision-rules such as Expectimax, Minimax and Expectiminimax. We show how
these decision-rules can be derived from a single free energy principle that
assigns a resource parameter to each node in the decision tree. These resource
parameters express a concrete computational cost that can be measured as the
amount of samples that are needed from the distribution that belongs to each
node. The free energy principle therefore provides the normative basis for
generalized optimality equations that account for both adversarial and
stochastic environments.
"
"  Suppose the data consist of a set $S$ of points $x_j$, $1\leq j \leq J$,
distributed in a bounded domain $D\subset R^N$, where $N$ is a large number. An
algorithm is given for finding the sets $L_k$ of dimension $k\ll N$,
$k=1,2,...K$, in a neighborhood of which maximal amount of points $x_j\in S$
lie. The algorithm is different from PCA (principal component analysis)
"
"  We consider the problems of clustering, classification, and visualization of
high-dimensional data when no straightforward Euclidean representation exists.
Typically, these tasks are performed by first reducing the high-dimensional
data to some lower dimensional Euclidean space, as many manifold learning
methods have been developed for this task. In many practical problems however,
the assumption of a Euclidean manifold cannot be justified. In these cases, a
more appropriate assumption would be that the data lies on a statistical
manifold, or a manifold of probability density functions (PDFs). In this paper
we propose using the properties of information geometry in order to define
similarities between data sets using the Fisher information metric. We will
show this metric can be approximated using entirely non-parametric methods, as
the parameterization of the manifold is generally unknown. Furthermore, by
using multi-dimensional scaling methods, we are able to embed the corresponding
PDFs into a low-dimensional Euclidean space. This not only allows for
classification of the data, but also visualization of the manifold. As a whole,
we refer to our framework as Fisher Information Non-parametric Embedding
(FINE), and illustrate its uses on a variety of practical problems, including
bio-medical applications and document classification.
"
"  Relative to the large literature on upper bounds on complexity of convex
optimization, lesser attention has been paid to the fundamental hardness of
these problems. Given the extensive use of convex optimization in machine
learning and statistics, gaining an understanding of these complexity-theoretic
issues is important. In this paper, we study the complexity of stochastic
convex optimization in an oracle model of computation. We improve upon known
results and obtain tight minimax complexity estimates for various function
classes.
"
"  Stochastic Gradient Descent (SGD) is one of the simplest and most popular
stochastic optimization methods. While it has already been theoretically
studied for decades, the classical analysis usually required non-trivial
smoothness assumptions, which do not apply to many modern applications of SGD
with non-smooth objective functions such as support vector machines. In this
paper, we investigate the performance of SGD without such smoothness
assumptions, as well as a running average scheme to convert the SGD iterates to
a solution with optimal optimization accuracy. In this framework, we prove that
after T rounds, the suboptimality of the last SGD iterate scales as
O(log(T)/\sqrt{T}) for non-smooth convex objective functions, and O(log(T)/T)
in the non-smooth strongly convex case. To the best of our knowledge, these are
the first bounds of this kind, and almost match the minimax-optimal rates
obtainable by appropriate averaging schemes. We also propose a new and simple
averaging scheme, which not only attains optimal rates, but can also be easily
computed on-the-fly (in contrast, the suffix averaging scheme proposed in
Rakhlin et al. (2011) is not as simple to implement). Finally, we provide some
experimental illustrations.
"
"  We generalize stochastic subgradient descent methods to situations in which
we do not receive independent samples from the distribution over which we
optimize, but instead receive samples that are coupled over time. We show that
as long as the source of randomness is suitably ergodic---it converges quickly
enough to a stationary distribution---the method enjoys strong convergence
guarantees, both in expectation and with high probability. This result has
implications for stochastic optimization in high-dimensional spaces,
peer-to-peer distributed optimization schemes, decision problems with dependent
data, and stochastic optimization problems over combinatorial spaces.
"
"  The optimal and robust design of structures has gained much attention in the
past ten years due to the ever increasing need for manufacturers to build
robust systems at the lowest cost. Reliability-based design optimization (RBDO)
allows the analyst to minimize some cost function while ensuring some minimal
performances cast as admissible probabilities of failure for a set of
performance functions. In order to address real-world problems in which the
performance is assessed through computational models (e.g. large scale finite
element models) meta-modelling techniques have been developed in the past
decade. This paper introduces adaptive kriging surrogate models to solve the
RBDO problem. The latter is cast in an augmented space that ""sums up"" the range
of the design space and the aleatory uncertainty in the design parameters and
the environmental conditions. Thus the surrogate model is used (i) for
evaluating robust estimates of the probabilities of failure (and for enhancing
the computational experimental design by adaptive sampling) in order to achieve
the requested accuracy and (ii) for applying the gradient-based optimization
algorithm. The approach is applied to the optimal design of imperfect stiffened
cylinder shells used in submarine engineering. For this application the
performance of the structure is related to buckling which is addressed here by
means of the asymptotic numerical method.
"
"  We consider the problem of learning a structured multi-task regression, where
the output consists of multiple responses that are related by a graph and the
correlated response variables are dependent on the common inputs in a sparse
but synergistic manner. Previous methods such as l1/l2-regularized multi-task
regression assume that all of the output variables are equally related to the
inputs, although in many real-world problems, outputs are related in a complex
manner. In this paper, we propose graph-guided fused lasso (GFlasso) for
structured multi-task regression that exploits the graph structure over the
output variables. We introduce a novel penalty function based on fusion penalty
to encourage highly correlated outputs to share a common set of relevant
inputs. In addition, we propose a simple yet efficient proximal-gradient method
for optimizing GFlasso that can also be applied to any optimization problems
with a convex smooth loss and the general class of fusion penalty defined on
arbitrary graph structures. By exploiting the structure of the non-smooth
''fusion penalty'', our method achieves a faster convergence rate than the
standard first-order method, sub-gradient method, and is significantly more
scalable than the widely adopted second-order cone-programming and
quadratic-programming formulations. In addition, we provide an analysis of the
consistency property of the GFlasso model. Experimental results not only
demonstrate the superiority of GFlasso over the standard lasso but also show
the efficiency and scalability of our proximal-gradient method.
"
"  In conducting preliminary analysis during an epidemic, data on reported
disease cases offer key information in guiding the direction to the in-depth
analysis. Models for growth and transmission dynamics are heavily dependent on
preliminary analysis results. When a particular disease case is reported more
than once or alternatively is never reported or detected in the population,
then in such a situation, there is a possibility of existence of multiple
reporting or under reporting in the population. In this work, a theoretical
approach for studying reporting error in epidemiology is explored. The upper
bound for the error that arises due to multiple reporting is higher than that
which arises due to under reporting. Numerical examples are provided to support
the arguments. This article mainly treats reporting error as deterministic and
one can explore a stochastic model for the same.
"
"  We present a probabilistic method for linking multiple datafiles. This task
is not trivial in the absence of unique identifiers for the individuals
recorded. This is a common scenario when linking census data to coverage
measurement surveys for census coverage evaluation, and in general when
multiple record-systems need to be integrated for posterior analysis. Our
method generalizes the Fellegi-Sunter theory for linking records from two
datafiles and its modern implementations. The multiple record linkage goal is
to classify the record K-tuples coming from K datafiles according to the
different matching patterns. Our method incorporates the transitivity of
agreement in the computation of the data used to model matching probabilities.
We use a mixture model to fit matching probabilities via maximum likelihood
using the EM algorithm. We present a method to decide the record K-tuples
membership to the subsets of matching patterns and we prove its optimality. We
apply our method to the integration of three Colombian homicide record systems
and we perform a simulation study in order to explore the performance of the
method under measurement error and different scenarios. The proposed method
works well and opens some directions for future research.
"
"  Network inference approaches are now widely used in biological applications
to probe regulatory relationships between molecular components such as genes or
proteins. Many methods have been proposed for this setting, but the connections
and differences between their statistical formulations have received less
attention. In this paper, we show how a broad class of statistical network
inference methods, including a number of existing approaches, can be described
in terms of variable selection for the linear model. This reveals some subtle
but important differences between the methods, including the treatment of time
intervals in discretely observed data. In developing a general formulation, we
also explore the relationship between single-cell stochastic dynamics and
network inference on averages over cells. This clarifies the link between
biochemical networks as they operate at the cellular level and network
inference as carried out on data that are averages over populations of cells.
We present empirical results, comparing thirty-two network inference methods
that are instances of the general formulation we describe, using two published
dynamical models. Our investigation sheds light on the applicability and
limitations of network inference and provides guidance for practitioners and
suggestions for experimental design.
"
"  In this article, we derive an explicit formula for computing confidence
interval for the mean of a bounded random variable. Moreover, we have developed
multistage point estimation methods for estimating the mean value with
prescribed precision and confidence level based on the proposed confidence
interval.
"
"  Statistical arbitrage strategies, such as pairs trading and its
generalizations, rely on the construction of mean-reverting spreads enjoying a
certain degree of predictability. Gaussian linear state-space processes have
recently been proposed as a model for such spreads under the assumption that
the observed process is a noisy realization of some hidden states. Real-time
estimation of the unobserved spread process can reveal temporary market
inefficiencies which can then be exploited to generate excess returns. Building
on previous work, we embrace the state-space framework for modeling spread
processes and extend this methodology along three different directions. First,
we introduce time-dependency in the model parameters, which allows for quick
adaptation to changes in the data generating process. Second, we provide an
on-line estimation algorithm that can be constantly run in real-time. Being
computationally fast, the algorithm is particularly suitable for building
aggressive trading strategies based on high-frequency data and may be used as a
monitoring device for mean-reversion. Finally, our framework naturally provides
informative uncertainty measures of all the estimated parameters. Experimental
results based on Monte Carlo simulations and historical equity data are
discussed, including a co-integration relationship involving two
exchange-traded funds.
"
"  The U.S. Census Bureau provides an estimate of the true population as a
supplement to the basic census numbers. This estimate is constructed from data
in a post-censal survey. The overall procedure is referred to as dual system
estimation. Dual system estimation is designed to produce revised estimates at
all levels of geography, via a synthetic estimation procedure. We design three
alternative formulas for dual system estimation and investigate the differences
in area estimates produced as a result of using those formulas. The primary
target of this exercise is to better understand the nature of the homogeneity
assumptions involved in dual system estimation and their consequences when used
for the enumeration data that occurs in an actual large scale application like
the Census. (Assumptions of this nature are sometimes collectively referred to
as the ``synthetic assumption'' for dual system estimation.) The specific focus
of our study is the treatment of the category of census counts referred to as
imputations in dual system estimation. Our results show the degree to which
varying treatment of these imputation counts can result in differences in
population estimates for local areas such as states or counties.
"
"  MicroRNAs (miRNAs) are small RNA molecules composed of 19-22 nt, which play
important regulatory roles in post-transcriptional gene regulation by
inhibiting the translation of the mRNA into proteins or otherwise cleaving the
target mRNA. Inferring miRNA targets provides useful information for
understanding the roles of miRNA in biological processes that are potentially
involved in complex diseases. Statistical methodologies for point estimation,
such as the Least Absolute Shrinkage and Selection Operator (LASSO) algorithm,
have been proposed to identify the interactions of miRNA and mRNA based on
sequence and expression data. In this paper, we propose using the Bayesian
LASSO (BLASSO) and the non-negative Bayesian LASSO (nBLASSO) to analyse the
interactions between miRNA and mRNA using expression data. The proposed
Bayesian methods explore the posterior distributions for those parameters
required to model the miRNA-mRNA interactions. These approaches can be used to
observe the inferred effects of the miRNAs on the targets by plotting the
posterior distributions of those parameters. For comparison purposes, the Least
Squares Regression (LSR), Ridge Regression (RR), LASSO, non-negative LASSO
(nLASSO), and the proposed Bayesian approaches were applied to four public
datasets. We concluded that nLASSO and nBLASSO perform best in terms of
sensitivity and specificity. Compared to the point estimate algorithms, which
only provide single estimates for those parameters, the Bayesian methods are
more meaningful and provide credible intervals, which take into account the
uncertainty of the inferred interactions of the miRNA and mRNA. Furthermore,
Bayesian methods naturally provide statistical significance to select
convincing inferred interactions, while point estimate algorithms require a
manually chosen threshold, which is less meaningful, to choose the possible
interactions.
"
"  Background: Many authors have described MELD as a predictor of short-term
mortality in the liver transplantation waiting list. However MELD score
accuracy to predict long term mortality has not been statistically evaluated.
Objective: The aim of this study is to analyze the MELD score as well as other
variables as a predictor of long-term mortality using a new model: the Survival
Tree analysis. Study Design and Setting: The variables obtained at the time of
liver transplantation list enrollment and considered in this study are: sex,
age, blood type, body mass index, etiology of liver disease, hepatocellular
carcinoma, waiting time for transplant and MELD. Mortality on the waiting list
is the outcome. Exclusion, transplantation or still in the transplantation list
at the end of the study are censored data. Results: The graphical
representation of the survival trees showed that the most statistically
significant cut off is related to MELD score at point 16. Conclusion: The
results are compatible with the cut off point of MELD indicated in the clinical
literature.
"
"  Leo Breiman was a highly creative, influential researcher with a
down-to-earth personal style and an insistence on working on important real
world problems and producing useful solutions. This paper is a short review of
Breiman's extensive contributions to the field of applied statistics.
"
"  The problem of matching unlabelled point sets using Bayesian inference is
considered. Two recently proposed models for the likelihood are compared, based
on the Procrustes size-and-shape and the full configuration. Bayesian inference
is carried out for matching point sets using Markov chain Monte Carlo
simulation. An improvement to the existing Procrustes algorithm is proposed
which improves convergence rates, using occasional large jumps in the burn-in
period. The Procrustes and configuration methods are compared in a simulation
study and using real data, where it is of interest to estimate the strengths of
matches between protein binding sites. The performance of both methods is
generally quite similar, and a connection between the two models is made using
a Laplace approximation.
"
"  Consider an attributed graph whose vertices are colored green or red, but
only a few are observed to be red. The color of the other vertices is
unobserved. Typically, the unknown total number of red vertices is small. The
vertex nomination problem is to nominate one of the unobserved vertices as
being red. The edge set of the graph is a subset of the set of unordered pairs
of vertices. Suppose that each edge is also colored green or red and this is
observed for all edges. The context statistic of a vertex is defined as the
number of observed red vertices connected to it, and its content statistic is
the number of red edges incident to it. Assuming that these statistics are
independent between vertices and that red edges are more likely between red
vertices, Coppersmith and Priebe (2012) proposed a likelihood model based on
these statistics. Here, we formulate a Bayesian model using the proposed
likelihood together with prior distributions chosen for the unknown parameters
and unobserved vertex colors. From the resulting posterior distribution, the
nominated vertex is the one with the highest posterior probability of being
red. Inference is conducted using a Metropolis-within-Gibbs algorithm, and
performance is illustrated by a simulation study. Results show that (i) the
Bayesian model performs significantly better than chance; (ii) the probability
of correct nomination increases with increasing posterior probability that the
nominated vertex is red; and (iii) the Bayesian model either matches or
performs better than the method in Coppersmith and Priebe. An application
example is provided using the Enron email corpus, where vertices represent
Enron employees and their associates, observed red vertices are known
fraudsters, red edges represent email communications perceived as fraudulent,
and we wish to identify one of the latent vertices as most likely to be a
fraudster.
"
"  We have developed a sophisticated statistical model for predicting the
hitting performance of Major League baseball players. The Bayesian paradigm
provides a principled method for balancing past performance with crucial
covariates, such as player age and position. We share information across time
and across players by using mixture distributions to control shrinkage for
improved accuracy. We compare the performance of our model to current
sabermetric methods on a held-out season (2006), and discuss both successes and
limitations.
"
"  High dimensional time series are endemic in applications of machine learning
such as robotics (sensor data), computational biology (gene expression data),
vision (video sequences) and graphics (motion capture data). Practical
nonlinear probabilistic approaches to this data are required. In this paper we
introduce the variational Gaussian process dynamical system. Our work builds on
recent variational approximations for Gaussian process latent variable models
to allow for nonlinear dimensionality reduction simultaneously with learning a
dynamical prior in the latent space. The approach also allows for the
appropriate dimensionality of the latent space to be automatically determined.
We demonstrate the model on a human motion capture data set and a series of
high resolution video sequences.
"
"  Gaia will obtain astrometry and spectrophotometry for essentially all sources
in the sky down to a broad band magnitude limit of G=20, an expected yield of
10^9 stars. Its main scientific objective is to reveal the formation and
evolution of our Galaxy through chemo-dynamical analysis. In addition to
inferring positions, parallaxes and proper motions from the astrometry, we must
also infer the astrophysical parameters of the stars from the
spectrophotometry, the BP/RP spectrum. Here we investigate the performance of
three different algorithms (SVM, ILIUM, Aeneas) for estimating the effective
temperature, line-of-sight interstellar extinction, metallicity and surface
gravity of A-M stars over a wide range of these parameters and over the full
magnitude range Gaia will observe (G=6-20mag). One of the algorithms, Aeneas,
infers the posterior probability density function over all parameters, and can
optionally take into account the parallax and the Hertzsprung-Russell diagram
to improve the estimates. For all algorithms the accuracy of estimation depends
on G and on the value of the parameters themselves, so a broad summary of
performance is only approximate. For stars at G=15 with less than two
magnitudes extinction, we expect to be able to estimate Teff to within 1%, logg
to 0.1-0.2dex, and [Fe/H] (for FGKM stars) to 0.1-0.2dex, just using the BP/RP
spectrum (mean absolute error statistics are quoted). Performance degrades at
larger extinctions, but not always by a large amount. Extinction can be
estimated to an accuracy of 0.05-0.2mag for stars across the full parameter
range with a priori unknown extinction between 0 and 10mag. Performance
degrades at fainter magnitudes, but even at G=19 we can estimate logg to better
than 0.2dex for all spectral types, and [Fe/H] to within 0.35dex for FGKM
stars, for extinctions below 1mag.
"
"  There is a lot of statistical researches of Russian elections 04.12.2011. The
purpose of this activity is to give a mathematical proof of large
falsifications and to estimate possible 'real results of elections'. My purpose
is to show that
  1. Statistical argumentation allows to prove existence of falsifications and
to give a lower estimate of falsification, near 1-2 percents.
  2. Statistical proofs of stronger statements are incorrect from both points
of view of mathematics and of natural sciences.
  3. This problem is not a problem of pure mathematics (since it includes
strong indeterminacy of sociological nature).
"
"  We present a new approach to factor rotation for functional data. This is
achieved by rotating the functional principal components toward a predefined
space of periodic functions designed to decompose the total variation into
components that are nearly-periodic and nearly-aperiodic with a predefined
period. We show that the factor rotation can be obtained by calculation of
canonical correlations between appropriate spaces which make the methodology
computationally efficient. Moreover, we demonstrate that our proposed rotations
provide stable and interpretable results in the presence of highly complex
covariance. This work is motivated by the goal of finding interpretable sources
of variability in gridded time series of vegetation index measurements obtained
from remote sensing, and we demonstrate our methodology through an application
of factor rotation of this data.
"
"  This paper applies machine learning and the mathematics of chaos to the task
of designing indoor rock-climbing routes. Chaotic variation has been used to
great advantage on music and dance, but the challenges here are quite
different, beginning with the representation. We present a formalized system
for transcribing rock climbing problems, then describe a variation generator
that is designed to support human route-setters in designing new and
interesting climbing problems. This variation generator, termed Strange Beta,
combines chaos and machine learning, using the former to introduce novelty and
the latter to smooth transitions in a manner that is consistent with the style
of the climbs This entails parsing the domain-specific natural language that
rock climbers use to describe routes and movement and then learning the
patterns in the results. We validated this approach with a pilot study in a
small university rock climbing gym, followed by a large blinded study in a
commercial climbing gym, in cooperation with experienced climbers and expert
route setters. The results show that {\sc Strange Beta} can help a human setter
produce routes that are at least as good as, and in some cases better than,
those produced in the traditional manner.
"
"  Efficient global optimization is the problem of minimizing an unknown
function f, using as few evaluations f(x) as possible. It can be considered as
a continuum-armed bandit problem, with noiseless data and simple regret.
Expected improvement is perhaps the most popular method for solving this
problem; the algorithm performs well in experiments, but little is known about
its theoretical properties. Implementing expected improvement requires a choice
of Gaussian process prior, which determines an associated space of functions,
its reproducing-kernel Hilbert space (RKHS). When the prior is fixed, expected
improvement is known to converge on the minimum of any function in the RKHS. We
begin by providing convergence rates for this procedure. The rates are optimal
for functions of low smoothness, and we modify the algorithm to attain optimal
rates for smoother functions. For practitioners, however, these results are
somewhat misleading. Priors are typically not held fixed, but depend on
parameters estimated from the data. For standard estimators, we show this
procedure may never discover the minimum of f. We then propose alternative
estimators, chosen to minimize the constants in the rate of convergence, and
show these estimators retain the convergence rates of a fixed prior.
"
"  An ongoing challenge in the analysis of document collections is how to
summarize content in terms of a set of inferred themes that can be interpreted
substantively in terms of topics. The current practice of parametrizing the
themes in terms of most frequent words limits interpretability by ignoring the
differential use of words across topics. We argue that words that are both
common and exclusive to a theme are more effective at characterizing topical
content. We consider a setting where professional editors have annotated
documents to a collection of topic categories, organized into a tree, in which
leaf-nodes correspond to the most specific topics. Each document is annotated
to multiple categories, at different levels of the tree. We introduce a
hierarchical Poisson convolution model to analyze annotated documents in this
setting. The model leverages the structure among categories defined by
professional editors to infer a clear semantic description for each topic in
terms of words that are both frequent and exclusive. We carry out a large
randomized experiment on Amazon Turk to demonstrate that topic summaries based
on the FREX score are more interpretable than currently established frequency
based summaries, and that the proposed model produces more efficient estimates
of exclusivity than with currently models. We also develop a parallelized
Hamiltonian Monte Carlo sampler that allows the inference to scale to millions
of documents.
"
"  The expansion of tools against HIV transmission has brought increased
interest in epidemiological models that can predict the impact of these
interventions. The EMOD-HIV model was recently compared to eleven other
independently developed mathematical models of HIV transmission to determine
the extent to which they agree about the potential impact of expanded use of
antiretroviral therapy in South Africa. Here we describe in detail the modeling
methodology used to produce the results in this comparison, which we term
EMOD-HIV v0.7. We include a discussion of the structure and a full list of
model parameters. We also discuss the architecture of the model, and its
potential utility in comparing structural assumptions within a single modeling
framework.
"
"  We propose a scalable, efficient and statistically motivated computational
framework for Graphical Lasso (Friedman et al., 2007b) - a covariance
regularization framework that has received significant attention in the
statistics community over the past few years. Existing algorithms have trouble
in scaling to dimensions larger than a thousand. Our proposal significantly
enhances the state-of-the-art for such moderate sized problems and gracefully
scales to larger problems where other algorithms become practically infeasible.
This requires a few key new ideas. We operate on the primal problem and use a
subtle variation of block-coordinate-methods which drastically reduces the
computational complexity by orders of magnitude. We provide rigorous
theoretical guarantees on the convergence and complexity of our algorithm and
demonstrate the effectiveness of our proposal via experiments. We believe that
our framework extends the applicability of Graphical Lasso to large-scale
modern applications like bioinformatics, collaborative filtering and social
networks, among others.
"
"  In ""Caveats for using statistical significance tests in research
assessments,""--Journal of Informetrics 7(1)(2013) 50-62, available at
arXiv:1112.2516 -- Schneider (2013) focuses on Opthof & Leydesdorff (2010) as
an example of the misuse of statistics in the social sciences. However, our
conclusions are theoretical since they are not dependent on the use of one
statistics or another. We agree with Schneider insofar as he proposes to
develop further statistical instruments (such as effect sizes). Schneider
(2013), however, argues on meta-theoretical grounds against the specification
of uncertainty because, in his opinion, the presence of statistics would
legitimate decision-making. We disagree: uncertainty can also be used for
opening a debate. Scientometric results in which error bars are suppressed for
meta-theoretical reasons should not be trusted.
"
"  Detailed and systematic understanding of the biological effects of millions
of available compounds on living cells is a significant challenge. As most
compounds impact multiple targets and pathways, traditional methods for
analyzing structure-function relationships are not comprehensive enough.
Therefore more advanced integrative models are needed for predicting biological
effects elicited by specific chemical features. As a step towards creating such
computational links we developed a data-driven chemical systems biology
approach to comprehensively study the relationship of 76 structural
3D-descriptors (VolSurf, chemical space) of 1159 drugs with the gene expression
responses (biological space) they elicited in three cancer cell lines. The
analysis covering 11350 genes was based on data from the Connectivity Map. We
decomposed these biological response profiles into components, each linked to a
characteristic chemical descriptor profile. The integrated quantitative
analysis of the chemical and biological spaces was more informative about
protein-target based drug similarity than either dataset separately. We
identified ten major components that link distinct VolSurf features across
multiple compounds to specific biological activity types. For example,
component 2 (hydrophobic properties) strongly links to DNA damage response,
while component 3 (hydrogen bonding) connects to metabolic stress. Individual
structural and biological features were often linked to one cell line only,
such as leukemia cells (HL-60) specifically responding to cardiac glycosides.
In summary, our approach identified specific chemical structures shared across
multiple drugs causing distinct biological responses. The decoding of such
systematic chemical-biological relationships is necessary to build better
models of drug effects, including unidentified types of molecular properties
with strong biological effects.
"
"  In this paper we discuss four problems regarding Markov equivalences for
subclasses of loopless mixed graphs. We classify these four problems as finding
conditions for internal Markov equivalence, which is Markov equivalence within
a subclass, for external Markov equivalence, which is Markov equivalence
between subclasses, for representational Markov equivalence, which is the
possibility of a graph from a subclass being Markov equivalent to a graph from
another subclass, and finding algorithms to generate a graph from a certain
subclass that is Markov equivalent to a given graph. We particularly focus on
the class of maximal ancestral graphs and its subclasses, namely regression
graphs, bidirected graphs, undirected graphs, and directed acyclic graphs, and
present novel results for representational Markov equivalence and algorithms.
"
"  We consider the problem of identifying patterns in a data set that exhibit
anomalous behavior, often referred to as anomaly detection. In most anomaly
detection algorithms, the dissimilarity between data samples is calculated by a
single criterion, such as Euclidean distance. However, in many cases there may
not exist a single dissimilarity measure that captures all possible anomalous
patterns. In such a case, multiple criteria can be defined, and one can test
for anomalies by scalarizing the multiple criteria using a linear combination
of them. If the importance of the different criteria are not known in advance,
the algorithm may need to be executed multiple times with different choices of
weights in the linear combination. In this paper, we introduce a novel
non-parametric multi-criteria anomaly detection method using Pareto depth
analysis (PDA). PDA uses the concept of Pareto optimality to detect anomalies
under multiple criteria without having to run an algorithm multiple times with
different choices of weights. The proposed PDA approach scales linearly in the
number of criteria and is provably better than linear combinations of the
criteria.
"
"  We present a new, efficient method for automatically detecting severe
conflicts `edit wars' in Wikipedia and evaluate this method on six different
language WPs. We discuss how the number of edits, reverts, the length of
discussions, the burstiness of edits and reverts deviate in such pages from
those following the general workflow, and argue that earlier work has
significantly over-estimated the contentiousness of the Wikipedia editing
process.
"
"  The market events of 2007-2009 have reinvigorated the search for realistic
return models that capture greater likelihoods of extreme movements. In this
paper we model the medium-term log-return dynamics in a market with both
fundamental and technical traders. This is based on a Poisson trade arrival
model with variable size orders. With simplifications we are led to a hybrid
SDE mixing both arithmetic and geometric Brownian motions, whose solution is
given by a class of integrals of exponentials of one Brownian motion against
another, in forms considered by Yor and collaborators. The reduction of the
hybrid SDE to a single Brownian motion leads to an SDE of the form considered
by Nagahara, which is a type of ""Pearson diffusion"", or equivalently a
hyperbolic OU SDE. Various dynamics and equilibria are possible depending on
the balance of trades. Under mean-reverting circumstances we arrive naturally
at an equilibrium fat-tailed return distribution with a Student or Pearson Type
IV form. Under less restrictive assumptions richer dynamics are possible,
including bimodal structures. The phenomenon of variance explosion is
identified that gives rise to much larger price movements that might have a
priori been expected, so that ""$25\sigma$"" events are significantly more
probable. We exhibit simple example solutions of the Fokker-Planck equation
that shows how such variance explosion can hide beneath a standard Gaussian
facade. These are elementary members of an extended class of distributions with
a rich and varied structure, capable of describing a wide range of market
behaviours. Several approaches to the density function are possible, and an
example of the computation of a hyperbolic VaR is given. The model also
suggests generalizations of the Bougerol identity.
"
"  Data collection at a massive scale is becoming ubiquitous in a wide variety
of settings, from vast offline databases to streaming real-time information.
Learning algorithms deployed in such contexts must rely on single-pass
inference, where the data history is never revisited. In streaming contexts,
learning must also be temporally adaptive to remain up-to-date against
unforeseen changes in the data generating mechanism. Although rapidly growing,
the online Bayesian inference literature remains challenged by massive data and
transient, evolving data streams. Non-parametric modelling techniques can prove
particularly ill-suited, as the complexity of the model is allowed to increase
with the sample size. In this work, we take steps to overcome these challenges
by porting standard streaming techniques, like data discarding and
downweighting, into a fully Bayesian framework via the use of informative
priors and active learning heuristics. We showcase our methods by augmenting a
modern non-parametric modelling framework, dynamic trees, and illustrate its
performance on a number of practical examples. The end product is a powerful
streaming regression and classification tool, whose performance compares
favourably to the state-of-the-art.
"
"  We generated a dataset of 200 GB with 10^9 features, to test our recent b-bit
minwise hashing algorithms for training very large-scale logistic regression
and SVM. The results confirm our prior work that, compared with the VW hashing
algorithm (which has the same variance as random projections), b-bit minwise
hashing is substantially more accurate at the same storage. For example, with
merely 30 hashed values per data point, b-bit minwise hashing can achieve
similar accuracies as VW with 2^14 hashed values per data point.
  We demonstrate that the preprocessing cost of b-bit minwise hashing is
roughly on the same order of magnitude as the data loading time. Furthermore,
by using a GPU, the preprocessing cost can be reduced to a small fraction of
the data loading time.
  Minwise hashing has been widely used in industry, at least in the context of
search. One reason for its popularity is that one can efficiently simulate
permutations by (e.g.,) universal hashing. In other words, there is no need to
store the permutation matrix. In this paper, we empirically verify this
practice, by demonstrating that even using the simplest 2-universal hashing
does not degrade the learning performance.
"
"  Bayesian networks are now being used in enormous fields, for example,
diagnosis of a system, data mining, clustering and so on. In spite of their
wide range of applications, the statistical properties have not yet been
clarified, because the models are nonidentifiable and non-regular. In a
Bayesian network, the set of its parameter for a smaller model is an analytic
set with singularities in the space of large ones. Because of these
singularities, the Fisher information matrices are not positive definite. In
other words, the mathematical foundation for learning was not constructed. In
recent years, however, we have developed a method to analyze non-regular models
using algebraic geometry. This method revealed the relation between the models
singularities and its statistical properties. In this paper, applying this
method to Bayesian networks with latent variables, we clarify the order of the
stochastic complexities.Our result claims that the upper bound of those is
smaller than the dimension of the parameter space. This means that the Bayesian
generalization error is also far smaller than that of regular model, and that
Schwarzs model selection criterion BIC needs to be improved for Bayesian
networks.
"
"  We present a novel statistical treatment, the ""metastatistics of extreme
events"", for calculating the frequency of extreme events. This approach, which
is of general validity, is the proper statistical framework to address the
problem of data with statistical inhomogeneities. By use of artificial
sequences, we show that the metastatistics produce the correct predictions
while the traditional approach based on the generalized extreme value
distribution does not. An application of the metastatistics methodology to the
case of extreme event to rainfall daily precipitation is also presented.
"
"  A vast amount of textual web streams is influenced by events or phenomena
emerging in the real world. The social web forms an excellent modern paradigm,
where unstructured user generated content is published on a regular basis and
in most occasions is freely distributed. The present Ph.D. Thesis deals with
the problem of inferring information - or patterns in general - about events
emerging in real life based on the contents of this textual stream. We show
that it is possible to extract valuable information about social phenomena,
such as an epidemic or even rainfall rates, by automatic analysis of the
content published in Social Media, and in particular Twitter, using Statistical
Machine Learning methods. An important intermediate task regards the formation
and identification of features which characterise a target event; we select and
use those textual features in several linear, non-linear and hybrid inference
approaches achieving a significantly good performance in terms of the applied
loss function. By examining further this rich data set, we also propose methods
for extracting various types of mood signals revealing how affective norms - at
least within the social web's population - evolve during the day and how
significant events emerging in the real world are influencing them. Lastly, we
present some preliminary findings showing several spatiotemporal
characteristics of this textual information as well as the potential of using
it to tackle tasks such as the prediction of voting intentions.
"
"  Introduction to papers on the modeling and analysis of network data---II
"
"  RNA interference (RNAi) is an endogenous cellular process in which small
double-stranded RNAs lead to the destruction of mRNAs with complementary
nucleoside sequence. With the production of RNAi libraries, large-scale RNAi
screening in human cells can be conducted to identify unknown genes involved in
a biological pathway. One challenge researchers face is how to deal with the
multiple testing issue and the related false positive rate (FDR) and false
negative rate (FNR). This paper proposes a Bayesian hierarchical measurement
error model for the analysis of data from a two-channel RNAi high-throughput
experiment with replicates, in which both the activity of a particular
biological pathway and cell viability are monitored and the goal is to identify
short hair-pin RNAs (shRNAs) that affect the pathway activity without affecting
cell activity. Simulation studies demonstrate the flexibility and robustness of
the Bayesian method and the benefits of having replicates in the experiment.
This method is illustrated through analyzing the data from a RNAi
high-throughput screening that searches for cellular factors affecting HCV
replication without affecting cell viability; comparisons of the results from
this HCV study and some of those reported in the literature are included.
"
"  We propose to describe the variety of galaxies from SDSS by using only one
affine parameter. To this aim, we build the Principal Curve (P-curve) passing
through the spine of the data point cloud, considering the eigenspace derived
from Principal Component Analysis of morphological, physical and photometric
galaxy properties. Thus, galaxies can be labeled, ranked and classified by a
single arc length value of the curve, measured at the unique closest projection
of the data points on the P-curve. We find that the P-curve has a ""W"" letter
shape with 3 turning points, defining 4 branches that represent distinct galaxy
populations. This behavior is controlled mainly by 2 properties, namely u-r and
SFR. We further present the variations of several galaxy properties as a
function of arc length. Luminosity functions variate from steep Schechter fits
at low arc length, to double power law and ending in Log-normal fits at high
arc length. Galaxy clustering shows increasing autocorrelation power at large
scales as arc length increases. PCA analysis allowed to find peculiar galaxy
populations located apart from the main cloud of data points, such as small red
galaxies dominated by a disk, of relatively high stellar mass-to-light ratio
and surface mass density. The P-curve allows not only dimensionality reduction,
but also provides supporting evidence for relevant physical models and
scenarios in extragalactic astronomy: 1) Evidence for the hierarchical merging
scenario in the formation of a selected group of red massive galaxies. These
galaxies present a log-normal r-band luminosity function, which might arise
from multiplicative processes involved in this scenario. 2) Connection between
the onset of AGN activity and star formation quenching, which appears in green
galaxies when transitioning from blue to red populations. (Full abstract in
downloadable version)
"
"  Oracle inequalities and variable selection properties for the Lasso in linear
models have been established under a variety of different assumptions on the
design matrix. We show in this paper how the different conditions and concepts
relate to each other. The restricted eigenvalue condition (Bickel et al., 2009)
or the slightly weaker compatibility condition (van de Geer, 2007) are
sufficient for oracle results. We argue that both these conditions allow for a
fairly general class of design matrices. Hence, optimality of the Lasso for
prediction and estimation holds for more general situations than what it
appears from coherence (Bunea et al, 2007b,c) or restricted isometry (Candes
and Tao, 2005) assumptions.
"
"  In a variety of disciplines such as social sciences, psychology, medicine and
economics, the recorded data are considered to be noisy measurements of latent
variables connected by some causal structure. This corresponds to a family of
graphical models known as the structural equation model with latent variables.
While linear non-Gaussian variants have been well-studied, inference in
nonparametric structural equation models is still underdeveloped. We introduce
a sparse Gaussian process parameterization that defines a non-linear structure
connecting latent variables, unlike common formulations of Gaussian process
latent variable models. The sparse parameterization is given a full Bayesian
treatment without compromising Markov chain Monte Carlo efficiency. We compare
the stability of the sampling procedure and the predictive ability of the model
against the current practice.
"
"  We present some nonparametric methods for graphical modeling. In the discrete
case, where the data are binary or drawn from a finite alphabet, Markov random
fields are already essentially nonparametric, since the cliques can take only a
finite number of values. Continuous data are different. The Gaussian graphical
model is the standard parametric model for continuous data, but it makes
distributional assumptions that are often unrealistic. We discuss two
approaches to building more flexible graphical models. One allows arbitrary
graphs and a nonparametric extension of the Gaussian; the other uses kernel
density estimation and restricts the graphs to trees and forests. Examples of
both methods are presented. We also discuss possible future research directions
for nonparametric graphical modeling.
"
"  Participants in longitudinal studies on the effects of drug treatment and
criminal justice system interventions are at high risk for institutionalization
(e.g., spending time in an environment where their freedom to use drugs, commit
crimes, or engage in risky behavior may be circumscribed). Methods used for
estimating treatment effects in the presence of institutionalization during
follow-up can be highly sensitive to assumptions that are unlikely to be met in
applications and thus likely to yield misleading inferences. In this paper we
consider the use of principal stratification to control for
institutionalization at follow-up. Principal stratification has been suggested
for similar problems where outcomes are unobservable for samples of study
participants because of dropout, death or other forms of censoring. The method
identifies principal strata within which causal effects are well defined and
potentially estimable. We extend the method of principal stratification to
model institutionalization at follow-up and estimate the effect of residential
substance abuse treatment versus outpatient services in a large scale study of
adolescent substance abuse treatment programs. Additionally, we discuss
practical issues in applying the principal stratification model to data. We
show via simulation studies that the model can only recover true effects
provided the data meet strenuous demands and that there must be caution taken
when implementing principal stratification as a technique to control for
post-treatment confounders such as institutionalization.
"
"  A version of indifference valuation of a European call option is proposed
that includes statistical regularities of nonstochastic randomness. Classical
relations (forward contract value and Black-Scholes formula) are obtained as
particular cases. We show that in the general case of nonstochastic randomness
the minimal expected profit of uncovered European option position is always
negative. A version of delta hedge is proposed.
"
"  The beta-Bernoulli process provides a Bayesian nonparametric prior for models
involving collections of binary-valued features. A draw from the beta process
yields an infinite collection of probabilities in the unit interval, and a draw
from the Bernoulli process turns these into binary-valued features. Recent work
has provided stick-breaking representations for the beta process analogous to
the well-known stick-breaking representation for the Dirichlet process. We
derive one such stick-breaking representation directly from the
characterization of the beta process as a completely random measure. This
approach motivates a three-parameter generalization of the beta process, and we
study the power laws that can be obtained from this generalized beta process.
We present a posterior inference algorithm for the beta-Bernoulli process that
exploits the stick-breaking representation, and we present experimental results
for a discrete factor-analysis model.
"
"  Despite rapid advances in experimental cell biology, the in vivo behavior of
hematopoietic stem cells (HSC) cannot be directly observed and measured.
Previously we modeled feline hematopoiesis using a two-compartment hidden
Markov process that had birth and emigration events in the first compartment.
Here we perform Bayesian statistical inference on models which contain two
additional events in the first compartment in order to determine if HSC fate
decisions are linked to cell division or occur independently. Pareto Optimal
Model Assessment approach is used to cross check the estimates from Bayesian
inference. Our results show that HSC must divide symmetrically (i.e., produce
two HSC daughter cells) in order to maintain hematopoiesis. We then demonstrate
that the augmented model that adds asymmetric division events provides a better
fit to the competitive transplantation data, and we thus provide evidence that
HSC fate determination in vivo occurs both in association with cell division
and at a separate point in time. Last we show that assuming each cat has a
unique set of parameters leads to either a significant decrease or a
nonsignificant increase in model fit, suggesting that the kinetic parameters
for HSC are not unique attributes of individual animals, but shared within a
species.
"
"  Interactions among multiple genes across the genome may contribute to the
risks of many complex human diseases. Whole-genome single nucleotide
polymorphisms (SNPs) data collected for many thousands of SNP markers from
thousands of individuals under the case--control design promise to shed light
on our understanding of such interactions. However, nearby SNPs are highly
correlated due to linkage disequilibrium (LD) and the number of possible
interactions is too large for exhaustive evaluation. We propose a novel
Bayesian method for simultaneously partitioning SNPs into LD-blocks and
selecting SNPs within blocks that are associated with the disease, either
individually or interactively with other SNPs. When applied to homogeneous
population data, the method gives posterior probabilities for LD-block
boundaries, which not only result in accurate block partitions of SNPs, but
also provide measures of partition uncertainty. When applied to case--control
data for association mapping, the method implicitly filters out SNP
associations created merely by LD with disease loci within the same blocks.
Simulation study showed that this approach is more powerful in detecting
multi-locus associations than other methods we tested, including one of ours.
When applied to the WTCCC type 1 diabetes data, the method identified many
previously known T1D associated genes, including PTPN22, CTLA4, MHC, and IL2RA.
"
"  Hierarchical statistical models are widely employed in information science
and data engineering. The models consist of two types of variables: observable
variables that represent the given data and latent variables for the
unobservable labels. An asymptotic analysis of the models plays an important
role in evaluating the learning process; the result of the analysis is applied
not only to theoretical but also to practical situations, such as optimal model
selection and active learning. There are many studies of generalization errors,
which measure the prediction accuracy of the observable variables. However, the
accuracy of estimating the latent variables has not yet been elucidated. For a
quantitative evaluation of this, the present paper formulates
distribution-based functions for the errors in the estimation of the latent
variables. The asymptotic behavior is analyzed for both the maximum likelihood
and the Bayes methods.
"
"  Extreme value data with a high clump-at-zero occur in many domains. Moreover,
it might happen that the observed data are either truncated below a given
threshold and/or might not be reliable enough below that threshold because of
the recording devices. These situations occur, in particular, with radio
audience data measured using personal meters that record environmental noise
every minute, that is then matched to one of the several radio programs. There
are therefore genuine zeros for respondents not listening to the radio, but
also zeros corresponding to real listeners for whom the match between the
recorded noise and the radio program could not be achieved. Since radio
audiences are important for radio broadcasters in order, for example, to
determine advertisement price policies, possibly according to the type of
audience at different time points, it is essential to be able to explain not
only the probability of listening to a radio but also the average time spent
listening to the radio by means of the characteristics of the listeners. In
this paper we propose a generalized linear model for zero-inflated truncated
Pareto distribution (ZITPo) that we use to fit audience radio data. Because it
is based on the generalized Pareto distribution, the ZITPo model has nice
properties such as model invariance to the choice of the threshold and from
which a natural residual measure can be derived to assess the model fit to the
data. From a general formulation of the most popular models for zero-inflated
data, we derive our model by considering successively the truncated case, the
generalized Pareto distribution and then the inclusion of covariates to explain
the nonzero proportion of listeners and their average listening time. By means
of simulations, we study the performance of the maximum likelihood estimator
(and derived inference) and use the model to fully analyze the audience data of
a radio station in a certain area of Switzerland.
"
"  We demonstrate an equivalence between reproducing kernel Hilbert space (RKHS)
embeddings of conditional distributions and vector-valued regressors. This
connection introduces a natural regularized loss function which the RKHS
embeddings minimise, providing an intuitive understanding of the embeddings and
a justification for their use. Furthermore, the equivalence allows the
application of vector-valued regression methods and results to the problem of
learning conditional distributions. Using this link we derive a sparse version
of the embedding by considering alternative formulations. Further, by applying
convergence results for vector-valued regression to the embedding problem we
derive minimax convergence rates which are O(\log(n)/n) -- compared to current
state of the art rates of O(n^{-1/4}) -- and are valid under milder and more
intuitive assumptions. These minimax upper rates coincide with lower rates up
to a logarithmic factor, showing that the embedding method achieves nearly
optimal rates. We study our sparse embedding algorithm in a reinforcement
learning task where the algorithm shows significant improvement in sparsity
over an incomplete Cholesky decomposition.
"
"  Sparse modeling is a powerful framework for data analysis and processing.
Traditionally, encoding in this framework is performed by solving an
L1-regularized linear regression problem, commonly referred to as Lasso or
Basis Pursuit. In this work we combine the sparsity-inducing property of the
Lasso model at the individual feature level, with the block-sparsity property
of the Group Lasso model, where sparse groups of features are jointly encoded,
obtaining a sparsity pattern hierarchically structured. This results in the
Hierarchical Lasso (HiLasso), which shows important practical modeling
advantages. We then extend this approach to the collaborative case, where a set
of simultaneously coded signals share the same sparsity pattern at the higher
(group) level, but not necessarily at the lower (inside the group) level,
obtaining the collaborative HiLasso model (C-HiLasso). Such signals then share
the same active groups, or classes, but not necessarily the same active set.
This model is very well suited for applications such as source identification
and separation. An efficient optimization procedure, which guarantees
convergence to the global optimum, is developed for these new models. The
underlying presentation of the new framework and optimization approach is
complemented with experimental examples and theoretical results regarding
recovery guarantees for the proposed models.
"
"  A local bootstrap method is proposed for the analysis of electoral vote-count
first-digit frequencies, complementing the Benford's Law limit. The method is
calibrated on five presidential-election first rounds (2002--2006) and applied
to the 2009 Iranian presidential-election first round. Candidate K has a highly
significant (p< 0.15%) excess of vote counts starting with the digit 7. This
leads to other anomalies, two of which are individually significant at p\sim
0.1%, and one at p\sim 1%. Independently, Iranian pre-election opinion polls
significantly reject the official results unless the five polls favouring
candidate A are considered alone. If the latter represent normalised data and a
linear, least-squares, equal-weighted fit is used, then either candidates R and
K suffered a sudden, dramatic (70%\pm 15%) loss of electoral support just prior
to the election, or the official results are rejected (p\sim 0.01%).
"
"  Penalized regression models are popularly used in high-dimensional data
analysis to conduct variable selection and model fitting simultaneously.
Whereas success has been widely reported in literature, their performances
largely depend on the tuning parameters that balance the trade-off between
model fitting and model sparsity. Existing tuning criteria mainly follow the
route of minimizing the estimated prediction error or maximizing the posterior
model probability, such as cross-validation, AIC and BIC. This article
introduces a general tuning parameter selection criterion based on a novel
concept of variable selection stability. The key idea is to select the tuning
parameters so that the resultant penalized regression model is stable in
variable selection. The asymptotic selection consistency is established for
both fixed and diverging dimensions. The effectiveness of the proposed
criterion is also demonstrated in a variety of simulated examples as well as an
application to the prostate cancer data.
"
"  We consider the problem of jointly estimating the parameters as well as the
structure of binary valued Markov Random Fields, in contrast to earlier work
that focus on one of the two problems. We formulate the problem as a
maximization of $\ell_1$-regularized surrogate likelihood that allows us to
find a sparse solution. Our optimization technique efficiently incorporates the
cutting-plane algorithm in order to obtain a tighter outer bound on the
marginal polytope, which results in improvement of both parameter estimates and
approximation to marginals. On synthetic data, we compare our algorithm on the
two estimation tasks to the other existing methods. We analyze the method in
the high-dimensional setting, where the number of dimensions $p$ is allowed to
grow with the number of observations $n$. The rate of convergence of the
estimate is demonstrated to depend explicitly on the sparsity of the underlying
graph.
"
"  We are settling a longstanding quarrel in quantitative finance by proving the
existence of trends in financial time series thanks to a theorem due to P.
Cartier and Y. Perrin, which is expressed in the language of nonstandard
analysis (Integration over finite sets, F. & M. Diener (Eds): Nonstandard
Analysis in Practice, Springer, 1995, pp. 195--204). Those trends, which might
coexist with some altered random walk paradigm and efficient market hypothesis,
seem nevertheless difficult to reconcile with the celebrated Black-Scholes
model. They are estimated via recent techniques stemming from control and
signal theory. Several quite convincing computer simulations on the forecast of
various financial quantities are depicted. We conclude by discussing the r\^ole
of probability theory.
"
"  In this paper, we propose a signal-selective spectrum sensing method for
cognitive radio networks and specifically targeted for receivers with
multiple-antenna capability. This method is used for detecting the presence or
absence of primary users based on the eigenvalues of the cyclic covariance
matrix of received signals. In particular, the cyclic correlation significance
test is used to detect a specific signal-of-interest by exploiting knowledge of
its cyclic frequencies. The analytical threshold for achieving constant false
alarm rate using this detection method is presented, verified through
simulations, and shown to be independent of both the number of samples used and
the noise variance, effectively eliminating the dependence on accurate noise
estimation. The proposed method is also shown, through numerical simulations,
to outperform existing multiple-antenna cyclostationary-based spectrum sensing
algorithms under a quasi-static Rayleigh fading channel, in both spatially
correlated and uncorrelated noise environments. The algorithm also has
significantly lower computational complexity than these other approaches.
"
"  We show how to reduce the problem of computing VaR and CVaR with Student T
return distributions to evaluation of analytical functions of the moments. This
allows an analysis of the risk properties of systems to be carefully attributed
between choices of risk function (e.g. VaR vs CVaR); choice of return
distribution (power law tail vs Gaussian) and choice of event frequency, for
risk assessment. We exploit this to provide a simple method for portfolio
optimization when the asset returns follow a standard multivariate T
distribution. This may be used as a semi-analytical verification tool for more
general optimizers, and for practical assessment of the impact of fat tails on
asset allocation for shorter time horizons.
"
"  We derive an integral expression for the plane-wave expansion of the
time-varying (nonstationary) random field inside a mode-stirred reverberation
chamber. It is shown that this expansion is a so-called oscillatory process,
whose kernel can be expressed explicitly in closed form. The effect of
nonstationarity is a modulation of the spectral field on a time scale that is a
function of the cavity relaxation time. It is also shown how the contribution
by a nonzero initial value of the field can be incorporated into the expansion.
The results are extended to a special class of second-order processes, relevant
to the perception of a mode-stirred reverberation field by a device under test
with a first-order (relaxation-type) frequency response.
"
"  This paper proposes a nonparametric Bayesian method for exploratory data
analysis and feature construction in continuous time series. Our method focuses
on understanding shared features in a set of time series that exhibit
significant individual variability. Our method builds on the framework of
latent Diricihlet allocation (LDA) and its extension to hierarchical Dirichlet
processes, which allows us to characterize each series as switching between
latent ``topics'', where each topic is characterized as a distribution over
``words'' that specify the series dynamics. However, unlike standard
applications of LDA, we discover the words as we learn the model. We apply this
model to the task of tracking the physiological signals of premature infants;
our model obtains clinically significant insights as well as useful features
for supervised learning tasks.
"
"  We propose directed time series regression, a new approach to estimating
parameters of time-series models for use in certainty equivalent model
predictive control. The approach combines merits of least squares regression
and empirical optimization. Through a computational study involving a
stochastic version of a well known inverted pendulum balancing problem, we
demonstrate that directed time series regression can generate significant
improvements in controller performance over either of the aforementioned
alternatives.
"
"  When using the K-nearest neighbors method, one often ignores uncertainty in
the choice of K. To account for such uncertainty, Holmes and Adams (2002)
proposed a Bayesian framework for K-nearest neighbors (KNN). Their Bayesian KNN
(BKNN) approach uses a pseudo-likelihood function, and standard Markov chain
Monte Carlo (MCMC) techniques to draw posterior samples. Holmes and Adams
(2002) focused on the performance of BKNN in terms of misclassification error
but did not assess its ability to quantify uncertainty. We present some
evidence to show that BKNN still significantly underestimates model
uncertainty.
"
"  Much information available on the web is copied, reused or rephrased. The
phenomenon that multiple web sources pick up certain information is often
called trend. A central problem in the context of web data mining is to detect
those web sources that are first to publish information which will give rise to
a trend. We present a simple and efficient method for finding trends dominating
a pool of web sources and identifying those web sources that publish the
information relevant to a trend before others. We validate our approach on real
data collected from influential technology news feeds.
"
"  The most studies on functional connectivity have been done by analyzing the
brain's hemodynamic response to a stimulation. On the other hand, the
low-frequency spontaneous fluctuations in the blood oxygen level dependent
(BOLD) signals of functional MRI have been observed in the resting state.
However, the BOLD signals in resting state are significantly corrupted by huge
noises arising from cardiac pulsation, respiration, subject motion, scanner,
and so forth. Especially, the noise compounds are stronger in the rat brain
than in the human brain. To overcome such an artifact, we assumed that fractal
behavior in BOLD signals reflects low frequency neural activity, and applied
the theorem such that the wavelet correlation spectrum between long memory
processes is scale-invariant over low frequency scales. Here, we report an
experiment that shows special correlation patterns not only in correlation of
scaling coefficients in very low-frequency band (less than 0.0078Hz) but also
in asymptotic wavelet correlation. In addition, we show the distribution of the
Hurst exponents in the rat brain.
"
"  We study online learnability of a wide class of problems, extending the
results of (Rakhlin, Sridharan, Tewari, 2010) to general notions of performance
measure well beyond external regret. Our framework simultaneously captures such
well-known notions as internal and general Phi-regret, learning with
non-additive global cost functions, Blackwell's approachability, calibration of
forecasters, adaptive regret, and more. We show that learnability in all these
situations is due to control of the same three quantities: a martingale
convergence term, a term describing the ability to perform well if future is
known, and a generalization of sequential Rademacher complexity, studied in
(Rakhlin, Sridharan, Tewari, 2010). Since we directly study complexity of the
problem instead of focusing on efficient algorithms, we are able to improve and
extend many known results which have been previously derived via an algorithmic
construction.
"
"  Sparse coding consists in representing signals as sparse linear combinations
of atoms selected from a dictionary. We consider an extension of this framework
where the atoms are further assumed to be embedded in a tree. This is achieved
using a recently introduced tree-structured sparse regularization norm, which
has proven useful in several applications. This norm leads to regularized
problems that are difficult to optimize, and we propose in this paper efficient
algorithms for solving them. More precisely, we show that the proximal operator
associated with this norm is computable exactly via a dual approach that can be
viewed as the composition of elementary proximal operators. Our procedure has a
complexity linear, or close to linear, in the number of atoms, and allows the
use of accelerated gradient techniques to solve the tree-structured sparse
approximation problem at the same computational cost as traditional ones using
the L1-norm. Our method is efficient and scales gracefully to millions of
variables, which we illustrate in two types of applications: first, we consider
fixed hierarchical dictionaries of wavelets to denoise natural images. Then, we
apply our optimization tools in the context of dictionary learning, where
learned dictionary elements naturally organize in a prespecified arborescent
structure, leading to a better performance in reconstruction of natural image
patches. When applied to text documents, our method learns hierarchies of
topics, thus providing a competitive alternative to probabilistic topic models.
"
"  The predictability of a time series is determined by the sensitivity to
initial conditions of its data generating process. In this paper our goal is to
characterize this sensitivity from a finite sample by assuming few hypotheses
on the data generating model structure. In order to measure the distance
between two trajectories induced by a same noisy chaotic dynamic from two close
initial conditions, a symmetric Kullback-Leiber divergence measure is used. Our
approach allows to take into account the dependence of the residual variance on
initial conditions. We show it is linked to a Fisher information matrix and we
investigated its expressions in the cases of covariance-stationary processes
and ARCH($\infty$) processes. Moreover, we propose a consistent non-parametric
estimator of this sensitivity matrix in the case of conditionally
heteroscedastic autoregressive nonlinear processes. Various statistical
hypotheses can so be tested as for instance the hypothesis that the data
generating process is ""almost"" independently distributed at a given moment.
Applications to simulated data and to the stock market index S&P500 illustrate
our findings. More particularly, we highlight a significant relationship
between the sensitivity to initial conditions of the daily returns of the S&P
500 and their volatility.
"
"  We propose a method to generate a warning system for the early detection of
time clusters applied to public health surveillance data. This new method
relies on the evaluation of a return period associated to any new count of a
particular infection reported to a surveillance system. The method is applied
to Salmonella surveillance in France and compared to the model developed by
Farrington et al.
"
"  This paper addresses estimation in a longitudinal regression model for
association between a scalar outcome and a set of longitudinally-collected
functional covariates or predictor curves. The framework consists of estimating
a time-varying coefficient function that is modeled as a linear combination of
time-invariant functions but having time-varying coefficients. The estimation
procedure exploits the equivalence between penalized least squares estimation
and a linear mixed model representation. The process is empirically evaluated
with several simulations and it is applied to analyze the neurocognitive
impairment of HIV patients and its association with longitudinally-collected
magnetic resonance spectroscopy curves.
"
"  This paper describes a compound Poisson-based random effects structure for
modeling zero-inflated data. Data with large proportion of zeros are found in
many fields of applied statistics, for example in ecology when trying to model
and predict species counts (discrete data) or abundance distributions
(continuous data). Standard methods for modeling such data include mixture and
two-part conditional models. Conversely to these methods, the stochastic models
proposed here behave coherently with regards to a change of scale, since they
mimic the harvesting of a marked Poisson process in the modeling steps. Random
effects are used to account for inhomogeneity. In this paper, model design and
inference both rely on conditional thinking to understand the links between
various layers of quantities : parameters, latent variables including random
effects and zero-inflated observations. The potential of these parsimonious
hierarchical models for zero-inflated data is exemplified using two marine
macroinvertebrate abundance datasets from a large scale scientific bottom-trawl
survey. The EM algorithm with a Monte Carlo step based on importance sampling
is checked for this model structure on a simulated dataset : it proves to work
well for parameter estimation but parameter values matter when re-assessing the
actual coverage level of the confidence regions far from the asymptotic
conditions.
"
"  Regression adjustments are often made to experimental data. Since
randomization does not justify the models, bias is likely; nor are the usual
variance calculations to be trusted. Here, we evaluate regression adjustments
using Neyman's nonparametric model. Previous results are generalized, and more
intuitive proofs are given. A bias term is isolated, and conditions are given
for unbiased estimation in finite samples.
"
"  We consider a binary unsupervised classification problem where each
observation is associated with an unobserved label that we want to retrieve.
More precisely, we assume that there are two groups of observation: normal and
abnormal. The `normal' observations are coming from a known distribution
whereas the distribution of the `abnormal' observations is unknown. Several
models have been developed to fit this unknown distribution. In this paper, we
propose an alternative based on a mixture of Gaussian distributions. The
inference is done within a variational Bayesian framework and our aim is to
infer the posterior probability of belonging to the class of interest. To this
end, it makes no sense to estimate the mixture component number since each
mixture model provides more or less relevant information to the posterior
probability estimation. By computing a weighted average (named aggregated
estimator) over the model collection, Bayesian Model Averaging (BMA) is one way
of combining models in order to account for information provided by each model.
The aim is then the estimation of the weights and the posterior probability for
one specific model. In this work, we derive optimal approximations of these
quantities from the variational theory and propose other approximations of the
weights. To perform our method, we consider that the data are dependent
(Markovian dependency) and hence we consider a Hidden Markov Model. A
simulation study is carried out to evaluate the accuracy of the estimates in
terms of classification. We also present an application to the analysis of
public health surveillance systems.
"
"  There are various methods to analyze different kinds of data sets. Spatial
data is defined when data is dependent on each other based on their respective
locations. Spline and Kriging are two methods for interpolating and predicting
spatial data. Under certain conditions, these methods are equivalent, but in
practice they show different behaviors. Amount of data can be observed only at
some positions that are chosen as positions of sample points, therefore,
prediction of data values in other positions is important. In this paper, the
link between Spline and Kriging methods is described, then for an
epidemiological two dimensional real data set, data is observed in geological
longitude and in latitude dimensions, and behavior of these methods are
investigated. Comparison of these performances show that for this data set,
Kriging method has a better performance than Spline method.
"
"  The problem of learning tree-structured Gaussian graphical models from
independent and identically distributed (i.i.d.) samples is considered. The
influence of the tree structure and the parameters of the Gaussian distribution
on the learning rate as the number of samples increases is discussed.
Specifically, the error exponent corresponding to the event that the estimated
tree structure differs from the actual unknown tree structure of the
distribution is analyzed. Finding the error exponent reduces to a least-squares
problem in the very noisy learning regime. In this regime, it is shown that the
extremal tree structure that minimizes the error exponent is the star for any
fixed set of correlation coefficients on the edges of the tree. If the
magnitudes of all the correlation coefficients are less than 0.63, it is also
shown that the tree structure that maximizes the error exponent is the Markov
chain. In other words, the star and the chain graphs represent the hardest and
the easiest structures to learn in the class of tree-structured Gaussian
graphical models. This result can also be intuitively explained by correlation
decay: pairs of nodes which are far apart, in terms of graph distance, are
unlikely to be mistaken as edges by the maximum-likelihood estimator in the
asymptotic regime.
"
"  We give a simple proof of Bell's inequality in quantum mechanics which, in
conjunction with experiments, demonstrates that the local hidden variables
assumption is false. The proof sheds light on relationships between the notion
of causal interaction and interference between particles.
"
"  A classical condition for fast learning rates is the margin condition, first
introduced by Mammen and Tsybakov. We tackle in this paper the problem of
adaptivity to this condition in the context of model selection, in a general
learning framework. Actually, we consider a weaker version of this condition
that allows one to take into account that learning within a small model can be
much easier than within a large one. Requiring this ""strong margin adaptivity""
makes the model selection problem more challenging. We first prove, in a
general framework, that some penalization procedures (including local
Rademacher complexities) exhibit this adaptivity when the models are nested.
Contrary to previous results, this holds with penalties that only depend on the
data. Our second main result is that strong margin adaptivity is not always
possible when the models are not nested: for every model selection procedure
(even a randomized one), there is a problem for which it does not demonstrate
strong margin adaptivity.
"
"  Score matching is a recently developed parameter learning method that is
particularly effective to complicated high dimensional density models with
intractable partition functions. In this paper, we study two issues that have
not been completely resolved for score matching. First, we provide a formal
link between maximum likelihood and score matching. Our analysis shows that
score matching finds model parameters that are more robust with noisy training
data. Second, we develop a generalization of score matching. Based on this
generalization, we further demonstrate an extension of score matching to models
of discrete data.
"
"  We unify f-divergences, Bregman divergences, surrogate loss bounds (regret
bounds), proper scoring rules, matching losses, cost curves, ROC-curves and
information. We do this by systematically studying integral and variational
representations of these objects and in so doing identify their primitives
which all are related to cost-sensitive binary classification. As well as
clarifying relationships between generative and discriminative views of
learning, the new machinery leads to tight and more general surrogate loss
bounds and generalised Pinsker inequalities relating f-divergences to
variational divergence. The new viewpoint illuminates existing algorithms: it
provides a new derivation of Support Vector Machines in terms of divergences
and relates Maximum Mean Discrepancy to Fisher Linear Discriminants. It also
suggests new techniques for estimating f-divergences.
"
"  I propose a path integral description of the Su-Schrieffer-Heeger
Hamiltonian, both in one and two dimensions, after mapping the real space model
onto the time scale. While the lattice degrees of freedom are classical
functions of time and are integrated out exactly, the electron particle paths
are treated quantum mechanically. The method accounts for the variable range of
the electronic hopping processes. The free energy of the system and its
temperature derivatives are computed by summing at any $T$ over the ensemble of
relevant particle paths which mainly contribute to the total partition
function. In the low $T$ regime, the {\it heat capacity over T} ratio shows un
upturn peculiar to a glass-like behavior. This feature is more sizeable in the
square lattice than in the linear chain as the overall hopping potential
contribution to the total action is larger in higher dimensionality. The
effects of the electron-phonon anharmonic interactions on the phonon subsystem
are studied by the path integral cumulant expansion method.
"
"  Financial markets, with their vast range of different investment
opportunities, can be seen as a system of many different simultaneous games
with diverse and often unknown levels of risk and reward. We introduce
generalizations to the classic Kelly investment game [Kelly (1956)] that
incorporates these features, and use them to investigate the influence of
diversification and limited information on Kelly-optimal portfolios. In
particular we present approximate formulas for optimizing diversified
portfolios and exact results for optimal investment in unknown games where the
only available information is past outcomes.
"
"  When a defendant's DNA matches a sample found at a crime scene, how
compelling is the match? To answer this question, DNA analysts typically use
relative frequencies, random-match probabilities or likelihood ratios. They
compute these quantities for the major racial or ethnic groups in the United
States, supplying prosecutors with such mind-boggling figures as ``one in nine
hundred and fifty sextillion African Americans, one in one hundred and thirty
septillion Caucasians, and one in nine hundred and thirty sextillion
Hispanics."" In People v. Prince, a California Court of Appeals rejected this
practice on the theory that only the perpetrator's race is relevant to the
crime; hence, it is impermissible to introduce statistics about other races.
This paper critiques this reasoning. Relying on the concept of likelihood, it
presents a logical justification for referring to a range of races and
identifies some problems with the one-race-only rule. The paper also notes some
ways to express the probative value of a DNA match quantitatively without
referring to variations in DNA profile frequencies among races or ethnic
groups.
"
"  Based on World Health Organization (WHO) fact sheet in the 2011, outbreaks of
poultry diseases especially Avian Influenza in poultry may raise global public
health concerns due to their effect on poultry populations, their potential to
cause serious disease in people, and their pandemic potential. In this
research, we built a Poultry Diseases Expert System using Dempster-Shafer
Theory. In this Poultry Diseases Expert System We describe five symptoms which
include depression, combs, wattle, bluish face region, swollen face region,
narrowness of eyes, and balance disorders. The result of the research is that
Poultry Diseases Expert System has been successfully identifying poultry
diseases.
"
"  We present two Bayesian procedures to infer the interactions and external
currents in an assembly of stochastic integrate-and-fire neurons from the
recording of their spiking activity. The first procedure is based on the exact
calculation of the most likely time courses of the neuron membrane potentials
conditioned by the recorded spikes, and is exact for a vanishing noise variance
and for an instantaneous synaptic integration. The second procedure takes into
account the presence of fluctuations around the most likely time courses of the
potentials, and can deal with moderate noise levels. The running time of both
procedures is proportional to the number S of spikes multiplied by the squared
number N of neurons. The algorithms are validated on synthetic data generated
by networks with known couplings and currents. We also reanalyze previously
published recordings of the activity of the salamander retina (including from
32 to 40 neurons, and from 65,000 to 170,000 spikes). We study the dependence
of the inferred interactions on the membrane leaking time; the differences and
similarities with the classical cross-correlation analysis are discussed.
"
"  Data on functional disability are of widespread policy interest in the United
States, especially with respect to planning for Medicare and Social Security
for a growing population of elderly adults. We consider an extract of
functional disability data from the National Long Term Care Survey (NLTCS) and
attempt to develop disability profiles using variations of the Grade of
Membership (GoM) model. We first describe GoM as an individual-level mixture
model that allows individuals to have partial membership in several mixture
components simultaneously. We then prove the equivalence between
individual-level and population-level mixture models, and use this property to
develop a Markov Chain Monte Carlo algorithm for Bayesian estimation of the
model. We use our approach to analyze functional disability data from the
NLTCS.
"
"  This paper deals with the problem of estimating the volume of the excursion
set of a function $f:\mathbb{R}^d \to \mathbb{R}$ above a given threshold,
under a probability measure on $\mathbb{R}^d$ that is assumed to be known. In
the industrial world, this corresponds to the problem of estimating a
probability of failure of a system. When only an expensive-to-simulate model of
the system is available, the budget for simulations is usually severely limited
and therefore classical Monte Carlo methods ought to be avoided. One of the
main contributions of this article is to derive SUR (stepwise uncertainty
reduction) strategies from a Bayesian-theoretic formulation of the problem of
estimating a probability of failure. These sequential strategies use a Gaussian
process model of $f$ and aim at performing evaluations of $f$ as efficiently as
possible to infer the value of the probability of failure. We compare these
strategies to other strategies also based on a Gaussian process model for
estimating a probability of failure.
"
"  Inference in general Markov random fields (MRFs) is NP-hard, though
identifying the maximum a posteriori (MAP) configuration of pairwise MRFs with
submodular cost functions is efficiently solvable using graph cuts. Marginal
inference, however, even for this restricted class, is in #P. We prove new
formulations of derivatives of the Bethe free energy, provide bounds on the
derivatives and bracket the locations of stationary points, introducing a new
technique called Bethe bound propagation. Several results apply to pairwise
models whether associative or not. Applying these to discretized
pseudo-marginals in the associative case we present a polynomial time
approximation scheme for global optimization provided the maximum degree is
$O(\log n)$, and discuss several extensions.
"
"  This paper addresses the problem of segmenting a time-series with respect to
changes in the mean value or in the variance. The first case is when the time
data is modeled as a sequence of independent and normal distributed random
variables with unknown, possibly changing, mean value but fixed variance. The
main assumption is that the mean value is piecewise constant in time, and the
task is to estimate the change times and the mean values within the segments.
The second case is when the mean value is constant, but the variance can
change. The assumption is that the variance is piecewise constant in time, and
we want to estimate change times and the variance values within the segments.
To find solutions to these problems, we will study an l_1 regularized maximum
likelihood method, related to the fused lasso method and l_1 trend filtering,
where the parameters to be estimated are free to vary at each sample. To
penalize variations in the estimated parameters, the l_1-norm of the time
difference of the parameters is used as a regularization term. This idea is
closely related to total variation denoising. The main contribution is that a
convex formulation of this variance estimation problem, where the
parametrization is based on the inverse of the variance, can be formulated as a
certain l_1 mean estimation problem. This implies that results and methods for
mean estimation can be applied to the challenging problem of variance
segmentation/estimation.
"
"  The valuation of real estates (e.g., house, land, among others) is of extreme
importance for decision making. Their singular characteristics make valuation
through hedonic pricing methods dificult since the theory does not specify the
correct regression functional form nor which explanatory variables should be
included in the hedonic equation. In this article we perform real estate
appraisal using a class of regression models proposed by Rigby & Stasinopoulos
(2005): generalized additive models for location, scale and shape (GAMLSS). Our
empirical analysis shows that these models seem to be more appropriate for
estimation of the hedonic prices function than the regression models currently
used to that end.
"
"  The variance of the concentration in a sample can be estimated using
knowledge of the particle masses, concentrations and the parameter for the
dependent selection of particles. A number of variance estimators are
constructed including a class of hybrid estimators.
"
"  A wide class of regularization problems in machine learning and statistics
employ a regularization term which is obtained by composing a simple convex
function \omega with a linear transformation. This setting includes Group Lasso
methods, the Fused Lasso and other total variation methods, multi-task learning
methods and many more. In this paper, we present a general approach for
computing the proximity operator of this class of regularizers, under the
assumption that the proximity operator of the function \omega is known in
advance. Our approach builds on a recent line of research on optimal first
order optimization methods and uses fixed point iterations for numerically
computing the proximity operator. It is more general than current approaches
and, as we show with numerical simulations, computationally more efficient than
available first order methods which do not achieve the optimal rate. In
particular, our method outperforms state of the art O(1/T) methods for
overlapping Group Lasso and matches optimal O(1/T^2) methods for the Fused
Lasso and tree structured Group Lasso.
"
"  In most countries CD4+ cell counts are still used for deciding when to start
HIV-positive people on anti-retroviral therapy. However, various CD4+
thresholds, 200, 350 or 500/\muL, are chosen arbitrarily and for historical
reasons. Here we consider the optimal CD4+ threshold at which asymptomatic
HIV-positive people living in Botswana, South Africa and Zimbabwe should start
treatment depending on their prognosis given their CD4+ cell counts or viral
load. We also examine the optimal interval at which people should be retested
if they are HIV-negative. This analysis shows that while the use of CD4+ cell
counts or viral load tests could have been useful in deciding how to triage
patients for treatment at the start of the epidemic this is no longer the case
except possibly for those aged about 15 to 25 years. In order not to do harm to
individual patients everyone should be started on ART as soon as they are found
to be HIV-positive.
"
"  The Garman-Klass unbiased estimator of the variance per unit time of a
zero-drift Brownian Motion B, based on the usual financial data that reports
for time windows of equal length the open (OPEN), minimum (MIN), maximum (MAX)
and close (CLOSE) values, is quadratic in the statistic S1=(CLOSE-OPEN,
OPEN-MIN, MAX-OPEN). This estimator, with efficiency 7.4 with respect to the
classical estimator (CLOSE-OPEN)^2, is widely believed to be of minimal
variance. The current report disproves this belief by exhibiting an unbiased
estimator with slightly but strictly higher efficiency 7.7322. The essence of
the improvement lies in the observation that the data should be compressed to
the statistic S2 defined on W(t)= B(0)+[B(t)-B(0)]sign[(B(1)-B(0)] as S1 was
defined on the Brownian path B(t). The best S2-based quadratic unbiased
estimator is presented explicitly. The Cramer-Rao upper bound for the
efficiency of unbiased estimators, corresponding to the efficiency of
large-sample Maximum Likelihood estimators, is 8.471. This bound cannot be
attained because the distribution is not of exponential type. Regression-fitted
quadratic functions of S2 (with mean 1) markedly out-perform those of S1 when
applied to random walks with heavy-tail-distributed increments. Performance is
empirically studied in terms of the tail parameter.
"
"  We present and prove properties of a new offline policy evaluator for an
exploration learning setting which is superior to previous evaluators. In
particular, it simultaneously and correctly incorporates techniques from
importance weighting, doubly robust evaluation, and nonstationary policy
evaluation approaches. In addition, our approach allows generating longer
histories by careful control of a bias-variance tradeoff, and further decreases
variance by incorporating information about randomness of the target policy.
Empirical evidence from synthetic and realworld exploration learning problems
shows the new evaluator successfully unifies previous approaches and uses
information an order of magnitude more efficiently.
"
"  The goal of this paper is to develop an adjusted plus-minus statistic for NHL
players that is independent of both teammates and opponents. We use data from
the shift reports on NHL.com in a weighted least squares regression to estimate
an NHL player's effect on his team's success in scoring and preventing goals at
even strength. Both offensive and defensive components of adjusted plus-minus
are given, estimates in terms of goals per 60 minutes and goals per season are
given, and estimates for forwards, defensemen, and goalies are given.
"
"  Random utility theory models an agent's preferences on alternatives by
drawing a real-valued score on each alternative (typically independently) from
a parameterized distribution, and then ranking the alternatives according to
scores. A special case that has received significant attention is the
Plackett-Luce model, for which fast inference methods for maximum likelihood
estimators are available. This paper develops conditions on general random
utility models that enable fast inference within a Bayesian framework through
MC-EM, providing concave loglikelihood functions and bounded sets of global
maxima solutions. Results on both real-world and simulated data provide support
for the scalability of the approach and capability for model selection among
general random utility models including Plackett-Luce.
"
"  Singular Spectrum Analysis (SSA) as a tool for analysis and forecasting of
time series is considered. The main features of the Rssa package, which
implements the SSA algorithms and methodology in R, are described and examples
of its use are presented. Analysis, forecasting and parameter estimation are
demonstrated by means of case study with an accompanying code in R.
"
"  Identifying viral pathogens and characterizing their transmission is
essential to developing effective public health measures in response to a
pandemic. Phylogenetics, though currently the most popular tool used to
characterize the likely host of a virus, can be ambiguous when studying species
very distant to known species and when there is very little reliable sequence
information available in the early stages of the pandemic. Motivated by an
existing framework for representing biological sequence information, we learn
sparse, tree-structured models, built from decision rules based on
subsequences, to predict viral hosts from protein sequence data using popular
discriminative machine learning tools. Furthermore, the predictive motifs
robustly selected by the learning algorithm are found to show strong
host-specificity and occur in highly conserved regions of the viral proteome.
"
"  Compliance to standardized highway design criteria is considered essential to
ensure the roadway safety. However, for a variety of reasons, situations arise
where exceptions to standard-design criteria are requested and accepted after
review. This research explores the impact that design exceptions have on the
accident severity and accident frequency in Indiana. Data on accidents at
roadway sites with and without design exceptions are used to estimate
appropriate statistical models for the frequency and severity accidents at
these sites using some of the most recent statistical advances with mixing
distributions. The results of the modeling process show that presence of
approved design exceptions has not had a statistically significant effect on
the average frequency or severity of accidents -- suggesting that current
procedures for granting design exceptions have been sufficiently rigorous to
avoid adverse safety impacts.
"
"  This paper presents an improvement to model learning when using multi-class
LogitBoost for classification. Motivated by the statistical view, LogitBoost
can be seen as additive tree regression. Two important factors in this setting
are: 1) coupled classifier output due to a sum-to-zero constraint, and 2) the
dense Hessian matrices that arise when computing tree node split gain and node
value fittings. In general, this setting is too complicated for a tractable
model learning algorithm. However, too aggressive simplification of the setting
may lead to degraded performance. For example, the original LogitBoost is
outperformed by ABC-LogitBoost due to the latter's more careful treatment of
the above two factors.
  In this paper we propose techniques to address the two main difficulties of
the LogitBoost setting: 1) we adopt a vector tree (i.e. each node value is
vector) that enforces a sum-to-zero constraint, and 2) we use an adaptive block
coordinate descent that exploits the dense Hessian when computing tree split
gain and node values. Higher classification accuracy and faster convergence
rates are observed for a range of public data sets when compared to both the
original and the ABC-LogitBoost implementations.
"
"  Used to estimate the risk of an estimator or to perform model selection,
cross-validation is a widespread strategy because of its simplicity and its
apparent universality. Many results exist on the model selection performances
of cross-validation procedures. This survey intends to relate these results to
the most recent advances of model selection theory, with a particular emphasis
on distinguishing empirical statements from rigorous theoretical results. As a
conclusion, guidelines are provided for choosing the best cross-validation
procedure according to the particular features of the problem in hand.
"
"  We describe a network clustering framework, based on finite mixture models,
that can be applied to discrete-valued networks with hundreds of thousands of
nodes and billions of edge variables. Relative to other recent model-based
clustering work for networks, we introduce a more flexible modeling framework,
improve the variational-approximation estimation algorithm, discuss and
implement standard error estimation via a parametric bootstrap approach, and
apply these methods to much larger data sets than those seen elsewhere in the
literature. The more flexible framework is achieved through introducing novel
parameterizations of the model, giving varying degrees of parsimony, using
exponential family models whose structure may be exploited in various
theoretical and algorithmic ways. The algorithms are based on variational
generalized EM algorithms, where the E-steps are augmented by a
minorization-maximization (MM) idea. The bootstrapped standard error estimates
are based on an efficient Monte Carlo network simulation idea. Last, we
demonstrate the usefulness of the model-based clustering framework by applying
it to a discrete-valued network with more than 131,000 nodes and 17 billion
edge variables.
"
"  We describe an expert system, MAIES, developed for analysing forensic
identification problems involving DNA mixture traces using quantitative peak
area information. Peak area information is represented by conditional Gaussian
distributions, and inference based on exact junction tree propagation
ascertains whether individuals, whose profiles have been measured, have
contributed to the mixture. The system can also be used to predict DNA profiles
of unknown contributors by separating the mixture into its individual
components. The use of the system is illustrated with an application to a real
world example. The system implements a novel MAP (maximum a posteriori) search
algorithm that is described in an appendix.
"
"  In sparse Bayesian learning (SBL), Gaussian scale mixtures (GSMs) have been
used to model sparsity-inducing priors that realize a class of concave penalty
functions for the regression task in real-valued signal models. Motivated by
the relative scarcity of formal tools for SBL in complex-valued models, this
paper proposes a GSM model - the Bessel K model - that induces concave penalty
functions for the estimation of complex sparse signals. The properties of the
Bessel K model are analyzed when it is applied to Type I and Type II
estimation. This analysis reveals that, by tuning the parameters of the mixing
pdf different penalty functions are invoked depending on the estimation type
used, the value of the noise variance, and whether real or complex signals are
estimated. Using the Bessel K model, we derive a sparse estimator based on a
modification of the expectation-maximization algorithm formulated for Type II
estimation. The estimator includes as a special instance the algorithms
proposed by Tipping and Faul [1] and by Babacan et al. [2]. Numerical results
show the superiority of the proposed estimator over these state-of-the-art
estimators in terms of convergence speed, sparseness, reconstruction error, and
robustness in low and medium signal-to-noise ratio regimes.
"
"  Optimization of complex functions, such as the output of computer simulators,
is a difficult task that has received much attention in the literature. A less
studied problem is that of optimization under unknown constraints, i.e., when
the simulator must be invoked both to determine the typical real-valued
response and to determine if a constraint has been violated, either for
physical or policy reasons. We develop a statistical approach based on Gaussian
processes and Bayesian learning to both approximate the unknown function and
estimate the probability of meeting the constraints. A new integrated
improvement criterion is proposed to recognize that responses from inputs that
violate the constraint may still be informative about the function, and thus
could potentially be useful in the optimization. The new criterion is
illustrated on synthetic data, and on a motivating optimization problem from
health care policy.
"
"  Support Vector Machines, SVMs, and the Large Margin Nearest Neighbor
algorithm, LMNN, are two very popular learning algorithms with quite different
learning biases. In this paper we bring them into a unified view and show that
they have a much stronger relation than what is commonly thought. We analyze
SVMs from a metric learning perspective and cast them as a metric learning
problem, a view which helps us uncover the relations of the two algorithms. We
show that LMNN can be seen as learning a set of local SVM-like models in a
quadratic space. Along the way and inspired by the metric-based interpretation
of SVM s we derive a novel variant of SVMs, epsilon-SVM, to which LMNN is even
more similar. We give a unified view of LMNN and the different SVM variants.
Finally we provide some preliminary experiments on a number of benchmark
datasets in which show that epsilon-SVM compares favorably both with respect to
LMNN and SVM.
"
"  Agents learning to act autonomously in real-world domains must acquire a
model of the dynamics of the domain in which they operate. Learning domain
dynamics can be challenging, especially where an agent only has partial access
to the world state, and/or noisy external sensors. Even in standard STRIPS
domains, existing approaches cannot learn from noisy, incomplete observations
typical of real-world domains. We propose a method which learns STRIPS action
models in such domains, by decomposing the problem into first learning a
transition function between states in the form of a set of classifiers, and
then deriving explicit STRIPS rules from the classifiers' parameters. We
evaluate our approach on simulated standard planning domains from the
International Planning Competition, and show that it learns useful domain
descriptions from noisy, incomplete observations.
"
"  We propose a novel algorithm for greedy forward feature selection for
regularized least-squares (RLS) regression and classification, also known as
the least-squares support vector machine or ridge regression. The algorithm,
which we call greedy RLS, starts from the empty feature set, and on each
iteration adds the feature whose addition provides the best leave-one-out
cross-validation performance. Our method is considerably faster than the
previously proposed ones, since its time complexity is linear in the number of
training examples, the number of features in the original data set, and the
desired size of the set of selected features. Therefore, as a side effect we
obtain a new training algorithm for learning sparse linear RLS predictors which
can be used for large scale learning. This speed is possible due to matrix
calculus based short-cuts for leave-one-out and feature addition. We
experimentally demonstrate the scalability of our algorithm and its ability to
find good quality feature sets.
"
"  In this article we identify social communities among gang members in the
Hollenbeck policing district in Los Angeles, based on sparse observations of a
combination of social interactions and geographic locations of the individuals.
This information, coming from LAPD Field Interview cards, is used to construct
a similarity graph for the individuals. We use spectral clustering to identify
clusters in the graph, corresponding to communities in Hollenbeck, and compare
these with the LAPD's knowledge of the individuals' gang membership. We discuss
different ways of encoding the geosocial information using a graph structure
and the influence on the resulting clusterings. Finally we analyze the
robustness of this technique with respect to noisy and incomplete data, thereby
providing suggestions about the relative importance of quantity versus quality
of collected data.
"
"  In this article, we analyze the SPICE method developed in [1], and establish
its connections with other standard sparse estimation methods such as the Lasso
and the LAD-Lasso. This result positions SPICE as a computationally efficient
technique for the calculation of Lasso-type estimators. Conversely, this
connection is very useful for establishing the asymptotic properties of SPICE
under several problem scenarios and for suggesting suitable modifications in
cases where the naive version of SPICE would not work.
"
"  Clusters traverse a gas and collide with gas particles. The gas particles are
adsorbed and the clusters become hosts. If the clusters are size selected, the
number of guests will be Poisson distributed. We review this by showcasing four
laboratory procedures that all rely on the validity of the Poisson model. The
effects of a statistical distribution of the cluster sizes in a beam of
clusters are discussed. We derive the average collision rates. Additionally, we
present Poisson mixture models that involve also standard deviations. We derive
the collision statistics for common size distributions of hosts and also for
some generalizations thereof. The models can be applied to large noble gas
clusters traversing doping gas. While outlining how to fit a generalized
Poisson to the statistics, we still find even these Poisson models to be often
insufficient.
"
"  Regression splines are smooth, flexible, and parsimonious nonparametric
function estimators. They are known to be sensitive to knot number and
placement, but if assumptions such as monotonicity or convexity may be imposed
on the regression function, the shape-restricted regression splines are robust
to knot choices. Monotone regression splines were introduced by Ramsay
[Statist. Sci. 3 (1998) 425--461], but were limited to quadratic and lower
order. In this paper an algorithm for the cubic monotone case is proposed, and
the method is extended to convex constraints and variants such as
increasing-concave. The restricted versions have smaller squared error loss
than the unrestricted splines, although they have the same convergence rates.
The relatively small degrees of freedom of the model and the insensitivity of
the fits to the knot choices allow for practical inference methods; the
computational efficiency allows for back-fitting of additive models. Tests of
constant versus increasing and linear versus convex regression function, when
implemented with shape-restricted regression splines, have higher power than
the standard version using ordinary shape-restricted regression.
"
"  Unbiased, label-free proteomics is becoming a powerful technique for
measuring protein expression in almost any biological sample. The output of
these measurements after preprocessing is a collection of features and their
associated intensities for each sample. Subsets of features within the data are
from the same peptide, subsets of peptides are from the same protein, and
subsets of proteins are in the same biological pathways, therefore, there is
the potential for very complex and informative correlational structure inherent
in these data. Recent attempts to utilize this data often focus on the
identification of single features that are associated with a particular
phenotype that is relevant to the experiment. However, to date, there have been
no published approaches that directly model what we know to be multiple
different levels of correlation structure. Here we present a hierarchical
Bayesian model which is specifically designed to model such correlation
structure in unbiased, label-free proteomics. This model utilizes partial
identification information from peptide sequencing and database lookup as well
as the observed correlation in the data to appropriately compress features into
latent proteins and to estimate their correlation structure. We demonstrate the
effectiveness of the model using artificial/benchmark data and in the context
of a series of proteomics measurements of blood plasma from a collection of
volunteers who were infected with two different strains of viral influenza.
"
"  Research output and impact is currently the focus of serious debate
worldwide. Quantitative analyses based on a wide spectrum of indices indicate a
clear advantage of US institutions as compared to institutions in Europe and
the rest of the world. However the measures used to quantify research
performance are mostly static: Even though research output is the result of a
process that extends in time as well as in space, indices often only take into
account the current affiliation when assigning influential research to
institutions. In this paper, we focus on the field of mathematics and
investigate whether the image that emerges from static indices persists when
bringing in more dynamic information, through the study of the ""trajectories""
of highly cited mathematicians: birthplace, country of first degree, country of
PhD and current affiliation. While the dominance of the US remains apparent,
some interesting patterns -that perhaps explain this dominance- emerge.
"
"  We propose a method for inferring the existence of a latent common cause
('confounder') of two observed random variables. The method assumes that the
two effects of the confounder are (possibly nonlinear) functions of the
confounder plus independent, additive noise. We discuss under which conditions
the model is identifiable (up to an arbitrary reparameterization of the
confounder) from the joint distribution of the effects. We state and prove a
theoretical result that provides evidence for the conjecture that the model is
generically identifiable under suitable technical conditions. In addition, we
propose a practical method to estimate the confounder from a finite i.i.d.
sample of the effects and illustrate that the method works well on both
simulated and real-world data.
"
"  We discuss multi-task online learning when a decision maker has to deal
simultaneously with M tasks. The tasks are related, which is modeled by
imposing that the M-tuple of actions taken by the decision maker needs to
satisfy certain constraints. We give natural examples of such restrictions and
then discuss a general class of tractable constraints, for which we introduce
computationally efficient ways of selecting actions, essentially by reducing to
an on-line shortest path problem. We briefly discuss ""tracking"" and ""bandit""
versions of the problem and extend the model in various ways, including
non-additive global losses and uncountably infinite sets of tasks.
"
"  Metrics specifying distances between data points can be learned in a
discriminative manner or from generative models. In this paper, we show how to
unify generative and discriminative learning of metrics via a kernel learning
framework. Specifically, we learn local metrics optimized from parametric
generative models. These are then used as base kernels to construct a global
kernel that minimizes a discriminative training criterion. We consider both
linear and nonlinear combinations of local metric kernels. Our empirical
results show that these combinations significantly improve performance on
classification tasks. The proposed learning algorithm is also very efficient,
achieving order of magnitude speedup in training time compared to previous
discriminative baseline methods.
"
"  Due to myriads of classes, designing accurate and efficient classifiers
becomes very challenging for multi-class classification. Recent research has
shown that class structure learning can greatly facilitate multi-class
learning. In this paper, we propose a novel method to learn the class structure
for multi-class classification problems. The class structure is assumed to be a
binary hierarchical tree. To learn such a tree, we propose a maximum separating
margin method to determine the child nodes of any internal node. The proposed
method ensures that two classgroups represented by any two sibling nodes are
most separable. In the experiments, we evaluate the accuracy and efficiency of
the proposed method over other multi-class classification methods on real world
large-scale problems. The results show that the proposed method outperforms
benchmark methods in terms of accuracy for most datasets and performs
comparably with other class structure learning methods in terms of efficiency
for all datasets.
"
"  Conformal prediction uses past experience to determine precise levels of
confidence in new predictions. Given an error probability $\epsilon$, together
with a method that makes a prediction $\hat{y}$ of a label $y$, it produces a
set of labels, typically containing $\hat{y}$, that also contains $y$ with
probability $1-\epsilon$. Conformal prediction can be applied to any method for
producing $\hat{y}$: a nearest-neighbor method, a support-vector machine, ridge
regression, etc.
  Conformal prediction is designed for an on-line setting in which labels are
predicted successively, each one being revealed before the next is predicted.
The most novel and valuable feature of conformal prediction is that if the
successive examples are sampled independently from the same distribution, then
the successive predictions will be right $1-\epsilon$ of the time, even though
they are based on an accumulating dataset rather than on independent datasets.
  In addition to the model under which successive examples are sampled
independently, other on-line compression models can also use conformal
prediction. The widely used Gaussian linear model is one of these.
  This tutorial presents a self-contained account of the theory of conformal
prediction and works through several numerical examples. A more comprehensive
treatment of the topic is provided in ""Algorithmic Learning in a Random World"",
by Vladimir Vovk, Alex Gammerman, and Glenn Shafer (Springer, 2005).
"
"  Recently it has become popular to learn sparse Gaussian graphical models
(GGMs) by imposing l1 or group l1,2 penalties on the elements of the precision
matrix. Thispenalized likelihood approach results in a tractable convex
optimization problem. In this paper, we reinterpret these results as performing
MAP estimation under a novel prior which we call the group l1 and l1,2
positivedefinite matrix distributions. This enables us to build a hierarchical
model in which the l1 regularization terms vary depending on which group the
entries are assigned to, which in turn allows us to learn block structured
sparse GGMs with unknown group assignments. Exact inference in this
hierarchical model is intractable, due to the need to compute the normalization
constant of these matrix distributions. However, we derive upper bounds on the
partition functions, which lets us use fast variational inference (optimizing a
lower bound on the joint posterior). We show that on two real world data sets
(motion capture and financial data), our method which infers the block
structure outperforms a method that uses a fixed block structure, which in turn
outperforms baseline methods that ignore block structure.
"
"  Existing methods for sparse channel estimation typically provide an estimate
computed as the solution maximizing an objective function defined as the sum of
the log-likelihood function and a penalization term proportional to the l1-norm
of the parameter of interest. However, other penalization terms have proven to
have strong sparsity-inducing properties. In this work, we design
pilot-assisted channel estimators for OFDM wireless receivers within the
framework of sparse Bayesian learning by defining hierarchical Bayesian prior
models that lead to sparsity-inducing penalization terms. The estimators result
as an application of the variational message-passing algorithm on the factor
graph representing the signal model extended with the hierarchical prior
models. Numerical results demonstrate the superior performance of our channel
estimators as compared to traditional and state-of-the-art sparse methods.
"
"  This paper addresses the general problem of modelling and learning rank data
with ties. We propose a probabilistic generative model, that models the process
as permutations over partitions. This results in super-exponential
combinatorial state space with unknown numbers of partitions and unknown
ordering among them. We approach the problem from the discrete choice theory,
where subsets are chosen in a stagewise manner, reducing the state space per
each stage significantly. Further, we show that with suitable parameterisation,
we can still learn the models in linear time. We evaluate the proposed models
on the problem of learning to rank with the data from the recently held Yahoo!
challenge, and demonstrate that the models are competitive against well-known
rivals.
"
"  The purpose of the New York Workshop on Computer, Earth and Space Sciences is
to bring together the New York area's finest Astronomers, Statisticians,
Computer Scientists, Space and Earth Scientists to explore potential synergies
between their respective fields. The 2011 edition (CESS2011) was a great
success, and we would like to thank all of the presenters and participants for
attending. This year was also special as it included authors from the upcoming
book titled ""Advances in Machine Learning and Data Mining for Astronomy"". Over
two days, the latest advanced techniques used to analyze the vast amounts of
information now available for the understanding of our universe and our planet
were presented. These proceedings attempt to provide a small window into what
the current state of research is in this vast interdisciplinary field and we'd
like to thank the speakers who spent the time to contribute to this volume.
"
"  We show that the Gamma distribution is not an adequate fit for the
probability density function of drop diameters using the Kolmogorov-Smirnov
goodness of fit test. We propose a different parametrization of drop size
distributions, which not depending by any particular functional form, is based
on the adoption of standardized central moments. The first three standardized
central moments are sufficient to characterize the distribution of drop
diamters at the ground. These parameters together with the drop count form a
4-tuple which fully describe the variability of the drop size distributions.
The Cartesian product of this 4-tuple of parameters is the rainfall phase
space. Using disdrometer data from 10 different locations we identify
invariant, not depending on location, properties of the rainfall phenomenon.
"
"  We present a method for learning the parameters of a Bayesian network with
prior knowledge about the signs of influences between variables. Our method
accommodates not just the standard signs, but provides for context-specific
signs as well. We show how the various signs translate into order constraints
on the network parameters and how isotonic regression can be used to compute
order-constrained estimates from the available data. Our experimental results
show that taking prior knowledge about the signs of influences into account
leads to an improved fit of the true distribution, especially when only a small
sample of data is available. Moreover, the computed estimates are guaranteed to
be consistent with the specified signs, thereby resulting in a network that is
more likely to be accepted by experts in its domain of application.
"
"  We study statistical risk minimization problems under a privacy model in
which the data is kept confidential even from the learner. In this local
privacy framework, we establish sharp upper and lower bounds on the convergence
rates of statistical estimation procedures. As a consequence, we exhibit a
precise tradeoff between the amount of privacy the data preserves and the
utility, as measured by convergence rate, of any statistical estimator or
learning procedure.
"
"  Recently, the concept of tail dependence has been discussed in financial
applications related to market or credit risk. The multivariate extreme value
theory is a proper tool to measure and model dependence, for example, of large
loss events. A common measure of tail dependence is given by the so-called
tail-dependence coefficient. We present a simple estimator of this latter that
avoids the drawbacks of the estimation procedure that has been used so far. We
prove strong consistency and asymptotic normality and analyze the finite sample
behavior through simulation. We illustrate with an application to financial
data.
"
"  We present and implement a probabilistic (Bayesian) method for producing
catalogs from images of stellar fields. The method is capable of inferring the
number of sources N in the image and can also handle the challenges introduced
by noise, overlapping sources, and an unknown point spread function (PSF). The
luminosity function of the stars can also be inferred even when the precise
luminosity of each star is uncertain, via the use of a hierarchical Bayesian
model. The computational feasibility of the method is demonstrated on two
simulated images with different numbers of stars. We find that our method
successfully recovers the input parameter values along with principled
uncertainties even when the field is crowded. We also compare our results with
those obtained from the SExtractor software. While the two approaches largely
agree about the fluxes of the bright stars, the Bayesian approach provides more
accurate inferences about the faint stars and the number of stars, particularly
in the crowded case.
"
"  Massive datasets in the gigabyte and terabyte range combined with the
availability of increasingly sophisticated statistical tools yield analyses at
the boundary of what is computationally feasible. Compromising in the face of
this computational burden by partitioning the dataset into more tractable sizes
results in stratified analyses, removed from the context that justified the
initial data collection. In a Bayesian framework, these stratified analyses
generate intermediate realizations, often compared using point estimates that
fail to account for the variability within and correlation between the
distributions these realizations approximate. However, although the initial
concession to stratify generally precludes the more sensible analysis using a
single joint hierarchical model, we can circumvent this outcome and capitalize
on the intermediate realizations by extending the dynamic iterative reweighting
MCMC algorithm. In doing so, we reuse the available realizations by reweighting
them with importance weights, recycling them into a now tractable joint
hierarchical model. We apply this technique to intermediate realizations
generated from stratified analyses of 687 influenza A genomes spanning 13 years
allowing us to revisit hypotheses regarding the evolutionary history of
influenza within a hierarchical statistical framework.
"
"  This paper proposes a new interpretation of sparse penalties such as the
elastic-net and the group-lasso. Beyond providing a new viewpoint on these
penalization schemes, our approach results in a unified optimization strategy.
Our experiments demonstrate that this strategy, implemented on the elastic-net,
is computationally extremely efficient for small to medium size problems. Our
accompanying software solves problems very accurately, at machine precision, in
the time required to get a rough estimate with competing state-of-the-art
algorithms. We illustrate on real and artificial datasets that this accuracy is
required to for the correctness of the support of the solution, which is an
important element for the interpretability of sparsity-inducing penalties.
"
"  In this paper we propose a Bayesian approach for inference about dependence
of high throughput gene expression. Our goals are to use prior knowledge about
pathways to anchor inference about dependence among genes; to account for this
dependence while making inferences about differences in mean expression across
phenotypes; and to explore differences in the dependence itself across
phenotypes. Useful features of the proposed approach are a model-based
parsimonious representation of expression as an ordinal outcome, a novel and
flexible representation of prior information on the nature of dependencies, and
the use of a coherent probability model over both the structure and strength of
the dependencies of interest. We evaluate our approach through simulations and
in the analysis of data on expression of genes in the Complement and
Coagulation Cascade pathway in ovarian cancer.
"
"  Sparse feature selection has been demonstrated to be effective in handling
high-dimensional data. While promising, most of the existing works use convex
methods, which may be suboptimal in terms of the accuracy of feature selection
and parameter estimation. In this paper, we expand a nonconvex paradigm to
sparse group feature selection, which is motivated by applications that require
identifying the underlying group structure and performing feature selection
simultaneously. The main contributions of this article are twofold: (1)
statistically, we introduce a nonconvex sparse group feature selection model
which can reconstruct the oracle estimator. Therefore, consistent feature
selection and parameter estimation can be achieved; (2) computationally, we
propose an efficient algorithm that is applicable to large-scale problems.
Numerical results suggest that the proposed nonconvex method compares favorably
against its competitors on synthetic data and real-world applications, thus
achieving desired goal of delivering high performance.
"
"  Single-particle electron microscopy is a modern technique that biophysicists
employ to learn the structure of proteins. It yields data that consist of noisy
random projections of the protein structure in random directions, with the
added complication that the projection angles cannot be observed. In order to
reconstruct a three-dimensional model, the projection directions need to be
estimated by use of an ad-hoc starting estimate of the unknown particle. In
this paper we propose a methodology that does not rely on knowledge of the
projection angles, to construct an objective data-dependent low-resolution
approximation of the unknown structure that can serve as such a starting
estimate. The approach assumes that the protein admits a suitable sparse
representation, and employs discrete $L^1$-regularization (LASSO) as well as
notions from shape theory to tackle the peculiar challenges involved in the
associated inverse problem. We illustrate the approach by application to the
reconstruction of an E. coli protein component called the Klenow fragment.
"
"  In cluster analysis, it can be useful to interpret the partition built from
the data in the light of external categorical variables which were not directly
involved to cluster the data. An approach is proposed in the model-based
clustering context to select a model and a number of clusters which both fit
the data well and take advantage of the potential illustrative ability of the
external variables. This approach makes use of the integrated joint likelihood
of the data and the partitions at hand, namely the model-based partition and
the partitions associated to the external variables. It is noteworthy that each
mixture model is fitted by the maximum likelihood methodology to the data,
excluding the external variables which are used to select a relevant mixture
model only. Numerical experiments illustrate the promising behaviour of the
derived criterion.
"
"  This paper studies the problem of learning clusters which are consistently
present in different (continuously valued) representations of observed data.
Our setup differs slightly from the standard approach of (co-) clustering as we
use the fact that some form of `labeling' becomes available in this setup: a
cluster is only interesting if it has a counterpart in the alternative
representation. The contribution of this paper is twofold: (i) the problem
setting is explored and an analysis in terms of the PAC-Bayesian theorem is
presented, (ii) a practical kernel-based algorithm is derived exploiting the
inherent relation to Canonical Correlation Analysis (CCA), as well as its
extension to multiple views. A content based information retrieval (CBIR) case
study is presented on the multi-lingual aligned Europal document dataset which
supports the above findings.
"
"  Let M be a random (alpha n) x n matrix of rank r<<n, and assume that a
uniformly random subset E of its entries is observed. We describe an efficient
algorithm that reconstructs M from |E| = O(rn) observed entries with relative
root mean square error RMSE <= C(rn/|E|)^0.5 . Further, if r=O(1), M can be
reconstructed exactly from |E| = O(n log(n)) entries. These results apply
beyond random matrices to general low-rank incoherent matrices.
  This settles (in the case of bounded rank) a question left open by Candes and
Recht and improves over the guarantees for their reconstruction algorithm. The
complexity of our algorithm is O(|E|r log(n)), which opens the way to its use
for massive data sets. In the process of proving these statements, we obtain a
generalization of a celebrated result by Friedman-Kahn-Szemeredi and Feige-Ofek
on the spectrum of sparse random matrices.
"
"  This article proposes a novel density estimation based algorithm for carrying
out supervised machine learning. The proposed algorithm features O(n) time
complexity for generating a classifier, where n is the number of sampling
instances in the training dataset. This feature is highly desirable in
contemporary applications that involve large and still growing databases. In
comparison with the kernel density estimation based approaches, the
mathe-matical fundamental behind the proposed algorithm is not based on the
assump-tion that the number of training instances approaches infinite. As a
result, a classifier generated with the proposed algorithm may deliver higher
prediction accuracy than the kernel density estimation based classifier in some
cases.
"
"  Research in reinforcement learning has produced algorithms for optimal
decision making under uncertainty that fall within two main types. The first
employs a Bayesian framework, where optimality improves with increased
computational time. This is because the resulting planning task takes the form
of a dynamic programming problem on a belief tree with an infinite number of
states. The second type employs relatively simple algorithm which are shown to
suffer small regret within a distribution-free framework. This paper presents a
lower bound and a high probability upper bound on the optimal value function
for the nodes in the Bayesian belief tree, which are analogous to similar
bounds in POMDPs. The bounds are then used to create more efficient strategies
for exploring the tree. The resulting algorithms are compared with the
distribution-free algorithm UCB1, as well as a simpler baseline algorithm on
multi-armed bandit problems.
"
"  This is a case study discussing the supervised artificial neural network for
the purpose of forecasting with comparison of the Box-Jenkins methodology by
using the data of well known emergency service Rescue 1122. We fits a variety
of neural network (NN) models and many problems were revealed while fitting the
ANNs model to achieve the local minima. Moreover ANNs model is giving much
better out of sample forecasts as compare to the ARIMA model. However we use
diagnostic checks for the comparison of models.
"
"  When data is sampled from an unknown subspace, principal component analysis
(PCA) provides an effective way to estimate the subspace and hence reduce the
dimension of the data. At the heart of PCA is the Eckart-Young-Mirsky theorem,
which characterizes the best rank k approximation of a matrix. In this paper,
we prove a generalization of the Eckart-Young-Mirsky theorem under all
unitarily invariant norms. Using this result, we obtain closed-form solutions
for a set of rank/norm regularized problems, and derive closed-form solutions
for a general class of subspace clustering problems (where data is modelled by
unions of unknown subspaces). From these results we obtain new theoretical
insights and promising experimental results.
"
"  This paper proposes a novel framework for delay-tolerant particle filtering
that is computationally efficient and has limited memory requirements. Within
this framework the informativeness of a delayed (out-of-sequence) measurement
(OOSM) is estimated using a lightweight procedure and uninformative
measurements are immediately discarded. The framework requires the
identification of a threshold that separates informative from uninformative;
this threshold selection task is formulated as a constrained optimization
problem, where the goal is to minimize tracking error whilst controlling the
computational requirements. We develop an algorithm that provides an
approximate solution for the optimization problem. Simulation experiments
provide an example where the proposed framework processes less than 40% of all
OOSMs with only a small reduction in tracking accuracy.
"
"  Both linear mixed models (LMMs) and sparse regression models are widely used
in genetics applications, including, recently, polygenic modeling in
genome-wide association studies. These two approaches make very different
assumptions, so are expected to perform well in different situations. However,
in practice, for a given data set one typically does not know which assumptions
will be more accurate. Motivated by this, we consider a hybrid of the two,
which we refer to as a ""Bayesian sparse linear mixed model"" (BSLMM) that
includes both these models as special cases. We address several key
computational and statistical issues that arise when applying BSLMM, including
appropriate prior specification for the hyper-parameters, and a novel Markov
chain Monte Carlo algorithm for posterior inference. We apply BSLMM and compare
it with other methods for two polygenic modeling applications: estimating the
proportion of variance in phenotypes explained (PVE) by available genotypes,
and phenotype (or breeding value) prediction. For PVE estimation, we
demonstrate that BSLMM combines the advantages of both standard LMMs and sparse
regression modeling. For phenotype prediction it considerably outperforms
either of the other two methods, as well as several other large-scale
regression methods previously suggested for this problem. Software implementing
our method is freely available from
http://stephenslab.uchicago.edu/software.html
"
"  We consider solving the $\ell_1$-regularized least-squares ($\ell_1$-LS)
problem in the context of sparse recovery, for applications such as compressed
sensing. The standard proximal gradient method, also known as iterative
soft-thresholding when applied to this problem, has low computational cost per
iteration but a rather slow convergence rate. Nevertheless, when the solution
is sparse, it often exhibits fast linear convergence in the final stage. We
exploit the local linear convergence using a homotopy continuation strategy,
i.e., we solve the $\ell_1$-LS problem for a sequence of decreasing values of
the regularization parameter, and use an approximate solution at the end of
each stage to warm start the next stage. Although similar strategies have been
studied in the literature, there have been no theoretical analysis of their
global iteration complexity. This paper shows that under suitable assumptions
for sparse recovery, the proposed homotopy strategy ensures that all iterates
along the homotopy solution path are sparse. Therefore the objective function
is effectively strongly convex along the solution path, and geometric
convergence at each stage can be established. As a result, the overall
iteration complexity of our method is $O(\log(1/\epsilon))$ for finding an
$\epsilon$-optimal solution, which can be interpreted as global geometric rate
of convergence. We also present empirical results to support our theoretical
analysis.
"
"  In microarray technology, a number of critical steps are required to convert
the raw measurements into the data relied upon by biologists and clinicians.
These data manipulations, referred to as preprocessing, influence the quality
of the ultimate measurements and studies that rely upon them. Standard
operating procedure for microarray researchers is to use preprocessed data as
the starting point for the statistical analyses that produce reported results.
This has prevented many researchers from carefully considering their choice of
preprocessing methodology. Furthermore, the fact that the preprocessing step
affects the stochastic properties of the final statistical summaries is often
ignored. In this paper we propose a statistical framework that permits the
integration of preprocessing into the standard statistical analysis flow of
microarray data. This general framework is relevant in many microarray
platforms and motivates targeted analysis methods for specific applications. We
demonstrate its usefulness by applying the idea in three different applications
of the technology.
"
"  In this paper we study a broad class of structured nonlinear programming
(SNLP) problems. In particular, we first establish the first-order optimality
conditions for them. Then we propose sequential convex programming (SCP)
methods for solving them in which each iteration is obtained by solving a
convex programming problem. Under some suitable assumptions, we establish that
any accumulation point of the sequence generated by the methods is a KKT point
of the SNLP problems. In addition, we propose a variant of the SCP method for
SNLP in which nonmonotone scheme and ``local'' Lipschitz constants of the
associated functions are used. A similar convergence result as mentioned above
is established.
"
"  This paper considers the challenge of evaluating a set of classifiers, as
done in shared task evaluations like the KDD Cup or NIST TREC, without expert
labels. While expert labels provide the traditional cornerstone for evaluating
statistical learners, limited or expensive access to experts represents a
practical bottleneck. Instead, we seek methodology for estimating performance
of the classifiers which is more scalable than expert labeling yet preserves
high correlation with evaluation based on expert labels. We consider both: 1)
using only labels automatically generated by the classifiers (blind
evaluation); and 2) using labels obtained via crowdsourcing. While
crowdsourcing methods are lauded for scalability, using such data for
evaluation raises serious concerns given the prevalence of label noise. In
regard to blind evaluation, two broad strategies are investigated: combine &
score and score & combine methods infer a single pseudo-gold label set by
aggregating classifier labels; classifiers are then evaluated based on this
single pseudo-gold label set. On the other hand, score & combine methods: 1)
sample multiple label sets from classifier outputs, 2) evaluate classifiers on
each label set, and 3) average classifier performance across label sets. When
additional crowd labels are also collected, we investigate two alternative
avenues for exploiting them: 1) direct evaluation of classifiers; or 2)
supervision of combine & score methods. To assess generality of our techniques,
classifier performance is measured using four common classification metrics,
with statistical significance tests. Finally, we measure both score and rank
correlations between estimated classifier performance vs. actual performance
according to expert judgments. Rigorous evaluation of classifiers from the TREC
2011 Crowdsourcing Track shows reliable evaluation can be achieved without
reliance on expert labels.
"
"  The graphics processing unit (GPU) has emerged as a powerful and cost
effective processor for general performance computing. GPUs are capable of an
order of magnitude more floating-point operations per second as compared to
modern central processing units (CPUs), and thus provide a great deal of
promise for computationally intensive statistical applications. Fitting complex
statistical models with a large number of parameters and/or for large datasets
is often very computationally expensive. In this study, we focus on Gaussian
process (GP) models -- statistical models commonly used for emulating expensive
computer simulators. We demonstrate that the computational cost of implementing
GP models can be significantly reduced by using a CPU+GPU heterogeneous
computing system over an analogous implementation on a traditional computing
system with no GPU acceleration. Our small study suggests that GP models are
fertile ground for further implementation on CPU+GPU systems.
"
"  A rigorous methodology is proposed to study cell division data consisting in
several observed genealogical trees of possibly different shapes. The procedure
takes into account missing observations, data from different trees, as well as
the dependence structure within genealogical trees. Its main new feature is the
joint use of all available information from several data sets instead of single
data set estimation, to avoid the drawbacks of low accuracy for estimators or
low power for tests on small single-trees. The data is modeled by an asymmetric
bifurcating autoregressive process and possibly missing observations are taken
into account by modeling the genealogies with a two-type Galton-Watson process.
Least-squares estimators of the unknown parameters of the processes are given
and symmetry tests are derived. Results are applied on real data of Escherichia
coli division and an empirical study of the convergence rates of the estimators
and power of the tests is conducted on simulated data.
"
"  We consider a collection of prediction experiments, which are clustered in
the sense that groups of experiments ex- hibit similar relationship between the
predictor and response variables. The experiment clusters as well as the
regres- sion relationships are unknown. The regression relation- ships define
the experiment clusters, and in general, the predictor and response variables
may not exhibit any clus- tering. We call this prediction problem clustered
regres- sion with unknown clusters (CRUC) and in this paper we focus on linear
regression. We study and compare several methods for CRUC, demonstrate their
applicability to the Yahoo Learning-to-rank Challenge (YLRC) dataset, and in-
vestigate an associated mathematical model. CRUC is at the crossroads of many
prior works and we study several prediction algorithms with diverse origins: an
adaptation of the expectation-maximization algorithm, an approach in- spired by
K-means clustering, the singular value threshold- ing approach to matrix rank
minimization under quadratic constraints, an adaptation of the Curds and Whey
method in multiple regression, and a local regression (LoR) scheme reminiscent
of neighborhood methods in collaborative filter- ing. Based on empirical
evaluation on the YLRC dataset as well as simulated data, we identify the LoR
method as a good practical choice: it yields best or near-best prediction
performance at a reasonable computational load, and it is less sensitive to the
choice of the algorithm parameter. We also provide some analysis of the LoR
method for an asso- ciated mathematical model, which sheds light on optimal
parameter choice and prediction performance.
"
"  A density ratio is defined by the ratio of two probability densities. We
study the inference problem of density ratios and apply a semi-parametric
density-ratio estimator to the two-sample homogeneity test. In the proposed
test procedure, the f-divergence between two probability densities is estimated
using a density-ratio estimator. The f-divergence estimator is then exploited
for the two-sample homogeneity test. We derive the optimal estimator of
f-divergence in the sense of the asymptotic variance, and then investigate the
relation between the proposed test procedure and the existing score test based
on empirical likelihood estimator. Through numerical studies, we illustrate the
adequacy of the asymptotic theory for finite-sample inference.
"
"  Atmospheric Carbon Monoxide (CO) provides a window on the chemistry of the
atmosphere since it is one of few chemical constituents that can be remotely
sensed, and it can be used to determine budgets of other greenhouse gases such
as ozone and OH radicals. Remote sensing platforms in geostationary Earth orbit
will soon provide regional observations of CO at several vertical layers with
high spatial and temporal resolution. However, cloudy locations cannot be
observed and estimates of the complete CO concentration fields have to be
estimated based on the cloud-free observations. The current state-of-the-art
solution of this interpolation problem is to combine cloud-free observations
with prior information, computed by a deterministic physical model, which might
introduce uncertainties that do not derive from data. While sharing features
with the physical model, this paper suggests a Bayesian hierarchical model to
estimate the complete CO concentration fields. The paper also provides a direct
comparison to state-of-the-art methods. To our knowledge, such a model and
comparison have not been considered before.
"
"  This paper provides lower bounds on the convergence rate of Derivative Free
Optimization (DFO) with noisy function evaluations, exposing a fundamental and
unavoidable gap between the performance of algorithms with access to gradients
and those with access to only function evaluations. However, there are
situations in which DFO is unavoidable, and for such situations we propose a
new DFO algorithm that is proved to be near optimal for the class of strongly
convex objective functions. A distinctive feature of the algorithm is that it
uses only Boolean-valued function comparisons, rather than function
evaluations. This makes the algorithm useful in an even wider range of
applications, such as optimization based on paired comparisons from human
subjects, for example. We also show that regardless of whether DFO is based on
noisy function evaluations or Boolean-valued function comparisons, the
convergence rate is the same.
"
"  In this letter, we propose a method for period estimation in light curves
from periodic variable stars using correntropy. Light curves are astronomical
time series of stellar brightness over time, and are characterized as being
noisy and unevenly sampled. We propose to use slotted time lags in order to
estimate correntropy directly from irregularly sampled time series. A new
information theoretic metric is proposed for discriminating among the peaks of
the correntropy spectral density. The slotted correntropy method outperformed
slotted correlation, string length, VarTools (Lomb-Scargle periodogram and
Analysis of Variance), and SigSpec applications on a set of light curves drawn
from the MACHO survey.
"
"  Many clustering schemes are defined by optimizing an objective function
defined on the partitions of the underlying set of a finite metric space. In
this paper, we construct a framework for studying what happens when we instead
impose various structural conditions on the clustering schemes, under the
general heading of functoriality. Functoriality refers to the idea that one
should be able to compare the results of clustering algorithms as one varies
the data set, for example by adding points or by applying functions to it. We
show that within this framework, one can prove a theorems analogous to one of
J. Kleinberg, in which for example one obtains an existence and uniqueness
theorem instead of a non-existence result.
  We obtain a full classification of all clustering schemes satisfying a
condition we refer to as excisiveness. The classification can be changed by
varying the notion of maps of finite metric spaces. The conditions occur
naturally when one considers clustering as the statistical version of the
geometric notion of connected components. By varying the degree of
functoriality that one requires from the schemes it is possible to construct
richer families of clustering schemes that exhibit sensitivity to density.
"
"  Sensitivity analysis plays an important role in the understanding of complex
models. It helps to identify influence of input parameters in relation to the
outputs. It can be also a tool to understand the behavior of the model and then
can help in its development stage. This study aims to analyze and illustrate
the potential usefulness of combining first and second-order sensitivity
analysis, applied to a building energy model (ESP-r). Through the example of a
collective building, a sensitivity analysis is performed using the method of
elementary effects (also known as Morris method), including an analysis of
interactions between the input parameters (second order analysis). Importance
of higher-order analysis to better support the results of first order analysis,
highlighted especially in such complex model. Several aspects are tackled to
implement efficiently the multi-order sensitivity analysis: interval size of
the variables, management of non-linearity, usefulness of various outputs.
"
"  Singular Value Decomposition (SVD) is the basic body of many statistical
algorithms and few users question whether SVD is properly handling its job.
  SVD aims at evaluating the decomposition that best approximates a data
matrix, given some rank restriction. However often we are interested in the
best components of the decomposition rather than in the best approximation .
This conflict of objectives leads us to introduce {\em Total SVD}, where the
word ""Total"" is taken as in ""Total"" least squares.
  SVD is a least squares method and, therefore, is very sensitive to gross
errors in the data matrix. We make SVD robust by imposing a weight to each of
the matrix entries. Breakdown properties are excellent.
  Algorithmic aspects are handled; they rely on high dimension fixed point
computations.
"
"  We will establish that the VC dimension of the class of d-dimensional
ellipsoids is (d^2+3d)/2, and that maximum likelihood estimate with N-component
d-dimensional Gaussian mixture models induces a geometric class having VC
dimension at least N(d^2+3d)/2.
  Keywords: VC dimension; finite dimensional ellipsoid; Gaussian mixture model
"
"  Sparsity-promoting priors have become increasingly popular over recent years
due to an increased number of regression and classification applications
involving a large number of predictors. In time series applications where
observations are collected over time, it is often unrealistic to assume that
the underlying sparsity pattern is fixed. We propose here an original class of
flexible Bayesian linear models for dynamic sparsity modelling. The proposed
class of models expands upon the existing Bayesian literature on sparse
regression using generalized multivariate hyperbolic distributions. The
properties of the models are explored through both analytic results and
simulation studies. We demonstrate the model on a financial application where
it is shown that it accurately represents the patterns seen in the analysis of
stock and derivative data, and is able to detect major events by filtering an
artificial portfolio of assets.
"
"  We consider rules for discarding predictors in lasso regression and related
problems, for computational efficiency. El Ghaoui et al (2010) propose ""SAFE""
rules that guarantee that a coefficient will be zero in the solution, based on
the inner products of each predictor with the outcome. In this paper we propose
strong rules that are not foolproof but rarely fail in practice. These can be
complemented with simple checks of the Karush- Kuhn-Tucker (KKT) conditions to
provide safe rules that offer substantial speed and space savings in a variety
of statistical convex optimization problems.
"
"  This paper discusses the family of distributions on the Grassmannian of the
linear span of r central gaussian vectors parametrized by the covariance
matrix. Our main result is an existence and uniqueness criterion for the
maximum likelihood estimate of a sample.
"
"  In this article we give our contribution to the problem of segmentation with
plug-in procedures. We give general sufficient conditions under which plug in
procedure are efficient. We also give an algorithm that satisfy these
conditions. We give an application of the used algorithm to hyperspectral
images segmentation. Hyperspectral images are images that have both spatial and
spectral coherence with thousands of spectral bands on each pixel. In the
proposed procedure we combine a reduction dimension technique and a spatial
regularisation technique. This regularisation is based on the mixlet
modelisation of Kolaczyck and Al.
"
"  The hidden Markov model (HMM) is a widely-used generative model that copes
with sequential data, assuming that each observation is conditioned on the
state of a hidden Markov chain. In this paper, we derive a novel algorithm to
cluster HMMs based on the hierarchical EM (HEM) algorithm. The proposed
algorithm i) clusters a given collection of HMMs into groups of HMMs that are
similar, in terms of the distributions they represent, and ii) characterizes
each group by a ""cluster center"", i.e., a novel HMM that is representative for
the group, in a manner that is consistent with the underlying generative model
of the HMM. To cope with intractable inference in the E-step, the HEM algorithm
is formulated as a variational optimization problem, and efficiently solved for
the HMM case by leveraging an appropriate variational approximation. The
benefits of the proposed algorithm, which we call variational HEM (VHEM), are
demonstrated on several tasks involving time-series data, such as hierarchical
clustering of motion capture sequences, and automatic annotation and retrieval
of music and of online hand-writing data, showing improvements over current
methods. In particular, our variational HEM algorithm effectively leverages
large amounts of data when learning annotation models by using an efficient
hierarchical estimation procedure, which reduces learning times and memory
requirements, while improving model robustness through better regularization.
"
"  This paper proposes a Hilbert space embedding for Dirichlet Process mixture
models via a stick-breaking construction of Sethuraman. Although Bayesian
nonparametrics offers a powerful approach to construct a prior that avoids the
need to specify the model size/complexity explicitly, an exact inference is
often intractable. On the other hand, frequentist approaches such as kernel
machines, which suffer from the model selection/comparison problems, often
benefit from efficient learning algorithms. This paper discusses the
possibility to combine the best of both worlds by using the Dirichlet Process
mixture model as a case study.
"
"  This article contributes to the search for a notion of postural style,
focusing on the issue of classifying subjects in terms of how they maintain
posture. Longer term, the hope is to make it possible to determine on a case by
case basis which sensorial information is prevalent in postural control, and to
improve/adapt protocols for functional rehabilitation among those who show
deficits in maintaining posture, typically seniors. Here, we specifically
tackle the statistical problem of classifying subjects sampled from a two-class
population. Each subject (enrolled in a cohort of 54 participants) undergoes
four experimental protocols which are designed to evaluate potential deficits
in maintaining posture. These protocols result in four complex trajectories,
from which we can extract four small-dimensional summary measures. Because
undergoing several protocols can be unpleasant, and sometimes painful, we try
to limit the number of protocols needed for the classification. Therefore, we
first rank the protocols by decreasing order of relevance, then we derive four
plug-in classifiers which involve the best (i.e., more informative), the two
best, the three best and all four protocols. This two-step procedure relies on
the cutting-edge methodologies of targeted maximum likelihood learning (a
methodology for robust and efficient inference) and super-learning (a machine
learning procedure for aggregating various estimation procedures into a single
better estimation procedure). A simulation study is carried out. The
performances of the procedure applied to the real data set (and evaluated by
the leave-one-out rule) go as high as an 87% rate of correct classification (47
out of 54 subjects correctly classified), using only the best protocol.
"
"  Low-rank matrix approximations are often used to help scale standard machine
learning algorithms to large-scale problems. Recently, matrix coherence has
been used to characterize the ability to extract global information from a
subset of matrix entries in the context of these low-rank approximations and
other sampling-based algorithms, e.g., matrix com- pletion, robust PCA. Since
coherence is defined in terms of the singular vectors of a matrix and is
expensive to compute, the practical significance of these results largely
hinges on the following question: Can we efficiently and accurately estimate
the coherence of a matrix? In this paper we address this question. We propose a
novel algorithm for estimating coherence from a small number of columns,
formally analyze its behavior, and derive a new coherence-based matrix
approximation bound based on this analysis. We then present extensive
experimental results on synthetic and real datasets that corroborate our
worst-case theoretical analysis, yet provide strong support for the use of our
proposed algorithm whenever low-rank approximation is being considered. Our
algorithm efficiently and accurately estimates matrix coherence across a wide
range of datasets, and these coherence estimates are excellent predictors of
the effectiveness of sampling-based matrix approximation on a case-by-case
basis.
"
"  We study the problem of prediction for evolving graph data. We formulate the
problem as the minimization of a convex objective encouraging sparsity and
low-rank of the solution, that reflect natural graph properties. The convex
formulation allows to obtain oracle inequalities and efficient solvers. We
provide empirical results for our algorithm and comparison with competing
methods, and point out two open questions related to compressed sensing and
algebra of low-rank and sparse matrices.
"
"  The performance of the Self-Organizing Map (SOM) algorithm is dependent on
the initial weights of the map. The different initialization methods can
broadly be classified into random and data analysis based initialization
approach. In this paper, the performance of random initialization (RI) approach
is compared to that of principal component initialization (PCI) in which the
initial map weights are chosen from the space of the principal component.
Performance is evaluated by the fraction of variance unexplained (FVU).
Datasets were classified into quasi-linear and non-linear and it was observed
that RI performed better for non-linear datasets; however the performance of
PCI approach remains inconclusive for quasi-linear datasets.
"
"  We develop a Bayesian nonparametric extension of the popular Plackett-Luce
choice model that can handle an infinite number of choice items. Our framework
is based on the theory of random atomic measures, with the prior specified by a
gamma process. We derive a posterior characterization and a simple and
effective Gibbs sampler for posterior simulation. We develop a time-varying
extension of our model, and apply it to the New York Times lists of weekly
bestselling books.
"
"  Discussion on ""Brownian distance covariance"" by G\'{a}bor J. Sz\'{e}kely and
Maria L. Rizzo [arXiv:1010.0297]
"
"  In this note, we present a new averaging technique for the projected
stochastic subgradient method. By using a weighted average with a weight of t+1
for each iterate w_t at iteration t, we obtain the convergence rate of O(1/t)
with both an easy proof and an easy implementation. The new scheme is compared
empirically to existing techniques, with similar performance behavior.
"
"  The research questions that motivate transportation safety studies are causal
in nature. Safety researchers typically use observational data to answer such
questions, but often without appropriate causal inference methodology. The
field of causal inference presents several modeling frameworks for probing
empirical data to assess causal relations. This paper focuses on exploring the
applicability of two such modeling frameworks---Causal Diagrams and Potential
Outcomes---for a specific transportation safety problem. The causal effects of
pavement marking retroreflectivity on safety of a road segment were estimated.
More specifically, the results based on three different implementations of
these frameworks on a real data set were compared: Inverse Propensity Score
Weighting with regression adjustment and Propensity Score Matching with
regression adjustment versus Causal Bayesian Network. The effect of increased
pavement marking retroreflectivity was generally found to reduce the
probability of target nighttime crashes. However, we found that the magnitude
of the causal effects estimated are sensitive to the method used and to the
assumptions being violated.
"
"  This paper illustrates the measurement and the applications of the
observable, entropy production rate (EPR), in human subject social interaction
systems. To this end, we show (1) how to test the minimax randomization model
with experimental economics' 2$\times$2 games data and with the Wimbledon
Tennis data; (2) how to identify the Edgeworth price cycle in experimental
market data; and (3) the relationship within EPR and motion in data. As a
result, in human subject social interaction systems, EPR can be measured
practically and can be employed to test models and to search for facts
efficiently.
"
"  The derivation of statistical properties for Partial Least Squares regression
can be a challenging task. The reason is that the construction of latent
components from the predictor variables also depends on the response variable.
While this typically leads to good performance and interpretable models in
practice, it makes the statistical analysis more involved. In this work, we
study the intrinsic complexity of Partial Least Squares Regression. Our
contribution is an unbiased estimate of its Degrees of Freedom. It is defined
as the trace of the first derivative of the fitted values, seen as a function
of the response. We establish two equivalent representations that rely on the
close connection of Partial Least Squares to matrix decompositions and Krylov
subspace techniques. We show that the Degrees of Freedom depend on the
collinearity of the predictor variables: The lower the collinearity is, the
higher the Degrees of Freedom are. In particular, they are typically higher
than the naive approach that defines the Degrees of Freedom as the number of
components. Further, we illustrate how the Degrees of Freedom approach can be
used for the comparison of different regression methods. In the experimental
section, we show that our Degrees of Freedom estimate in combination with
information criteria is useful for model selection.
"
"  This study examined the teaching practices of 227 college instructors of
introductory statistics (from the health and behavioral sciences). Using
primarily multidimensional scaling (MDS) techniques, a two-dimensional, 10-item
teaching practice scale, TISS (Teaching of Introductory Statistics Scale), was
developed and validated. The two dimensions (subscales) were characterized as
constructivist, and behaviorist, and are orthogonal to each other. Criterion
validity of this scale was established in relation to instructors' attitude
toward teaching, and acceptable levels of reliability were obtained. A
significantly higher level of behaviorist practice (less reform-oriented) was
reported by instructors from the USA, and instructors with academic degrees in
mathematics and engineering. This new scale (TISS) will allow us to empirically
assess and describe the pedagogical approach (teaching practice) of instructors
of introductory statistics. Further research is required in order to be
conclusive about the structural and psychometric properties of this scale.
"
"  Compressed sensing (CS) is an important theory for sub-Nyquist sampling and
recovery of compressible data. Recently, it has been extended by Pham and
Venkatesh to cope with the case where corruption to the CS data is modeled as
impulsive noise. The new formulation, termed as robust CS, combines robust
statistics and CS into a single framework to suppress outliers in the CS
recovery. To solve the newly formulated robust CS problem, Pham and Venkatesh
suggested a scheme that iteratively solves a number of CS problems, the
solutions from which converge to the true robust compressed sensing solution.
However, this scheme is rather inefficient as it has to use existing CS solvers
as a proxy. To overcome limitation with the original robust CS algorithm, we
propose to solve the robust CS problem directly in this paper and drive more
computationally efficient algorithms by following latest advances in
large-scale convex optimization for non-smooth regularization. Furthermore, we
also extend the robust CS formulation to various settings, including additional
affine constraints, $\ell_1$-norm loss function, mixed-norm regularization, and
multi-tasking, so as to further improve robust CS. We also derive simple but
effective algorithms to solve these extensions. We demonstrate that the new
algorithms provide much better computational advantage over the original robust
CS formulation, and effectively solve more sophisticated extensions where the
original methods simply cannot. We demonstrate the usefulness of the extensions
on several CS imaging tasks.
"
"  We prove rates of convergence in the statistical sense for kernel-based least
squares regression using a conjugate gradient algorithm, where regularization
against overfitting is obtained by early stopping. This method is directly
related to Kernel Partial Least Squares, a regression method that combines
supervised dimensionality reduction with least squares projection. The rates
depend on two key quantities: first, on the regularity of the target regression
function and second, on the intrinsic dimensionality of the data mapped into
the kernel space. Lower bounds on attainable rates depending on these two
quantities were established in earlier literature, and we obtain upper bounds
for the considered method that match these lower bounds (up to a log factor) if
the true regression function belongs to the reproducing kernel Hilbert space.
If this assumption is not fulfilled, we obtain similar convergence rates
provided additional unlabeled data are available. The order of the learning
rates match state-of-the-art results that were recently obtained for least
squares support vector machines and for linear regularization operators.
"
"  Various methods have been proposed for creating a binary version of a complex
network with valued ties. Rather than the default method of choosing a single
threshold value about which to dichotomize, we consider a method of choosing
the highest k outbound arcs from each person and assigning a binary tie, as
this has the advantage of minimizing the isolation of nodes that may otherwise
be weakly connected. However, simulations and real data sets establish that
this method is worse than the default thresholding method and should not be
generally considered to deal with valued networks.
"
"  A setting of a trivairate survival function using semi-competing risks
concept is proposed. The Stanford Heart Transplant data is reanalyzed using a
trivariate Weibull distribution model with the proposed survival function.
"
"  Let us assume that $f$ is a continuous function defined on the unit ball of
$\mathbb R^d$, of the form $f(x) = g (A x)$, where $A$ is a $k \times d$ matrix
and $g$ is a function of $k$ variables for $k \ll d$. We are given a budget $m
\in \mathbb N$ of possible point evaluations $f(x_i)$, $i=1,...,m$, of $f$,
which we are allowed to query in order to construct a uniform approximating
function. Under certain smoothness and variation assumptions on the function
$g$, and an {\it arbitrary} choice of the matrix $A$, we present in this paper
  1. a sampling choice of the points $\{x_i\}$ drawn at random for each
function approximation;
  2. algorithms (Algorithm 1 and Algorithm 2) for computing the approximating
function, whose complexity is at most polynomial in the dimension $d$ and in
the number $m$ of points.
  Due to the arbitrariness of $A$, the choice of the sampling points will be
according to suitable random distributions and our results hold with
overwhelming probability. Our approach uses tools taken from the {\it
compressed sensing} framework, recent Chernoff bounds for sums of
positive-semidefinite matrices, and classical stability bounds for invariant
subspaces of singular value decompositions.
"
"  A wide variety of machine learning algorithms such as support vector machine
(SVM), minimax probability machine (MPM), and Fisher discriminant analysis
(FDA), exist for binary classification. The purpose of this paper is to provide
a unified classification model that includes the above models through a robust
optimization approach. This unified model has several benefits. One is that the
extensions and improvements intended for SVM become applicable to MPM and FDA,
and vice versa. Another benefit is to provide theoretical results to above
learning methods at once by dealing with the unified model. We give a
statistical interpretation of the unified classification model and propose a
non-convex optimization algorithm that can be applied to non-convex variants of
existing learning methods.
"
"  In binary-transaction data-mining, traditional frequent itemset mining often
produces results which are not straightforward to interpret. To overcome this
problem, probability models are often used to produce more compact and
conclusive results, albeit with some loss of accuracy. Bayesian statistics have
been widely used in the development of probability models in machine learning
in recent years and these methods have many advantages, including their
abilities to avoid overfitting. In this paper, we develop two Bayesian mixture
models with the Dirichlet distribution prior and the Dirichlet process (DP)
prior to improve the previous non-Bayesian mixture model developed for
transaction dataset mining. We implement the inference of both mixture models
using two methods: a collapsed Gibbs sampling scheme and a variational
approximation algorithm. Experiments in several benchmark problems have shown
that both mixture models achieve better performance than a non-Bayesian mixture
model. The variational algorithm is the faster of the two approaches while the
Gibbs sampling method achieves a more accurate results. The Dirichlet process
mixture model can automatically grow to a proper complexity for a better
approximation. Once the model is built, it can be very fast to query and run
analysis on (typically 10 times faster than Eclat, as we will show in the
experiment section). However, these approaches also show that mixture models
underestimate the probabilities of frequent itemsets. Consequently, these
models have a higher sensitivity but a lower specificity.
"
"  Singular Value Decomposition (and Principal Component Analysis) is one of the
most widely used techniques for dimensionality reduction: successful and
efficiently computable, it is nevertheless plagued by a well-known,
well-documented sensitivity to outliers. Recent work has considered the setting
where each point has a few arbitrarily corrupted components. Yet, in
applications of SVD or PCA such as robust collaborative filtering or
bioinformatics, malicious agents, defective genes, or simply corrupted or
contaminated experiments may effectively yield entire points that are
completely corrupted.
  We present an efficient convex optimization-based algorithm we call Outlier
Pursuit, that under some mild assumptions on the uncorrupted points (satisfied,
e.g., by the standard generative assumption in PCA problems) recovers the exact
optimal low-dimensional subspace, and identifies the corrupted points. Such
identification of corrupted points that do not conform to the low-dimensional
approximation, is of paramount interest in bioinformatics and financial
applications, and beyond. Our techniques involve matrix decomposition using
nuclear norm minimization, however, our results, setup, and approach,
necessarily differ considerably from the existing line of work in matrix
completion and matrix decomposition, since we develop an approach to recover
the correct column space of the uncorrupted matrix, rather than the exact
matrix itself. In any problem where one seeks to recover a structure rather
than the exact initial matrices, techniques developed thus far relying on
certificates of optimality, will fail. We present an important extension of
these methods, that allows the treatment of such problems.
"
"  In this paper we propose two new algorithms based on biclustering analysis,
which can be used at the basis of a recommender system for educational
orientation of Russian School graduates. The first algorithm was designed to
help students make a choice between different university faculties when some of
their preferences are known. The second algorithm was developed for the special
situation when nothing is known about their preferences. The final version of
this recommender system will be used by Higher School of Economics.
"
"  Directed acyclic graphs (DAGs) are a popular framework to express
multivariate probability distributions. Acyclic directed mixed graphs (ADMGs)
are generalizations of DAGs that can succinctly capture much richer sets of
conditional independencies, and are especially useful in modeling the effects
of latent variables implicitly. Unfortunately there are currently no good
parameterizations of general ADMGs. In this paper, we apply recent work on
cumulative distribution networks and copulas to propose one one general
construction for ADMG models. We consider a simple parameter estimation
approach, and report some encouraging experimental results.
"
"  Motivated by a computer experiment for the design of a rocket booster, this
paper explores nonstationary modeling methodologies that couple stationary
Gaussian processes with treed partitioning. Partitioning is a simple but
effective method for dealing with nonstationarity. The methodological
developments and statistical computing details which make this approach
efficient are described in detail. In addition to providing an analysis of the
rocket booster simulator, our approach is demonstrated to be effective in other
arenas.
"
"  This is an index to the papers that appear in the Proceedings of the 29th
International Conference on Machine Learning (ICML-12). The conference was held
in Edinburgh, Scotland, June 27th - July 3rd, 2012.
"
"  We describe a system for meta-analysis where a wiki stores numerical data in
a simple format and a web service performs the numerical computation.
  We initially apply the system on multiple meta-analyses of structural
neuroimaging data results. The described system allows for mass meta-analysis,
e.g., meta-analysis across multiple brain regions and multiple mental
disorders.
"
"  Multi-sample microarray experiments have become a standard experimental
method for studying biological systems. A frequent goal in such studies is to
unravel the regulatory relationships between genes. During the last few years,
regression models have been proposed for the de novo discovery of cis-acting
regulatory sequences using gene expression data. However, when applied to
multi-sample experiments, existing regression based methods model each
individual sample separately. To better capture the dynamic relationships in
multi-sample microarray experiments, we propose a flexible method for the joint
modeling of promoter sequence and multivariate expression data. In higher order
eukaryotic genomes expression regulation usually involves combinatorial
interaction between several transcription factors. Experiments have shown that
spacing between transcription factor binding sites can significantly affect
their strength in activating gene expression. We propose an adaptive model
building procedure to capture such spacing dependent cis-acting regulatory
modules. We apply our methods to the analysis of microarray time-course
experiments in yeast and in Arabidopsis. These experiments exhibit very
different dynamic temporal relationships. For both data sets, we have found all
of the well-known cis-acting regulatory elements in the related context, as
well as being able to predict novel elements.
"
"  Motivation: Prediction of ligands for proteins of known 3D structure is
important to understand structure-function relationship, predict molecular
function, or design new drugs. Results: We explore a new approach for ligand
prediction in which binding pockets are represented by atom clouds. Each target
pocket is compared to an ensemble of pockets of known ligands. Pockets are
aligned in 3D space with further use of convolution kernels between clouds of
points. Performance of the new method for ligand prediction is compared to
those of other available measures and to docking programs. We discuss two
criteria to compare the quality of similarity measures: area under ROC curve
(AUC) and classification based scores. We show that the latter is better suited
to evaluate the methods with respect to ligand prediction. Our results on
existing and new benchmarks indicate that the new method outperforms other
approaches, including docking. Availability: The new method is available at
http://cbio.ensmp.fr/paris/ Contact: mikhail.zaslavskiy@mines-paristech.fr
"
"  The $k$th-nearest neighbor rule is arguably the simplest and most intuitively
appealing nonparametric classification procedure. However, application of this
method is inhibited by lack of knowledge about its properties, in particular,
about the manner in which it is influenced by the value of $k$; and by the
absence of techniques for empirical choice of $k$. In the present paper we
detail the way in which the value of $k$ determines the misclassification
error. We consider two models, Poisson and Binomial, for the training samples.
Under the first model, data are recorded in a Poisson stream and are ""assigned""
to one or other of the two populations in accordance with the prior
probabilities. In particular, the total number of data in both training samples
is a Poisson-distributed random variable. Under the Binomial model, however,
the total number of data in the training samples is fixed, although again each
data value is assigned in a random way. Although the values of risk and regret
associated with the Poisson and Binomial models are different, they are
asymptotically equivalent to first order, and also to the risks associated with
kernel-based classifiers that are tailored to the case of two derivatives.
These properties motivate new methods for choosing the value of $k$.
"
"  Privacy Preserving Data Mining (PPDM) addresses the problem of developing
accurate models about aggregated data without access to precise information in
individual data record. A widely studied \emph{perturbation-based PPDM}
approach introduces random perturbation to individual values to preserve
privacy before data is published. Previous solutions of this approach are
limited in their tacit assumption of single-level trust on data miners.
  In this work, we relax this assumption and expand the scope of
perturbation-based PPDM to Multi-Level Trust (MLT-PPDM). In our setting, the
more trusted a data miner is, the less perturbed copy of the data it can
access. Under this setting, a malicious data miner may have access to
differently perturbed copies of the same data through various means, and may
combine these diverse copies to jointly infer additional information about the
original data that the data owner does not intend to release. Preventing such
\emph{diversity attacks} is the key challenge of providing MLT-PPDM services.
We address this challenge by properly correlating perturbation across copies at
different trust levels. We prove that our solution is robust against diversity
attacks with respect to our privacy goal. That is, for data miners who have
access to an arbitrary collection of the perturbed copies, our solution prevent
them from jointly reconstructing the original data more accurately than the
best effort using any individual copy in the collection. Our solution allows a
data owner to generate perturbed copies of its data for arbitrary trust levels
on-demand. This feature offers data owners maximum flexibility.
"
"  Classifying streaming data requires the development of methods which are
computationally efficient and able to cope with changes in the underlying
distribution of the stream, a phenomenon known in the literature as concept
drift. We propose a new method for detecting concept drift which uses an
Exponentially Weighted Moving Average (EWMA) chart to monitor the
misclassification rate of an streaming classifier. Our approach is modular and
can hence be run in parallel with any underlying classifier to provide an
additional layer of concept drift detection. Moreover our method is
computationally efficient with overhead O(1) and works in a fully online manner
with no need to store data points in memory. Unlike many existing approaches to
concept drift detection, our method allows the rate of false positive
detections to be controlled and kept constant over time.
"
"  This paper investigates the cross-correlations across multiple climate model
errors. We build a Bayesian hierarchical model that accounts for the spatial
dependence of individual models as well as cross-covariances across different
climate models. Our method allows for a nonseparable and nonstationary
cross-covariance structure. We also present a covariance approximation approach
to facilitate the computation in the modeling and analysis of very large
multivariate spatial data sets. The covariance approximation consists of two
parts: a reduced-rank part to capture the large-scale spatial dependence, and a
sparse covariance matrix to correct the small-scale dependence error induced by
the reduced rank approximation. We pay special attention to the case that the
second part of the approximation has a block-diagonal structure. Simulation
results of model fitting and prediction show substantial improvement of the
proposed approximation over the predictive process approximation and the
independent blocks analysis. We then apply our computational approach to the
joint statistical modeling of multiple climate model errors.
"
"  When analysing time series an important issue is to decide whether the time
series is stationary or a random walk. Relaxing these notions, we consider the
problem to decide in favor of the I(0)- or I(1)-property. Fixed-sample
statistical tests for that problem are well studied in the literature. In this
paper we provide first results for the problem to monitor sequentially a time
series. Our stopping times are based on a sequential version of a
kernel-weighted variance-ratio statistic. The asymptotic distributions are
established for I(1) processes, a rich class of stationary processes, possibly
affected by local nonpara- metric alternatives, and the local-to-unity model.
Further, we consider the two interesting change-point models where the time
series changes its behaviour after a certain fraction of the observations and
derive the associated limiting laws. Our Monte-Carlo studies show that the
proposed detection procedures have high power when interpreted as a hypothesis
test, and that the decision can often be made very early.
"
"  This article shows how to fit reticulate finite and infinite sites sequence
spectra to aligned data from five modern human genomes (San, Yoruba, French,
Han and Papuan) plus two archaic humans (Denisovan and Neanderthal), to better
infer demographic parameters. These include interbreeding between distinct
lineages. Major improvements in the fit of the sequence spectrum are made with
successively more complicated models. Findings include some evidence of a male
biased gene flow from the Denisova lineage to Papuan ancestors and possibly
even more archaic gene flow. It is unclear if there is evidence for more than
one Neanderthal interbreeding, as the evidence suggesting this largely
disappears when a finite sites model is fitted.
"
"  Recently, Petrik et al. demonstrated that L1Regularized Approximate Linear
Programming (RALP) could produce value functions and policies which compared
favorably to established linear value function approximation techniques like
LSPI. RALP's success primarily stems from the ability to solve the feature
selection and value function approximation steps simultaneously. RALP's
performance guarantees become looser if sampled next states are used. For very
noisy domains, RALP requires an accurate model rather than samples, which can
be unrealistic in some practical scenarios. In this paper, we demonstrate this
weakness, and then introduce Locally Smoothed L1-Regularized Approximate Linear
Programming (LS-RALP). We demonstrate that LS-RALP mitigates inaccuracies
stemming from noise even without an accurate model. We show that, given some
smoothness assumptions, as the number of samples increases, error from noise
approaches zero, and provide experimental examples of LS-RALP's success on
common reinforcement learning benchmark problems.
"
"  Isotonic regression is a nonparametric approach for fitting monotonic models
to data that has been widely studied from both theoretical and practical
perspectives. However, this approach encounters computational and statistical
overfitting issues in higher dimensions. To address both concerns, we present
an algorithm, which we term Isotonic Recursive Partitioning (IRP), for isotonic
regression based on recursively partitioning the covariate space through
solution of progressively smaller ""best cut"" subproblems. This creates a
regularized sequence of isotonic models of increasing model complexity that
converges to the global isotonic regression solution. The models along the
sequence are often more accurate than the unregularized isotonic regression
model because of the complexity control they offer. We quantify this complexity
control through estimation of degrees of freedom along the path. Success of the
regularized models in prediction and IRPs favorable computational properties
are demonstrated through a series of simulated and real data experiments. We
discuss application of IRP to the problem of searching for gene--gene
interactions and epistasis, and demonstrate it on data from genome-wide
association studies of three common diseases.
"
"  We introduce a data-based approach to estimating key quantities which arise
in the study of nonlinear control systems and random nonlinear dynamical
systems. Our approach hinges on the observation that much of the existing
linear theory may be readily extended to nonlinear systems - with a reasonable
expectation of success - once the nonlinear system has been mapped into a high
or infinite dimensional feature space. In particular, we develop computable,
non-parametric estimators approximating controllability and observability
energy functions for nonlinear systems, and study the ellipsoids they induce.
In all cases the relevant quantities are estimated from simulated or observed
data. It is then shown that the controllability energy estimator provides a key
means for approximating the invariant measure of an ergodic, stochastically
forced nonlinear system.
"
"  Cell adhesion experiments are biomechanical experiments studying the binding
of a cell to another cell at the level of single molecules. Such a study plays
an important role in tumor metastasis in cancer study. Motivated by analyzing a
repeated cell adhesion experiment, a new class of nonlinear time series models
with an order selection procedure is developed in this paper. Due to the
nonlinearity, there are two types of overfitting. Therefore, a double penalized
approach is introduced for order selection. To implement this approach, a
global optimization algorithm using mixed integer programming is discussed. The
procedure is shown to be asymptotically consistent in estimating both the order
and parameters of the proposed model. Simulations show that the new order
selection approach outperforms standard methods. The finite-sample performance
of the estimator is also examined via a simulation study. The application of
the proposed methodology to a T-cell experiment provides a better understanding
of the kinetics and mechanics of cell adhesion, including quantifying the
memory effect on a repeated unbinding force experiment and identifying the
order of the memory.
"
"  The starting point of this article is the question ""How to retrieve
fingerprints of rhythm in written texts?"" We address this problem in the case
of Brazilian and European Portuguese. These two dialects of Modern Portuguese
share the same lexicon and most of the sentences they produce are superficially
identical. Yet they are conjectured, on linguistic grounds, to implement
different rhythms. We show that this linguistic question can be formulated as a
problem of model selection in the class of variable length Markov chains. To
carry on this approach, we compare texts from European and Brazilian
Portuguese. These texts are previously encoded according to some basic rhythmic
features of the sentences which can be automatically retrieved. This is an
entirely new approach from the linguistic point of view. Our statistical
contribution is the introduction of the smallest maximizer criterion which is a
constant free procedure for model selection. As a by-product, this provides a
solution for the problem of optimal choice of the penalty constant when using
the BIC to select a variable length Markov chain. Besides proving the
consistency of the smallest maximizer criterion when the sample size diverges,
we also make a simulation study comparing our approach with both the standard
BIC selection and the Peres-Shields order estimation. Applied to the linguistic
sample constituted for our case study, the smallest maximizer criterion assigns
different context-tree models to the two dialects of Portuguese. The features
of the selected models are compatible with current conjectures discussed in the
linguistic literature.
"
"  Inspired by correlations recently discovered between Google search data and
financial markets, we show correlations between Google search data mortality
rates. Words with negative connotations may provide for increased mortality
rates, while words with positive connotations may provide for decreased
mortality rates, and so statistical methods were employed to determine to
investigate further.
"
"  We present a new online boosting algorithm for adapting the weights of a
boosted classifier, which yields a closer approximation to Freund and
Schapire's AdaBoost algorithm than previous online boosting algorithms. We also
contribute a new way of deriving the online algorithm that ties together
previous online boosting work. We assume that the weak hypotheses were selected
beforehand, and only their weights are updated during online boosting. The
update rule is derived by minimizing AdaBoost's loss when viewed in an
incremental form. The equations show that optimization is computationally
expensive. However, a fast online approximation is possible. We compare
approximation error to batch AdaBoost on synthetic datasets and generalization
error on face datasets and the MNIST dataset.
"
"  We consider a Bayesian method for learning the Bayesian network structure
from complete data. Recently, Koivisto and Sood (2004) presented an algorithm
that for any single edge computes its marginal posterior probability in O(n
2^n) time, where n is the number of attributes; the number of parents per
attribute is bounded by a constant. In this paper we show that the posterior
probabilities for all the n (n - 1) potential edges can be computed in O(n 2^n)
total time. This result is achieved by a forward-backward technique and fast
Moebius transform algorithms, which are of independent interest. The resulting
speedup by a factor of about n^2 allows us to experimentally study the
statistical power of learning moderate-size networks. We report results from a
simulation study that covers data sets with 20 to 10,000 records over 5 to 25
discrete attributes
"
"  Logitboost is an influential boosting algorithm for classification. In this
paper, we develop robust logitboost to provide an explicit formulation of
tree-split criterion for building weak learners (regression trees) for
logitboost. This formulation leads to a numerically stable implementation of
logitboost. We then propose abc-logitboost for multi-class classification, by
combining robust logitboost with the prior work of abc-boost. Previously,
abc-boost was implemented as abc-mart using the mart algorithm. Our extensive
experiments on multi-class classification compare four algorithms: mart,
abcmart, (robust) logitboost, and abc-logitboost, and demonstrate the
superiority of abc-logitboost. Comparisons with other learning methods
including SVM and deep learning are also available through prior publications.
"
"  In this paper we introduce a new method to add a parameter to a family of
distributions. The additional parameter is completely studied and a full
description of its behaviour in the distribution is given. We obtain several
mathematical properties of the new class of distributions such as
Kullback-Leibler divergence, Shannon entropy, moments, order statistics,
estimation of the parameters and inference for large sample. Further, we showed
that the new distribution have the reference distribution as special case, and
that the usual inference procedures also hold in this case. Furthermore, we
applied our method to yield three-parameter extensions of the Weibull and beta
distributions. To motivate the use of our class of distributions, we present a
successful application to fatigue life data.
"
"  This paper outlines a methodology for semi-parametric spatio-temporal
modelling of data which is dense in time but sparse in space, obtained from a
split panel design, the most feasible approach to covering space and time with
limited equipment. The data are hourly averaged particle number concentration
(PNC) and were collected, as part of the Ultrafine Particles from Transport
Emissions and Child Health (UPTECH) project. Two weeks of continuous
measurements were taken at each of a number of government primary schools in
the Brisbane Metropolitan Area. The monitoring equipment was taken to each
school sequentially. The school data are augmented by data from long term
monitoring stations at three locations in Brisbane, Australia.
  Fitting the model helps describe the spatial and temporal variability at a
subset of the UPTECH schools and the long-term monitoring sites. The temporal
variation is modelled hierarchically with penalised random walk terms, one
common to all sites and a term accounting for the remaining temporal trend at
each site. Parameter estimates and their uncertainty are computed in a
computationally efficient approximate Bayesian inference environment, R-INLA.
  The temporal part of the model explains daily and weekly cycles in PNC at the
schools, which can be used to estimate the exposure of school children to
ultrafine particles (UFPs) emitted by vehicles. At each school and long-term
monitoring site, peaks in PNC can be attributed to the morning and afternoon
rush hour traffic and new particle formation events. The spatial component of
the model describes the school to school variation in mean PNC at each school
and within each school ground. It is shown how the spatial model can be
expanded to identify spatial patterns at the city scale with the inclusion of
more spatial locations.
"
"  We introduce a new family of matrix norms, the ""local max"" norms,
generalizing existing methods such as the max norm, the trace norm (nuclear
norm), and the weighted or smoothed weighted trace norms, which have been
extensively used in the literature as regularizers for matrix reconstruction
problems. We show that this new family can be used to interpolate between the
(weighted or unweighted) trace norm and the more conservative max norm. We test
this interpolation on simulated data and on the large-scale Netflix and
MovieLens ratings data, and find improved accuracy relative to the existing
matrix norms. We also provide theoretical results showing learning guarantees
for some of the new norms.
"
"  This paper deals with chain graphs under the alternative
Andersson-Madigan-Perlman (AMP) interpretation. In particular, we present a
constraint based algorithm for learning an AMP chain graph a given probability
distribution is faithful to. We also show that the extension of Meek's
conjecture to AMP chain graphs does not hold, which compromises the development
of efficient and correct score+search learning algorithms under assumptions
weaker than faithfulness.
"
"  There have been several instances in our nation's history in which the
presidential candidate who received the most popular votes did not win the
presidency. Using a principal components analysis of recent presidential
elections, we estimate the likelihood of such an outcome under modern
conditions to be approximately 5%. We also investigate the effect on this
estimate of eliminating from the Electoral College the two electors per state
awarded due to their representation in the Senate, as well as the effect of
increasing them. We conclude with an analysis of the likely consequences of The
National Popular Vote Bill, which would award the presidency to the candidate
with the largest national popular vote total if it took effect.
"
"  The statistical analysis of covariance matrices occurs in many important
applications, e.g. in diffusion tensor imaging and longitudinal data analysis.
We consider the situation where it is of interest to estimate an average
covariance matrix, describe its anisotropy, to carry out principal geodesic
analysis and to interpolate between covariance matrices. There are many choices
of metric available, each with its advantages. The particular choice of what is
best will depend on the particular application. The use of the Procrustes
size-and-shape metric is particularly appropriate when the covariance matrices
are close to being deficient in rank. We discuss the use of different metrics
for diffusion tensor analysis, and we also introduce certain types of
regularization for tensors.
"
"  We consider structural equation models in which variables can be written as a
function of their parents and noise terms, which are assumed to be jointly
independent. Corresponding to each structural equation model, there is a
directed acyclic graph describing the relationships between the variables. In
Gaussian structural equation models with linear functions, the graph can be
identified from the joint distribution only up to Markov equivalence classes,
assuming faithfulness. In this work, we prove full identifiability if all noise
variables have the same variances: the directed acyclic graph can be recovered
from the joint Gaussian distribution. Our result has direct implications for
causal inference: if the data follow a Gaussian structural equation model with
equal error variances and assuming that all variables are observed, the causal
structure can be inferred from observational data only. We propose a
statistical method and an algorithm that exploit our theoretical findings.
"
"  Since many environmental processes such as heat waves or precipitation are
spatial in extent, it is likely that a single extreme event affects several
locations and the areal modelling of extremes is therefore essential if the
spatial dependence of extremes has to be appropriately taken into account. This
paper proposes a framework for conditional simulations of max-stable processes
and give closed forms for Brown-Resnick and Schlather processes. We test the
method on simulated data and give an application to extreme rainfall around
Zurich and extreme temperature in Switzerland. Results show that the proposed
framework provides accurate conditional simulations and can handle real-sized
problems.
"
"  Motivated by genetic association studies of pleiotropy, we propose here a
Bayesian latent variable approach to jointly study multiple outcomes or
phenotypes. The proposed method models both continuous and binary phenotypes,
and it accounts for serial and familial correlations when longitudinal and
pedigree data have been collected. We present a Bayesian estimation method for
the model parameters, and we develop a novel MCMC algorithm that builds upon
hierarchical centering and parameter expansion techniques to efficiently sample
the posterior distribution. We discuss phenotype and model selection in the
Bayesian setting, and we study the performance of two selection strategies
based on Bayes factors and spike-and-slab priors. We evaluate the proposed
method via extensive simulations and demonstrate its utility with an
application to a genome-wide association study of various complication
phenotypes related to type 1 diabetes.
"
"  We have obtained a ""hierarchical regionalization"" of 3,107 county-level units
of the United States based upon census-recorded 1995-2000 intercounty migration
flows. The methodology employed was the two-stage (double-standardization and
strong component [directed graph] hierarchical clustering) algorithm described
in the 2009 PNAS (106 [26], E66) letter (arXiv:0904.4863). Various features (e.
g., cosmopolitan vs. provincial aspects, and indices of isolation) of the
regionalization have been previously discussed in arXiv:0907.2393,
arXiv:0903.3623 and arXiv:0809.2768. However, due to the lengthy (38-page)
nature of the associated dendrogram, the detailed tree structure itself was not
readily available for inspection. Here, we do present this (county-searchable)
dendrogram--and invite readers to explore it, based on their particular
interests/locations. An ordinal scale--rather than the originally-derived
cardinal scale of the doubly-standardized values--in which groupings/features
were more immediately apparent, was originally presented. Now, we append the
cardinal-scale dendrogram.
"
"  In this paper we use detailed data about the biology of the head louse
(pediculus humanus capitis) to build a model of the evolution of head lice
colonies. Using theory and computer simulations, we show that the model can be
used to assess the impact of the various strategies usually applied to
eradicate head lice, both conscious (treatments) and unconscious (grooming). In
the case of treatments, we study the difference in performance that arises when
they are applied in systematic and non-systematic ways. Using some reasonable
simplifying assumptions (as random mixing of human groups and the same mobility
for all life stages of head lice other than eggs) we model the contagion of
pediculosis using only one additional parameter. It is shown that this
parameter can be tuned to obtain collective infestations whose variables are
compatible with what is given in the literature on real infestations. We
analyze two scenarios: one where group members begin treatment when a similar
number of lice are present in each head, and another where there is one
individual who starts treatment with a much larger threshold ('superspreader').
For both cases we assess the impact of several collective strategies of
treatment.
"
"  We state the problem of inverse reinforcement learning in terms of preference
elicitation, resulting in a principled (Bayesian) statistical formulation. This
generalises previous work on Bayesian inverse reinforcement learning and allows
us to obtain a posterior distribution on the agent's preferences, policy and
optionally, the obtained reward sequence, from observations. We examine the
relation of the resulting approach to other statistical methods for inverse
reinforcement learning via analysis and experimental results. We show that
preferences can be determined accurately, even if the observed agent's policy
is sub-optimal with respect to its own preferences. In that case, significantly
improved policies with respect to the agent's preferences are obtained,
compared to both other methods and to the performance of the demonstrated
policy.
"
"  Many problems of low-level computer vision and image processing, such as
denoising, deconvolution, tomographic reconstruction or super-resolution, can
be addressed by maximizing the posterior distribution of a sparse linear model
(SLM). We show how higher-order Bayesian decision-making problems, such as
optimizing image acquisition in magnetic resonance scanners, can be addressed
by querying the SLM posterior covariance, unrelated to the density's mode. We
propose a scalable algorithmic framework, with which SLM posteriors over full,
high-resolution images can be approximated for the first time, solving a
variational optimization problem which is convex iff posterior mode finding is
convex. These methods successfully drive the optimization of sampling
trajectories for real-world magnetic resonance imaging through Bayesian
experimental design, which has not been attempted before. Our methodology
provides new insight into similarities and differences between sparse
reconstruction and approximate Bayesian inference, and has important
implications for compressive sensing of real-world images.
"
"  Randomized algorithms that base iteration-level decisions on samples from
some pool are ubiquitous in machine learning and optimization. Examples include
stochastic gradient descent and randomized coordinate descent. This paper makes
progress at theoretically evaluating the difference in performance between
sampling with- and without-replacement in such algorithms. Focusing on least
means squares optimization, we formulate a noncommutative arithmetic-geometric
mean inequality that would prove that the expected convergence rate of
without-replacement sampling is faster than that of with-replacement sampling.
We demonstrate that this inequality holds for many classes of random matrices
and for some pathological examples as well. We provide a deterministic
worst-case bound on the gap between the discrepancy between the two sampling
models, and explore some of the impediments to proving this inequality in full
generality. We detail the consequences of this inequality for stochastic
gradient descent and the randomized Kaczmarz algorithm for solving linear
systems.
"
"  The most popular approach in extreme value statistics is the modelling of
threshold exceedances using the asymptotically motivated generalised Pareto
distribution. This approach involves the selection of a high threshold above
which the model fits the data well. Sometimes, few observations of a
measurement process might be recorded in applications and so selecting a high
quantile of the sample as the threshold leads to almost no exceedances. In this
paper we propose extensions of the generalised Pareto distribution that
incorporate an additional shape parameter while keeping the tail behaviour
unaffected. The inclusion of this parameter offers additional structure for the
main body of the distribution, improves the stability of the modified scale,
tail index and return level estimates to threshold choice and allows a lower
threshold to be selected. We illustrate the benefits of the proposed models
with a simulation study and two case studies.
"
"  We present results of a forecast initiated Week 49 (beginning December 9,
2012) of the 2012-2013 influenza season for municipalities in the United
States. The forecast was made on December 14, 2012. Results from forecasts
initiated the two previous weeks (Weeks 47 and 48) are also presented. Also
results from the forecast generated with the SIRS model without AH forcing (no
AH) are shown
"
"  Methods for learning Bayesian network structure can discover dependency
structure between observed variables, and have been shown to be useful in many
applications. However, in domains that involve a large number of variables, the
space of possible network structures is enormous, making it difficult, for both
computational and statistical reasons, to identify a good model. In this paper,
we consider a solution to this problem, suitable for domains where many
variables have similar behavior. Our method is based on a new class of models,
which we call module networks. A module network explicitly represents the
notion of a module - a set of variables that have the same parents in the
network and share the same conditional probability distribution. We define the
semantics of module networks, and describe an algorithm that learns a module
network from data. The algorithm learns both the partitioning of the variables
into modules and the dependency structure between the variables. We evaluate
our algorithm on synthetic data, and on real data in the domains of gene
expression and the stock market. Our results show that module networks
generalize better than Bayesian networks, and that the learned module network
structure reveals regularities that are obscured in learned Bayesian networks.
"
"  I describe ongoing work on development of Bayesian methods for exploring
periodically varying phenomena in astronomy, addressing two classes of sources:
pulsars, and extrasolar planets (exoplanets). For pulsars, the methods aim to
detect and measure periodically varying signals in data consisting of photon
arrival times, modeled as non-homogeneous Poisson point processes. For
exoplanets, the methods address detection and estimation of planetary orbits
using observations of the reflex motion ""wobble"" of a host star, including
adaptive scheduling of observations to optimize inferences.
"
"  In practical nonlinear filtering, the assessment of achievable filtering
performance is important. In this paper, we focus on the problem of efficiently
approximate the posterior Cramer-Rao lower bound (CRLB) in a recursive manner.
By using Gaussian assumptions, two types of approximations for calculating the
CRLB are proposed: An exact model using the state estimate as well as a
Taylor-series-expanded model using both of the state estimate and its error
covariance, are derived. Moreover, the difference between the two approximated
CRLBs is also formulated analytically. By employing the particle filter (PF)
and the unscented Kalman filter (UKF) to compute, simulation results reveal
that the approximated CRLB using mean-covariance-based model outperforms that
using the mean-based exact model. It is also shown that the theoretical
difference between the estimated CRLBs can be improved through an improved
filtering method.
"
"  We study the detection error probability associated with a balanced binary
relay tree, where the leaves of the tree correspond to $N$ identical and
independent detectors. The root of the tree represents a fusion center that
makes the overall detection decision. Each of the other nodes in the tree are
relay nodes that combine two binary messages to form a single output binary
message. In this way, the information from the detectors is aggregated into the
fusion center via the intermediate relay nodes. In this context, we describe
the evolution of Type I and Type II error probabilities of the binary data as
it propagates from the leaves towards the root. Tight upper and lower bounds
for the total error probability at the fusion center as functions of $N$ are
derived. These characterize how fast the total error probability converges to 0
with respect to $N$, even if the individual sensors have error probabilities
that converge to 1/2.
"
"  This paper presents an approach to estimating the health effects of an
environmental hazard. The approach is general in nature, but is applied here to
the case of air pollution. It uses a computer model involving ambient pollution
and temperature inputs, to simulate the exposures experienced by individuals in
an urban area, whilst incorporating the mechanisms that determine exposures.
The output from the model comprises a set of daily exposures for a sample of
individuals from the population of interest. These daily exposures are
approximated by parametric distributions, so that the predictive exposure
distribution of a randomly selected individual can be generated. These
distributions are then incorporated into a hierarchical Bayesian framework
(with inference using Markov Chain Monte Carlo simulation) in order to examine
the relationship between short-term changes in exposures and health outcomes,
whilst making allowance for long-term trends, seasonality, the effect of
potential confounders and the possibility of ecological bias.
  The paper applies this approach to particulate pollution (PM$_{10}$) and
respiratory mortality counts for seniors in greater London ($\geq$65 years)
during 1997. Within this substantive epidemiological study, the effects on
health of ambient concentrations and (estimated) personal exposures are
compared.
"
"  Testing for nonlinearity is one of the most important preprocessing steps in
nonlinear time series analysis. Typically, this is done by means of the linear
surrogate data methods. But it is a known fact that the validity of the results
heavily depends on the stationarity of the time series. Since most
physiological signals are non-stationary, it is easy to falsely detect
nonlinearity using the linear surrogate data methods. In this document, we
propose a methodology to extend the procedure for generating constrained
surrogate time series in order to assess nonlinearity in non-stationary data.
The method is based on the band-phase-randomized surrogates, which consists
(contrary to the linear surrogate data methods) in randomizing only a portion
of the Fourier phases in the high frequency band. Analysis of simulated time
series showed that in comparison to the linear surrogate data method, our
method is able to discriminate between linear stationarity, linear
non-stationary and nonlinear time series. When applying our methodology to
heart rate variability (HRV) time series that present spikes and other kinds of
nonstationarities, we where able to obtain surrogate time series that look like
the data and preserves linear correlations, something that is not possible to
do with the existing surrogate data methods.
"
"  In multi-label learning, each sample is associated with several labels.
Existing works indicate that exploring correlations between labels improve the
prediction performance. However, embedding the label correlations into the
training process significantly increases the problem size. Moreover, the
mapping of the label structure in the feature space is not clear. In this
paper, we propose a novel multi-label learning method ""Structured Decomposition
+ Group Sparsity (SDGS)"". In SDGS, we learn a feature subspace for each label
from the structured decomposition of the training data, and predict the labels
of a new sample from its group sparse representation on the multi-subspace
obtained from the structured decomposition. In particular, in the training
stage, we decompose the data matrix $X\in R^{n\times p}$ as
$X=\sum_{i=1}^kL^i+S$, wherein the rows of $L^i$ associated with samples that
belong to label $i$ are nonzero and consist a low-rank matrix, while the other
rows are all-zeros, the residual $S$ is a sparse matrix. The row space of $L_i$
is the feature subspace corresponding to label $i$. This decomposition can be
efficiently obtained via randomized optimization. In the prediction stage, we
estimate the group sparse representation of a new sample on the multi-subspace
via group \emph{lasso}. The nonzero representation coefficients tend to
concentrate on the subspaces of labels that the sample belongs to, and thus an
effective prediction can be obtained. We evaluate SDGS on several real datasets
and compare it with popular methods. Results verify the effectiveness and
efficiency of SDGS.
"
"  In this thesis I explore challenging discrete energy minimization problems
that arise mainly in the context of computer vision tasks. This work motivates
the use of such ""hard-to-optimize"" non-submodular functionals, and proposes
methods and algorithms to cope with the NP-hardness of their optimization.
Consequently, this thesis revolves around two axes: applications and
approximations. The applications axis motivates the use of such
""hard-to-optimize"" energies by introducing new tasks. As the energies become
less constrained and structured one gains more expressive power for the
objective function achieving more accurate models. Results show how
challenging, hard-to-optimize, energies are more adequate for certain computer
vision applications. To overcome the resulting challenging optimization tasks
the second axis of this thesis proposes approximation algorithms to cope with
the NP-hardness of the optimization. Experiments show that these new methods
yield good results for representative challenging problems.
"
"  We present the nested Chinese restaurant process (nCRP), a stochastic process
which assigns probability distributions to infinitely-deep,
infinitely-branching trees. We show how this stochastic process can be used as
a prior distribution in a Bayesian nonparametric model of document collections.
Specifically, we present an application to information retrieval in which
documents are modeled as paths down a random tree, and the preferential
attachment dynamics of the nCRP leads to clustering of documents according to
sharing of topics at multiple levels of abstraction. Given a corpus of
documents, a posterior inference algorithm finds an approximation to a
posterior distribution over trees, topics and allocations of words to levels of
the tree. We demonstrate this algorithm on collections of scientific abstracts
from several journals. This model exemplifies a recent trend in statistical
machine learning--the use of Bayesian nonparametric methods to infer
distributions on flexible data structures.
"
"  We present arguments for the formulation of unified approach to different
standard continuous inference methods from partial information. It is claimed
that an explicit partition of information into a priori (prior knowledge) and a
posteriori information (data) is an important way of standardizing inference
approaches so that they can be compared on a normative scale, and so that
notions of optimal algorithms become farther-reaching. The inference methods
considered include neural network approaches, information-based complexity, and
Monte Carlo, spline, and regularization methods. The model is an extension of
currently used continuous complexity models, with a class of algorithms in the
form of optimization methods, in which an optimization functional (involving
the data) is minimized. This extends the family of current approaches in
continuous complexity theory, which include the use of interpolatory algorithms
in worst and average case settings.
"
"  We provide a model to understand how adverse weather conditions modify
traffic flow dynamic. We first prove that the microscopic Free Flow Speed of
the vehicles is changed and then provide a rule to model this change. For this,
we consider a thresholded linear model, corresponding to an application of a
MARS model to road trafficking. This model adapts itself locally to the whole
road network and provides accurate unbiased forecasted speed using live or
short term forecasted weather data information.
"
"  We study notions of robustness of Markov kernels and probability distribution
of a system that is described by $n$ input random variables and one output
random variable. Markov kernels can be expanded in a series of potentials that
allow to describe the system's behaviour after knockouts. Robustness imposes
structural constraints on these potentials. Robustness of probability
distributions is defined via conditional independence statements. These
statements can be studied algebraically. The corresponding conditional
independence ideals are related to binary edge ideals. The set of robust
probability distributions lies on an algebraic variety. We compute a Gr\""obner
basis of this ideal and study the irreducible decomposition of the variety.
These algebraic results allow to parametrize the set of all robust probability
distributions.
"
"  We prove a computable version of de Finetti's theorem on exchangeable
sequences of real random variables. As a consequence, exchangeable stochastic
processes expressed in probabilistic functional programming languages can be
automatically rewritten as procedures that do not modify non-local state. Along
the way, we prove that a distribution on the unit interval is computable if and
only if its moments are uniformly computable.
"
"  It has often been taken as a working assumption that directed links in
information networks are frequently formed by ""short-cutting"" a two-step path
between the source and the destination -- a kind of implicit ""link copying""
analogous to the process of triadic closure in social networks. Despite the
role of this assumption in theoretical models such as preferential attachment,
it has received very little direct empirical investigation. Here we develop a
formalization and methodology for studying this type of directed closure
process, and we provide evidence for its important role in the formation of
links on Twitter. We then analyze a sequence of models designed to capture the
structural phenomena related to directed closure that we observe in the Twitter
data.
"
"  The statistical problem for network tomography is to infer the distribution
of $\mathbf{X}$, with mutually independent components, from a measurement model
$\mathbf{Y}=A\mathbf{X}$, where $A$ is a given binary matrix representing the
routing topology of a network under consideration. The challenge is that the
dimension of $\mathbf{X}$ is much larger than that of $\mathbf{Y}$ and thus the
problem is often called ill-posed. This paper studies some statistical aspects
of network tomography. We first address the identifiability issue and prove
that the $\mathbf{X}$ distribution is identifiable up to a shift parameter
under mild conditions. We then use a mixture model of characteristic functions
to derive a fast algorithm for estimating the distribution of $\mathbf{X}$
based on the General method of Moments. Through extensive model simulation and
real Internet trace driven simulation, the proposed approach is shown to be
favorable comparing to previous methods using simple discretization for
inferring link delays in a heterogeneous network.
"
"  Caesium fountain frequency-standards realize the second in the International
System of Units with a relative uncertainty approaching 10^-16. Among the main
contributions to the accuracy budget, cold collisions play an important role
because of the atomic density shift of the reference atomic transition. This
paper describes an application of the Bayesian analysis of the clock frequency
to estimate the density shift and describes how the Bayes theorem allows the a
priori knowledge of the sign of the collisional coefficient to be rigourously
embedded into the analysis. As an application, data from the INRIM caesium
fountain are used and the Bayesian and orthodox analyses are compared. The
Bayes theorem allows the orthodox uncertainty to be reduced by 28% and
demonstrates to be an important tool in primary frequency-metrology.
"
"  Biomarker discovery and gene ranking is a standard task in genomic high
throughput analysis. Typically, the ordering of markers is based on a
stabilized variant of the t-score, such as the moderated t or the SAM
statistic. However, these procedures ignore gene-gene correlations, which may
have a profound impact on the gene orderings and on the power of the subsequent
tests.
  We propose a simple procedure that adjusts gene-wise t-statistics to take
account of correlations among genes. The resulting correlation-adjusted
t-scores (""cat"" scores) are derived from a predictive perspective, i.e. as a
score for variable selection to discriminate group membership in two-class
linear discriminant analysis. In the absence of correlation the cat score
reduces to the standard t-score. Moreover, using the cat score it is
straightforward to evaluate groups of features (i.e. gene sets). For
computation of the cat score from small sample data we propose a shrinkage
procedure. In a comparative study comprising six different synthetic and
empirical correlation structures we show that the cat score improves estimation
of gene orderings and leads to higher power for fixed true discovery rate, and
vice versa. Finally, we also illustrate the cat score by analyzing metabolomic
data.
  The shrinkage cat score is implemented in the R package ""st"" available from
URL http://cran.r-project.org/web/packages/st/
"
"  Principal component analysis is a useful dimension reduction and data
visualization method. However, in high dimension, low sample size asymptotic
contexts, where the sample size is fixed and the dimension goes to infinity,a
paradox has arisen. In particular, despite the useful real data insights
commonly obtained from principal component score visualization, these scores
are not consistent even when the sample eigen-vectors are consistent. This
paradox is resolved by asymptotic study of the ratio between the sample and
population principal component scores. In particular, it is seen that this
proportion converges to a non-degenerate random variable. The realization is
the same for each data point, i.e. there is a common random rescaling, which
appears for each eigen-direction. This then gives inconsistent axis labels for
the standard scores plot, yet the relative positions of the points (typically
the main visual content) are consistent. This paradox disappears when the
sample size goes to infinity.
"
"  The breakdown point in its different variants is one of the central notions
to quantify the global robustness of a procedure. We propose a simple
supplementary variant which is useful in situations where we have no obvious or
only partial equivariance: Extending the Donoho and Huber(1983) Finite Sample
Breakdown Point, we propose the Expected Finite Sample Breakdown Point to
produce less configuration-dependent values while still preserving the finite
sample aspect of the former definition. We apply this notion for joint
estimation of scale and shape (with only scale-equivariance available),
exemplified for generalized Pareto, generalized extreme value, Weibull, and
Gamma distributions. In these settings, we are interested in highly-robust,
easy-to-compute initial estimators; to this end we study Pickands-type and
Location-Dispersion-type estimators and compute their respective breakdown
points.
"
"  This paper is concerned with statistical methods for the segmental
classification of linear sequence data where the task is to segment and
classify the data according to an underlying hidden discrete state sequence.
Such analysis is commonplace in the empirical sciences including genomics,
finance and speech processing. In particular, we are interested in answering
the following question: given data $y$ and a statistical model $\pi(x,y)$ of
the hidden states $x$, what should we report as the prediction $\hat{x}$ under
the posterior distribution $\pi (x|y)$? That is, how should you make a
prediction of the underlying states? We demonstrate that traditional approaches
such as reporting the most probable state sequence or most probable set of
marginal predictions can give undesirable classification artefacts and offer
limited control over the properties of the prediction. We propose a decision
theoretic approach using a novel class of Markov loss functions and report
$\hat{x}$ via the principle of minimum expected loss (maximum expected
utility). We demonstrate that the sequence of minimum expected loss under the
Markov loss function can be enumerated exactly using dynamic programming
methods and that it offers flexibility and performance improvements over
existing techniques. The result is generic and applicable to any probabilistic
model on a sequence, such as Hidden Markov models, change point or product
partition models.
"
"  In this work, we propose the kernel Pitman-Yor process (KPYP) for
nonparametric clustering of data with general spatial or temporal
interdependencies. The KPYP is constructed by first introducing an infinite
sequence of random locations. Then, based on the stick-breaking construction of
the Pitman-Yor process, we define a predictor-dependent random probability
measure by considering that the discount hyperparameters of the
Beta-distributed random weights (stick variables) of the process are not
uniform among the weights, but controlled by a kernel function expressing the
proximity between the location assigned to each weight and the given
predictors.
"
"  The ubiquity of integrating detectors in imaging and other applications
implies that a variety of real-world data are well modeled as Poisson random
variables whose means are in turn proportional to an underlying vector-valued
signal of interest. In this article, we first show how the so-called Skellam
distribution arises from the fact that Haar wavelet and filterbank transform
coefficients corresponding to measurements of this type are distributed as sums
and differences of Poisson counts. We then provide two main theorems on Skellam
shrinkage, one showing the near-optimality of shrinkage in the Bayesian setting
and the other providing for unbiased risk estimation in a frequentist context.
These results serve to yield new estimators in the Haar transform domain,
including an unbiased risk estimate for shrinkage of Haar-Fisz
variance-stabilized data, along with accompanying low-complexity algorithms for
inference. We conclude with a simulation study demonstrating the efficacy of
our Skellam shrinkage estimators both for the standard univariate wavelet test
functions as well as a variety of test images taken from the image processing
literature, confirming that they offer substantial performance improvements
over existing alternatives.
"
"  This paper proposes a new algorithm for multiple sparse regression in high
dimensions, where the task is to estimate the support and values of several
(typically related) sparse vectors from a few noisy linear measurements. Our
algorithm is a ""forward-backward"" greedy procedure that -- uniquely -- operates
on two distinct classes of objects. In particular, we organize our target
sparse vectors as a matrix; our algorithm involves iterative addition and
removal of both (a) individual elements, and (b) entire rows (corresponding to
shared features), of the matrix.
  Analytically, we establish that our algorithm manages to recover the supports
(exactly) and values (approximately) of the sparse vectors, under assumptions
similar to existing approaches based on convex optimization. However, our
algorithm has a much smaller computational complexity. Perhaps most
interestingly, it is seen empirically to require visibly fewer samples. Ours
represents the first attempt to extend greedy algorithms to the class of models
that can only/best be represented by a combination of component structural
assumptions (sparse and group-sparse, in our case).
"
"  We investigate the dynamical transition from free-flow to jammed traffic,
which is related to the divergence of the relaxation time and susceptibility of
the energy dissipation rate $E_d$, in the Nagel-Schreckenberg (NS) model with
two different initial configurations. Different initial configurations give
rise to distinct phase transition. We argue that the phase transition of the
deterministic NS model with megajam and random initial configuration is first-
and second-order phase transition, respectively. The energy dissipation rate
$E_d$ and relaxation time follow power-law behavior in some cases. The
associated dynamic exponents have also been presented.
"
"  A statistical model or a learning machine is called regular if the map taking
a parameter to a probability distribution is one-to-one and if its Fisher
information matrix is always positive definite. If otherwise, it is called
singular. In regular statistical models, the Bayes free energy, which is
defined by the minus logarithm of Bayes marginal likelihood, can be
asymptotically approximated by the Schwarz Bayes information criterion (BIC),
whereas in singular models such approximation does not hold.
  Recently, it was proved that the Bayes free energy of a singular model is
asymptotically given by a generalized formula using a birational invariant, the
real log canonical threshold (RLCT), instead of half the number of parameters
in BIC. Theoretical values of RLCTs in several statistical models are now being
discovered based on algebraic geometrical methodology. However, it has been
difficult to estimate the Bayes free energy using only training samples,
because an RLCT depends on an unknown true distribution.
  In the present paper, we define a widely applicable Bayesian information
criterion (WBIC) by the average log likelihood function over the posterior
distribution with the inverse temperature $1/\log n$, where $n$ is the number
of training samples. We mathematically prove that WBIC has the same asymptotic
expansion as the Bayes free energy, even if a statistical model is singular for
and unrealizable by a statistical model. Since WBIC can be numerically
calculated without any information about a true distribution, it is a
generalized version of BIC onto singular statistical models.
"
"  Measurement involves the determination of quantitative estimates of physical
quantities from experiment, along with estimates of their associated
uncertainties. Herewith an experimental system model is the key to extracting
information from the experimental data. The measurement information obtained
depends directly on the quality of the model. With this concern novel
regression modelling techniques have been fashioned by data integration from
computer-simulation and physical designed experiments. These techniques have
allowed attaining the advanced level of model completeness, parsimony, and
precision via approximation of the exact unknown model by mathematical product
of available theoretical and appropriate empirical functions. The purpose of
this approximation is to represent adequately the true model on the considered
region of factor space with all advantages of theoretical modelling. This
allows a further focus on the measurement science of issue. Pneumatic gauge
hybrid data candidate model building, solving and validation reviled that such
adequate models permit to attain minimum discrepancy from empirical evidence.
"
"  This paper proposes a new estimation algorithm for the parameters of an HMM
as to best account for the observed data. In this model, in addition to the
observation sequence, we have \emph{partial} and \emph{noisy} access to the
hidden state sequence as side information. This access can be seen as ""partial
labeling"" of the hidden states. Furthermore, we model possible mislabeling in
the side information in a joint framework and derive the corresponding EM
updates accordingly. In our simulations, we observe that using this side
information, we considerably improve the state recognition performance, up to
70%, with respect to the ""achievable margin"" defined by the baseline
algorithms. Moreover, our algorithm is shown to be robust to the training
conditions.
"
"  The problem of hierarchical clustering items from pairwise similarities is
found across various scientific disciplines, from biology to networking. Often,
applications of clustering techniques are limited by the cost of obtaining
similarities between pairs of items. While prior work has been developed to
reconstruct clustering using a significantly reduced set of pairwise
similarities via adaptive measurements, these techniques are only applicable
when choice of similarities are available to the user. In this paper, we
examine reconstructing hierarchical clustering under similarity observations
at-random. We derive precise bounds which show that a significant fraction of
the hierarchical clustering can be recovered using fewer than all the pairwise
similarities. We find that the correct hierarchical clustering down to a
constant fraction of the total number of items (i.e., clusters sized O(N)) can
be found using only O(N log N) randomly selected pairwise similarities in
expectation.
"
"  Mobile robots require basic information to navigate through an environment:
they need to know where they are (localization) and they need to know where
they are going. For the latter, robots need a map of the environment. Using
sensors of a variety of forms, robots gather information as they move through
an environment in order to build a map. In this paper we present a novel
sampling algorithm to solving the simultaneous mapping and localization (SLAM)
problem in indoor environments. We approach the problem from a Bayesian
statistics perspective. The data correspond to a set of range finder and
odometer measurements, obtained at discrete time instants. We focus on the
estimation of the posterior distribution over the space of possible maps given
the data. By exploiting different factorizations of this distribution, we
derive three sampling algorithms based on importance sampling. We illustrate
the results of our approach by testing the algorithms with two real data sets
obtained through robot navigation inside office buildings at Carnegie Mellon
University and the Pontificia Universidad Catolica de Chile.
"
"  This paper concerns the problem of matrix completion, which is to estimate a
matrix from observations in a small subset of indices. We propose a calibrated
spectrum elastic net method with a sum of the nuclear and Frobenius penalties
and develop an iterative algorithm to solve the convex minimization problem.
The iterative algorithm alternates between imputing the missing entries in the
incomplete matrix by the current guess and estimating the matrix by a scaled
soft-thresholding singular value decomposition of the imputed matrix until the
resulting matrix converges. A calibration step follows to correct the bias
caused by the Frobenius penalty. Under proper coherence conditions and for
suitable penalties levels, we prove that the proposed estimator achieves an
error bound of nearly optimal order and in proportion to the noise level. This
provides a unified analysis of the noisy and noiseless matrix completion
problems. Simulation results are presented to compare our proposal with
previous ones.
"
"  The sparse group lasso optimization problem is solved using a coordinate
gradient descent algorithm. The algorithm is applicable to a broad class of
convex loss functions. Convergence of the algorithm is established, and the
algorithm is used to investigate the performance of the multinomial sparse
group lasso classifier. On three different real data examples the multinomial
group lasso clearly outperforms multinomial lasso in terms of achieved
classification error rate and in terms of including fewer features for the
classification. The run-time of our sparse group lasso implementation is of the
same order of magnitude as the multinomial lasso algorithm implemented in the R
package glmnet. Our implementation scales well with the problem size. One of
the high dimensional examples considered is a 50 class classification problem
with 10k features, which amounts to estimating 500k parameters. The
implementation is available as the R package msgl.
"
"  We introduce an extension of nonparametric DS inference for arbitrary
univariate CDFs to the case in which some failure times are (right)-censored,
and then apply this to the problem of assessing evidence regarding assertions
about relative risks across two populations. The approach enables exploration
of the sensitivity of survival analyses to assumed independence of the missing
data process and the failure proces. We present an application to the partially
efficacious RV144 (HIV-1) vaccine trial, and show that the strength of
conclusions of vaccine efficacy depend on assumptions about the maximum failure
rates of the subjects lost-to-followup.
"
"  Latent variable models are used to estimate variables of interest quantities
which are observable only up to some measurement error. In many studies, such
variables are known but not precisely quantifiable (such as ""job satisfaction""
in social sciences and marketing, ""analytical ability"" in educational testing,
or ""inflation"" in economics). This leads to the development of measurement
instruments to record noisy indirect evidence for such unobserved variables
such as surveys, tests and price indexes. In such problems, there are
postulated latent variables and a given measurement model. At the same time,
other unantecipated latent variables can add further unmeasured confounding to
the observed variables. The problem is how to deal with unantecipated latents
variables. In this paper, we provide a method loosely inspired by canonical
correlation that makes use of background information concerning the ""known""
latent variables. Given a partially specified structure, it provides a
structure learning approach to detect ""unknown unknowns,"" the confounding
effect of potentially infinitely many other latent variables. This is done
without explicitly modeling such extra latent factors. Because of the special
structure of the problem, we are able to exploit a new variation of composite
likelihood fitting to efficiently learn this structure. Validation is provided
with experiments in synthetic data and the analysis of a large survey done with
a sample of over 100,000 staff members of the National Health Service of the
United Kingdom.
"
"  Power laws pervade statistical physics and complex systems, but,
traditionally, researchers in these fields have paid little attention to
properly fit these distributions. Who has not seen (or even shown) a log-log
plot of a completely curved line pretending to be a power law? Recently,
Clauset et al. have proposed a method to decide if a set of values of a
variable has a distribution whose tail is a power law. The key of their
procedure is the identification of the minimum value of the variable for which
the fit holds, which is selected as the value for which the Kolmogorov-Smirnov
distance between the empirical distribution and its maximum-likelihood fit is
minimum. However, it has been shown that this method can reject the power-law
hypothesis even in the case of power-law simulated data. Here we propose a
simpler selection criterion, which is illustrated with the more involving case
of discrete power-law distributions.
"
"  We consider to learn a causal ordering of variables in a linear non-Gaussian
acyclic model called LiNGAM. Several existing methods have been shown to
consistently estimate a causal ordering assuming that all the model assumptions
are correct. But, the estimation results could be distorted if some assumptions
actually are violated. In this paper, we propose a new algorithm for learning
causal orders that is robust against one typical violation of the model
assumptions: latent confounders. We demonstrate the effectiveness of our method
using artificial data.
"
"  Motivation: Cell-biological processes are regulated through a complex network
of interactions between genes and their products. The processes, their
activating conditions, and the associated transcriptional responses are often
unknown. Organism-wide modeling of network activation can reveal unique and
shared mechanisms between physiological conditions, and potentially as yet
unknown processes. We introduce a novel approach for organism-wide discovery
and analysis of transcriptional responses in interaction networks. The method
searches for local, connected regions in a network that exhibit coordinated
transcriptional response in a subset of conditions. Known interactions between
genes are used to limit the search space and to guide the analysis. Validation
on a human pathway network reveals physiologically coherent responses,
functional relatedness between physiological conditions, and coordinated,
context-specific regulation of the genes. Availability: Implementation is
freely available in R and Matlab at http://netpro.r-forge.r-project.org
"
"  The arrival process of bidders and bids in online auctions is important for
studying and modeling supply and demand in the online marketplace. A popular
assumption in the online auction literature is that a Poisson bidder arrival
process is a reasonable approximation. This approximation underlies theoretical
derivations, statistical models and simulations used in field studies. However,
when it comes to the bid arrivals, empirical research has shown that the
process is far from Poisson, with early bidding and last-moment bids taking
place. An additional feature that has been reported by various authors is an
apparent self-similarity in the bid arrival process. Despite the wide evidence
for the changing bidding intensities and the self-similarity, there has been no
rigorous attempt at developing a model that adequately approximates bid
arrivals and accounts for these features. The goal of this paper is to
introduce a family of distributions that well-approximate the bid time
distribution in hard-close auctions. We call this the BARISTA process (Bid
ARrivals In STAges) because of its ability to generate different intensities at
different stages. We describe the properties of this model, show how to
simulate bid arrivals from it, and how to use it for estimation and inference.
We illustrate its power and usefulness by fitting simulated and real data from
eBay.com. Finally, we show how a Poisson bidder arrival process relates to a
BARISTA bid arrival process.
"
"  In this paper, the Weiss-Weinstein bound is analyzed in the context of
sources localization with a planar array of sensors. Both conditional and
unconditional source signal models are studied. First, some results are given
in the multiple sources context without specifying the structure of the
steering matrix and of the noise covariance matrix. Moreover, the case of an
uniform or Gaussian prior are analyzed. Second, these results are applied to
the particular case of a single source for two kinds of array geometries: a
non-uniform linear array (elevation only) and an arbitrary planar (azimuth and
elevation) array.
"
"  Kernel Induced Random Survival Forests (KIRSF) is a statistical learning
algorithm which aims to improve prediction accuracy for survival data. As in
Random Survival Forests (RSF), Cumulative Hazard Function is predicted for each
individual in the test set. Prediction error is estimated using Harrell's
concordance index (C index) [Harrell et al. (1982)]. The C-index can be
interpreted as a misclassification probability and does not depend on a single
fixed time for evaluation. The C-index also specifically accounts for
censoring. By utilizing kernel functions, KIRSF achieves better results than
RSF in many situations. In this report, we show how to incorporate kernel
functions into RSF. We test the performance of KIRSF and compare our method to
RSF. We find that the KIRSF's performance is better than RSF in many occasions.
"
"  The accurate quantification of gene expression levels is crucial for
transcriptome study. Microarray platforms are commonly used for simultaneously
interrogating thousands of genes in the past decade, and recently RNA-Seq has
emerged as a promising alternative. The gene expression measurements obtained
by microarray and RNA-Seq are, however, subject to various measurement errors.
A third platform called qRT-PCR is acknowledged to provide more accurate
quantification of gene expression levels than microarray and RNA-Seq, but it
has limited throughput capacity. In this article, we propose to use a system of
functional measurement error models to model gene expression measurements and
calibrate the microarray and RNA-Seq platforms with qRT-PCR. Based on the
system, a two-step approach was developed to estimate the biases and error
variance components of the three platforms and calculate calibrated estimates
of gene expression levels. The estimated biases and variance components shed
light on the relative strengths and weaknesses of the three platforms and the
calibrated estimates provide a more accurate and consistent quantification of
gene expression levels. Theoretical and simulation studies were conducted to
establish the properties of those estimates. The system was applied to analyze
two gene expression data sets from the Microarray Quality Control (MAQC) and
Sequencing Quality Control (SEQC) projects.
"
"  Minimizing the rank of a matrix subject to constraints is a challenging
problem that arises in many applications in control theory, machine learning,
and discrete geometry. This class of optimization problems, known as rank
minimization, is NP-HARD, and for most practical problems there are no
efficient algorithms that yield exact solutions. A popular heuristic algorithm
replaces the rank function with the nuclear norm--equal to the sum of the
singular values--of the decision variable. In this paper, we provide a
necessary and sufficient condition that quantifies when this heuristic
successfully finds the minimum rank solution of a linear constraint set. We
additionally provide a probability distribution over instances of the affine
rank minimization problem such that instances sampled from this distribution
satisfy our conditions for success with overwhelming probability provided the
number of constraints is appropriately large. Finally, we give empirical
evidence that these probabilistic bounds provide accurate predictions of the
heuristic's performance in non-asymptotic scenarios.
"
"  Imaging spectrometers measure electromagnetic energy scattered in their
instantaneous field view in hundreds or thousands of spectral channels with
higher spectral resolution than multispectral cameras. Imaging spectrometers
are therefore often referred to as hyperspectral cameras (HSCs). Higher
spectral resolution enables material identification via spectroscopic analysis,
which facilitates countless applications that require identifying materials in
scenarios unsuitable for classical spectroscopic analysis. Due to low spatial
resolution of HSCs, microscopic material mixing, and multiple scattering,
spectra measured by HSCs are mixtures of spectra of materials in a scene. Thus,
accurate estimation requires unmixing. Pixels are assumed to be mixtures of a
few materials, called endmembers. Unmixing involves estimating all or some of:
the number of endmembers, their spectral signatures, and their abundances at
each pixel. Unmixing is a challenging, ill-posed inverse problem because of
model inaccuracies, observation noise, environmental conditions, endmember
variability, and data set size. Researchers have devised and investigated many
models searching for robust, stable, tractable, and accurate unmixing
algorithms. This paper presents an overview of unmixing methods from the time
of Keshava and Mustard's unmixing tutorial [1] to the present. Mixing models
are first discussed. Signal-subspace, geometrical, statistical, sparsity-based,
and spatial-contextual unmixing algorithms are described. Mathematical problems
and potential solutions are described. Algorithm characteristics are
illustrated experimentally.
"
"  Sinopoli et al. (TAC, 2004) considered the problem of optimal estimation for
linear systems with Gaussian noise and intermittent observations, available
according to a Bernoulli arrival process. They showed that there is a
""critical"" arrival probability of the observations, such that under that
threshold the expected value of the covariance matrix (i.e., the quadratic
error) of the estimate is unbounded. Sinopoli et al., and successive authors,
interpreted this result implying that the behavior of the system is
qualitatively different above and below the threshold. This paper shows that
this is not necessarily the only interpretation. In fact, the critical
probability is different if one considers the average error instead of the
average quadratic error. More generally, finding a meaningful ""average""
covariance is not as simple as taking the algebraic expected value. A rigorous
way to frame the problem is in a differential geometric framework, by
recognizing that the set of covariance matrices (or better, the manifold of
Gaussian distributions) is not a flat space, and then studying the intrinsic
Riemannian mean. Several metrics on this manifold are considered that lead to
different critical probabilities, or no critical probability at all.
"
"  In this paper we present a spatially-adaptive method for image reconstruction
that is based on the concept of statistical multiresolution estimation as
introduced in [Frick K, Marnitz P, and Munk A. ""Statistical multiresolution
Dantzig estimation in imaging: Fundamental concepts and algorithmic framework"".
Electron. J. Stat., 6:231-268, 2012]. It constitutes a variational
regularization technique that uses an supremum-type distance measure as
data-fidelity combined with a convex cost functional. The resulting convex
optimization problem is approached by a combination of an inexact alternating
direction method of multipliers and Dykstra's projection algorithm. We describe
a novel method for balancing data-fit and regularity that is fully automatic
and allows for a sound statistical interpretation. The performance of our
estimation approach is studied for various problems in imaging. Among others,
this includes deconvolution problems that arise in Poisson nanoscale
fluorescence microscopy.
"
"  A new class of stochastic field models is constructed using nested stochastic
partial differential equations (SPDEs). The model class is computationally
efficient, applicable to data on general smooth manifolds, and includes both
the Gaussian Mat\'{e}rn fields and a wide family of fields with oscillating
covariance functions. Nonstationary covariance models are obtained by spatially
varying the parameters in the SPDEs, and the model parameters are estimated
using direct numerical optimization, which is more efficient than standard
Markov Chain Monte Carlo procedures. The model class is used to estimate daily
ozone maps using a large data set of spatially irregular global total column
ozone data.
"
"  This paper considers the problem of clustering a partially observed
unweighted graph---i.e., one where for some node pairs we know there is an edge
between them, for some others we know there is no edge, and for the remaining
we do not know whether or not there is an edge. We want to organize the nodes
into disjoint clusters so that there is relatively dense (observed)
connectivity within clusters, and sparse across clusters.
  We take a novel yet natural approach to this problem, by focusing on finding
the clustering that minimizes the number of ""disagreements""---i.e., the sum of
the number of (observed) missing edges within clusters, and (observed) present
edges across clusters. Our algorithm uses convex optimization; its basis is a
reduction of disagreement minimization to the problem of recovering an
(unknown) low-rank matrix and an (unknown) sparse matrix from their partially
observed sum. We evaluate the performance of our algorithm on the classical
Planted Partition/Stochastic Block Model. Our main theorem provides sufficient
conditions for the success of our algorithm as a function of the minimum
cluster size, edge density and observation probability; in particular, the
results characterize the tradeoff between the observation probability and the
edge density gap. When there are a constant number of clusters of equal size,
our results are optimal up to logarithmic factors.
"
"  Given a finite family of functions, the goal of model selection aggregation
is to construct a procedure that mimics the function from this family that is
the closest to an unknown regression function. More precisely, we consider a
general regression model with fixed design and measure the distance between
functions by the mean squared error at the design points. While procedures
based on exponential weights are known to solve the problem of model selection
aggregation in expectation, they are, surprisingly, sub-optimal in deviation.
We propose a new formulation called Q-aggregation that addresses this
limitation; namely, its solution leads to sharp oracle inequalities that are
optimal in a minimax sense. Moreover, based on the new formulation, we design
greedy Q-aggregation procedures that produce sparse aggregation models
achieving the optimal rate. The convergence and performance of these greedy
procedures are illustrated and compared with other standard methods on
simulated examples.
"
"  Approximate Bayesian computation (ABC), also known as likelihood-free
methods, have become a favourite tool for the analysis of complex stochastic
models, primarily in population genetics but also in financial analyses. We
advocated in Grelaud et al. (2009) the use of ABC for Bayesian model choice in
the specific case of Gibbs random fields (GRF), relying on a sufficiency
property mainly enjoyed by GRFs to show that the approach was legitimate.
Despite having previously suggested the use of ABC for model choice in a wider
range of models in the DIY ABC software (Cornuet et al., 2008), we present
theoretical evidence that the general use of ABC for model choice is fraught
with danger in the sense that no amount of computation, however large, can
guarantee a proper approximation of the posterior probabilities of the models
under comparison.
"
"  We propose a novel approach which employs random sampling to generate an
accurate non-uniform mesh for numerically solving Partial Differential Equation
Boundary Value Problems (PDE-BVP's). From a uniform probability distribution U
over a 1D domain, we sample M discretizations of size N where M>>N. The
statistical moments of the solutions to a given BVP on each of the M
ultra-sparse meshes provide insight into identifying highly accurate
non-uniform meshes. Essentially, we use the pointwise mean and variance of the
coarse-grid solutions to construct a mapping Q(x) from uniformly to
non-uniformly spaced mesh-points. The error convergence properties of the
approximate solution to the PDE-BVP on the non-uniform mesh are superior to a
uniform mesh for a certain class of BVP's. In particular, the method works well
for BVP's with locally non-smooth solutions. We present a framework for
studying the sampled sparse-mesh solutions and provide numerical evidence for
the utility of this approach as applied to a set of example BVP's. We conclude
with a discussion of how the near-perfect paralellizability of our approach
suggests that these strategies have the potential for highly efficient
utilization of massively parallel multi-core technologies such as General
Purpose Graphics Processing Units (GPGPU's). We believe that the proposed
algorithm is beyond embarrassingly parallel; implementing it on anything but a
massively multi-core architecture would be scandalous.
"
"  High-dimensional data common in genomics, proteomics, and chemometrics often
contains complicated correlation structures. Recently, partial least squares
(PLS) and Sparse PLS methods have gained attention in these areas as dimension
reduction techniques in the context of supervised data analysis. We introduce a
framework for Regularized PLS by solving a relaxation of the SIMPLS
optimization problem with penalties on the PLS loadings vectors. Our approach
enjoys many advantages including flexibility, general penalties, easy
interpretation of results, and fast computation in high-dimensional settings.
We also outline extensions of our methods leading to novel methods for
Non-negative PLS and Generalized PLS, an adaption of PLS for structured data.
We demonstrate the utility of our methods through simulations and a case study
on proton Nuclear Magnetic Resonance (NMR) spectroscopy data.
"
"  Linear principal component analysis (PCA) can be extended to a nonlinear PCA
by using artificial neural networks. But the benefit of curved components
requires a careful control of the model complexity. Moreover, standard
techniques for model selection, including cross-validation and more generally
the use of an independent test set, fail when applied to nonlinear PCA because
of its inherent unsupervised characteristics. This paper presents a new
approach for validating the complexity of nonlinear PCA models by using the
error in missing data estimation as a criterion for model selection. It is
motivated by the idea that only the model of optimal complexity is able to
predict missing values with the highest accuracy. While standard test set
validation usually favours over-fitted nonlinear PCA models, the proposed model
validation approach correctly selects the optimal model complexity.
"
"  In the context of clustering, we consider a generative model in a Euclidean
ambient space with clusters of different shapes, dimensions, sizes and
densities. In an asymptotic setting where the number of points becomes large,
we obtain theoretical guaranties for a few emblematic methods based on pairwise
distances: a simple algorithm based on the extraction of connected components
in a neighborhood graph; the spectral clustering method of Ng, Jordan and
Weiss; and hierarchical clustering with single linkage. The methods are shown
to enjoy some near-optimal properties in terms of separation between clusters
and robustness to outliers. The local scaling method of Zelnik-Manor and Perona
is shown to lead to a near-optimal choice for the scale in the first two
methods. We also provide a lower bound on the spectral gap to consistently
choose the correct number of clusters in the spectral method.
"
"  Bayesian network classifiers are used in many fields, and one common class of
classifiers are naive Bayes classifiers. In this paper, we introduce an
approach for reasoning about Bayesian network classifiers in which we
explicitly convert them into Ordered Decision Diagrams (ODDs), which are then
used to reason about the properties of these classifiers. Specifically, we
present an algorithm for converting any naive Bayes classifier into an ODD, and
we show theoretically and experimentally that this algorithm can give us an ODD
that is tractable in size even given an intractable number of instances. Since
ODDs are tractable representations of classifiers, our algorithm allows us to
efficiently test the equivalence of two naive Bayes classifiers and
characterize discrepancies between them. We also show a number of additional
results including a count of distinct classifiers that can be induced by
changing some CPT in a naive Bayes classifier, and the range of allowable
changes to a CPT which keeps the current classifier unchanged.
"
"  Expander graphs are widely used in communication problems and construction of
error correcting codes. In such graphs, information gets through very quickly.
Typically, it is not true for social or biological networks, though we may find
a partition of the vertices such that the induced subgraphs on them and the
bipartite subgraphs between any pair of them exhibit regular behavior of
information flow within or between the subsets. Implications between spectral
and regularity properties are discussed.
"
"  We investigate the problem of sequentially predicting the binary labels on
the nodes of an arbitrary weighted graph. We show that, under a suitable
parametrization of the problem, the optimal number of prediction mistakes can
be characterized (up to logarithmic factors) by the cutsize of a random
spanning tree of the graph. The cutsize is induced by the unknown adversarial
labeling of the graph nodes. In deriving our characterization, we obtain a
simple randomized algorithm achieving in expectation the optimal mistake bound
on any polynomially connected weighted graph. Our algorithm draws a random
spanning tree of the original graph and then predicts the nodes of this tree in
constant expected amortized time and linear space. Experiments on real-world
datasets show that our method compares well to both global (Perceptron) and
local (label propagation) methods, while being generally faster in practice.
"
"  Conventional wisdom holds that influenza A and B are such genetically
dissimilar viruses that infection with one cannot confer cross-immunity to the
other. However, our examination of the records of the past 25 influenza seasons
in the U.S. reveals that almost every time there is an early and severe
influenza A outbreak, the annual influenza B epidemic is almost entirely
suppressed (and is never suppressed otherwise). Temporary broad-spectrum (aka
""heterologous"") immunity in the aftermath of influenza infection is the most
direct explanation for this phenomenon. We find a remarkably weak degree of
temporary cross-immunity is needed to explain these patterns, and that indeed
influenza B provides an ideal setting for the observation of heterologous
immune effects outside of the carefully controlled environment of a laboratory.
"
"  A stochastic epidemic model is defined in which each individual belongs to a
household, a secondary grouping (typically school or workplace) and also the
community as a whole. Moreover, infectious contacts take place in these three
settings according to potentially different rates. For this model we consider
how different kinds of data can be used to estimate the infection rate
parameters with a view to understanding what can and cannot be inferred, and
with what precision. Among other things we find that temporal data can be of
considerable inferential benefit compared to final size data, that the degree
of heterogeneity in the data can have a considerable effect on inference for
non-household transmission, and that inferences can be materially different
from those obtained from a model with two levels of mixing.
  Keywords: Basic reproduction number, Bayesian inference, Epidemic model,
Infectious disease data, Markov chain Monte Carlo, Networks.
"
"  The goal of cross-domain object matching (CDOM) is to find correspondence
between two sets of objects in different domains in an unsupervised way. Photo
album summarization is a typical application of CDOM, where photos are
automatically aligned into a designed frame expressed in the Cartesian
coordinate system. CDOM is usually formulated as finding a mapping from objects
in one domain (photos) to objects in the other domain (frame) so that the
pairwise dependency is maximized. A state-of-the-art CDOM method employs a
kernel-based dependency measure, but it has a drawback that the kernel
parameter needs to be determined manually. In this paper, we propose
alternative CDOM methods that can naturally address the model selection
problem. Through experiments on image matching, unpaired voice conversion, and
photo album summarization tasks, the effectiveness of the proposed methods is
demonstrated.
"
"  Sequential prediction problems such as imitation learning, where future
observations depend on previous predictions (actions), violate the common
i.i.d. assumptions made in statistical learning. This leads to poor performance
in theory and often in practice. Some recent approaches provide stronger
guarantees in this setting, but remain somewhat unsatisfactory as they train
either non-stationary or stochastic policies and require a large number of
iterations. In this paper, we propose a new iterative algorithm, which trains a
stationary deterministic policy, that can be seen as a no regret algorithm in
an online learning setting. We show that any such no regret algorithm, combined
with additional reduction assumptions, must find a policy with good performance
under the distribution of observations it induces in such sequential settings.
We demonstrate that this new approach outperforms previous approaches on two
challenging imitation learning problems and a benchmark sequence labeling
problem.
"
"  Minimizing the relative inertia of a statistical group with respect to the
inertia of the overall sample defines an unique point, the in-focus, which
constitutes a context-dependent measure of typical group tendency, biased in
comparison to the group centroid. Maximizing the relative inertia yields an
unique out-focal point, polarized in the reverse direction. This mechanism
evokes the relative variability reduction of the outgroup reported in Social
Psychology, and the stereotypic-like behavior of the in-focus, whose bias
vanishes if the outgroup is constituted of a single individual. In this
picture, the out-focus plays the role of an anti-stereotypical position,
identical to the in-focus of the complementary group.
"
"  The 2004 US Presidential Election cycle marked the debut of Internet-based
media such as blogs and social networking websites as institutionally
recognized features of the American political landscape. Using a longitudinal
sample of all DNC/RNC-designated blog-citation networks we are able to test the
influence of various strategic, institutional, and balance-theoretic mechanisms
and exogenous factors such as seasonality and political events on the
propensity of blogs to cite one another over time. Capitalizing on the temporal
resolution of our data, we utilize an autoregressive network regression
framework to carry out inference for a logistic choice process. Using a
combination of deviance-based model selection criteria and simulation-based
model adequacy tests, we identify the combination of processes that best
characterizes the choice behavior of the contending blogs.
"
"  The class of Schoenberg transformations, embedding Euclidean distances into
higher dimensional Euclidean spaces, is presented, and derived from theorems on
positive definite and conditionally negative definite matrices. Original
results on the arc lengths, angles and curvature of the transformations are
proposed, and visualized on artificial data sets by classical multidimensional
scaling. A simple distance-based discriminant algorithm illustrates the theory,
intimately connected to the Gaussian kernels of Machine Learning.
"
