abstract,category
"  In the last four years, daily deals have emerged from nowhere to become a
multi-billion dollar industry world-wide. Daily deal sites such as Groupon and
Livingsocial offer products and services at deep discounts to consumers via
email and social networks. As the industry matures, there are many questions
regarding the impact of daily deals on the marketplace. Important questions in
this regard concern the reasons why businesses decide to offer daily deals and
their longer-term impact on businesses. In the present paper, we investigate
whether the unobserved factors that make marketers run daily deals are
correlated with the unobserved factors that influence the business, In
particular, we employ the framework of seemingly unrelated regression to model
the correlation between the errors in predicting whether a business uses a
daily deal and the errors in predicting the business' survival. Our analysis
consists of the survival of 985 small businesses that offered daily deals
between January and July 2011 in the city of Chicago. Our results indicate that
there is a statistically significant correlation between the unobserved factors
that influence the business' decision to offer a daily deal and the unobserved
factors that impact its survival. Furthermore, our results indicate that the
correlation coefficient is significant in certain business categories (e.g.
restaurants).
",Applied
"  We propose a novel approach for density estimation with exponential families
for the case when the true density may not fall within the chosen family. Our
approach augments the sufficient statistics with features designed to
accumulate probability mass in the neighborhood of the observed points,
resulting in a non-parametric model similar to kernel density estimators. We
show that under mild conditions, the resulting model uses only the sufficient
statistics if the density is within the chosen exponential family, and
asymptotically, it approximates densities outside of the chosen exponential
family. Using the proposed approach, we modify the exponential random graph
model, commonly used for modeling small-size graph distributions, to address
the well-known issue of model degeneracy.
",ML
"  In this research, two-state Markov switching models are proposed to study
accident frequencies and severities. These models assume that there are two
unobserved states of roadway safety, and that roadway entities (e.g., roadway
segments) can switch between these states over time. The states are distinct,
in the sense that in the different states accident frequencies or severities
are generated by separate processes (e.g., Poisson, negative binomial,
multinomial logit). Bayesian inference methods and Markov Chain Monte Carlo
(MCMC) simulations are used for estimation of Markov switching models. To
demonstrate the applicability of the approach, we conduct the following three
studies. In the first study, two-state Markov switching count data models are
considered as an alternative to zero-inflated models for annual accident
frequencies, in order to account for preponderance of zeros typically observed
in accident frequency data. In the second study, two-state Markov switching
Poisson model and two-state Markov switching negative binomial model are
estimated using weekly accident frequencies on selected Indiana interstate
highway segments over a five-year time period. In the third study, two-state
Markov switching multinomial logit models are estimated for severity outcomes
of accidents occurring on Indiana roads over a four-year time period. One of
the most important results found in each of the three studies, is that in each
case the estimated Markov switching models are strongly favored by roadway
safety data and result in a superior statistical fit, as compared to the
corresponding standard (non-switching) models.
",Applied
"  This article considers the estimation of the number of severely disabled
people using data from the Italian survey on Health Conditions and Appeal to
Medicare. Disability is indirectly measured using a set of categorical items,
which survey a set of functions concerning the ability of a person to
accomplish everyday tasks. Latent Class Models can be employed to classify the
population according to different levels of a latent variable connected with
disability. The survey, however, is designed to provide reliable estimates at
the level of Administrative Regions (NUTS2 level), while local authorities are
interested in quantifying the amount of population that belongs to each latent
class at a sub-regional level. Therefore, small area estimation techniques
should be used. The challenge of the present application is that the variable
of interest is not directly observed. Adopting a full Bayesian approach, we
base small area estimation on a Latent Class model in which the probability of
belonging to each latent class changes with covariates and the influence of age
is learnt from the data using penalized splines. Deimmler-Reisch bases are
shown to improve speed and mixing of MCMC chains used to simulate posteriors.
",Applied
"  Markowitz's celebrated mean--variance portfolio optimization theory assumes
that the means and covariances of the underlying asset returns are known. In
practice, they are unknown and have to be estimated from historical data.
Plugging the estimates into the efficient frontier that assumes known
parameters has led to portfolios that may perform poorly and have
counter-intuitive asset allocation weights; this has been referred to as the
""Markowitz optimization enigma."" After reviewing different approaches in the
literature to address these difficulties, we explain the root cause of the
enigma and propose a new approach to resolve it. Not only is the new approach
shown to provide substantial improvements over previous methods, but it also
allows flexible modeling to incorporate dynamic features and fundamental
analysis of the training sample of historical data, as illustrated in
simulation and empirical studies.
",Applied
"  There is much interest in the Hierarchical Dirichlet Process Hidden Markov
Model (HDP-HMM) as a natural Bayesian nonparametric extension of the ubiquitous
Hidden Markov Model for learning from sequential and time-series data. However,
in many settings the HDP-HMM's strict Markovian constraints are undesirable,
particularly if we wish to learn or encode non-geometric state durations. We
can extend the HDP-HMM to capture such structure by drawing upon
explicit-duration semi-Markovianity, which has been developed mainly in the
parametric frequentist setting, to allow construction of highly interpretable
models that admit natural prior information on state durations.
  In this paper we introduce the explicit-duration Hierarchical Dirichlet
Process Hidden semi-Markov Model (HDP-HSMM) and develop sampling algorithms for
efficient posterior inference. The methods we introduce also provide new
methods for sampling inference in the finite Bayesian HSMM. Our modular Gibbs
sampling methods can be embedded in samplers for larger hierarchical Bayesian
models, adding semi-Markov chain modeling as another tool in the Bayesian
inference toolbox. We demonstrate the utility of the HDP-HSMM and our inference
methods on both synthetic and real experiments.
",Applied
"  In a multiple testing context, we consider a semiparametric mixture model
with two components where one component is known and corresponds to the
distribution of $p$-values under the null hypothesis and the other component
$f$ is nonparametric and stands for the distribution under the alternative
hypothesis. Motivated by the issue of local false discovery rate estimation, we
focus here on the estimation of the nonparametric unknown component $f$ in the
mixture, relying on a preliminary estimator of the unknown proportion $\theta$
of true null hypotheses. We propose and study the asymptotic properties of two
different estimators for this unknown component. The first estimator is a
randomly weighted kernel estimator. We establish an upper bound for its
pointwise quadratic risk, exhibiting the classical nonparametric rate of
convergence over a class of H\""older densities. To our knowledge, this is the
first result establishing convergence as well as corresponding rate for the
estimation of the unknown component in this nonparametric mixture. The second
estimator is a maximum smoothed likelihood estimator. It is computed through an
iterative algorithm, for which we establish a descent property. In addition,
these estimators are used in a multiple testing procedure in order to estimate
the local false discovery rate. Their respective performances are then compared
on synthetic data.
",Applied
"  Bayesian phylogenetic methods are generating noticeable enthusiasm in the
field of molecular systematics. Many phylogenetic models are often at stake and
different approaches are used to compare them within a Bayesian framework. The
Bayes factor, defined as the ratio of the marginal likelihoods of two competing
models, plays a key role in Bayesian model selection. We focus on an
alternative estimator of the marginal likelihood whose computation is still a
challenging problem. Several computational solutions have been proposed none of
which can be considered outperforming the others simultaneously in terms of
simplicity of implementation, computational burden and precision of the
estimates. Practitioners and researchers, often led by available software, have
privileged so far the simplicity of the harmonic mean estimator (HM) and the
arithmetic mean estimator (AM). However it is known that the resulting
estimates of the Bayesian evidence in favor of one model are biased and often
inaccurate up to having an infinite variance so that the reliability of the
corresponding conclusions is doubtful. Our new implementation of the
generalized harmonic mean (GHM) idea recycles MCMC simulations from the
posterior, shares the computational simplicity of the original HM estimator,
but, unlike it, overcomes the infinite variance issue. The alternative
estimator is applied to simulated phylogenetic data and produces fully
satisfactory results outperforming those simple estimators currently provided
by most of the publicly available software.
",Applied
"  A perturbative approach is used to derive approximations of arbitrary order
to estimate high percentiles of sums of positive independent random variables
that exhibit heavy tails. Closed-form expressions for the successive
approximations are obtained both when the number of terms in the sum is
deterministic and when it is random. The zeroth order approximation is the
percentile of the maximum term in the sum. Higher orders in the perturbative
series involve the right-truncated moments of the individual random variables
that appear in the sum. These censored moments are always finite. As a result,
and in contrast to previous approximations proposed in the literature, the
perturbative series has the same form regardless of whether these random
variables have a finite mean or not. The accuracy of the approximations is
illustrated for a variety of distributions and a wide range of parameters. The
quality of the estimate improves as more terms are included in the perturbative
series, specially for higher percentiles and heavier tails.
",Applied
"  The mean field methods, which entail approximating intractable probability
distributions variationally with distributions from a tractable family, enjoy
high efficiency, guaranteed convergence, and provide lower bounds on the true
likelihood. But due to requirement for model-specific derivation of the
optimization equations and unclear inference quality in various models, it is
not widely used as a generic approximate inference algorithm. In this paper, we
discuss a generalized mean field theory on variational approximation to a broad
class of intractable distributions using a rich set of tractable distributions
via constrained optimization over distribution spaces. We present a class of
generalized mean field (GMF) algorithms for approximate inference in complex
exponential family models, which entails limiting the optimization over the
class of cluster-factorizable distributions. GMF is a generic method requiring
no model-specific derivations. It factors a complex model into a set of
disjoint variable clusters, and uses a set of canonical fix-point equations to
iteratively update the cluster distributions, and converge to locally optimal
cluster marginals that preserve the original dependency structure within each
cluster, hence, fully decomposed the overall inference problem. We empirically
analyzed the effect of different tractable family (clusters of different
granularity) on inference quality, and compared GMF with BP on several
canonical models. Possible extension to higher-order MF approximation is also
discussed.
",ML
"  We present a max-margin nonparametric latent feature model, which unites the
ideas of max-margin learning and Bayesian nonparametrics to discover
discriminative latent features for link prediction and automatically infer the
unknown latent social dimension. By minimizing a hinge-loss using the linear
expectation operator, we can perform posterior inference efficiently without
dealing with a highly nonlinear link likelihood function; by using a
fully-Bayesian formulation, we can avoid tuning regularization constants.
Experimental results on real datasets appear to demonstrate the benefits
inherited from max-margin learning and fully-Bayesian nonparametric inference.
",ML
"  Boundary distance (BD) plotting is a technique for making orientation
invariant comparisons of the spatial distribution of biochemical markers within
and across cells/nuclei. Marker expression is aggregated over points with the
same distance from the boundary. We present a suite of tools for improved data
analysis and statistical inference using BD plotting. BD is computed using the
Euclidean distance transform after presmoothing and oversampling of nuclear
boundaries. Marker distribution profiles are averaged using smoothing with
linearly decreasing bandwidth. Average expression curves are scaled and
registered by x-axis dilation to compensate for uneven lighting and errors in
nuclear boundary marking. Penalized discriminant analysis is used to
characterize the quality of separation between average marker distributions. An
adaptive piecewise linear model is used to compare expression gradients in
intra, peri and extra nuclear zones. The techniques are illustrated by the
following: (a) a two sample problem involving a pair of voltage gated calcium
channels (Cav1.2 and AB70) marked in different cells; (b) a paired sample
problem of calcium channels (Y1F4 and RyR1) marked in the same cell.
",Applied
"  We introduce a recursive adaptive group lasso algorithm for real-time
penalized least squares prediction that produces a time sequence of optimal
sparse predictor coefficient vectors. At each time index the proposed algorithm
computes an exact update of the optimal $\ell_{1,\infty}$-penalized recursive
least squares (RLS) predictor. Each update minimizes a convex but
nondifferentiable function optimization problem. We develop an online homotopy
method to reduce the computational complexity. Numerical simulations
demonstrate that the proposed algorithm outperforms the $\ell_1$ regularized
RLS algorithm for a group sparse system identification problem and has lower
implementation complexity than direct group lasso solvers.
",ML
"  The use of 24-hour ambulatory blood pressure monitoring (ABPM) in clinical
practice and observational epidemiological studies has grown considerably in
the past 25 years. ABPM is a very effective technique for assessing biological,
environmental, and drug effects on blood pressure. In order to enhance the
effectiveness of ABPM for clinical and observational research studies via
analytical and graphical results, developing alternative data analysis
approaches are important. The linear mixed model for the analysis of
longitudinal data is particularly well-suited for the estimation of, inference
about, and interpretation of both population and subject-specific trajectories
for ABPM data. Subject-specific trajectories are of great importance in ABPM
studies, especially in clinical research, but little emphasis has been placed
on this dimension of the problem in the statistical analyses of the data. We
propose using a linear mixed model with orthonormal polynomials across time in
both the fixed and random effects to analyze ABPM data. Orthonormal polynomials
in the linear mixed model may be used to develop model-based, subject-specific
24-hour ABPM correlates of cardiovascular disease outcomes. We demonstrate the
proposed analysis technique using data from the Dietary Approaches to Stop
Hypertension (DASH) study, a multicenter, randomized, parallel arm feeding
study that tested the effects of dietary patterns on blood pressure.
",Applied
"  We study the statistical meaning of the minimization of distortion measure
and the relation between the equilibrium points of the SOM algorithm and the
minima of distortion measure. If we assume that the observations and the map
lie in an compact Euclidean space, we prove the strong consistency of the map
which almost minimizes the empirical distortion. Moreover, after calculating
the derivatives of the theoretical distortion measure, we show that the points
minimizing this measure and the equilibria of the Kohonen map do not match in
general. We illustrate, with a simple example, how this occurs.
",ML
"  We describe the underlying probabilistic interpretation of alpha and beta
divergences. We first show that beta divergences are inherently tied to Tweedie
distributions, a particular type of exponential family, known as exponential
dispersion models. Starting from the variance function of a Tweedie model, we
outline how to get alpha and beta divergences as special cases of Csisz\'ar's
$f$ and Bregman divergences. This result directly generalizes the well-known
relationship between the Gaussian distribution and least squares estimation to
Tweedie models and beta divergence minimization.
",ML
"  Sparse coding has been popularly used as an effective data representation
method in various applications, such as computer vision, medical imaging and
bioinformatics, etc. However, the conventional sparse coding algorithms and its
manifold regularized variants (graph sparse coding and Laplacian sparse
coding), learn the codebook and codes in a unsupervised manner and neglect the
class information available in the training set. To address this problem, in
this paper we propose a novel discriminative sparse coding method based on
multi-manifold, by learning discriminative class-conditional codebooks and
sparse codes from both data feature space and class labels. First, the entire
training set is partitioned into multiple manifolds according to the class
labels. Then, we formulate the sparse coding as a manifold-manifold matching
problem and learn class-conditional codebooks and codes to maximize the
manifold margins of different classes. Lastly, we present a data point-manifold
matching error based strategy to classify the unlabeled data point.
Experimental results on somatic mutations identification and breast tumors
classification in ultrasonic images tasks demonstrate the efficacy of the
proposed data representation-classification approach.
",ML
"  Graphical models with bi-directed edges (<->) represent marginal
independence: the absence of an edge between two vertices indicates that the
corresponding variables are marginally independent. In this paper, we consider
maximum likelihood estimation in the case of continuous variables with a
Gaussian joint distribution, sometimes termed a covariance graph model. We
present a new fitting algorithm which exploits standard regression techniques
and establish its convergence properties. Moreover, we contrast our procedure
to existing estimation methods.
",ML
"  We introduce a new method for training deep Boltzmann machines jointly. Prior
methods require an initial learning pass that trains the deep Boltzmann machine
greedily, one layer at a time, or do not perform well on classifi- cation
tasks.
",ML
"  Variable selection in high-dimensional space characterizes many contemporary
problems in scientific discovery and decision making. Many frequently-used
techniques are based on independence screening; examples include correlation
ranking (Fan and Lv, 2008) or feature selection using a two-sample t-test in
high-dimensional classification (Tibshirani et al., 2003). Within the context
of the linear model, Fan and Lv (2008)showed that this simple correlation
ranking possesses a sure independence screening property under certain
conditions and that its revision, called iteratively sure independent screening
(ISIS), is needed when the features are marginally unrelated but jointly
related to the response variable. In this paper, we extend ISIS, without
explicit definition of residuals, to a general pseudo-likelihood framework,
which includes generalized linear models as a special case. Even in the
least-squares setting, the new method improves ISIS by allowing variable
deletion in the iterative process. Our technique allows us to select important
features in high-dimensional classification where the popularly used two-sample
t-method fails. A new technique is introduced to reduce the false discovery
rate in the feature screening stage. Several simulated and two real data
examples are presented to illustrate the methodology.
",ML
"  Approximate dynamic programming is a popular method for solving large Markov
decision processes. This paper describes a new class of approximate dynamic
programming (ADP) methods- distributionally robust ADP-that address the curse
of dimensionality by minimizing a pessimistic bound on the policy loss. This
approach turns ADP into an optimization problem, for which we derive new
mathematical program formulations and analyze its properties. DRADP improves on
the theoretical guarantees of existing ADP methods-it guarantees convergence
and L1 norm based error bounds. The empirical evaluation of DRADP shows that
the theoretical guarantees translate well into good performance on benchmark
problems.
",ML
"  Respondent-driven sampling (RDS) is a widely used method for sampling from
hard-to-reach human populations, especially groups most at-risk for HIV/AIDS.
Data are collected through a peer-referral process in which current sample
members harness existing social networks to recruit additional sample members.
RDS has proven to be a practical method of data collection in many difficult
settings and has been adopted by leading public health organizations around the
world. Unfortunately, inference from RDS data requires many strong assumptions
because the sampling design is not fully known and is partially beyond the
control of the researcher. In this paper, we introduce diagnostic tools for
most of the assumptions underlying RDS inference. We also apply these
diagnostics in a case study of 12 populations at increased risk for HIV/AIDS.
We developed these diagnostics to enable RDS researchers to better understand
their data and to encourage future statistical research on RDS.
",Applied
"  We analyze the generalized Mallows model, a popular exponential model over
rankings. Estimating the central (or consensus) ranking from data is NP-hard.
We obtain the following new results: (1) We show that search methods can
estimate both the central ranking pi0 and the model parameters theta exactly.
The search is n! in the worst case, but is tractable when the true distribution
is concentrated around its mode; (2) We show that the generalized Mallows model
is jointly exponential in (pi0; theta), and introduce the conjugate prior for
this model class; (3) The sufficient statistics are the pairwise marginal
probabilities that item i is preferred to item j. Preliminary experiments
confirm the theoretical predictions and compare the new algorithm and existing
heuristics.
",ML
"  Ensembles of classification and regression trees remain popular machine
learning methods because they define flexible non-parametric models that
predict well and are computationally efficient both during training and
testing. During induction of decision trees one aims to find predicates that
are maximally informative about the prediction target. To select good
predicates most approaches estimate an information-theoretic scoring function,
the information gain, both for classification and regression problems. We point
out that the common estimation procedures are biased and show that by replacing
them with improved estimators of the discrete and the differential entropy we
can obtain better decision trees. In effect our modifications yield improved
predictive performance and are simple to implement in any decision tree code.
",ML
"  Conditional Random Fields (CRFs) are undirected graphical models, a special
case of which correspond to conditionally-trained finite state machines. A key
advantage of these models is their great flexibility to include a wide array of
overlapping, multi-granularity, non-independent features of the input. In face
of this freedom, an important question that remains is, what features should be
used? This paper presents a feature induction method for CRFs. Founded on the
principle of constructing only those feature conjunctions that significantly
increase log-likelihood, the approach is based on that of Della Pietra et al
[1997], but altered to work with conditional rather than joint probabilities,
and with additional modifications for providing tractability specifically for a
sequence model. In comparison with traditional approaches, automated feature
induction offers both improved accuracy and more than an order of magnitude
reduction in feature count; it enables the use of richer, higher-order Markov
models, and offers more freedom to liberally guess about which atomic features
may be relevant to a task. The induction method applies to linear-chain CRFs,
as well as to more arbitrary CRF structures, also known as Relational Markov
Networks [Taskar & Koller, 2002]. We present experimental results on a named
entity extraction task.
",ML
"  We present a novel clustering approach for moving object trajectories that
are constrained by an underlying road network. The approach builds a similarity
graph based on these trajectories then uses modularity-optimization hiearchical
graph clustering to regroup trajectories with similar profiles. Our
experimental study shows the superiority of the proposed approach over classic
hierarchical clustering and gives a brief insight to visualization of the
clustering results.
",ML
"  Gaussian process (GP) predictors are an important component of many Bayesian
approaches to machine learning. However, even a straightforward implementation
of Gaussian process regression (GPR) requires O(n^2) space and O(n^3) time for
a dataset of n examples. Several approximation methods have been proposed, but
there is a lack of understanding of the relative merits of the different
approximations, and in what situations they are most useful. We recommend
assessing the quality of the predictions obtained as a function of the compute
time taken, and comparing to standard baselines (e.g., Subset of Data and
FITC). We empirically investigate four different approximation algorithms on
four different prediction problems, and make our code available to encourage
future comparisons.
",ML
"  For any family of measurable sets in a probability space, we show that either
(i) the family has infinite Vapnik-Chervonenkis (VC) dimension or (ii) for
every epsilon > 0 there is a finite partition pi such the pi-boundary of each
set has measure at most epsilon. Immediate corollaries include the fact that a
family with finite VC dimension has finite bracketing numbers, and satisfies
uniform laws of large numbers for every ergodic process. From these
corollaries, we derive analogous results for VC major and VC graph families of
functions.
",ML
"  The stationary distribution of allele frequencies under a variety of
Wright--Fisher $k$-allele models with selection and parent independent mutation
is well studied. However, the statistical properties of maximum likelihood
estimates of parameters under these models are not well understood. Under each
of these models there is a point in data space which carries the strongest
possible signal for selection, yet, at this point, the likelihood is unbounded.
This result remains valid even if all of the mutation parameters are assumed to
be known. Therefore, standard simulation approaches used to approximate the
sampling distribution of the maximum likelihood estimate produce numerically
unstable results in the presence of substantial selection. We describe the
Bayesian alternative where the posterior distribution tends to produce more
accurate and reliable interval estimates for the selection intensity at a
locus.
",Applied
"  We describe a nonparametric topic model for labeled data. The model uses a
mixture of random measures (MRM) as a base distribution of the Dirichlet
process (DP) of the HDP framework, so we call it the DP-MRM. To model labeled
data, we define a DP distributed random measure for each label, and the
resulting model generates an unbounded number of topics for each label. We
apply DP-MRM on single-labeled and multi-labeled corpora of documents and
compare the performance on label prediction with MedLDA, LDA-SVM, and
Labeled-LDA. We further enhance the model by incorporating ddCRP and modeling
multi-labeled images for image segmentation and object labeling, comparing the
performance with nCuts and rddCRP.
",ML
"  The stochastic block model is a powerful tool for inferring community
structure from network topology. However, it predicts a Poisson degree
distribution within each community, while most real-world networks have a
heavy-tailed degree distribution. The degree-corrected block model can
accommodate arbitrary degree distributions within communities. But since it
takes the vertex degrees as parameters rather than generating them, it cannot
use them to help it classify the vertices, and its natural generalization to
directed graphs cannot even use the orientations of the edges. In this paper,
we present variants of the block model with the best of both worlds: they can
use vertex degrees and edge orientations in the classification process, while
tolerating heavy-tailed degree distributions within communities. We show that
for some networks, including synthetic networks and networks of word
adjacencies in English text, these new block models achieve a higher accuracy
than either standard or degree-corrected block models.
",ML
"  Typical cohorts in brain imaging studies are not large enough for systematic
testing of all the information contained in the images. To build testable
working hypotheses, investigators thus rely on analysis of previous work,
sometimes formalized in a so-called meta-analysis. In brain imaging, this
approach underlies the specification of regions of interest (ROIs) that are
usually selected on the basis of the coordinates of previously detected
effects. In this paper, we propose to use a database of images, rather than
coordinates, and frame the problem as transfer learning: learning a
discriminant model on a reference task to apply it to a different but related
new task. To facilitate statistical analysis of small cohorts, we use a sparse
discriminant model that selects predictive voxels on the reference task and
thus provides a principled procedure to define ROIs. The benefits of our
approach are twofold. First it uses the reference database for prediction, i.e.
to provide potential biomarkers in a clinical setting. Second it increases
statistical power on the new task. We demonstrate on a set of 18 pairs of
functional MRI experimental conditions that our approach gives good prediction.
In addition, on a specific transfer situation involving different scanners at
different locations, we show that voxel selection based on transfer learning
leads to higher detection power on small cohorts.
",ML
"  This paper proposes a new probabilistic classification algorithm using a
Markov random field approach. The joint distribution of class labels is
explicitly modelled using the distances between feature vectors. Intuitively, a
class label should depend more on class labels which are closer in the feature
space, than those which are further away. Our approach builds on previous work
by Holmes and Adams (2002, 2003) and Cucala et al. (2008). Our work shares many
of the advantages of these approaches in providing a probabilistic basis for
the statistical inference. In comparison to previous work, we present a more
efficient computational algorithm to overcome the intractability of the Markov
random field model. The results of our algorithm are encouraging in comparison
to the k-nearest neighbour algorithm.
",Applied
"  This paper presents a dynamic linear model for modeling hourly ozone
concentrations over the eastern United States. That model, which is developed
within an Bayesian hierarchical framework, inherits the important feature of
such models that its coefficients, treated as states of the process, can change
with time. Thus the model includes a time--varying site invariant mean field as
well as time varying coefficients for 24 and 12 diurnal cycle components. This
cost of this model's great flexibility comes at the cost of computational
complexity, forcing us to use an MCMC approach and to restrict application of
our model domain to a small number of monitoring sites. We critically assess
this model and discover some of its weaknesses in this type of application.
",Applied
"  The proliferation of models for networks raises challenging problems of model
selection: the data are sparse and globally dependent, and models are typically
high-dimensional and have large numbers of latent variables. Together, these
issues mean that the usual model-selection criteria do not work properly for
networks. We illustrate these challenges, and show one way to resolve them, by
considering the key network-analysis problem of dividing a graph into
communities or blocks of nodes with homogeneous patterns of links to the rest
of the network. The standard tool for doing this is the stochastic block model,
under which the probability of a link between two nodes is a function solely of
the blocks to which they belong. This imposes a homogeneous degree distribution
within each block; this can be unrealistic, so degree-corrected block models
add a parameter for each node, modulating its over-all degree. The choice
between ordinary and degree-corrected block models matters because they make
very different inferences about communities. We present the first principled
and tractable approach to model selection between standard and degree-corrected
block models, based on new large-graph asymptotics for the distribution of
log-likelihood ratios under the stochastic block model, finding substantial
departures from classical results for sparse graphs. We also develop
linear-time approximations for log-likelihoods under both the stochastic block
model and the degree-corrected model, using belief propagation. Applications to
simulated and real networks show excellent agreement with our approximations.
Our results thus both solve the practical problem of deciding on degree
correction, and point to a general approach to model selection in network
analysis.
",ML
"  Detection of rare variants by resequencing is important for the
identification of individuals carrying disease variants. Rapid sequencing by
new technologies enables low-cost resequencing of target regions, although it
is still prohibitive to test more than a few individuals. In order to improve
cost trade-offs, it has recently been suggested to apply pooling designs which
enable the detection of carriers of rare alleles in groups of individuals.
However, this was shown to hold only for a relatively low number of individuals
in a pool, and requires the design of pooling schemes for particular cases.
  We propose a novel pooling design, based on a compressed sensing approach,
which is both general, simple and efficient. We model the experimental
procedure and show via computer simulations that it enables the recovery of
rare allele carriers out of larger groups than were possible before, especially
in situations where high coverage is obtained for each individual.
  Our approach can also be combined with barcoding techniques to enhance
performance and provide a feasible solution based on current resequencing
costs. For example, when targeting a small enough genomic region (~100
base-pairs) and using only ~10 sequencing lanes and ~10 distinct barcodes, one
can recover the identity of 4 rare allele carriers out of a population of over
4000 individuals.
",Applied
"  In this paper, we propose a Bayesian MAP estimator for solving the
deconvolution problems when the observations are corrupted by Poisson noise.
Towards this goal, a proper data fidelity term (log-likelihood) is introduced
to reflect the Poisson statistics of the noise. On the other hand, as a prior,
the images to restore are assumed to be positive and sparsely represented in a
dictionary of waveforms such as wavelets or curvelets. Both analysis and
synthesis-type sparsity priors are considered. Piecing together the data
fidelity and the prior terms, the deconvolution problem boils down to the
minimization of non-smooth convex functionals (for each prior). We establish
the well-posedness of each optimization problem, characterize the corresponding
minimizers, and solve them by means of proximal splitting algorithms
originating from the realm of non-smooth convex optimization theory.
Experimental results are conducted to demonstrate the potential applicability
of the proposed algorithms to astronomical imaging datasets.
",Applied
"  One conjecture in both deep learning and classical connectionist viewpoint is
that the biological brain implements certain kinds of deep networks as its
back-end. However, to our knowledge, a detailed correspondence has not yet been
set up, which is important if we want to bridge between neuroscience and
machine learning. Recent researches emphasized the biological plausibility of
Linear-Nonlinear-Poisson (LNP) neuron model. We show that with neurally
plausible settings, the whole network is capable of representing any Boltzmann
machine and performing a semi-stochastic Bayesian inference algorithm lying
between Gibbs sampling and variational inference.
",ML
"  Estimation of small failure probabilities is one of the most important and
challenging computational problems in reliability engineering. The failure
probability is usually given by an integral over a high-dimensional uncertain
parameter space that is difficult to evaluate numerically. This paper focuses
on enhancements to Subset Simulation (SS), proposed by Au and Beck, which
provides an efficient algorithm based on MCMC (Markov chain Monte Carlo)
simulation for computing small failure probabilities for general
high-dimensional reliability problems. First, we analyze the Modified
Metropolis algorithm (MMA), an MCMC technique, which is used in SS for sampling
from high-dimensional conditional distributions. We present some observations
on the optimal scaling of MMA, and develop an optimal scaling strategy for this
algorithm when it is employed within SS. Next, we provide a theoretical basis
for the optimal value of the conditional failure probability $p_0$, an
important parameter one has to choose when using SS. Finally, a Bayesian
post-processor SS+ for the original SS method is developed where the uncertain
failure probability that one is estimating is modeled as a stochastic variable
whose possible values belong to the unit interval. Simulated samples from SS
are viewed as informative data relevant to the system's reliability. Instead of
a single real number as an estimate, SS+ produces the posterior PDF of the
failure probability, which takes into account both prior information and the
information in the sampled data. This PDF quantifies the uncertainty in the
value of the failure probability and it may be further used in risk analyses to
incorporate this uncertainty. The relationship between the original SS and SS+
is also discussed
",Applied
"  We study a problem of distributed detection of a stationary point event in a
large extent wireless sensor network ($\wsn$), where the event influences the
observations of the sensors only in the vicinity of where it occurs. An event
occurs at an unknown time and at a random location in the coverage region (or
region of interest ($\ROI$)) of the $\wsn$. We consider a general sensing model
in which the effect of the event at a sensor node depends on the distance
between the event and the sensor node; in particular, in the Boolean sensing
model, all sensors in a disk of a given radius around the event are equally
affected. Following the prior work reported in
\cite{nikiforov95change_isolation},
\cite{nikiforov03lower-bound-for-det-isolation},
\cite{tartakovsky08multi-decision}, {\em the problem is formulated as that of
detecting the event and locating it to a subregion of the $\ROI$ as early as
possible under the constraints that the average run length to false alarm
($\tfa$) is bounded below by $\gamma$, and the probability of false isolation
($\pfi$) is bounded above by $\alpha$}, where $\gamma$ and $\alpha$ are target
performance requirements. In this setting, we propose distributed procedures
for event detection and isolation (namely $\mx$, $\all$, and $\hall$), based on
the local fusion of $\CUSUM$s at the sensors. For these procedures, we obtain
bounds on the maximum mean detection/isolation delay ($\add$), and on $\tfa$
and $\pfi$, and thus provide an upper bound on $\add$ as
$\min\{\gamma,1/\alpha\} \to \infty$. For the Boolean sensing model, we show
that an asymptotic upper bound on the maximum mean detection/isolation delay of
our distributed procedure scales with $\gamma$ and $\alpha$ in the same way as
the asymptotically optimal centralised procedure
\cite{nikiforov03lower-bound-for-det-isolation}.
",Applied
"  We consider the problem of bandwidth selection by cross-validation from a
sequential point of view in a nonparametric regression model. Having in mind
that in applications one often aims at estimation, prediction and change
detection simultaneously, we investigate that approach for sequential kernel
smoothers in order to base these tasks on a single statistic. We provide
uniform weak laws of large numbers and weak consistency results for the
cross-validated bandwidth. Extensions to weakly dependent error terms are
discussed as well. The errors may be {\alpha}-mixing or L2-near epoch
dependent, which guarantees that the uniform convergence of the cross
validation sum and the consistency of the cross-validated bandwidth hold true
for a large class of time series. The method is illustrated by analyzing
photovoltaic data.
",Applied
"  Pattern recognition is a central topic in Learning Theory with numerous
applications such as voice and text recognition, image analysis, computer
diagnosis. The statistical set-up in classification is the following: we are
given an i.i.d. training set $(X_{1},Y_{1}),... (X_{n},Y_{n})$ where $X_{i}$
represents a feature and $Y_{i}\in \{0,1\}$ is a label attached to that
feature. The underlying joint distribution of $(X,Y)$ is unknown, but we can
learn about it from the training set and we aim at devising low error
classifiers $f:X\to Y$ used to predict the label of new incoming features.
  Here we solve a quantum analogue of this problem, namely the classification
of two arbitrary unknown qubit states. Given a number of `training' copies from
each of the states, we would like to `learn' about them by performing a
measurement on the training set. The outcome is then used to design mesurements
for the classification of future systems with unknown labels. We find the
asymptotically optimal classification strategy and show that typically, it
performs strictly better than a plug-in strategy based on state estimation.
  The figure of merit is the excess risk which is the difference between the
probability of error and the probability of error of the optimal measurement
when the states are known, that is the Helstrom measurement. We show that the
excess risk has rate $n^{-1}$ and compute the exact constant of the rate.
",ML
"  We develop Bayesian inference methods for a recently-emerging type of
epigenetic data to study the transmission fidelity of DNA methylation patterns
over cell divisions. The data consist of parent-daughter double-stranded DNA
methylation patterns with each pattern coming from a single cell and
represented as an unordered pair of binary strings. The data are technically
difficult and time-consuming to collect, putting a premium on an efficient
inference method. Our aim is to estimate rates for the maintenance and de novo
methylation events that gave rise to the observed patterns, while accounting
for measurement error. We model data at multiple sites jointly, thus using
whole-strand information, and considerably reduce confounding between
parameters. We also adopt a hierarchical structure that allows for variation in
rates across sites without an explosion in the effective number of parameters.
Our context-specific priors capture the expected stationarity, or
near-stationarity, of the stochastic process that generated the data analyzed
here. This expected stationarity is shown to greatly increase the precision of
the estimation. Applying our model to a data set collected at the human FMR1
locus, we find that measurement errors, generally ignored in similar studies,
occur at a nontrivial rate (inappropriate bisulfite conversion error: 1.6$%$
with 80$%$ CI: 0.9--2.3$%$). Accounting for these errors has a substantial
impact on estimates of key biological parameters. The estimated average failure
of maintenance rate and daughter de novo rate decline from 0.04 to 0.024 and
from 0.14 to 0.07, respectively, when errors are accounted for. Our results
also provide evidence that de novo events may occur on both parent and daughter
strands: the median parent and daughter de novo rates are 0.08 (80$%$ CI:
0.04--0.13) and 0.07 (80$%$ CI: 0.04--0.11), respectively.
",Applied
"  During the period 1962--1964, I had a tenure track Assistant Professorship in
Mathematics at Cornell University in Ithaca, New York, where I did research in
probability theory, especially on linear diffusion processes. Being somewhat
lonely there and not liking the cold winter weather, I decided around the
beginning of 1964 to try to get a job in the Mathematics Department at UCLA, in
the city in which I was born and raised. At that time, Leo Breiman was an
Associate Professor in that department. Presumably, he liked my research on
linear diffusion processes and other research as well, since the department
offered me a tenure track Assistant Professorship, which I happily accepted.
During the Summer of 1965, I worked on various projects with Sidney Port, then
at RAND Corporation, especially on random walks and related material. I was
promoted to Associate Professor, effective in Fall, 1966, presumably thanks in
part to Leo. Early in 1966, I~was surprised to be asked by Leo to participate
in a department meeting called to discuss the possible hiring of Sidney. The
conclusion was that Sidney was hired as Associate Professor in the department,
as of Fall, 1966. Leo communicated to me his view that he thought that Sidney
and I worked well together, which is why he had urged the department to hire
Sidney. Anyhow, Sidney and I had a very fruitful and enjoyable collaboration in
probability and, to a much lesser extent, in theoretical statistics, for a
number of years thereafter.
",Applied
"  It is natural to ask: what kinds of matrices satisfy the Restricted
Eigenvalue (RE) condition? In this paper, we associate the RE condition
(Bickel-Ritov-Tsybakov 09) with the complexity of a subset of the sphere in
$\R^p$, where $p$ is the dimensionality of the data, and show that a class of
random matrices with independent rows, but not necessarily independent columns,
satisfy the RE condition, when the sample size is above a certain lower bound.
Here we explicitly introduce an additional covariance structure to the class of
random matrices that we have known by now that satisfy the Restricted Isometry
Property as defined in Candes and Tao 05 (and hence the RE condition), in order
to compose a broader class of random matrices for which the RE condition holds.
In this case, tools from geometric functional analysis in characterizing the
intrinsic low-dimensional structures associated with the RE condition has been
crucial in analyzing the sample complexity and understanding its statistical
implications for high dimensional data.
",ML
"  The usual nonparametric approach to spectral analysis is revisited within the
regularization framework. Both usual and windowed periodograms are obtained as
the squared modulus of the minimizer of regularized least squares criteria.
Then, particular attention is paid to their interpretation within the Bayesian
statistical framework. Finally, the question of unsupervised hyperparameter and
window selection is addressed. It is shown that maximum likelihood solution is
both formally achievable and practically useful.
",Applied
"  We present an objective function for learning with unlabeled data that
utilizes auxiliary expectation constraints. We optimize this objective function
using a procedure that alternates between information and moment projections.
Our method provides an alternate interpretation of the posterior regularization
framework (Graca et al., 2008), maintains uncertainty during optimization
unlike constraint-driven learning (Chang et al., 2007), and is more efficient
than generalized expectation criteria (Mann & McCallum, 2008). Applications of
this framework include minimally supervised learning, semisupervised learning,
and learning with constraints that are more expressive than the underlying
model. In experiments, we demonstrate comparable accuracy to generalized
expectation criteria for minimally supervised learning, and use expressive
structural constraints to guide semi-supervised learning, providing a 3%-6%
improvement over stateof-the-art constraint-driven learning.
",ML
"  In real-world classification problems, the class balance in the training
dataset does not necessarily reflect that of the test dataset, which can cause
significant estimation bias. If the class ratio of the test dataset is known,
instance re-weighting or resampling allows systematical bias correction.
However, learning the class ratio of the test dataset is challenging when no
labeled data is available from the test domain. In this paper, we propose to
estimate the class ratio in the test dataset by matching probability
distributions of training and test input data. We demonstrate the utility of
the proposed approach through experiments.
",ML
"  US Presidential Election 2012 has been a very tight race between the two key
candidates. There were intense battle between the two key candidates. The
election reflects the sentiment of the electorate towards the achievements of
the incumbent President Obama. The campaign lasted several months and the
effects can be felt in the internet and twitter. The presidential debates
injected new vigor in the challenger's campaign and successfully captured the
electorate of several states posing a threat to the incumbent's position. Much
of the sentiment in the election has been captured in the online discussions.
In this paper, we will be using the original model described in Choy et. al.
(2011) using twitter data to forecast the next US president.
",Applied
"  This paper examines from an experimental perspective random forests, the
increasingly used statistical method for classification and regression problems
introduced by Leo Breiman in 2001. It first aims at confirming, known but
sparse, advice for using random forests and at proposing some complementary
remarks for both standard problems as well as high dimensional ones for which
the number of variables hugely exceeds the sample size. But the main
contribution of this paper is twofold: to provide some insights about the
behavior of the variable importance index based on random forests and in
addition, to propose to investigate two classical issues of variable selection.
The first one is to find important variables for interpretation and the second
one is more restrictive and try to design a good prediction model. The strategy
involves a ranking of explanatory variables using the random forests score of
importance and a stepwise ascending variable introduction strategy.
",ML
"  The CUR decomposition provides an approximation of a matrix $X$ that has low
reconstruction error and that is sparse in the sense that the resulting
approximation lies in the span of only a few columns of $X$. In this regard, it
appears to be similar to many sparse PCA methods. However, CUR takes a
randomized algorithmic approach, whereas most sparse PCA methods are framed as
convex optimization problems. In this paper, we try to understand CUR from a
sparse optimization viewpoint. We show that CUR is implicitly optimizing a
sparse regression objective and, furthermore, cannot be directly cast as a
sparse PCA method. We also observe that the sparsity attained by CUR possesses
an interesting structure, which leads us to formulate a sparse PCA method that
achieves a CUR-like sparsity.
",Applied
"  To identify novel dynamic patterns of gene expression, we develop a
statistical method to cluster noisy measurements of gene expression collected
from multiple replicates at multiple time points, with an unknown number of
clusters. We propose a random-effects mixture model coupled with a
Dirichlet-process prior for clustering. The mixture model formulation allows
for probabilistic cluster assignments. The random-effects formulation allows
for attributing the total variability in the data to the sources that are
consistent with the experimental design, particularly when the noise level is
high and the temporal dependence is not strong. The Dirichlet-process prior
induces a prior distribution on partitions and helps to estimate the number of
clusters (or mixture components) from the data. We further tackle two
challenges associated with Dirichlet-process prior-based methods. One is
efficient sampling. We develop a novel Metropolis-Hastings Markov Chain Monte
Carlo (MCMC) procedure to sample the partitions. The other is efficient use of
the MCMC samples in forming clusters. We propose a two-step procedure for
posterior inference, which involves resampling and relabeling, to estimate the
posterior allocation probability matrix. This matrix can be directly used in
cluster assignments, while describing the uncertainty in clustering. We
demonstrate the effectiveness of our model and sampling procedure through
simulated data. Applying our method to a real data set collected from
Drosophila adult muscle cells after five-minute Notch activation, we identify
14 clusters of different transcriptional responses among 163 differentially
expressed genes, which provides novel insights into underlying transcriptional
mechanisms in the Notch signaling pathway. The algorithm developed here is
implemented in the R package DIRECT, available on CRAN.
",Applied
"  Forecasting technological progress is of great interest to engineers, policy
makers, and private investors. Several models have been proposed for predicting
technological improvement, but how well do these models perform? An early
hypothesis made by Theodore Wright in 1936 is that cost decreases as a power
law of cumulative production. An alternative hypothesis is Moore's law, which
can be generalized to say that technologies improve exponentially with time.
Other alternatives were proposed by Goddard, Sinclair et al., and Nordhaus.
These hypotheses have not previously been rigorously tested. Using a new
database on the cost and production of 62 different technologies, which is the
most expansive of its kind, we test the ability of six different postulated
laws to predict future costs. Our approach involves hindcasting and developing
a statistical model to rank the performance of the postulated laws. Wright's
law produces the best forecasts, but Moore's law is not far behind. We discover
a previously unobserved regularity that production tends to increase
exponentially. A combination of an exponential decrease in cost and an
exponential increase in production would make Moore's law and Wright's law
indistinguishable, as originally pointed out by Sahal. We show for the first
time that these regularities are observed in data to such a degree that the
performance of these two laws is nearly tied. Our results show that
technological progress is forecastable, with the square root of the logarithmic
error growing linearly with the forecasting horizon at a typical rate of 2.5%
per year. These results have implications for theories of technological change,
and assessments of candidate technologies and policies for climate change
mitigation.
",Applied
"  Heavy-tailed distributions are frequently used to enhance the robustness of
regression and classification methods to outliers in output space. Often,
however, we are confronted with ""outliers"" in input space, which are isolated
observations in sparsely populated regions. We show that heavy-tailed
stochastic processes (which we construct from Gaussian processes via a copula),
can be used to improve robustness of regression and classification estimators
to such outliers by selectively shrinking them more strongly in sparse regions
than in dense regions. We carry out a theoretical analysis to show that
selective shrinkage occurs, provided the marginals of the heavy-tailed process
have sufficiently heavy tails. The analysis is complemented by experiments on
biological data which indicate significant improvements of estimates in sparse
regions while producing competitive results in dense regions.
",Applied
"  In this work we show that randomized (block) coordinate descent methods can
be accelerated by parallelization when applied to the problem of minimizing the
sum of a partially separable smooth convex function and a simple separable
convex function. The theoretical speedup, as compared to the serial method, and
referring to the number of iterations needed to approximately solve the problem
with high probability, is a simple expression depending on the number of
parallel processors and a natural and easily computable measure of separability
of the smooth component of the objective function. In the worst case, when no
degree of separability is present, there may be no speedup; in the best case,
when the problem is separable, the speedup is equal to the number of
processors. Our analysis also works in the mode when the number of blocks being
updated at each iteration is random, which allows for modeling situations with
busy or unreliable processors. We show that our algorithm is able to solve a
LASSO problem involving a matrix with 20 billion nonzeros in 2 hours on a large
memory node with 24 cores.
",ML
"  Many tasks require finding groups of elements in a matrix of numbers, symbols
or class likelihoods. One approach is to use efficient bi- or tri-linear
factorization techniques including PCA, ICA, sparse matrix factorization and
plaid analysis. These techniques are not appropriate when addition and
multiplication of matrix elements are not sensibly defined. More directly,
methods like bi-clustering can be used to classify matrix elements, but these
methods make the overly-restrictive assumption that the class of each element
is a function of a row class and a column class. We introduce a general
computational problem, `matrix tile analysis' (MTA), which consists of
decomposing a matrix into a set of non-overlapping tiles, each of which is
defined by a subset of usually nonadjacent rows and columns. MTA does not
require an algebra for combining tiles, but must search over discrete
combinations of tile assignments. Exact MTA is a computationally intractable
integer programming problem, but we describe an approximate iterative technique
and a computationally efficient sum-product relaxation of the integer program.
We compare the effectiveness of these methods to PCA and plaid on hundreds of
randomly generated tasks. Using double-gene-knockout data, we show that MTA
finds groups of interacting yeast genes that have biologically-related
functions.
",ML
"  Joint sparsity offers powerful structural cues for feature selection,
especially for variables that are expected to demonstrate a ""grouped"" behavior.
Such behavior is commonly modeled via group-lasso, multitask lasso, and related
methods where feature selection is effected via mixed-norms. Several mixed-norm
based sparse models have received substantial attention, and for some cases
efficient algorithms are also available. Surprisingly, several constrained
sparse models seem to be lacking scalable algorithms. We address this
deficiency by presenting batch and online (stochastic-gradient) optimization
methods, both of which rely on efficient projections onto mixed-norm balls. We
illustrate our methods by applying them to the multitask lasso. We conclude by
mentioning some open problems.
",ML
"  Models defined by stochastic differential equations (SDEs) allow for the
representation of random variability in dynamical systems. The relevance of
this class of models is growing in many applied research areas and is already a
standard tool to model e.g. financial, neuronal and population growth dynamics.
However inference for multidimensional SDE models is still very challenging,
both computationally and theoretically. Approximate Bayesian computation (ABC)
allow to perform Bayesian inference for models which are sufficiently complex
that the likelihood function is either analytically unavailable or
computationally prohibitive to evaluate. A computationally efficient ABC-MCMC
algorithm is proposed, halving the running time in our simulations. Focus is on
the case where the SDE describes latent dynamics in state-space models; however
the methodology is not limited to the state-space framework. Simulation studies
for a pharmacokinetics/pharmacodynamics model and for stochastic chemical
reactions are considered and a MATLAB package implementing our ABC-MCMC
algorithm is provided.
",Applied
"  The Minimum Description Length (MDL) principle selects the model that has the
shortest code for data plus model. We show that for a countable class of
models, MDL predictions are close to the true distribution in a strong sense.
The result is completely general. No independence, ergodicity, stationarity,
identifiability, or other assumption on the model class need to be made. More
formally, we show that for any countable class of models, the distributions
selected by MDL (or MAP) asymptotically predict (merge with) the true measure
in the class in total variation distance. Implications for non-i.i.d. domains
like time-series forecasting, discriminative learning, and reinforcement
learning are discussed.
",ML
"  The use of Reinforcement Learning in real-world scenarios is strongly limited
by issues of scale. Most RL learning algorithms are unable to deal with
problems composed of hundreds or sometimes even dozens of possible actions, and
therefore cannot be applied to many real-world problems. We consider the RL
problem in the supervised classification framework where the optimal policy is
obtained through a multiclass classifier, the set of classes being the set of
actions of the problem. We introduce error-correcting output codes (ECOCs) in
this setting and propose two new methods for reducing complexity when using
rollouts-based approaches. The first method consists in using an ECOC-based
classifier as the multiclass classifier, reducing the learning complexity from
O(A2) to O(Alog(A)). We then propose a novel method that profits from the
ECOC's coding dictionary to split the initial MDP into O(log(A)) seperate
two-action MDPs. This second method reduces learning complexity even further,
from O(A2) to O(log(A)), thus rendering problems with large action sets
tractable. We finish by experimentally demonstrating the advantages of our
approach on a set of benchmark problems, both in speed and performance.
",ML
"  The paper introduces a penalized matrix estimation procedure aiming at
solutions which are sparse and low-rank at the same time. Such structures arise
in the context of social networks or protein interactions where underlying
graphs have adjacency matrices which are block-diagonal in the appropriate
basis. We introduce a convex mixed penalty which involves $\ell_1$-norm and
trace norm simultaneously. We obtain an oracle inequality which indicates how
the two effects interact according to the nature of the target matrix. We bound
generalization error in the link prediction problem. We also develop proximal
descent strategies to solve the optimization problem efficiently and evaluate
performance on synthetic and real data sets.
",ML
"  Cross-validation (CV) is widely used for tuning a model with respect to
user-selected parameters and for selecting a ""best"" model. For example, the
method of $k$-nearest neighbors requires the user to choose $k$, the number of
neighbors, and a neural network has several tuning parameters controlling the
network complexity. Once such parameters are optimized for a particular data
set, the next step is often to compare the various optimized models and choose
the method with the best predictive performance. Both tuning and model
selection boil down to comparing models, either across different values of the
tuning parameters or across different classes of statistical models and/or sets
of explanatory variables. For multiple large sets of data, like the PubChem
drug discovery cheminformatics data which motivated this work, reliable CV
comparisons are computationally demanding, or even infeasible. In this paper we
develop an efficient sequential methodology for model comparison based on CV.
It also takes into account the randomness in CV. The number of models is
reduced via an adaptive, multiplicity-adjusted sequential algorithm, where poor
performers are quickly eliminated. By exploiting matching of individual
observations, it is sometimes even possible to establish the statistically
significant inferiority of some models with just one execution of CV.
",Applied
"  We analyze the results of the German Team Handball Bundesliga for ten seasons
in a model-free statistical time series approach. We will show that the home
advantage is nearly negligible compared to the total sum of goals. Specific
interest has been spent on the time evolution of the team fitness expressed in
terms of the goal difference. In contrast to soccer, our results indicate a
decay of the team fitness values over a season while the long time correlation
behavior over years is nearly comparable. We are able to explain the dominance
of a few teams by the large value for the total number of goals in a match. A
method for the prediction of match winners is presented in good accuracy with
the real results. We analyze the properties of promoted teams and indicate
drastic level changes between the Bundesliga and the second league. Our
findings reflect in good agreement recent discussions on modern successful
attack strategies.
",Applied
"  In this paper we consider sparse and identifiable linear latent variable
(factor) and linear Bayesian network models for parsimonious analysis of
multivariate data. We propose a computationally efficient method for joint
parameter and model inference, and model comparison. It consists of a fully
Bayesian hierarchy for sparse models using slab and spike priors (two-component
delta-function and continuous mixtures), non-Gaussian latent factors and a
stochastic search over the ordering of the variables. The framework, which we
call SLIM (Sparse Linear Identifiable Multivariate modeling), is validated and
bench-marked on artificial and real biological data sets. SLIM is closest in
spirit to LiNGAM (Shimizu et al., 2006), but differs substantially in
inference, Bayesian network structure learning and model comparison.
Experimentally, SLIM performs equally well or better than LiNGAM with
comparable computational complexity. We attribute this mainly to the stochastic
search strategy used, and to parsimony (sparsity and identifiability), which is
an explicit part of the model. We propose two extensions to the basic i.i.d.
linear framework: non-linear dependence on observed variables, called SNIM
(Sparse Non-linear Identifiable Multivariate modeling) and allowing for
correlations between latent variables, called CSLIM (Correlated SLIM), for the
temporal and/or spatial data. The source code and scripts are available from
http://cogsys.imm.dtu.dk/slim/.
",ML
